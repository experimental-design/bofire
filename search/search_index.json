{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>BoFire is a framework to define and solve black-box optimization problems. These problems can arise in a number of closely related fields including experimental design, multi-objective optimization and active learning.</p> <p>BoFire problem specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.</p> <p>You can find code-examples in the Getting Started section of this document, as well as full worked-out examples of code-usage in the /tutorials section of this repository!</p>"},{"location":"#experimental-design","title":"Experimental design","text":"<p>In the context of experimental design BoFire allows to define a design space</p> \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] <p>where the design parameters may take values depending on their type and domain, e.g.</p> <ul> <li>continuous: \\(x_1 \\in [0, 1]\\)</li> <li>discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\)</li> <li>categorical: \\(x_3 \\in \\{A, B, C\\}\\)</li> </ul> <p>and a set of equations define additional experimental constraints, e.g.</p> <ul> <li>linear equality: \\(\\sum x_i = 1\\)</li> <li>linear inequality: \\(2 x_1 \\leq x_2\\)</li> <li>non-linear inequality: \\(\\sum x_i^2 \\leq 1\\)</li> <li>n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.</li> </ul>"},{"location":"#multi-objective-optimization","title":"Multi-objective optimization","text":"<p>In the context of multi-objective optimization BoFire allows to define a vector-valued optimization problem</p> \\[ \\mathrm{argmax}_{x \\in \\mathbb{X}} s(y(x)) \\] <p>where</p> <ul> <li>\\(\\mathbb{X}\\) is again the experimental design space</li> <li>\\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and</li> <li>\\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be maximized. For instance, \\(s_1\\) is the identity function if \\(y_1\\) is to be maximized.</li> </ul> <p>Since the objectives are usually conflicting, there is no point \\(x\\) that simultaneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises.</p> <p>A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.</p>"},{"location":"#bayesian-optimization","title":"Bayesian optimization","text":"<p>In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, \\mathrm{data})\\) that estimates the distribution of possible outcomes for \\(y\\). An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation</p> \\[ \\mathrm{argmax}_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] <p>and the maximizer \\(x_\\mathrm{opt}\\) of this acquisition function determines the next experiment \\(y(x)\\) to run.</p> <p>When there are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.</p>"},{"location":"#design-of-experiments","title":"Design of Experiments","text":"<p>BoFire can be used to generate optimal experimental designs with respect to various optimality criteria like D-optimality, A-optimality or uniform space filling.</p> <p>For this, the user specifies a design space and a model formula, then chooses an optimality criterion and the desired number of experiments in the design. The resulting optimization problem is then solved by IPOPT.</p> <p>The doe subpackage also supports a wide range of constraints on the design space including linear and nonlinear equalities and inequalities as well a (limited) use of NChooseK constraints. The user can provide fixed experiments that will be treated as part of the design but remain fixed during the optimization process. While some of the optimization algorithms support non-continuous design variables, the doe subpackage only supports those that are continuous.</p> <p>By default IPOPT uses the freely available linear solver MUMPS. For large models choosing a different linear solver (e.g. ma57 from Coin-HSL) can vastly reduce optimization time. A free academic license for Coin-HSL can be obtained here. Instructions on how to install additional linear solvers for IPOPT are given in the IPOPT documentation. For choosing a specific (HSL) linear solver in BoFire you can just pass the name of the solver to <code>find_local_max_ipopt()</code> with the <code>linear_solver</code> option together with the library's name in the option <code>hsllib</code>, e.g. <pre><code>find_local_max_ipopt(domain, \"fully-quadratic\", ipopt_options={\"linear_solver\":\"ma57\", \"hsllib\":\"libcoinhsl.so\"})\n</code></pre></p>"},{"location":"#reference","title":"Reference","text":"<p>We would love for you to use BoFire in your work! If you do, please cite our paper:</p> <pre><code>@misc{durholt2024bofire,\n  title={BoFire: Bayesian Optimization Framework Intended for Real Experiments},\n  author={Johannes P. D{\\\"{u}}rholt and Thomas S. Asche and Johanna Kleinekorte and Gabriel Mancino-Ball and Benjamin Schiller and Simon Sung and Julian Keupp and Aaron Osburg and Toby Boyne and Ruth Misener and Rosona Eldred and Wagner Steuer Costa and Chrysoula Kappatou and Robert M. Lee and Dominik Linzner and David Walz and Niklas Wulkow and Behrang Shafei},\n  year={2024},\n  eprint={2408.05040},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2408.05040},\n}\n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Contributions to BoFire are highly welcome!</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Pull requests are highly welcome:</p> <ol> <li>Create a fork from main.</li> <li>Add or adapt unit tests according to your change.</li> <li>Add doc-strings and update the documentation. You might consider contributing to the tutorials section.</li> <li>Make sure that the GitHub pipelines passes.</li> </ol>"},{"location":"CONTRIBUTING/#development-environment","title":"Development Environment","text":"<p>We recommend an editable installation. After cloning the repository via <pre><code>git clone https://github.com/experimental-design/bofire.git\n</code></pre> and cd <code>bofire</code>, you can proceed with <pre><code>pip install -e \".[all]\"\n</code></pre> Afterwards, you can check that the tests are successful via <pre><code>pytest tests/\n</code></pre></p>"},{"location":"CONTRIBUTING/#coding-style","title":"Coding Style","text":"<p>We use Ruff for linting, sorting and formatting of our code. Our doc-strings are in Google-style.</p> <p>In our CI/CD pipeline we check if contributions are compliant to Ruff. To make contributors' lives easier, we have pre-commit hooks for Ruff configured in the versions corresponding to the pipeline. They can be installed via</p> <p><pre><code>pip install pre-commit\npre-commit install\n</code></pre> in you local project root folder, if you want to use <code>pre-commit</code>.</p>"},{"location":"CONTRIBUTING/#type-checks","title":"Type checks","text":"<p>We make heavy use of Pydantic to enforce type checks during runtime. Further, we use Pyright for static type checking. We enforce Pyright type checks in our CI/CD pipeline.</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>If you add new functionality, make sure that it is tested properly and that it does not break existing code. Our tests run in our CI/CD pipeline. The test coverage is hidden from our Readme because it is not a very robust metric. However, you can find it in the outputs of our test-CI/CD-pipeline. See example.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>We use MkDocs with material theme and deploy our documentation to https://experimental-design.github.io/bofire/. Thereby, an API description is extracted from the doc-strings. Additionally, we have tutorials and getting-started-sections.</p>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing you agree that your contributions will be licensed under the same BSD 3-Clause License as BoFire.</p>"},{"location":"basic_examples/","title":"Basic Examples for the DoE Subpackage","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    InterpointEqualityConstraint,\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NonlinearEqualityConstraint,\n    NonlinearInequalityConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion, IOptimalityCriterion\n</pre> import matplotlib.pyplot as plt import numpy as np from matplotlib.ticker import FormatStrFormatter  import bofire.strategies.api as strategies from bofire.data_models.constraints.api import (     InterpointEqualityConstraint,     LinearEqualityConstraint,     LinearInequalityConstraint,     NonlinearEqualityConstraint,     NonlinearInequalityConstraint, ) from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import ContinuousInput, ContinuousOutput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion, IOptimalityCriterion In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0.1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 0.6)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n        LinearInequalityConstraint(features=[\"x1\", \"x2\"], coefficients=[5, 4], rhs=3.9),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\"],\n            coefficients=[-20, 5],\n            rhs=-3,\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(0, 1)),         ContinuousInput(key=\"x2\", bounds=(0.1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 0.6)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         LinearEqualityConstraint(             features=[\"x1\", \"x2\", \"x3\"],             coefficients=[1, 1, 1],             rhs=1,         ),         LinearInequalityConstraint(features=[\"x1\", \"x2\"], coefficients=[5, 4], rhs=3.9),         LinearInequalityConstraint(             features=[\"x1\", \"x2\"],             coefficients=[-20, 5],             rhs=-3,         ),     ], )  data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(candidate_count=12) <pre>Tried to set Option: disp. It is not a valid option. Please check the list of available options.\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[2], line 29\n     23 data_model = DoEStrategy(\n     24     domain=domain,\n     25     criterion=DOptimalityCriterion(formula=\"linear\"),\n     26     ipopt_options={\"disp\": 0},\n     27 )\n     28 strategy = strategies.map(data_model=data_model)\n---&gt; 29 candidates = strategy.ask(candidate_count=12)\n\nFile ~/Documents/temporary/bofire/bofire/strategies/strategy.py:128, in Strategy.ask(self, candidate_count, add_pending, raise_validation_error)\n    123 if not self.has_sufficient_experiments():\n    124     raise ValueError(\n    125         \"Not enough experiments available to execute the strategy.\",\n    126     )\n--&gt; 128 candidates = self._ask(candidate_count=candidate_count)\n    130 self.domain.validate_candidates(\n    131     candidates=candidates,\n    132     only_inputs=True,\n    133     raise_validation_error=raise_validation_error,\n    134 )\n    136 if candidate_count is not None:\n\nFile ~/Documents/temporary/bofire/bofire/strategies/doe_strategy.py:113, in DoEStrategy._ask(self, candidate_count)\n    103 num_discrete_vars = len(new_discretes)\n    104 if (\n    105     self.data_model.optimization_strategy == \"relaxed\"\n    106     or (num_binary_vars == 0 and num_discrete_vars == 0)\n   (...)\n    111     )\n    112 ):\n--&gt; 113     design = find_local_max_ipopt(\n    114         new_domain,\n    115         n_experiments=_candidate_count,\n    116         fixed_experiments=None,\n    117         partially_fixed_experiments=adapted_partially_fixed_candidates,\n    118         ipopt_options=self.data_model.ipopt_options,\n    119         criterion=self.data_model.criterion,\n    120         use_hessian=self.data_model.use_hessian,\n    121     )\n    122 # TODO adapt to when exhaustive search accepts discrete variables\n    123 elif (\n    124     self.data_model.optimization_strategy == \"exhaustive\"\n    125     and num_discrete_vars == 0\n    126 ):\n\nFile ~/Documents/temporary/bofire/bofire/strategies/doe/design.py:188, in find_local_max_ipopt(domain, n_experiments, criterion, ipopt_options, sampling, fixed_experiments, partially_fixed_experiments, use_hessian)\n    182     problem = FirstOrderDoEProblem(\n    183         doe_objective=objective_function,\n    184         bounds=bounds,\n    185         constraints=constraints,\n    186     )\n    187 for key in _ipopt_options.keys():\n--&gt; 188     problem.add_option(key, _ipopt_options[key])\n    190 x, info = problem.solve(x0)\n    192 design = pd.DataFrame(\n    193     x.reshape(n_experiments, len(domain.inputs)),\n    194     columns=domain.inputs.get_keys(),\n    195     index=[f\"exp{i}\" for i in range(n_experiments)],\n    196 )\n\nFile ~/anaconda3/envs/bofire/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:495, in ipopt_wrapper.Problem.add_option()\n\nTypeError: Error while assigning an option</pre> <p>Let's visualize the experiments that were chosen. We will see that such a design puts the experiments at the extremes of the experimental space - these are the points that best allow us to estimate the parameters of the linear model we chose.</p> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"Linear model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n</pre> fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.view_init(45, 45) ax.set_title(\"Linear model\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(     xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],     ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],     zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],     linewidth=2, )  # plot D-optimal solutions ax.scatter(     xs=candidates[\"x1\"],     ys=candidates[\"x2\"],     zs=candidates[\"x3\"],     marker=\"o\",     s=40,     color=\"orange\",     label=\"optimal_design solution, 12 points\", )  plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x312f78d50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>data_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\"\n    ),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12)\n</pre> data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(         formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\"     ),     ipopt_options={\"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(12) <p>In this case we can compare with the result reported in the paper of Coetzer and Haines.</p> In\u00a0[\u00a0]: Copied! <pre>d_opt = np.array(\n    [\n        [\n            0.7,\n            0.3,\n            0.2,\n            0.3,\n            0.5902,\n            0.4098,\n            0.2702,\n            0.2279,\n            0.4118,\n            0.5738,\n            0.4211,\n            0.3360,\n        ],\n        [0.1, 0.6, 0.2, 0.1, 0.2373, 0.4628, 0.4808, 0.3117, 0.1, 0.1, 0.2911, 0.2264],\n        [\n            0.2,\n            0.1,\n            0.6,\n            0.6,\n            0.1725,\n            0.1274,\n            0.249,\n            0.4604,\n            0.4882,\n            0.3262,\n            0.2878,\n            0.4376,\n        ],\n    ],\n)  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solution\nax.scatter(\n    xs=d_opt[0],\n    ys=d_opt[1],\n    zs=d_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"D-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n</pre> d_opt = np.array(     [         [             0.7,             0.3,             0.2,             0.3,             0.5902,             0.4098,             0.2702,             0.2279,             0.4118,             0.5738,             0.4211,             0.3360,         ],         [0.1, 0.6, 0.2, 0.1, 0.2373, 0.4628, 0.4808, 0.3117, 0.1, 0.1, 0.2911, 0.2264],         [             0.2,             0.1,             0.6,             0.6,             0.1725,             0.1274,             0.249,             0.4604,             0.4882,             0.3262,             0.2878,             0.4376,         ],     ], )  # values taken from paper   fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.set_title(\"cubic model\") ax.view_init(45, 45) ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(     xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],     ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],     zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],     linewidth=2, )  # plot D-optimal solution ax.scatter(     xs=d_opt[0],     ys=d_opt[1],     zs=d_opt[2],     marker=\"o\",     s=40,     color=\"darkgreen\",     label=\"D-optimal design, 12 points\", )  ax.scatter(     xs=candidates[\"x1\"],     ys=candidates[\"x2\"],     zs=candidates[\"x3\"],     marker=\"o\",     s=40,     color=\"orange\",     label=\"optimal_design solution, 12 points\", )  plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x32fb49710&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>data_model = DoEStrategy(\n    domain=domain,\n    criterion=IOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\",\n        n_space_filling_points=60,\n        ipopt_options={\"max_iter\": 500},\n    ),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12).to_numpy().T\n\n\ni_opt = np.array(\n    [\n        [0.7000, 0.1000, 0.2000],\n        [0.3000, 0.6000, 0.1000],\n        [0.2031, 0.1969, 0.6000],\n        [0.5899, 0.2376, 0.1725],\n        [0.4105, 0.4619, 0.1276],\n        [0.2720, 0.4882, 0.2398],\n        [0.2281, 0.3124, 0.4595],\n        [0.3492, 0.1000, 0.5508],\n        [0.5614, 0.1000, 0.3386],\n        [0.4621, 0.2342, 0.3037],\n        [0.3353, 0.2228, 0.4419],\n        [0.3782, 0.3618, 0.2600],\n    ]\n).T  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot I-optimal solution\nax.scatter(\n    xs=i_opt[0],\n    ys=i_opt[1],\n    zs=i_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"I-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[0],\n    ys=candidates[1],\n    zs=candidates[2],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n</pre> data_model = DoEStrategy(     domain=domain,     criterion=IOptimalityCriterion(         formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\",         n_space_filling_points=60,         ipopt_options={\"max_iter\": 500},     ),     ipopt_options={\"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(12).to_numpy().T   i_opt = np.array(     [         [0.7000, 0.1000, 0.2000],         [0.3000, 0.6000, 0.1000],         [0.2031, 0.1969, 0.6000],         [0.5899, 0.2376, 0.1725],         [0.4105, 0.4619, 0.1276],         [0.2720, 0.4882, 0.2398],         [0.2281, 0.3124, 0.4595],         [0.3492, 0.1000, 0.5508],         [0.5614, 0.1000, 0.3386],         [0.4621, 0.2342, 0.3037],         [0.3353, 0.2228, 0.4419],         [0.3782, 0.3618, 0.2600],     ] ).T  # values taken from paper   fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.set_title(\"cubic model\") ax.view_init(45, 45) ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(     xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],     ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],     zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],     linewidth=2, )  # plot I-optimal solution ax.scatter(     xs=i_opt[0],     ys=i_opt[1],     zs=i_opt[2],     marker=\"o\",     s=40,     color=\"darkgreen\",     label=\"I-optimal design, 12 points\", )  ax.scatter(     xs=candidates[0],     ys=candidates[1],     zs=candidates[2],     marker=\"o\",     s=40,     color=\"orange\",     label=\"optimal_design solution, 12 points\", )  plt.legend() <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/objective.py:222: UserWarning: Equality constraints were detected. No equidistant grid of points can be generated. The design space will be filled via SpaceFilling.\n  warnings.warn(\n</pre> Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x32fb7ded0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_results_3d(result, surface_func):\n    u, v = np.mgrid[0 : 2 * np.pi : 100j, 0 : np.pi : 80j]\n    X = np.cos(u) * np.sin(v)\n    Y = np.sin(u) * np.sin(v)\n    Z = surface_func(X, Y)\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(X, Y, Z, alpha=0.3)\n    ax.scatter(\n        xs=result[\"x1\"],\n        ys=result[\"x2\"],\n        zs=result[\"x3\"],\n        marker=\"o\",\n        s=40,\n        color=\"red\",\n    )\n    ax.set(xlabel=\"x1\", ylabel=\"x2\", zlabel=\"x3\")\n    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n</pre> def plot_results_3d(result, surface_func):     u, v = np.mgrid[0 : 2 * np.pi : 100j, 0 : np.pi : 80j]     X = np.cos(u) * np.sin(v)     Y = np.sin(u) * np.sin(v)     Z = surface_func(X, Y)      fig = plt.figure(figsize=(8, 8))     ax = fig.add_subplot(111, projection=\"3d\")     ax.plot_surface(X, Y, Z, alpha=0.3)     ax.scatter(         xs=result[\"x1\"],         ys=result[\"x2\"],         zs=result[\"x3\"],         marker=\"o\",         s=40,         color=\"red\",     )     ax.set(xlabel=\"x1\", ylabel=\"x2\", zlabel=\"x3\")     ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))     ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\")) In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(-1, 1)),         ContinuousInput(key=\"x2\", bounds=(-1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         NonlinearInequalityConstraint(             expression=\"(x1**2 + x2**2)**0.5 - x3\",             features=[\"x1\", \"x2\", \"x3\"],         ),     ], )  data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"max_iter\": 100, \"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(     strategy.get_required_number_of_experiments(), raise_validation_error=False ) result.round(3) plot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2)) <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:106: UserWarning: Nonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:130: UserWarning: Sampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:204: UserWarning: Some points do not lie inside the domain or violate constraints. Please check if the                 results lie within your tolerance.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/data_models/domain/domain.py:454: UserWarning: Not all constraints are fulfilled.\n  warnings.warn(\"Not all constraints are fulfilled.\")\n</pre> <p>We can do the same for a design space limited by an elliptical cone $x_1^2 + x_2^2 - x_3 \\leq 0$.</p> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"x1**2 + x2**2 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: x1**2 + x2**2)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(-1, 1)),         ContinuousInput(key=\"x2\", bounds=(-1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         NonlinearInequalityConstraint(             expression=\"x1**2 + x2**2 - x3\",             features=[\"x1\", \"x2\", \"x3\"],         ),     ], ) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"max_iter\": 100, \"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(     strategy.get_required_number_of_experiments(), raise_validation_error=False ) result.round(3) plot_results_3d(result, surface_func=lambda x1, x2: x1**2 + x2**2) <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:106: UserWarning: Nonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:130: UserWarning: Sampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearEqualityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12, raise_validation_error=False)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(-1, 1)),         ContinuousInput(key=\"x2\", bounds=(-1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         NonlinearEqualityConstraint(             expression=\"(x1**2 + x2**2)**0.5 - x3\",             features=[\"x1\", \"x2\", \"x3\"],         ),     ], ) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"max_iter\": 100, \"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(12, raise_validation_error=False) result.round(3) plot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2)) <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:106: UserWarning: Nonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:130: UserWarning: Sampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[InterpointEqualityConstraint(feature=\"x1\", multiplicity=3)],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 500, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12)\nresult.round(3)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(0, 1)),         ContinuousInput(key=\"x2\", bounds=(0, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[InterpointEqualityConstraint(feature=\"x1\", multiplicity=3)], ) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"max_iter\": 500, \"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(12) result.round(3) Out[\u00a0]: x1 x2 x3 0 1.0 -0.0 -0.0 1 1.0 1.0 1.0 2 1.0 1.0 1.0 3 1.0 1.0 -0.0 4 1.0 -0.0 -0.0 5 1.0 -0.0 1.0 6 -0.0 -0.0 1.0 7 -0.0 1.0 -0.0 8 -0.0 -0.0 -0.0 9 -0.0 -0.0 -0.0 10 -0.0 1.0 1.0 11 -0.0 -0.0 1.0"},{"location":"basic_examples/#basic-examples-for-the-doe-subpackage","title":"Basic Examples for the DoE Subpackage\u00b6","text":"<p>The following example has been taken from the paper \"The construction of D- and I-optimal designs for mixture experiments with linear constraints on the components\" by R. Coetzer and L. M. Haines (https://www.sciencedirect.com/science/article/pii/S0169743917303106).</p>"},{"location":"basic_examples/#linear-model","title":"Linear model\u00b6","text":"<p>Creating an experimental design that is D-optimal with respect to a linear model is done the same way as making proposals using other methods in BoFire; you</p> <ol> <li>create a domain</li> <li>construct a stategy data model (here we want DoEStrategy)</li> <li>map the strategy to its functional version, and finally</li> <li>ask the strategy for proposals.</li> </ol> <p>We will start with the simplest case: make a design based on a linear model containing main-effects (i.e., simply the inputs themselves and an intercept, without any second-order terms).</p>"},{"location":"basic_examples/#cubic-model","title":"cubic model\u00b6","text":"<p>While the previous design is optimal for the main-effects model, we might prefer to see something that does not allocate all the experimental effort to values at the boundary of the space. This implies that we think there might be some higher-order effects present in the system - if we were sure that the target variable would follow straight-line behavior across the domain, we would not need to investigate any points away from the extremes.</p> <p>We can address this by specifying our own linear model that includes higher-order terms.</p>"},{"location":"basic_examples/#nonlinear-constraints","title":"Nonlinear Constraints\u00b6","text":"<p>Design generation also supports nonlinear constraints. The following 3 examples show what is possible.</p> <p>First, a convenience function for plotting.</p>"},{"location":"basic_examples/#example-1-design-inside-a-cone-nonlinear-inequality","title":"Example 1: Design inside a cone / nonlinear inequality\u00b6","text":"<p>In the following example we have three design variables. We impose the constraint that all experiments have to be contained in the interior of a cone, which corresponds to the nonlinear inequality constraint $\\sqrt{x_1^2 + x_2^2} - x_3 \\leq 0$. The optimization is done for a linear model and we will see that it places the points on the surface of the cone so as to maximize the distance between them (although this is not explicitly the objective of the optimization).</p>"},{"location":"basic_examples/#example-2-design-on-the-surface-of-a-cone-nonlinear-equality","title":"Example 2: Design on the surface of a cone / nonlinear equality\u00b6","text":"<p>We can also limit the design space to the surface of a cone, defined by the equality constraint $\\sqrt{x_1^2 + x_2^2} - x_3 = 0$. Before, we observed that the experimental proposals happened to be on the surface of the cone, but now they are constrained so that this must be the case.</p> <p>Remark: Due to missing sampling methods, the initial points provided to IPOPT don't satisfy the constraints. But this does not matter for the solution.</p>"},{"location":"basic_examples/#example-3-batch-constraints","title":"Example 3: Batch constraints\u00b6","text":"<p>Batch constraints can be used to create designs where each set of <code>multiplicity</code> subsequent experiments have the same value for a certain feature. This can be useful for setups where experiments are done in parallel and some parameters must be shared by experiments in the same parallel batch.</p> <p>In the following example we fix the value of the decision variable <code>x1</code> for each batch of 3 experiments.</p>"},{"location":"data_models_functionals/","title":"Data Models vs. Functional Components","text":"<p>Data models in BoFire hold static data of an optimization problem. These are input and output features as well as constraints making up the domain. They further include possible optimization objectives, acquisition functions, and kernels.</p> <p>All data models in <code>bofire.data_models</code>, are specified as pydantic models and inherit from <code>bofire.data_models.base.BaseModel</code>. These data models can be (de)serialized via <code>.dict()</code> and <code>.model_dump_json()</code> (provided by pydantic). A json schema of each data model can be obtained using <code>.schema()</code>.</p> <p>For surrogates and strategies, all functional parts are located in <code>bofire.surrogates</code> and <code>bofire.strategies</code>. These functionalities include the <code>ask</code> and <code>tell</code> as well as <code>fit</code> and <code>predict</code> methods. All class attributes (used by these method) are also removed from the data models. Each functional entity is initialized using the corresponding data model. As an example, consider the following data model of a <code>RandomStrategy</code>:</p> <pre><code>import bofire.data_models.domain.api as dm_domain\nimport bofire.data_models.features.api as dm_features\nimport bofire.data_models.strategies.api as dm_strategies\n\nin1 = dm_features.ContinuousInput(key=\"in1\", bounds=[0.0,1.0])\nin2 = dm_features.ContinuousInput(key=\"in2\", bounds=[0.0,2.0])\nin3 = dm_features.ContinuousInput(key=\"in3\", bounds=[0.0,3.0])\n\nout1 = dm_features.ContinuousOutput(key=\"out1\")\n\ninputs = dm_domain.Inputs(features=[in1, in2, in3])\noutputs = dm_domain.Outputs(features=[out1])\nconstraints = dm_domain.Constraints()\n\ndomain = dm_domain.Domain(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints,\n)\n\ndata_model = dm_strategies.RandomStrategy(domain=domain)\n</code></pre> <p>Such a data model can be (de)serialized as follows:</p> <p><pre><code>from pydantic import TypeAdapter\nfrom bofire.data_models.strategies.api import AnyStrategy\n\nserialized = data_model.model_dump_json()\n\ndata_model_ = TypeAdapter(AnyStrategy).validate_json(serialized)\n\nassert data_model_ == data_model\n</code></pre> The data model of a strategy contains its hyperparameters. Using this data model of a strategy, we can create an instance of a (functional) strategy:</p> <pre><code>import bofire.strategies.api as strategies\nstrategy = strategies.RandomStrategy(data_model=data_model)\n</code></pre> <p>As each strategy data model should be mapped to a specific (functional) strategy, we provide such a mapping:</p> <pre><code>strategy = strategies.map(data_model)\n</code></pre>"},{"location":"design_with_explicit_formula/","title":"Design with explicit Formula","text":"In\u00a0[\u00a0]: Copied! <pre>import bofire.strategies.api as strategies\nfrom bofire.data_models.api import Domain, Inputs\nfrom bofire.data_models.features.api import ContinuousInput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\nfrom bofire.utils.doe import get_confounding_matrix\n</pre> import bofire.strategies.api as strategies from bofire.data_models.api import Domain, Inputs from bofire.data_models.features.api import ContinuousInput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion from bofire.utils.doe import get_confounding_matrix In\u00a0[\u00a0]: Copied! <pre>input_features = Inputs(\n    features=[\n        ContinuousInput(key=\"a\", bounds=(0, 5)),\n        ContinuousInput(key=\"b\", bounds=(40, 800)),\n        ContinuousInput(key=\"c\", bounds=(80, 180)),\n        ContinuousInput(key=\"d\", bounds=(200, 800)),\n    ],\n)\ndomain = Domain(inputs=input_features)\n</pre> input_features = Inputs(     features=[         ContinuousInput(key=\"a\", bounds=(0, 5)),         ContinuousInput(key=\"b\", bounds=(40, 800)),         ContinuousInput(key=\"c\", bounds=(80, 180)),         ContinuousInput(key=\"d\", bounds=(200, 800)),     ], ) domain = Domain(inputs=input_features) In\u00a0[\u00a0]: Copied! <pre>model_type = \"a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d\"\nmodel_type\n</pre> model_type = \"a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d\" model_type Out[\u00a0]: <pre>'a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d'</pre> In\u00a0[\u00a0]: Copied! <pre>data_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=model_type),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ndesign = strategy.ask(17)\ndesign\n</pre> data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=model_type),     ipopt_options={\"max_iter\": 100, \"print_level\": 0}, ) strategy = strategies.map(data_model=data_model) design = strategy.ask(17) design <pre>Tried to set Option: disp. It is not a valid option. Please check the list of available options.\n</pre> <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 7\n      1 data_model = DoEStrategy(\n      2     domain=domain,\n      3     criterion=DOptimalityCriterion(formula=model_type),\n      4     ipopt_options={\"max_iter\": 100, \"disp\": 0},\n      5 )\n      6 strategy = strategies.map(data_model=data_model)\n----&gt; 7 design = strategy.ask(17)\n      8 design\n\nFile ~/Documents/temporary/bofire/bofire/strategies/strategy.py:128, in Strategy.ask(self, candidate_count, add_pending, raise_validation_error)\n    123 if not self.has_sufficient_experiments():\n    124     raise ValueError(\n    125         \"Not enough experiments available to execute the strategy.\",\n    126     )\n--&gt; 128 candidates = self._ask(candidate_count=candidate_count)\n    130 self.domain.validate_candidates(\n    131     candidates=candidates,\n    132     only_inputs=True,\n    133     raise_validation_error=raise_validation_error,\n    134 )\n    136 if candidate_count is not None:\n\nFile ~/Documents/temporary/bofire/bofire/strategies/doe_strategy.py:113, in DoEStrategy._ask(self, candidate_count)\n    103 num_discrete_vars = len(new_discretes)\n    104 if (\n    105     self.data_model.optimization_strategy == \"relaxed\"\n    106     or (num_binary_vars == 0 and num_discrete_vars == 0)\n   (...)\n    111     )\n    112 ):\n--&gt; 113     design = find_local_max_ipopt(\n    114         new_domain,\n    115         n_experiments=_candidate_count,\n    116         fixed_experiments=None,\n    117         partially_fixed_experiments=adapted_partially_fixed_candidates,\n    118         ipopt_options=self.data_model.ipopt_options,\n    119         criterion=self.data_model.criterion,\n    120         use_hessian=self.data_model.use_hessian,\n    121     )\n    122 # TODO adapt to when exhaustive search accepts discrete variables\n    123 elif (\n    124     self.data_model.optimization_strategy == \"exhaustive\"\n    125     and num_discrete_vars == 0\n    126 ):\n\nFile ~/Documents/temporary/bofire/bofire/strategies/doe/design.py:188, in find_local_max_ipopt(domain, n_experiments, criterion, ipopt_options, sampling, fixed_experiments, partially_fixed_experiments, use_hessian)\n    182     problem = FirstOrderDoEProblem(\n    183         doe_objective=objective_function,\n    184         bounds=bounds,\n    185         constraints=constraints,\n    186     )\n    187 for key in _ipopt_options.keys():\n--&gt; 188     problem.add_option(key, _ipopt_options[key])\n    190 x, info = problem.solve(x0)\n    192 design = pd.DataFrame(\n    193     x.reshape(n_experiments, len(domain.inputs)),\n    194     columns=domain.inputs.get_keys(),\n    195     index=[f\"exp{i}\" for i in range(n_experiments)],\n    196 )\n\nFile ~/anaconda3/envs/bofire/lib/python3.11/site-packages/cyipopt/cython/ipopt_wrapper.pyx:495, in ipopt_wrapper.Problem.add_option()\n\nTypeError: Error while assigning an option</pre> In\u00a0[\u00a0]: Copied! <pre>import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nmatplotlib.rcParams[\"figure.dpi\"] = 120\n\nm = get_confounding_matrix(\n    domain.inputs,\n    design=design,\n    interactions=[2, 3],\n    powers=[2],\n)\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> import matplotlib import matplotlib.pyplot as plt import seaborn as sns   matplotlib.rcParams[\"figure.dpi\"] = 120  m = get_confounding_matrix(     domain.inputs,     design=design,     interactions=[2, 3],     powers=[2], )  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show()"},{"location":"design_with_explicit_formula/#design-with-explicit-formula","title":"Design with explicit Formula\u00b6","text":"<p>This tutorial notebook shows how to setup a D-optimal design with BoFire while providing an explicit formula and not just one of the four available keywords <code>linear</code>, <code>linear-and-interaction</code>, <code>linear-and-quadratic</code>, <code>fully-quadratic</code>.</p> <p>Make sure that <code>cyipopt</code>is installed. The recommend way is the installation via conda <code>conda install -c conda-forge cyipopt</code>.</p>"},{"location":"design_with_explicit_formula/#imports","title":"Imports\u00b6","text":""},{"location":"design_with_explicit_formula/#setup-of-the-problem","title":"Setup of the problem\u00b6","text":""},{"location":"design_with_explicit_formula/#definition-of-the-formula-for-which-the-optimal-points-should-be-found","title":"Definition of the formula for which the optimal points should be found\u00b6","text":""},{"location":"design_with_explicit_formula/#find-d-optimal-design","title":"Find D-optimal Design\u00b6","text":""},{"location":"design_with_explicit_formula/#analyze-confounding","title":"Analyze Confounding\u00b6","text":""},{"location":"examples/","title":"Examples","text":"<p>This is a collection of code examples to allow for an easy exploration of the functionalities that BoFire offers. We provide even more tutorials in the repository.</p>"},{"location":"examples/#doe","title":"DoE","text":"<ul> <li>Creating designs for constrained design spaces</li> <li>Optimizing designs with respect to various optimality criteria</li> <li>Creating designs for a custom model</li> <li>Creating designs with NChooseK constraints</li> <li>Creating full and fractional factorial designs</li> </ul>"},{"location":"examples/#bayesian-optimization-for-chemistry","title":"Bayesian Optimization for Chemistry","text":"<p>These examples show how the tools provided by BoFire can be used for Bayesian Optimization with some of the challenges faced in real-world experiments:</p> <ul> <li>A toy example for optimizing a reaction</li> <li>Using a Tanimoto fingerprint kernel to optimize over molecules</li> <li>Using a MultiFidelity strategy with cheap, approximate experiments</li> </ul>"},{"location":"examples/#api-with-bofire","title":"API with BoFire","text":"<p>You can find an examples of how BoFire can be used in APIs in separate repositories:</p> <ul> <li>The Candidates API demonstrates an API that provides get new experimental candidates based on DoE or Bayesian optimization.</li> <li>The Types API is an API to check serialized data models. For instance, a JavaScript frontend that allows the user to define an optimization domain can check its validity explicitly.</li> </ul>"},{"location":"fingerprint_bayesopt/","title":"Fingerprint bayesopt","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.data.photoswitches import EXPERIMENTS\nfrom bofire.benchmarks.LookupTableBenchmark import LookupTableBenchmark\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import CategoricalMolecularInput, ContinuousOutput\nfrom bofire.data_models.molfeatures.api import FingerprintsFragments\nfrom bofire.data_models.objectives.api import MaximizeObjective\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, TanimotoGPSurrogate\nfrom bofire.runners.api import run\n\n\nwarnings.filterwarnings(\"ignore\")\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n</pre> import os import warnings  import numpy as np import pandas as pd from matplotlib import pyplot as plt  import bofire.strategies.api as strategies from bofire.benchmarks.data.photoswitches import EXPERIMENTS from bofire.benchmarks.LookupTableBenchmark import LookupTableBenchmark from bofire.data_models.acquisition_functions.api import qLogEI from bofire.data_models.domain.api import Domain, Inputs, Outputs from bofire.data_models.features.api import CategoricalMolecularInput, ContinuousOutput from bofire.data_models.molfeatures.api import FingerprintsFragments from bofire.data_models.objectives.api import MaximizeObjective from bofire.data_models.strategies.api import RandomStrategy, SoboStrategy from bofire.data_models.surrogates.api import BotorchSurrogates, TanimotoGPSurrogate from bofire.runners.api import run   warnings.filterwarnings(\"ignore\")  SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") In\u00a0[\u00a0]: Copied! <pre>benchmark = {\n    \"input\": \"SMILES\",\n    \"output\": \"E isomer pi-pi* wavelength in nm\",\n}\ndf = pd.read_json(EXPERIMENTS)\nmain_file = pd.DataFrame(columns=[benchmark[\"input\"], benchmark[\"output\"]])\nnans = df[benchmark[\"output\"]].isnull().to_list()\nnan_indices = [nan for nan, x in enumerate(nans) if x]\nmain_file[benchmark[\"input\"]] = df[benchmark[\"input\"]].drop(nan_indices).to_list()\nmain_file[benchmark[\"output\"]] = (\n    df[benchmark[\"output\"]].dropna().to_numpy().reshape(-1, 1)\n)\ninput_feature = CategoricalMolecularInput(\n    key=benchmark[\"input\"],\n    categories=list(set(main_file[benchmark[\"input\"]].to_list())),\n)\nobjective = MaximizeObjective(\n    w=1.0,\n)\ninputs = Inputs(features=[input_feature])\noutput_feature = ContinuousOutput(key=benchmark[\"output\"], objective=objective)\noutputs = Outputs(features=[output_feature])\ndomain = Domain(inputs=inputs, outputs=outputs)\n</pre> benchmark = {     \"input\": \"SMILES\",     \"output\": \"E isomer pi-pi* wavelength in nm\", } df = pd.read_json(EXPERIMENTS) main_file = pd.DataFrame(columns=[benchmark[\"input\"], benchmark[\"output\"]]) nans = df[benchmark[\"output\"]].isnull().to_list() nan_indices = [nan for nan, x in enumerate(nans) if x] main_file[benchmark[\"input\"]] = df[benchmark[\"input\"]].drop(nan_indices).to_list() main_file[benchmark[\"output\"]] = (     df[benchmark[\"output\"]].dropna().to_numpy().reshape(-1, 1) ) input_feature = CategoricalMolecularInput(     key=benchmark[\"input\"],     categories=list(set(main_file[benchmark[\"input\"]].to_list())), ) objective = MaximizeObjective(     w=1.0, ) inputs = Inputs(features=[input_feature]) output_feature = ContinuousOutput(key=benchmark[\"output\"], objective=objective) outputs = Outputs(features=[output_feature]) domain = Domain(inputs=inputs, outputs=outputs) In\u00a0[\u00a0]: Copied! <pre>def sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(20)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments[domain.outputs.get_keys()[0]].max()\n\n\nn_iter = 20 if not SMOKE_TEST else 1\nbo_results_set = []\nrandom_results_set = []\nn_iterations = 49 if not SMOKE_TEST else 1\n\nfor _ in range(n_iter):\n    Benchmark = LookupTableBenchmark(domain=domain, lookup_table=main_file)\n    sampled = sample(Benchmark.domain)\n    sampled_xy = Benchmark.f(sampled, return_complete=True)\n    random_results = run(\n        Benchmark,\n        strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n        n_iterations=n_iterations,\n        metric=best,\n        initial_sampler=sampled_xy,\n        n_runs=1,\n        n_procs=1,\n    )\n\n    specs = {Benchmark.domain.inputs.get_keys()[0]: FingerprintsFragments(n_bits=2048)}\n    surrogate = TanimotoGPSurrogate(\n        inputs=Benchmark.domain.inputs,\n        outputs=Benchmark.domain.outputs,\n        input_preprocessing_specs=specs,\n    )\n\n    def sobo_factory(domain: Domain, surrogate=surrogate):\n        return strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qLogEI(),\n                surrogate_specs=BotorchSurrogates(surrogates=[surrogate]),\n            ),\n        )\n\n    qExpectedImprovement = qLogEI()\n    bo_results = run(\n        Benchmark,\n        strategy_factory=sobo_factory,\n        n_iterations=n_iterations,\n        metric=best,\n        initial_sampler=sampled_xy,\n        n_runs=1,\n        n_procs=1,\n    )\n    random_results_new = np.insert(\n        random_results[0][1].to_numpy(),\n        0,\n        best(Benchmark.domain, sampled_xy),\n    )\n    bo_results_new = np.insert(\n        bo_results[0][1].to_numpy(),\n        0,\n        best(Benchmark.domain, sampled_xy),\n    )\n    random_results_set.append(random_results_new)\n    bo_results_set.append(bo_results_new)\n</pre> def sample(domain):     datamodel = RandomStrategy(domain=domain)     sampler = strategies.map(data_model=datamodel)     sampled = sampler.ask(20)     return sampled   def best(domain: Domain, experiments: pd.DataFrame) -&gt; float:     return experiments[domain.outputs.get_keys()[0]].max()   n_iter = 20 if not SMOKE_TEST else 1 bo_results_set = [] random_results_set = [] n_iterations = 49 if not SMOKE_TEST else 1  for _ in range(n_iter):     Benchmark = LookupTableBenchmark(domain=domain, lookup_table=main_file)     sampled = sample(Benchmark.domain)     sampled_xy = Benchmark.f(sampled, return_complete=True)     random_results = run(         Benchmark,         strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),         n_iterations=n_iterations,         metric=best,         initial_sampler=sampled_xy,         n_runs=1,         n_procs=1,     )      specs = {Benchmark.domain.inputs.get_keys()[0]: FingerprintsFragments(n_bits=2048)}     surrogate = TanimotoGPSurrogate(         inputs=Benchmark.domain.inputs,         outputs=Benchmark.domain.outputs,         input_preprocessing_specs=specs,     )      def sobo_factory(domain: Domain, surrogate=surrogate):         return strategies.map(             SoboStrategy(                 domain=domain,                 acquisition_function=qLogEI(),                 surrogate_specs=BotorchSurrogates(surrogates=[surrogate]),             ),         )      qExpectedImprovement = qLogEI()     bo_results = run(         Benchmark,         strategy_factory=sobo_factory,         n_iterations=n_iterations,         metric=best,         initial_sampler=sampled_xy,         n_runs=1,         n_procs=1,     )     random_results_new = np.insert(         random_results[0][1].to_numpy(),         0,         best(Benchmark.domain, sampled_xy),     )     bo_results_new = np.insert(         bo_results[0][1].to_numpy(),         0,         best(Benchmark.domain, sampled_xy),     )     random_results_set.append(random_results_new)     bo_results_set.append(bo_results_new) In\u00a0[\u00a0]: Copied! <pre># Define a confience interval function for plotting.\ndef ci(y):\n    return 1.96 * y.std(axis=0) / np.sqrt(n_iter)\n\n\nif not SMOKE_TEST:\n    iters = np.arange(n_iterations + 1)\n    y_rnd = np.asarray(random_results_set)\n    y_ei = np.asarray(bo_results_set)\n\n    y_rnd_mean = y_rnd.mean(axis=0)\n    y_ei_mean = y_ei.mean(axis=0)\n    y_rnd_std = y_rnd.std(axis=0)\n    y_ei_std = y_ei.std(axis=0)\n\n    lower_rnd = y_rnd_mean - y_rnd_std\n    upper_rnd = y_rnd_mean + y_rnd_std\n    lower_ei = y_ei_mean - y_ei_std\n    upper_ei = y_ei_mean + y_ei_std\n\n    plt.plot(iters, y_rnd_mean, label=\"Random\")\n    plt.fill_between(iters, lower_rnd, upper_rnd, alpha=0.2)\n    plt.plot(iters, y_ei_mean, label=\"SOBO\")\n    plt.fill_between(iters, lower_ei, upper_ei, alpha=0.2)\n    plt.xlabel(\"Number of Iterations\")\n    plt.ylabel(\"Best Objective Value\")\n    plt.legend(loc=\"lower right\")\n    plt.show()\n</pre> # Define a confience interval function for plotting. def ci(y):     return 1.96 * y.std(axis=0) / np.sqrt(n_iter)   if not SMOKE_TEST:     iters = np.arange(n_iterations + 1)     y_rnd = np.asarray(random_results_set)     y_ei = np.asarray(bo_results_set)      y_rnd_mean = y_rnd.mean(axis=0)     y_ei_mean = y_ei.mean(axis=0)     y_rnd_std = y_rnd.std(axis=0)     y_ei_std = y_ei.std(axis=0)      lower_rnd = y_rnd_mean - y_rnd_std     upper_rnd = y_rnd_mean + y_rnd_std     lower_ei = y_ei_mean - y_ei_std     upper_ei = y_ei_mean + y_ei_std      plt.plot(iters, y_rnd_mean, label=\"Random\")     plt.fill_between(iters, lower_rnd, upper_rnd, alpha=0.2)     plt.plot(iters, y_ei_mean, label=\"SOBO\")     plt.fill_between(iters, lower_ei, upper_ei, alpha=0.2)     plt.xlabel(\"Number of Iterations\")     plt.ylabel(\"Best Objective Value\")     plt.legend(loc=\"lower right\")     plt.show()"},{"location":"fingerprint_bayesopt/#bayesian-optimisation-over-molecules","title":"Bayesian Optimisation Over Molecules\u00b6","text":"<p>An example notebook for Bayesian optimisation on a molecular dataset using a Tanimoto fingerprint kernel and the photoswitch dataset.$\\newline$ Paper: https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h $\\newline$ Code: https://github.com/Ryan-Rhys/The-Photoswitch-Dataset $\\newline$ This notebook is adapted from https://github.com/leojklarner/gauche/blob/main/notebooks/Bayesian%20Optimisation%20Over%20Molecules.ipynb $\\newline$ The method of obtaining new data from a discrete dataset is explained in the notebook and the details of the dataset and the method are explained in the code and the paper respectively.</p>"},{"location":"fingerprint_bayesopt/#imports","title":"Imports\u00b6","text":""},{"location":"fingerprint_bayesopt/#benchmark","title":"Benchmark\u00b6","text":"<p>input and output feature keys and extract them to get LookUpTable</p>"},{"location":"fingerprint_bayesopt/#random-vs-sobo-optimization","title":"Random vs SOBO optimization\u00b6","text":"<p>For molecules, we use Tanimoto GP which has a Tanimoto kernel as default</p>"},{"location":"fingerprint_bayesopt/#performance","title":"Performance\u00b6","text":"<p>SOBO outperforms random search in terms of selecting molecules with high E isomer pi-pi* transition wavelength.</p>"},{"location":"fractional_factorial/","title":"Full and Fractional Factorial Designs","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import CategoricalInput, ContinuousInput\nfrom bofire.data_models.strategies.api import FractionalFactorialStrategy\nfrom bofire.utils.doe import get_alias_structure, get_confounding_matrix, get_generator\n\n\ndef plot_design(design: pd.DataFrame):\n    # we do a plot with three subplots in one row in which the three degrees of freedom (temperature, time and ph) are plotted\n    _, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].scatter(design[\"temperature\"], design[\"time\"])\n    axs[0].set_xlabel(\"Temperature\")\n    axs[0].set_ylabel(\"Time\")\n    axs[1].scatter(design[\"temperature\"], design[\"ph\"])\n    axs[1].set_xlabel(\"Temperature\")\n    axs[1].set_ylabel(\"pH\")\n    axs[2].scatter(design[\"time\"], design[\"ph\"])\n    axs[2].set_xlabel(\"Time\")\n    axs[2].set_ylabel(\"pH\")\n    plt.show()\n</pre> import matplotlib.pyplot as plt import pandas as pd import seaborn as sns  import bofire.strategies.api as strategies from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import CategoricalInput, ContinuousInput from bofire.data_models.strategies.api import FractionalFactorialStrategy from bofire.utils.doe import get_alias_structure, get_confounding_matrix, get_generator   def plot_design(design: pd.DataFrame):     # we do a plot with three subplots in one row in which the three degrees of freedom (temperature, time and ph) are plotted     _, axs = plt.subplots(1, 3, figsize=(15, 5))     axs[0].scatter(design[\"temperature\"], design[\"time\"])     axs[0].set_xlabel(\"Temperature\")     axs[0].set_ylabel(\"Time\")     axs[1].scatter(design[\"temperature\"], design[\"ph\"])     axs[1].set_xlabel(\"Temperature\")     axs[1].set_ylabel(\"pH\")     axs[2].scatter(design[\"time\"], design[\"ph\"])     axs[2].set_xlabel(\"Time\")     axs[2].set_ylabel(\"pH\")     plt.show() <pre>/opt/homebrew/Caskroom/miniforge/base/envs/bofire-2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n    ],\n)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"temperature\", bounds=(20, 80)),         ContinuousInput(key=\"time\", bounds=(60, 120)),         ContinuousInput(key=\"ph\", bounds=(7, 13)),     ], ) In\u00a0[\u00a0]: Copied! <pre>strategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n</pre> strategy_data = FractionalFactorialStrategy(     domain=domain,     n_center=1,  # number of center points     n_repetitions=1,  # number of repetitions, we do only one round here ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design)  plot_design(design=design) ph temperature time 0 7.0 20.0 60.0 1 7.0 20.0 120.0 2 7.0 80.0 60.0 3 7.0 80.0 120.0 4 13.0 20.0 60.0 5 13.0 20.0 120.0 6 13.0 80.0 60.0 7 13.0 80.0 120.0 8 10.0 50.0 90.0 <p>The confounding structure is shown below, as expected for a full factorial design, no confound is present.</p> In\u00a0[\u00a0]: Copied! <pre>m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show() In\u00a0[\u00a0]: Copied! <pre>blocked_domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n        CategoricalInput(key=\"operator\", categories=[\"A\", \"B\", \"C\", \"D\"]),\n    ],\n)\n\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=blocked_domain,\n    n_center=1,  # number of center points per block\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    block_feature_key=\"operator\",\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n</pre> blocked_domain = Domain(     inputs=[         ContinuousInput(key=\"temperature\", bounds=(20, 80)),         ContinuousInput(key=\"time\", bounds=(60, 120)),         ContinuousInput(key=\"ph\", bounds=(7, 13)),         CategoricalInput(key=\"operator\", categories=[\"A\", \"B\", \"C\", \"D\"]),     ], )   strategy_data = FractionalFactorialStrategy(     domain=blocked_domain,     n_center=1,  # number of center points per block     n_repetitions=1,  # number of repetitions, we do only one round here     block_feature_key=\"operator\", ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design)  plot_design(design=design) ph temperature time operator 0 7.0 20.0 60.0 A 1 13.0 80.0 120.0 A 2 10.0 50.0 90.0 A 3 7.0 20.0 120.0 B 4 13.0 80.0 60.0 B 5 10.0 50.0 90.0 B 6 7.0 80.0 60.0 C 7 13.0 20.0 120.0 C 8 10.0 50.0 90.0 C 9 7.0 80.0 120.0 D 10 13.0 20.0 60.0 D 11 10.0 50.0 90.0 D <p>Here a fractional factorial design of the form $2^{3-1}$ is setup by specifying the number of generators (here 1). In comparison to the full factorial design with 9 candidates, it features only 5 experiments.</p> In\u00a0[\u00a0]: Copied! <pre>strategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    n_generators=1,  # number of generators, ie number of reducing factors\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n</pre> strategy_data = FractionalFactorialStrategy(     domain=domain,     n_center=1,  # number of center points     n_repetitions=1,  # number of repetitions, we do only one round here     n_generators=1,  # number of generators, ie number of reducing factors ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design) <p>The generator string is automatically generated by making use of the method <code>get_generator</code> and specifying the total number of factors (here 3) and the number of generators (here 1).</p> In\u00a0[\u00a0]: Copied! <pre>get_generator(n_factors=3, n_generators=1)\n</pre> get_generator(n_factors=3, n_generators=1) <p>As expected for a type III design the main effects are confounded with the two factor interactions:</p> In\u00a0[\u00a0]: Copied! <pre>m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show() <p>This can also be expressed by the so called alias structure that can be calculated as following:</p> In\u00a0[\u00a0]: Copied! <pre>get_alias_structure(\"a b ab\")\n</pre> get_alias_structure(\"a b ab\") <p>Here again a fractional factorial design of the form $2^{3-1}$ is setup by providing the complete generator string of the form <code>a b -ab</code> explicitly to the strategy.</p> In\u00a0[\u00a0]: Copied! <pre>strategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    generator=\"a b -ab\",  # the exact generator\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n</pre> strategy_data = FractionalFactorialStrategy(     domain=domain,     n_center=1,  # number of center points     n_repetitions=1,  # number of repetitions, we do only one round here     generator=\"a b -ab\",  # the exact generator ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design) <p>The last two designs differ only in the last feature <code>time</code>, since the generator strings are different. In the first one it holds <code>time=ph x temperature</code> whereas in the second it holds <code>time=-ph x temperature</code>, which is also reflected in the confounding structure.</p> In\u00a0[\u00a0]: Copied! <pre>m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show()"},{"location":"fractional_factorial/#full-and-fractional-factorial-designs","title":"Full and Fractional Factorial Designs\u00b6","text":"<p>BoFire can be used to setup full (two level) and fractional factorial designs (https://en.wikipedia.org/wiki/Fractional_factorial_design). This tutorial notebook shows how.</p>"},{"location":"fractional_factorial/#imports-and-helper-functions","title":"Imports and helper functions\u00b6","text":""},{"location":"fractional_factorial/#setup-the-problem-domain","title":"Setup the problem domain\u00b6","text":"<p>The designs are generated for a simple three dimensional problem comprised of three continuous factors/features.</p>"},{"location":"fractional_factorial/#setup-a-full-factorial-design","title":"Setup a full factorial design\u00b6","text":"<p>Here we setup a full two-level factorial design including a center point and plot it.</p>"},{"location":"fractional_factorial/#setup-a-full-factorial-design-with-blocking","title":"Setup a full factorial design with blocking\u00b6","text":"<p>Here we setup a blocked full two-level factorial design including a center point and plot it.</p>"},{"location":"fractional_factorial/#setup-a-fractional-factorial-design","title":"Setup a fractional factorial design\u00b6","text":""},{"location":"getting_started/","title":"Basic terminology","text":"In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.features.api import (\n    CategoricalDescriptorInput,\n    CategoricalInput,\n    ContinuousInput,\n    DiscreteInput,\n)\n\n\nx1 = ContinuousInput(key=\"conc_A\", bounds=[0, 1])\nx2 = ContinuousInput(key=\"conc_B\", bounds=[0, 1])\nx3 = ContinuousInput(key=\"conc_C\", bounds=[0, 1])\nx4 = DiscreteInput(key=\"temperature\", values=[20, 50, 90], unit=\"\u00b0C\")\n\nx5 = CategoricalInput(\n    key=\"catalyst\",\n    categories=[\"cat_X\", \"cat_Y\", \"cat_Z\"],\n    allowed=[\n        True,\n        True,\n        False,\n    ],  # we have run out of catalyst Z, but still want to model past experiments\n)\n\nx6 = CategoricalDescriptorInput(\n    key=\"solvent\",\n    categories=[\"water\", \"methanol\", \"ethanol\"],\n    descriptors=[\"viscosity (mPa s)\", \"density (kg/m3)\"],\n    values=[[1.0, 997], [0.59, 792], [1.2, 789]],\n)\n</pre> from bofire.data_models.features.api import (     CategoricalDescriptorInput,     CategoricalInput,     ContinuousInput,     DiscreteInput, )   x1 = ContinuousInput(key=\"conc_A\", bounds=[0, 1]) x2 = ContinuousInput(key=\"conc_B\", bounds=[0, 1]) x3 = ContinuousInput(key=\"conc_C\", bounds=[0, 1]) x4 = DiscreteInput(key=\"temperature\", values=[20, 50, 90], unit=\"\u00b0C\")  x5 = CategoricalInput(     key=\"catalyst\",     categories=[\"cat_X\", \"cat_Y\", \"cat_Z\"],     allowed=[         True,         True,         False,     ],  # we have run out of catalyst Z, but still want to model past experiments )  x6 = CategoricalDescriptorInput(     key=\"solvent\",     categories=[\"water\", \"methanol\", \"ethanol\"],     descriptors=[\"viscosity (mPa s)\", \"density (kg/m3)\"],     values=[[1.0, 997], [0.59, 792], [1.2, 789]], ) <p>We can define both continuous and categorical outputs. Each output feature should have an objective, which determines if we aim to minimize, maximize, or drive the feature to a given value. Furthermore, we can define weights between 0 and 1 in case the objectives should not be weighted equally.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective\n\n\nobjective1 = MaximizeObjective(\n    w=1.0,\n    bounds=[0.0, 1.0],\n)\ny1 = ContinuousOutput(key=\"yield\", objective=objective1)\n\nobjective2 = MinimizeObjective(w=1.0)\ny2 = ContinuousOutput(key=\"time_taken\", objective=objective2)\n</pre> from bofire.data_models.features.api import ContinuousOutput from bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective   objective1 = MaximizeObjective(     w=1.0,     bounds=[0.0, 1.0], ) y1 = ContinuousOutput(key=\"yield\", objective=objective1)  objective2 = MinimizeObjective(w=1.0) y2 = ContinuousOutput(key=\"time_taken\", objective=objective2) <p>In- and output features are collected in respective feature lists, which can be summarized with the <code>get_reps_df</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Inputs, Outputs\n\n\ninput_features = Inputs(features=[x1, x2, x3, x4, x5, x6])\noutput_features = Outputs(features=[y1, y2])\n\ninput_features.get_reps_df()\n</pre> from bofire.data_models.domain.api import Inputs, Outputs   input_features = Inputs(features=[x1, x2, x3, x4, x5, x6]) output_features = Outputs(features=[y1, y2])  input_features.get_reps_df() In\u00a0[\u00a0]: Copied! <pre>output_features.get_reps_df()\n</pre> output_features.get_reps_df() <p>Individual features can be retrieved by name, and a collection of features can be retrieved with a list of names.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get_by_key(\"catalyst\")\n</pre> input_features.get_by_key(\"catalyst\") In\u00a0[\u00a0]: Copied! <pre>input_features.get_by_keys([\"catalyst\", \"conc_B\"])\n</pre> input_features.get_by_keys([\"catalyst\", \"conc_B\"]) <p>Features of a specific type can be returned by the <code>get</code> method. By using the <code>exact</code> argument, we can force the method to only return features that match the class exactly.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get(CategoricalInput)\n</pre> input_features.get(CategoricalInput) In\u00a0[\u00a0]: Copied! <pre>input_features.get(CategoricalInput, exact=True)\n</pre> input_features.get(CategoricalInput, exact=True) <p>The <code>get_keys</code> method follows the same logic as the <code>get</code> method, but returns just the keys of the features instead of the features itself.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get_keys(CategoricalInput)\n</pre> input_features.get_keys(CategoricalInput) <p>The input feature container further provides methods to return a feature container with only all fixed or all free features.</p> In\u00a0[\u00a0]: Copied! <pre>free_inputs = input_features.get_free()\nfixed_inputs = input_features.get_fixed()\n</pre> free_inputs = input_features.get_free() fixed_inputs = input_features.get_fixed() <p>One can uniformly sample from individual input features.</p> In\u00a0[\u00a0]: Copied! <pre>x5.sample(2)\n</pre> x5.sample(2) <p>Or directly from input feature containers, uniform, sobol and LHS sampling is possible. A default, uniform sampling is used.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.enum import SamplingMethodEnum\n\n\nX = input_features.sample(n=10, method=SamplingMethodEnum.LHS)\n\nX\n</pre> from bofire.data_models.enum import SamplingMethodEnum   X = input_features.sample(n=10, method=SamplingMethodEnum.LHS)  X In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n)\n\n\n# A mixture: x1 + x2 + x3 = 1\nconstr1 = LinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    coefficients=[1, 1, 1],\n    rhs=1,\n)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = LinearInequalityConstraint(\n    features=[\"conc_A\", \"conc_C\"],\n    coefficients=[1, 2],\n    rhs=0.8,\n)\n</pre> from bofire.data_models.constraints.api import (     LinearEqualityConstraint,     LinearInequalityConstraint, )   # A mixture: x1 + x2 + x3 = 1 constr1 = LinearEqualityConstraint(     features=[\"conc_A\", \"conc_B\", \"conc_C\"],     coefficients=[1, 1, 1],     rhs=1, )  # x1 + 2 * x3 &lt; 0.8 constr2 = LinearInequalityConstraint(     features=[\"conc_A\", \"conc_C\"],     coefficients=[1, 2],     rhs=0.8, ) <p>Linear constraints can only operate on <code>ContinuousInput</code> features.</p> <p><code>NonlinearEqualityConstraint</code> and <code>NonlinearInequalityConstraint</code> take any expression that can be evaluated by pandas.eval, including mathematical operators such as <code>sin</code>, <code>exp</code>, <code>log10</code> or exponentiation. So far, they cannot be used in any optimizations.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.constraints.api import NonlinearEqualityConstraint\n\n\n# The unit circle: x1**2 + x2**2 = 1\nconst3 = NonlinearEqualityConstraint(expression=\"conc_A**2 + conc_B**2 - 1\")\nconst3\n</pre> from bofire.data_models.constraints.api import NonlinearEqualityConstraint   # The unit circle: x1**2 + x2**2 = 1 const3 = NonlinearEqualityConstraint(expression=\"conc_A**2 + conc_B**2 - 1\") const3 In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.constraints.api import NChooseKConstraint\n\n\n# Only 1 or 2 out of 3 compounds can be present (have non-zero concentration)\nconstr5 = NChooseKConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    min_count=1,\n    max_count=2,\n    none_also_valid=False,\n)\nconstr5\n</pre> from bofire.data_models.constraints.api import NChooseKConstraint   # Only 1 or 2 out of 3 compounds can be present (have non-zero concentration) constr5 = NChooseKConstraint(     features=[\"conc_A\", \"conc_B\", \"conc_C\"],     min_count=1,     max_count=2,     none_also_valid=False, ) constr5 <p>Note that we have to set a boolean, if none is also a valid selection, e.g. if we want to have 0, 1, or 2 of the ingredients in our recipe.</p> <p>Similar to the features, constraints can be grouped in a container which acts as the union constraints.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Constraints\n\n\nconstraints = Constraints(constraints=[constr1, constr2])\n</pre> from bofire.data_models.domain.api import Constraints   constraints = Constraints(constraints=[constr1, constr2]) <p>A summary of the constraints can be obtained by the method <code>get_reps_df</code>:</p> In\u00a0[\u00a0]: Copied! <pre>constraints.get_reps_df()\n</pre> constraints.get_reps_df() <p>We can check whether a point satisfies individual constraints or the list of constraints.</p> In\u00a0[\u00a0]: Copied! <pre>constr2.is_fulfilled(X).values\n</pre> constr2.is_fulfilled(X).values <p>Output constraints can be setup via sigmoid-shaped objectives passed as argument to the respective feature, which can then also be plotted.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.objectives.api import MinimizeSigmoidObjective\nfrom bofire.plot.api import plot_objective_plotly\n\n\noutput_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5)\ny3 = ContinuousOutput(key=\"y3\", objective=output_constraint)\n\noutput_features = Outputs(features=[y1, y2, y3])\n\nfig = plot_objective_plotly(feature=y3, lower=0, upper=1)\n\nfig.show()\n</pre> from bofire.data_models.objectives.api import MinimizeSigmoidObjective from bofire.plot.api import plot_objective_plotly   output_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5) y3 = ContinuousOutput(key=\"y3\", objective=output_constraint)  output_features = Outputs(features=[y1, y2, y3])  fig = plot_objective_plotly(feature=y3, lower=0, upper=1)  fig.show() In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Domain\n\n\ndomain = Domain(inputs=input_features, outputs=output_features, constraints=constraints)\n</pre> from bofire.data_models.domain.api import Domain   domain = Domain(inputs=input_features, outputs=output_features, constraints=constraints) <p>In addition one can instantiate the domain also just from lists.</p> In\u00a0[\u00a0]: Copied! <pre>domain_single_objective = Domain.from_lists(\n    inputs=[x1, x2, x3, x4, x5, x6],\n    outputs=[y1],\n    constraints=[],\n)\n</pre> domain_single_objective = Domain.from_lists(     inputs=[x1, x2, x3, x4, x5, x6],     outputs=[y1],     constraints=[], ) In\u00a0[\u00a0]: Copied! <pre>import bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy\n\n\nstrategy_data_model = RandomStrategy(domain=domain)\n\nrandom_strategy = strategies.map(strategy_data_model)\nrandom_candidates = random_strategy.ask(2)\n\nrandom_candidates\n</pre> import bofire.strategies.api as strategies from bofire.data_models.strategies.api import RandomStrategy   strategy_data_model = RandomStrategy(domain=domain)  random_strategy = strategies.map(strategy_data_model) random_candidates = random_strategy.ask(2)  random_candidates In\u00a0[\u00a0]: Copied! <pre>from bofire.benchmarks.single import Himmelblau\n\n\nbenchmark = Himmelblau()\n\n(benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df()\n</pre> from bofire.benchmarks.single import Himmelblau   benchmark = Himmelblau()  (benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df() <p>Generating some initial data works as follows:</p> In\u00a0[\u00a0]: Copied! <pre>samples = benchmark.domain.inputs.sample(10)\n\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments\n</pre> samples = benchmark.domain.inputs.sample(10)  experiments = benchmark.f(samples, return_complete=True)  experiments <p>Let's setup the SOBO strategy and ask for a candidate. First we need a serializable data model that contains the hyperparameters.</p> In\u00a0[\u00a0]: Copied! <pre>from pprint import pprint\n\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM\n\n\nsobo_strategy_data_model = SoboStrategyDM(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# print information about hyperparameters\nprint(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function)\nprint()\nprint(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type)\nprint()\nprint(\"Surrogate's kernel:\")\npprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump())\n</pre> from pprint import pprint  from bofire.data_models.acquisition_functions.api import qLogNEI from bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM   sobo_strategy_data_model = SoboStrategyDM(     domain=benchmark.domain,     acquisition_function=qLogNEI(), )  # print information about hyperparameters print(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function) print() print(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type) print() print(\"Surrogate's kernel:\") pprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump()) <p>The actual strategy can then be created via the mapper function.</p> In\u00a0[\u00a0]: Copied! <pre>sobo_strategy = strategies.map(sobo_strategy_data_model)\nsobo_strategy.tell(experiments=experiments)\nsobo_strategy.ask(candidate_count=1)\n</pre> sobo_strategy = strategies.map(sobo_strategy_data_model) sobo_strategy.tell(experiments=experiments) sobo_strategy.ask(candidate_count=1) <p>An alternative way is calling the strategy's constructor directly.</p> In\u00a0[\u00a0]: Copied! <pre>sobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model)\n</pre> sobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model) <p>The latter way is helpful to keep type information.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1])\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n</pre> import numpy as np  from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion   domain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1]) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"fully-quadratic\"), ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(candidate_count=12) np.round(candidates, 3) <p>The resulting design looks like this:</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"fully-quadratic model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[x1.key],\n    ys=candidates[x2.key],\n    zs=candidates[x3.key],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n)\n</pre> import matplotlib.pyplot as plt   fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.view_init(45, 45) ax.set_title(\"fully-quadratic model\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)  # plot D-optimal solutions ax.scatter(     xs=candidates[x1.key],     ys=candidates[x2.key],     zs=candidates[x3.key],     marker=\"o\",     s=40,     color=\"orange\", )"},{"location":"getting_started/#basic-terminology","title":"Basic terminology\u00b6","text":"<p>In the following it is showed how to setup optimization problems in BoFire and how to use strategies to solve them.</p>"},{"location":"getting_started/#setting-up-the-optimization-problem","title":"Setting up the optimization problem\u00b6","text":"<p>In BoFire, an optimization problem is defined by defining a domain containing input and output features, as well as optionally including constraints.</p>"},{"location":"getting_started/#features","title":"Features\u00b6","text":"<p>Input features can be continuous, discrete, categorical.</p> <p>We also support a range of specialized inputs that make defining your experiments easier, such as:</p> <ul> <li><code>MolecularInput</code> allows transformations of molecules to featurizations (<code>Fingerprints</code>, <code>Fragments</code> and more).</li> <li><code>TaskInput</code> enables transfer learning and multi-fidelity methods, where you have access to similar experiments that can inform your optimization.</li> <li><code>*DescriptorInput</code> gives additional information about its value, combining the data with its significance.</li> </ul>"},{"location":"getting_started/#constraints","title":"Constraints\u00b6","text":"<p>The search space can be further defined by constraints on the input features. BoFire supports linear equality and inequality constraints, as well as non-linear equality and inequality constraints.</p>"},{"location":"getting_started/#linear-constraints","title":"Linear constraints\u00b6","text":"<p><code>LinearEqualityConstraint</code> and <code>LinearInequalityConstraint</code> are expressions of the form $\\sum_i a_i x_i = b$ or $\\leq b$ for equality and inequality constraints respectively. They take a list of names of the input features they are operating on, a list of left-hand-side coefficients $a_i$ and a right-hand-side constant $b$.</p>"},{"location":"getting_started/#nonlinear-constraints","title":"Nonlinear constraints\u00b6","text":""},{"location":"getting_started/#combinatorial-constraint","title":"Combinatorial constraint\u00b6","text":"<p>Use <code>NChooseKConstraint</code> to express that we only want to have $k$ out of the $n$ parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe.</p>"},{"location":"getting_started/#the-domain","title":"The domain\u00b6","text":"<p>The domain holds then all information about an optimization problem and can be understood as a search space definition.</p>"},{"location":"getting_started/#optimization","title":"Optimization\u00b6","text":"<p>To solve the optimization problem, we further need a solving strategy. BoFire supports strategies without a prediction model such as a random strategy and predictive strategies which are based on a prediction model.</p> <p>All strategies contain an <code>ask</code> method returning a defined number of candidate experiments.</p>"},{"location":"getting_started/#random-strategy","title":"Random Strategy\u00b6","text":""},{"location":"getting_started/#single-objective-bayesian-optimization-strategy","title":"Single objective Bayesian Optimization strategy\u00b6","text":"<p>Since a predictive strategy includes a prediction model, we need to generate some historical data, which we can afterwards pass as training data to the strategy via the tell method.</p> <p>For didactic purposes we just choose here from one of our benchmark methods.</p>"},{"location":"getting_started/#design-of-experiments","title":"Design of Experiments\u00b6","text":"<p>As a simple example for the DoE functionalities we consider the task of finding a D-optimal design for a fully-quadratic model with three design variables with bounds (0,1) and a mixture constraint.</p> <p>We define the design space including the constraint as a domain. Then we pass it to the optimization routine and specify the model. If the user does not indicate a number of experiments it will be chosen automatically based on the number of model terms.</p>"},{"location":"install/","title":"Installation Guide","text":""},{"location":"install/#installation-from-python-package-index-pypi","title":"Installation from Python Package Index (PyPI)","text":"<p>BoFire can be installed to your Python environment by using <code>pip</code>. It can be done by executing</p> <pre><code>pip install bofire\n</code></pre> <p>Tip</p> <p>The command from above will install a minimal BoFire version, consisting only of the data models. To install BoFire's including its core optimization features, execute: <pre><code>pip install 'bofire[optimization]'\n</code></pre></p>"},{"location":"install/#additional-optional-dependencies","title":"Additional optional dependencies","text":"<p>In BoFire, there are several optional dependencies that can be selected during installation via pip, like</p> <pre><code>pip install 'bofire[optimization, cheminfo] # will install bofire with additional dependencies `optimization` and `cheminfo`\n</code></pre> <p>To get the most our of BoFire, it is recommended to install at least <pre><code>pip install 'bofire[optimization]'\n</code></pre></p> <p>The available dependencies are:</p> <ul> <li><code>optimization</code>: Core Bayesian optimization features.</li> <li><code>cheminfo</code>: Cheminformatics utilities.</li> <li><code>entmoot</code>: Entmoot functionality.</li> <li><code>xgb</code>: XGboost surrogates.</li> <li><code>tests</code>: Required for running the test suite.</li> <li><code>docs</code>: Required for building the documentation.</li> <li><code>tutorials</code>: Required for running the tutorials.</li> <li><code>all</code>: Install all possible options (except DoE)</li> </ul> <p>Warning</p> <p>BoFire has the functionalities for creating D, E, A, G, K and I-optimal experimental designs via the <code>DoEStrategy</code>. This feature depends on cyipopt which is a python interface to <code>ipopt</code>. Unfortunately, it is not possible to install <code>cyipopt</code> including <code>ipopt</code> via pip. A solution is to install <code>cyipopt</code> and its dependencies via conda:</p> <pre><code>conda install -c conda-forge cyipopt\n</code></pre> <p>We are working on a solution that makes BoFire's model based DoE functionalities also accessible to users which do not have <code>cyipopt</code> available.</p>"},{"location":"install/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to BoFire, it is recommended to install the repository in editable mode (<code>-e</code>).</p> <p>After cloning the repository via <pre><code>git clone https://github.com/experimental-design/bofire.git\n</code></pre> and navigating to the repositories root folder (<code>cd bofire</code>), you can proceed with <pre><code>pip install -e \".[optimization, tests]\" # include optional dependencies as you wish\n</code></pre></p>"},{"location":"multifidelity_bo/","title":"Multi-fidelity Bayesian Optimization","text":"In\u00a0[\u00a0]: Copied! <pre>import os\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Ackley, Benchmark, Branin\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import TaskInput\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, MultiTaskGPSurrogate\n</pre> import os  import numpy as np import pandas as pd from tqdm import tqdm  import bofire.strategies.api as strategies from bofire.benchmarks.api import Ackley, Benchmark, Branin from bofire.data_models.acquisition_functions.api import qLogEI from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import TaskInput from bofire.data_models.surrogates.api import BotorchSurrogates, MultiTaskGPSurrogate In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\n</pre> import matplotlib.pyplot as plt from matplotlib.axes import Axes In\u00a0[\u00a0]: Copied! <pre>SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_INIT_HF = 4\nNUM_INIT_LF = 10\nif SMOKE_TEST:\n    num_runs = 5\n    num_iters = 2\n    verbose = False\nelse:\n    num_runs = 10\n    num_iters = 10\n    verbose = True\n</pre> SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") NUM_INIT_HF = 4 NUM_INIT_LF = 10 if SMOKE_TEST:     num_runs = 5     num_iters = 2     verbose = False else:     num_runs = 10     num_iters = 10     verbose = True <p>This notebook is a sequel to \"Transfer Learning in BO\".</p> In\u00a0[\u00a0]: Copied! <pre>class BraninMultiTask(Benchmark):\n    def __init__(self, low_fidelity_allowed=False, **kwargs):\n        super().__init__(**kwargs)\n        self._branin = Branin()\n        self._ackley = Ackley()\n        task_input = TaskInput(\n            key=\"task\",\n            categories=[\"task_hf\", \"task_lf\"],\n            allowed=[True, low_fidelity_allowed],\n            fidelities=[0, 1],\n        )\n        self._domain = Domain(\n            inputs=self._branin.domain.inputs + (task_input,),\n            outputs=self._branin.domain.outputs,\n        )\n\n    def _f(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:\n        candidates_no_task = candidates.drop(columns=[\"task\"])\n        f_branin = self._branin.f(candidates_no_task)\n        f_ackley = self._ackley.f(candidates_no_task)\n        bias_scale = np.where(candidates[\"task\"] == \"task_hf\", 0.0, 0.15).reshape(-1, 1)\n        bias_scale = pd.DataFrame(bias_scale, columns=self._domain.outputs.get_keys())\n        bias_scale[\"valid_y\"] = 0.0\n        return f_branin + bias_scale * f_ackley\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        optima = self._branin.get_optima()\n        optima[\"task\"] = \"task_hf\"\n        return optima\n\n\nmf_benchmark = BraninMultiTask(low_fidelity_allowed=True)\ntl_benchmark = BraninMultiTask(low_fidelity_allowed=False)\n</pre> class BraninMultiTask(Benchmark):     def __init__(self, low_fidelity_allowed=False, **kwargs):         super().__init__(**kwargs)         self._branin = Branin()         self._ackley = Ackley()         task_input = TaskInput(             key=\"task\",             categories=[\"task_hf\", \"task_lf\"],             allowed=[True, low_fidelity_allowed],             fidelities=[0, 1],         )         self._domain = Domain(             inputs=self._branin.domain.inputs + (task_input,),             outputs=self._branin.domain.outputs,         )      def _f(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:         candidates_no_task = candidates.drop(columns=[\"task\"])         f_branin = self._branin.f(candidates_no_task)         f_ackley = self._ackley.f(candidates_no_task)         bias_scale = np.where(candidates[\"task\"] == \"task_hf\", 0.0, 0.15).reshape(-1, 1)         bias_scale = pd.DataFrame(bias_scale, columns=self._domain.outputs.get_keys())         bias_scale[\"valid_y\"] = 0.0         return f_branin + bias_scale * f_ackley      def get_optima(self) -&gt; pd.DataFrame:         optima = self._branin.get_optima()         optima[\"task\"] = \"task_hf\"         return optima   mf_benchmark = BraninMultiTask(low_fidelity_allowed=True) tl_benchmark = BraninMultiTask(low_fidelity_allowed=False) In\u00a0[\u00a0]: Copied! <pre>def create_data_set(seed: int):\n    # use the tl_benchmark to sample without the low fidelity\n    experiments = tl_benchmark.domain.inputs.sample(\n        NUM_INIT_HF + NUM_INIT_LF, seed=seed\n    )\n    experiments[\"task\"] = np.where(\n        experiments.index &lt; NUM_INIT_LF, \"task_lf\", \"task_hf\"\n    )\n\n    # then use the ml_benchmark to evaluate the low fidelity\n    return mf_benchmark.f(experiments, return_complete=True)\n\n\ncreate_data_set(0)\n</pre> def create_data_set(seed: int):     # use the tl_benchmark to sample without the low fidelity     experiments = tl_benchmark.domain.inputs.sample(         NUM_INIT_HF + NUM_INIT_LF, seed=seed     )     experiments[\"task\"] = np.where(         experiments.index &lt; NUM_INIT_LF, \"task_lf\", \"task_hf\"     )      # then use the ml_benchmark to evaluate the low fidelity     return mf_benchmark.f(experiments, return_complete=True)   create_data_set(0) In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.strategies.api import MultiFidelityStrategy\n\n\n# It isn't necessary to define the surrogate specs here, as the MFStrategy\n# will use a MultiTaskGP by default.\n\nmf_data_model = MultiFidelityStrategy(\n    domain=mf_benchmark.domain,\n    acquisition_function=qLogEI(),\n    fidelity_thresholds=0.1,\n)\nmf_data_model.surrogate_specs.surrogates[0].inputs\n</pre> from bofire.data_models.strategies.api import MultiFidelityStrategy   # It isn't necessary to define the surrogate specs here, as the MFStrategy # will use a MultiTaskGP by default.  mf_data_model = MultiFidelityStrategy(     domain=mf_benchmark.domain,     acquisition_function=qLogEI(),     fidelity_thresholds=0.1, ) mf_data_model.surrogate_specs.surrogates[0].inputs In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.strategies.api import SoboStrategy\n\n\nsurrogate_specs = BotorchSurrogates(\n    surrogates=[\n        MultiTaskGPSurrogate(\n            inputs=tl_benchmark.domain.inputs,\n            outputs=tl_benchmark.domain.outputs,\n        )\n    ]\n)\n\ntl_data_model = SoboStrategy(\n    domain=tl_benchmark.domain,\n    acquisition_function=qLogEI(),\n    surrogate_specs=surrogate_specs,\n)\n</pre> from bofire.data_models.strategies.api import SoboStrategy   surrogate_specs = BotorchSurrogates(     surrogates=[         MultiTaskGPSurrogate(             inputs=tl_benchmark.domain.inputs,             outputs=tl_benchmark.domain.outputs,         )     ] )  tl_data_model = SoboStrategy(     domain=tl_benchmark.domain,     acquisition_function=qLogEI(),     surrogate_specs=surrogate_specs, ) <p>We first optimize only on the target fidelity (the \"Transfer Learning\" baseline). This uses the <code>SoboStrategy</code> defined above.</p> In\u00a0[\u00a0]: Copied! <pre>tl_results = pd.DataFrame(columns=pd.MultiIndex.from_tuples([], names=(\"col\", \"run\")))\nfor run in range(num_runs):\n    seed = 2048 * run + 123\n    experiments = create_data_set(seed)\n\n    tl_strategy = strategies.map(tl_data_model)\n    tl_strategy.tell(experiments)\n\n    assert tl_strategy.experiments is not None\n\n    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n    for _ in pbar:\n        candidate = tl_strategy.ask(1)\n        y = tl_benchmark.f(candidate, return_complete=True)\n        tl_strategy.tell(y)\n\n        hf_experiments = tl_strategy.experiments[\n            tl_strategy.experiments[\"task\"] == \"task_hf\"\n        ]\n        regret = hf_experiments[\"y\"].min() - tl_benchmark.get_optima()[\"y\"][0].item()\n\n        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n\n    tl_results[\"fidelity\", f\"{run}\"] = tl_strategy.experiments[\"task\"]\n    tl_results[\"y\", f\"{run}\"] = tl_strategy.experiments[\"y\"]\n</pre> tl_results = pd.DataFrame(columns=pd.MultiIndex.from_tuples([], names=(\"col\", \"run\"))) for run in range(num_runs):     seed = 2048 * run + 123     experiments = create_data_set(seed)      tl_strategy = strategies.map(tl_data_model)     tl_strategy.tell(experiments)      assert tl_strategy.experiments is not None      pbar = tqdm(range(num_iters), desc=\"Optimizing\")     for _ in pbar:         candidate = tl_strategy.ask(1)         y = tl_benchmark.f(candidate, return_complete=True)         tl_strategy.tell(y)          hf_experiments = tl_strategy.experiments[             tl_strategy.experiments[\"task\"] == \"task_hf\"         ]         regret = hf_experiments[\"y\"].min() - tl_benchmark.get_optima()[\"y\"][0].item()          pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})      tl_results[\"fidelity\", f\"{run}\"] = tl_strategy.experiments[\"task\"]     tl_results[\"y\", f\"{run}\"] = tl_strategy.experiments[\"y\"] <p>We now repeat the experiment using multi-fidelity BO, allowing the strategy to query the low fidelity function as well as the high fidelity function:</p> In\u00a0[\u00a0]: Copied! <pre>mf_results = pd.DataFrame(columns=pd.MultiIndex.from_tuples([], names=(\"col\", \"run\")))\nfor run in range(num_runs):\n    seed = 2048 * run + 123\n    experiments = create_data_set(seed)\n\n    mf_strategy = strategies.map(mf_data_model)\n    mf_strategy.tell(experiments)\n\n    assert mf_strategy.experiments is not None\n\n    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n    for _ in pbar:\n        candidate = mf_strategy.ask(1)\n        y = mf_benchmark.f(candidate, return_complete=True)\n        mf_strategy.tell(y)\n\n        hf_experiments = mf_strategy.experiments[\n            mf_strategy.experiments[\"task\"] == \"task_hf\"\n        ]\n        regret = hf_experiments[\"y\"].min() - mf_benchmark.get_optima()[\"y\"][0].item()\n\n        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n\n    mf_results[\"fidelity\", f\"{run}\"] = mf_strategy.experiments[\"task\"]\n    mf_results[\"y\", f\"{run}\"] = mf_strategy.experiments[\"y\"]\n</pre> mf_results = pd.DataFrame(columns=pd.MultiIndex.from_tuples([], names=(\"col\", \"run\"))) for run in range(num_runs):     seed = 2048 * run + 123     experiments = create_data_set(seed)      mf_strategy = strategies.map(mf_data_model)     mf_strategy.tell(experiments)      assert mf_strategy.experiments is not None      pbar = tqdm(range(num_iters), desc=\"Optimizing\")     for _ in pbar:         candidate = mf_strategy.ask(1)         y = mf_benchmark.f(candidate, return_complete=True)         mf_strategy.tell(y)          hf_experiments = mf_strategy.experiments[             mf_strategy.experiments[\"task\"] == \"task_hf\"         ]         regret = hf_experiments[\"y\"].min() - mf_benchmark.get_optima()[\"y\"][0].item()          pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})      mf_results[\"fidelity\", f\"{run}\"] = mf_strategy.experiments[\"task\"]     mf_results[\"y\", f\"{run}\"] = mf_strategy.experiments[\"y\"] <p>When evaluating the performance, we need to consider how cheap the low-fidelity (LF) is to query. When the LF has the same cost as the target fidelity, then we gain very little from the multi-fidelity approach. However, if the LF is cheaper than the target (in the example below, 3x cheaper) then we observe an improvement in BO performance.</p> <p>Specifically, although both strategies have a budget of 10 function queries, the MF approach uses some of them on</p> In\u00a0[\u00a0]: Copied! <pre>def plot_regret(\n    ax: Axes, bo_results: pd.DataFrame, fidelity_cost_ratio: float, **plot_kwargs\n):\n    cummin = (\n        bo_results[\"y\"]\n        .where(bo_results[\"fidelity\"] == \"task_hf\", other=np.inf)\n        .cummin(axis=0)\n    )\n    # only select iterations, and the final training point\n    cummin = cummin.iloc[-num_iters - 1 :]\n    regret: np.ndarray = (cummin - mf_benchmark.get_optima()[\"y\"][0].item()).to_numpy()\n\n    # keep track of \"real time\", where low fidelities are cheaper to evaluate.\n    time_taken = np.where(bo_results[\"fidelity\"] == \"task_hf\", fidelity_cost_ratio, 1)[\n        -num_iters - 1 :\n    ].cumsum(axis=0)\n    time_taken -= time_taken[0, 0]  # start from T=0 after training data\n    iterations = np.arange(num_iters * fidelity_cost_ratio)\n    before_time = time_taken[:, :, np.newaxis] &lt;= iterations[np.newaxis, np.newaxis, :]\n    regret_before_time = regret[:, :, np.newaxis] * np.where(before_time, 1.0, np.inf)\n    # regret_before_time.shape == (num_iters+1, num_runs, len(iterations))\n    # project into time dimension\n    regret = regret_before_time.min(axis=0)\n\n    ax.plot(\n        iterations,\n        np.median(regret, axis=0),\n        label=plot_kwargs.get(\"label\"),\n        color=plot_kwargs.get(\"color\"),\n    )\n    ax.fill_between(\n        iterations,\n        np.quantile(regret, 0.75, axis=0),\n        np.quantile(regret, 0.25, axis=0),\n        color=plot_kwargs.get(\"color\"),\n        alpha=0.2,\n    )\n\n\nfig, axs = plt.subplots(ncols=2, figsize=(8, 4), sharey=True)\ncost_ratios = (1, 3)\n\nfor ax, cost_ratio in zip(axs, cost_ratios):\n    plot_regret(\n        ax,\n        tl_results,\n        fidelity_cost_ratio=cost_ratio,\n        label=\"Transfer Learning\",\n        color=\"blue\",\n    )\n    plot_regret(\n        ax,\n        mf_results,\n        fidelity_cost_ratio=cost_ratio,\n        label=\"Multi-fidelity\",\n        color=\"green\",\n    )\n    ax.set_xlabel(\"Time step\")\n    ax.set_title(f\"Fidelity cost ratio = {cost_ratio}\")\n\n\naxs[1].legend()\naxs[0].set_ylabel(\"Regret\")\n\nplt.show()\n</pre> def plot_regret(     ax: Axes, bo_results: pd.DataFrame, fidelity_cost_ratio: float, **plot_kwargs ):     cummin = (         bo_results[\"y\"]         .where(bo_results[\"fidelity\"] == \"task_hf\", other=np.inf)         .cummin(axis=0)     )     # only select iterations, and the final training point     cummin = cummin.iloc[-num_iters - 1 :]     regret: np.ndarray = (cummin - mf_benchmark.get_optima()[\"y\"][0].item()).to_numpy()      # keep track of \"real time\", where low fidelities are cheaper to evaluate.     time_taken = np.where(bo_results[\"fidelity\"] == \"task_hf\", fidelity_cost_ratio, 1)[         -num_iters - 1 :     ].cumsum(axis=0)     time_taken -= time_taken[0, 0]  # start from T=0 after training data     iterations = np.arange(num_iters * fidelity_cost_ratio)     before_time = time_taken[:, :, np.newaxis] &lt;= iterations[np.newaxis, np.newaxis, :]     regret_before_time = regret[:, :, np.newaxis] * np.where(before_time, 1.0, np.inf)     # regret_before_time.shape == (num_iters+1, num_runs, len(iterations))     # project into time dimension     regret = regret_before_time.min(axis=0)      ax.plot(         iterations,         np.median(regret, axis=0),         label=plot_kwargs.get(\"label\"),         color=plot_kwargs.get(\"color\"),     )     ax.fill_between(         iterations,         np.quantile(regret, 0.75, axis=0),         np.quantile(regret, 0.25, axis=0),         color=plot_kwargs.get(\"color\"),         alpha=0.2,     )   fig, axs = plt.subplots(ncols=2, figsize=(8, 4), sharey=True) cost_ratios = (1, 3)  for ax, cost_ratio in zip(axs, cost_ratios):     plot_regret(         ax,         tl_results,         fidelity_cost_ratio=cost_ratio,         label=\"Transfer Learning\",         color=\"blue\",     )     plot_regret(         ax,         mf_results,         fidelity_cost_ratio=cost_ratio,         label=\"Multi-fidelity\",         color=\"green\",     )     ax.set_xlabel(\"Time step\")     ax.set_title(f\"Fidelity cost ratio = {cost_ratio}\")   axs[1].legend() axs[0].set_ylabel(\"Regret\")  plt.show() <p>We can see that allowing the lower fidelities to be queries leads to stronger optimization performance. We can also see below that the MF approach only samples the target fidelity in later iterations, once the variance of the LF has been sufficiently reduced.</p> In\u00a0[\u00a0]: Copied! <pre>(mf_results[\"fidelity\"] == \"task_hf\")[-num_iters:].mean(axis=1)  # type: ignore\n</pre> (mf_results[\"fidelity\"] == \"task_hf\")[-num_iters:].mean(axis=1)  # type: ignore"},{"location":"multifidelity_bo/#multi-fidelity-bayesian-optimization","title":"Multi-fidelity Bayesian Optimization\u00b6","text":"<p>In the previous notebook, we saw how using low-fidelity approximations to our target function can improve the predictions from our surrogate model, leading to a faster optimization procedure. In this notebook, we show how we can gain even further performance gains by querying the cheap low-fidelity approximations during the BO loop.</p>"},{"location":"multifidelity_bo/#problem-definition","title":"Problem definition\u00b6","text":"<p>We use the same problem as the transfer learning notebook; optimizing the Branin benchmark, with a low-fidelity function biased by the Ackley function (with fewer initial points, to demonstrate the strength of being able to query low fidelities). Below, we define the problem domain, and the strategies we will use to optimize.</p> <p>As a baseline, we use the <code>SoboStrategy</code> with the <code>MultiTaskSurrogate</code>, as in the previous notebook. We also introduce the <code>MultiFidelityStrategy</code> here, which uses the same surrogate, but is able to query the lower fidelity functions using a variance-based acquisition function [Kandasamy et al. 2016, Folch et al. 2023].</p> <p>Both strategies first select a design point $x$ by optimizing the target fidelity. The <code>MultiFidelityStrategy</code> then selects the fidelity, $m$, by selecting the lowest fidelity that has a variance over a fixed threshold. This means that the strategy will explore the cheapest fidelities first, and only query the expensive fidelities when there is no information to be gained by the cheap approximations.</p>"},{"location":"multifidelity_bo/#multi-fidelity-bayesian-optimisation","title":"Multi-fidelity Bayesian Optimisation\u00b6","text":""},{"location":"nchoosek_constraint/","title":"Nchoosek constraint","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NChooseKConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(8)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[f\"x{i+1}\" for i in range(8)],\n            coefficients=[1, 1, 1, 1, 1, 1, 1, 1],\n            rhs=1,\n        ),\n        NChooseKConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            min_count=0,\n            max_count=1,\n            none_also_valid=True,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=0.7,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x7\", \"x8\"],\n            coefficients=[-1, -1],\n            rhs=-0.1,\n        ),\n        LinearInequalityConstraint(features=[\"x7\", \"x8\"], coefficients=[1, 1], rhs=0.9),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n    ipopt_options={\"max_iter\": 500},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n</pre> import numpy as np  import bofire.strategies.api as strategies from bofire.data_models.constraints.api import (     LinearEqualityConstraint,     LinearInequalityConstraint,     NChooseKConstraint, ) from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import ContinuousInput, ContinuousOutput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion   domain = Domain(     inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(8)],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         LinearEqualityConstraint(             features=[f\"x{i+1}\" for i in range(8)],             coefficients=[1, 1, 1, 1, 1, 1, 1, 1],             rhs=1,         ),         NChooseKConstraint(             features=[\"x1\", \"x2\", \"x3\"],             min_count=0,             max_count=1,             none_also_valid=True,         ),         LinearInequalityConstraint(             features=[\"x1\", \"x2\", \"x3\"],             coefficients=[1, 1, 1],             rhs=0.7,         ),         LinearInequalityConstraint(             features=[\"x7\", \"x8\"],             coefficients=[-1, -1],             rhs=-0.1,         ),         LinearInequalityConstraint(features=[\"x7\", \"x8\"], coefficients=[1, 1], rhs=0.9),     ], )  data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),     ipopt_options={\"max_iter\": 500}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(candidate_count=12) np.round(candidates, 3) <pre>/home/linznedd/miniforge3/envs/bofire/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n</pre> Out[\u00a0]: x1 x2 x3 x4 x5 x6 x7 x8 0 -0.0 -0.0 -0.0 0.9 -0.0 -0.0 -0.0 0.1 1 -0.0 -0.0 -0.0 0.1 -0.0 -0.0 -0.0 0.9 2 -0.0 0.7 -0.0 -0.0 -0.0 -0.0 0.3 -0.0 3 -0.0 -0.0 -0.0 -0.0 -0.0 0.1 -0.0 0.9 4 -0.0 -0.0 -0.0 -0.0 -0.0 0.9 0.1 -0.0 5 -0.0 -0.0 0.7 -0.0 -0.0 -0.0 -0.0 0.3 6 -0.0 -0.0 -0.0 0.9 -0.0 -0.0 0.1 -0.0 7 -0.0 -0.0 -0.0 -0.0 -0.0 0.9 -0.0 0.1 8 0.7 -0.0 -0.0 -0.0 -0.0 -0.0 0.3 -0.0 9 -0.0 -0.0 -0.0 -0.0 0.1 -0.0 0.9 -0.0 10 -0.0 -0.0 -0.0 -0.0 0.9 -0.0 -0.0 0.1 11 -0.0 -0.0 -0.0 -0.0 -0.0 0.9 -0.0 0.1"},{"location":"nchoosek_constraint/#design-with-nchoosek-constraint","title":"Design with NChooseK constraint\u00b6","text":"<p>The doe subpackage also supports problems with NChooseK constraints. Since IPOPT has problems finding feasible solutions using the gradient of the NChooseK constraint violation, a closely related (but stricter) constraint that suffices to fulfill the NChooseK constraint is imposed onto the problem: For each experiment $j$ N-K decision variables $x_{i_1,j},...,x_{i_{N-K,j}}$ from the NChooseK constraints' names attribute are picked that are forced to be zero. This is done by setting the upper and lower bounds of the picked variables are set to 0 in the corresponding experiments. This causes IPOPT to treat them as \"fixed variables\" (i.e. it will not optimize for them) and will always stick to the only feasible value (which is 0 here). However, this constraint is stricter than the original NChooseK constraint. In combination with other constraints on the same decision variables this can result in a situation where the constraints cannot be fulfilled even though the original constraints would allow for a solution. For example consider a problem with four decision variables $x_1, x_2, x_3, x_4$, an NChooseK constraint on the first four variable that restricts the number of nonzero variables to two. Additionally, we have a linear constraint $$ x_3 + x_4 \\geq 0.1 $$ We can easily find points that fulfill both constraints (e.g. $(0,0,0,0.1)$). Now consider the stricter, linear constraint from above. Eventually, it will happen that $x_3$ and $x_4$ are chosen to be zero for one experiment. For this experiment it is impossible to fulfill the linear constraint $x_3 + x_4 \\geq 0.1$ since $x_3 = x_4 = 0$.</p> <p>Therefore one has to be very careful when imposing linear constraints upon decision variables that already show up in an NChooseK constraint.</p> <p>For practical reasons it necessary that two NChooseK constraints of the same problem must not share any variables.</p> <p>You can find an example for a problem with NChooseK constraints and additional linear constraints imposed on the same variables.</p>"},{"location":"optimality_criteria/","title":"Optimality criteria","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import (\n    AOptimalityCriterion,\n    DOptimalityCriterion,\n    EOptimalityCriterion,\n    IOptimalityCriterion,\n    KOptimalityCriterion,\n    SpaceFillingCriterion,\n)\nfrom bofire.strategies.doe.objective import get_objective_function\n</pre> import matplotlib.pyplot as plt  import bofire.strategies.api as strategies from bofire.data_models.constraints.api import LinearEqualityConstraint from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import ContinuousInput, ContinuousOutput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import (     AOptimalityCriterion,     DOptimalityCriterion,     EOptimalityCriterion,     IOptimalityCriterion,     KOptimalityCriterion,     SpaceFillingCriterion, ) from bofire.strategies.doe.objective import get_objective_function <pre>/home/linznedd/miniforge3/envs/bofire/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[\u00a0]: Copied! <pre># Optimal designs for a quadratic model on the unit square\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(2)],\n    outputs=[ContinuousOutput(key=\"y\")],\n)\nmodel_type = \"fully-quadratic\"\nn_experiments = 13\n\ndesigns = {}\nfor crit in [\n    DOptimalityCriterion,\n    AOptimalityCriterion,\n    KOptimalityCriterion,\n    EOptimalityCriterion,\n    IOptimalityCriterion,\n]:\n    criterion = crit(formula=model_type)\n    data_model = DoEStrategy(\n        domain=domain,\n        criterion=criterion,\n        ipopt_options={\"max_iter\": 300},\n    )\n    strategy = strategies.map(data_model=data_model)\n    design = strategy.ask(candidate_count=n_experiments)\n    obj_value = get_objective_function(\n        criterion=criterion, domain=domain, n_experiments=n_experiments\n    ).evaluate(design.to_numpy().flatten())\n    designs[obj_value] = design.to_numpy()\n\nfig = plt.figure(figsize=((8, 8)))\nax = fig.add_subplot(111)\nax.set_title(\"Designs with different optimality criteria\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nfor obj, X in designs.items():\n    ax.scatter(X[:, 0], X[:, 1], s=40, label=obj)\nax.grid(alpha=0.3)\nax.legend();\n</pre> # Optimal designs for a quadratic model on the unit square domain = Domain(     inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(2)],     outputs=[ContinuousOutput(key=\"y\")], ) model_type = \"fully-quadratic\" n_experiments = 13  designs = {} for crit in [     DOptimalityCriterion,     AOptimalityCriterion,     KOptimalityCriterion,     EOptimalityCriterion,     IOptimalityCriterion, ]:     criterion = crit(formula=model_type)     data_model = DoEStrategy(         domain=domain,         criterion=criterion,         ipopt_options={\"max_iter\": 300},     )     strategy = strategies.map(data_model=data_model)     design = strategy.ask(candidate_count=n_experiments)     obj_value = get_objective_function(         criterion=criterion, domain=domain, n_experiments=n_experiments     ).evaluate(design.to_numpy().flatten())     designs[obj_value] = design.to_numpy()  fig = plt.figure(figsize=((8, 8))) ax = fig.add_subplot(111) ax.set_title(\"Designs with different optimality criteria\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") for obj, X in designs.items():     ax.scatter(X[:, 0], X[:, 1], s=40, label=obj) ax.grid(alpha=0.3) ax.legend(); <pre>\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Space filling design on the unit 2-simplex\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(3)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain, criterion=SpaceFillingCriterion(), ipopt_options={\"max_iter\": 500}\n)\nstrategy = strategies.map(data_model=data_model)\nX = strategy.ask(candidate_count=40).to_numpy()\n\nfig = plt.figure(figsize=((10, 8)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 20)\nax.set_title(\"Space filling design\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\n\n# plot feasible polytope\nax.plot(xs=[0, 0, 1, 0], ys=[0, 1, 0, 0], zs=[1, 0, 0, 1], linewidth=2)\n\n# plot design points\nax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], s=40)\n</pre> # Space filling design on the unit 2-simplex domain = Domain(     inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(3)],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         LinearEqualityConstraint(             features=[\"x1\", \"x2\", \"x3\"],             coefficients=[1, 1, 1],             rhs=1,         ),     ], ) data_model = DoEStrategy(     domain=domain, criterion=SpaceFillingCriterion(), ipopt_options={\"max_iter\": 500} ) strategy = strategies.map(data_model=data_model) X = strategy.ask(candidate_count=40).to_numpy()  fig = plt.figure(figsize=((10, 8))) ax = fig.add_subplot(111, projection=\"3d\") ax.view_init(45, 20) ax.set_title(\"Space filling design\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\")  # plot feasible polytope ax.plot(xs=[0, 0, 1, 0], ys=[0, 1, 0, 0], zs=[1, 0, 0, 1], linewidth=2)  # plot design points ax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], s=40) Out[\u00a0]: <pre>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7cf598509510&gt;</pre>"},{"location":"optimality_criteria/#designs-for-different-optimality-criteria","title":"Designs for different optimality criteria\u00b6","text":""},{"location":"optimality_criteria/#space-filling-design","title":"Space filling design\u00b6","text":""},{"location":"reaction_optimization/","title":"Getting started by Example: Optimization of Reaction Conditions","text":"In\u00a0[\u00a0]: Copied! <pre># python imports we'll need in this notebook\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nprint(f\"SMOKE_TEST: {SMOKE_TEST}\")\n</pre> # python imports we'll need in this notebook import os import time  import numpy as np import pandas as pd   SMOKE_TEST = os.environ.get(\"SMOKE_TEST\") print(f\"SMOKE_TEST: {SMOKE_TEST}\") In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import (  # we won't need all of those.\n    CategoricalInput,\n    ContinuousInput,\n    ContinuousOutput,\n)\n</pre> from bofire.data_models.domain.api import Domain, Inputs, Outputs from bofire.data_models.features.api import (  # we won't need all of those.     CategoricalInput,     ContinuousInput,     ContinuousOutput, ) In\u00a0[\u00a0]: Copied! <pre># We wish the temperature of the reaction to be between 0 and 60 \u00b0C\ntemperature_feature = ContinuousInput(key=\"Temperature\", bounds=[0.0, 60.0], unit=\"\u00b0C\")\n\n# Solvent Amount\nsolvent_amount_feature = ContinuousInput(key=\"Solvent Volume\", bounds=[20, 90])\n\n# we have a couple of solvents in stock, which we'd like to use\nsolvent_type_feature = CategoricalInput(\n    key=\"Solvent Type\",\n    categories=[\"MeOH\", \"THF\", \"Dioxane\"],\n)\n\n\n# gather all individual features\ninput_features = Inputs(\n    features=[\n        temperature_feature,\n        solvent_type_feature,\n        solvent_amount_feature,\n    ],\n)\n</pre> # We wish the temperature of the reaction to be between 0 and 60 \u00b0C temperature_feature = ContinuousInput(key=\"Temperature\", bounds=[0.0, 60.0], unit=\"\u00b0C\")  # Solvent Amount solvent_amount_feature = ContinuousInput(key=\"Solvent Volume\", bounds=[20, 90])  # we have a couple of solvents in stock, which we'd like to use solvent_type_feature = CategoricalInput(     key=\"Solvent Type\",     categories=[\"MeOH\", \"THF\", \"Dioxane\"], )   # gather all individual features input_features = Inputs(     features=[         temperature_feature,         solvent_type_feature,         solvent_amount_feature,     ], ) In\u00a0[\u00a0]: Copied! <pre># outputs: we wish to maximize the Yield\n# import Maximize Objective to tell the optimizer you wish to optimize\nfrom bofire.data_models.objectives.api import MaximizeObjective\n\n\nobjective = MaximizeObjective(\n    w=1.0,\n)\nyield_feature = ContinuousOutput(key=\"Yield\", objective=objective)\n# create an output feature\noutput_features = Outputs(features=[yield_feature])\n</pre> # outputs: we wish to maximize the Yield # import Maximize Objective to tell the optimizer you wish to optimize from bofire.data_models.objectives.api import MaximizeObjective   objective = MaximizeObjective(     w=1.0, ) yield_feature = ContinuousOutput(key=\"Yield\", objective=objective) # create an output feature output_features = Outputs(features=[yield_feature]) In\u00a0[\u00a0]: Copied! <pre>objective\n</pre> objective In\u00a0[\u00a0]: Copied! <pre># we now have\nprint(\"input_features:\", input_features)\nprint(\"output_features:\", output_features)\n</pre> # we now have print(\"input_features:\", input_features) print(\"output_features:\", output_features) In\u00a0[\u00a0]: Copied! <pre># The domain is now the object that holds the entire optimization problem / problem definition.\ndomain = Domain(\n    inputs=input_features,\n    outputs=output_features,\n)\n</pre> # The domain is now the object that holds the entire optimization problem / problem definition. domain = Domain(     inputs=input_features,     outputs=output_features, ) In\u00a0[\u00a0]: Copied! <pre># you can now have a pretty printout of your domain via\n(domain.inputs + domain.outputs).get_reps_df()\n</pre> # you can now have a pretty printout of your domain via (domain.inputs + domain.outputs).get_reps_df() In\u00a0[\u00a0]: Copied! <pre># and you can access your domain features via\nfor (\n    feature_key\n) in domain.inputs.get_keys():  # this will get all the feature names and loop over them\n    input_feature = domain.inputs.get_by_key(\n        feature_key,\n    )  # we can extract the individual feature object by asking for it by name\n    print(feature_key, \"|\", input_feature)\n</pre> # and you can access your domain features via for (     feature_key ) in domain.inputs.get_keys():  # this will get all the feature names and loop over them     input_feature = domain.inputs.get_by_key(         feature_key,     )  # we can extract the individual feature object by asking for it by name     print(feature_key, \"|\", input_feature) In\u00a0[\u00a0]: Copied! <pre># as well as the output features as\n# and you can access your domain features via\nfor feature_key in (\n    domain.outputs.get_keys()\n):  # this will get all the feature names and loop over them\n    output_feature = domain.outputs.get_by_key(\n        feature_key,\n    )  # we can extract the individual feature object by asking for it by name\n    print(feature_key, \" | \", output_feature.__repr__())\n</pre> # as well as the output features as # and you can access your domain features via for feature_key in (     domain.outputs.get_keys() ):  # this will get all the feature names and loop over them     output_feature = domain.outputs.get_by_key(         feature_key,     )  # we can extract the individual feature object by asking for it by name     print(feature_key, \" | \", output_feature.__repr__()) In\u00a0[\u00a0]: Copied! <pre>(domain.inputs + domain.outputs).get_reps_df()\n</pre> (domain.inputs + domain.outputs).get_reps_df() In\u00a0[\u00a0]: Copied! <pre># Reaction Optimization Notebook util code\nT0 = 25\nT1 = 100\ne0 = np.exp((T1 + 0) / T0)\ne60 = np.exp((T1 + 60) / T0)\nde = e60 - e0\n\nboiling_points = {  # in \u00b0C\n    \"MeOH\": 64.7,\n    \"THF\": 66.0,\n    \"Dioxane\": 101.0,\n}\ndensity = {  # in kg/l\n    \"MeOH\": 0.792,\n    \"THF\": 0.886,\n    \"Dioxane\": 1.03,\n}\n# create dict from individual dicts\ndescs = {\n    \"boiling_points\": boiling_points,\n    \"density\": density,\n}\nsolvent_descriptors = pd.DataFrame(descs)\n\n\n# these functions are for faking real experimental data ;)\ndef calc_volume_fact(V):\n    # 20-90\n    # max at 75 = 1\n    # min at 20 = 0.7\n    # at 90=0.5\n    x = (V - 20) / 70\n    x = 0.5 + (x - 0.75) * 0.1 + (x - 0.4) ** 2\n    return x\n\n\ndef calc_rhofact(solvent_type, Tfact):\n    #  between 0.7 and 1.1\n    x = solvent_descriptors[\"density\"][solvent_type]\n    x = (1.5 - x) * (Tfact + 0.5) / 2\n    return x.values\n\n\ndef calc_Tfact(T):\n    x = np.exp((T1 + T) / T0)\n    return (x - e0) / de\n\n\n# this can be used to create a dataframe of experiments including yields\ndef create_experiments(domain, nsamples=100, A=25, B=90, candidates=None):\n    Tf = domain.inputs.get_by_key(\"Temperature\")\n    Vf = domain.inputs.get_by_key(\"Solvent Volume\")\n    typef = domain.inputs.get_by_key(\"Solvent Type\")\n    yf = domain.outputs.get_by_key(\"Yield\")\n    if candidates is None:\n        T = np.random.uniform(low=Tf.lower_bound, high=Tf.upper_bound, size=(nsamples,))\n        V = np.random.uniform(low=Vf.lower_bound, high=Vf.upper_bound, size=(nsamples,))\n        solvent_types = [\n            domain.inputs.get_by_key(\"Solvent Type\").categories[np.random.randint(0, 3)]\n            for i in range(nsamples)\n        ]\n    else:\n        nsamples = len(candidates)\n        T = candidates[\"Temperature\"].values\n        V = candidates[\"Solvent Volume\"].values\n        solvent_types = candidates[\"Solvent Type\"].values\n\n    Tfact = calc_Tfact(T)\n    rhofact = calc_rhofact(solvent_types, Tfact)\n    Vfact = calc_volume_fact(V)\n    y = A * Tfact + B * rhofact\n    y = 0.5 * y + 0.5 * y * Vfact\n    # y = y.values\n    samples = pd.DataFrame(\n        {\n            Tf.key: T,\n            Vf.key: V,\n            yf.key: y,\n            typef.key: solvent_types,\n            \"valid_\" + yf.key: np.ones(nsamples),\n        },\n        # index=pd.RangeIndex(nsamples),\n    )\n    samples.index = pd.RangeIndex(nsamples)\n    return samples\n\n\ndef create_candidates(domain, nsamples=4):\n    experiments = create_experiments(domain, nsamples=nsamples)\n    candidates = experiments.drop([\"Yield\", \"valid_Yield\"], axis=1)\n    return candidates\n\n\n# this is for evaluating candidates that do not yet have a yield attributed to it.\ndef evaluate_experiments(domain, candidates):\n    return create_experiments(domain, candidates=candidates)\n</pre> # Reaction Optimization Notebook util code T0 = 25 T1 = 100 e0 = np.exp((T1 + 0) / T0) e60 = np.exp((T1 + 60) / T0) de = e60 - e0  boiling_points = {  # in \u00b0C     \"MeOH\": 64.7,     \"THF\": 66.0,     \"Dioxane\": 101.0, } density = {  # in kg/l     \"MeOH\": 0.792,     \"THF\": 0.886,     \"Dioxane\": 1.03, } # create dict from individual dicts descs = {     \"boiling_points\": boiling_points,     \"density\": density, } solvent_descriptors = pd.DataFrame(descs)   # these functions are for faking real experimental data ;) def calc_volume_fact(V):     # 20-90     # max at 75 = 1     # min at 20 = 0.7     # at 90=0.5     x = (V - 20) / 70     x = 0.5 + (x - 0.75) * 0.1 + (x - 0.4) ** 2     return x   def calc_rhofact(solvent_type, Tfact):     #  between 0.7 and 1.1     x = solvent_descriptors[\"density\"][solvent_type]     x = (1.5 - x) * (Tfact + 0.5) / 2     return x.values   def calc_Tfact(T):     x = np.exp((T1 + T) / T0)     return (x - e0) / de   # this can be used to create a dataframe of experiments including yields def create_experiments(domain, nsamples=100, A=25, B=90, candidates=None):     Tf = domain.inputs.get_by_key(\"Temperature\")     Vf = domain.inputs.get_by_key(\"Solvent Volume\")     typef = domain.inputs.get_by_key(\"Solvent Type\")     yf = domain.outputs.get_by_key(\"Yield\")     if candidates is None:         T = np.random.uniform(low=Tf.lower_bound, high=Tf.upper_bound, size=(nsamples,))         V = np.random.uniform(low=Vf.lower_bound, high=Vf.upper_bound, size=(nsamples,))         solvent_types = [             domain.inputs.get_by_key(\"Solvent Type\").categories[np.random.randint(0, 3)]             for i in range(nsamples)         ]     else:         nsamples = len(candidates)         T = candidates[\"Temperature\"].values         V = candidates[\"Solvent Volume\"].values         solvent_types = candidates[\"Solvent Type\"].values      Tfact = calc_Tfact(T)     rhofact = calc_rhofact(solvent_types, Tfact)     Vfact = calc_volume_fact(V)     y = A * Tfact + B * rhofact     y = 0.5 * y + 0.5 * y * Vfact     # y = y.values     samples = pd.DataFrame(         {             Tf.key: T,             Vf.key: V,             yf.key: y,             typef.key: solvent_types,             \"valid_\" + yf.key: np.ones(nsamples),         },         # index=pd.RangeIndex(nsamples),     )     samples.index = pd.RangeIndex(nsamples)     return samples   def create_candidates(domain, nsamples=4):     experiments = create_experiments(domain, nsamples=nsamples)     candidates = experiments.drop([\"Yield\", \"valid_Yield\"], axis=1)     return candidates   # this is for evaluating candidates that do not yet have a yield attributed to it. def evaluate_experiments(domain, candidates):     return create_experiments(domain, candidates=candidates) In\u00a0[\u00a0]: Copied! <pre># create some trial experiments (at unitform random)\ncandidates = create_candidates(domain, nsamples=4)\n</pre> # create some trial experiments (at unitform random) candidates = create_candidates(domain, nsamples=4) In\u00a0[\u00a0]: Copied! <pre>candidates\n</pre> candidates In\u00a0[\u00a0]: Copied! <pre># we can evaluate the yield of those candidates\nexperiments = evaluate_experiments(domain, candidates)\n</pre> # we can evaluate the yield of those candidates experiments = evaluate_experiments(domain, candidates) In\u00a0[\u00a0]: Copied! <pre>experiments\n</pre> experiments In\u00a0[\u00a0]: Copied! <pre>import bofire.strategies.api as strategies\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.strategies.api import SoboStrategy\n</pre> import bofire.strategies.api as strategies from bofire.data_models.acquisition_functions.api import qLogEI from bofire.data_models.strategies.api import SoboStrategy In\u00a0[\u00a0]: Copied! <pre># a single objective BO strategy\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=domain,\n    acquisition_function=qLogEI(),\n)\n\n# map the strategy data model to the actual strategy that has functionality\nsobo_strategy = strategies.map(sobo_strategy_data_model)\n</pre> # a single objective BO strategy  sobo_strategy_data_model = SoboStrategy(     domain=domain,     acquisition_function=qLogEI(), )  # map the strategy data model to the actual strategy that has functionality sobo_strategy = strategies.map(sobo_strategy_data_model) In\u00a0[\u00a0]: Copied! <pre># first we fit the model of the strategy\n\nsobo_strategy.tell(experiments)\n</pre> # first we fit the model of the strategy  sobo_strategy.tell(experiments) <p>Each implemented strategy has a <code>strategy.ask(n)</code> method implemented, where new experiment candidates can be fetched from.</p> In\u00a0[\u00a0]: Copied! <pre># uncomment and run me to see what's happening!\n# sobo_strategy.ask(1)\n</pre> # uncomment and run me to see what's happening! # sobo_strategy.ask(1) <p>Since a BO strategy requires an underlying regression model for predictions, it requires a certain amount of initial experiments for it to be able to build such a model.</p> <p>In order to obtain initial experiments, one way is to (pseudo)randomly sample candidate points in the reaction domain. This can e.g. be done by the RandomStrategy.</p> In\u00a0[\u00a0]: Copied! <pre># a random strategy\nfrom bofire.data_models.strategies.api import RandomStrategy as RandomStrategyModel\n\n\nrandom_strategy_model = RandomStrategyModel(domain=domain)\n# we have to provide the strategy with our optimization problem so it knows where to sample from.\nrandom_strategy = strategies.map(random_strategy_model)\n</pre> # a random strategy from bofire.data_models.strategies.api import RandomStrategy as RandomStrategyModel   random_strategy_model = RandomStrategyModel(domain=domain) # we have to provide the strategy with our optimization problem so it knows where to sample from. random_strategy = strategies.map(random_strategy_model) In\u00a0[\u00a0]: Copied! <pre>domain\n</pre> domain In\u00a0[\u00a0]: Copied! <pre># let's ask for five random sets of conditions\ncandidates = random_strategy.ask(5)\n</pre> # let's ask for five random sets of conditions candidates = random_strategy.ask(5) <p>you can have a look at the candidates</p> In\u00a0[\u00a0]: Copied! <pre>candidates\n</pre> candidates <p>In order to use those experiments as data foundation of the bo strategy above, the output values of these candidates have to be provided. Herein we'll use a dummy function to evaluate some more or less realistic yields given the proposed input candidates as.</p> In\u00a0[\u00a0]: Copied! <pre>experiments = evaluate_experiments(domain, candidates)\n</pre> experiments = evaluate_experiments(domain, candidates) In\u00a0[\u00a0]: Copied! <pre>experiments\n</pre> experiments <p>note, that the columns <code>Yield</code> and <code>valid_Yield</code> have been added. <code>Yield</code> contains the actual output, whereas <code>valid_Yield</code> labels the experiment as valid w.r.t. this respective measured output.</p> <p>This info can now be given to the bo strategy so it can use it to fit the underlying regression model it utilizes via the <code>strategy.tell()</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nsobo_strategy.tell(experiments, replace=True, retrain=True)\nprint(f\"fit took {(time.time()-t1):.2f} seconds\")\n</pre> t1 = time.time() sobo_strategy.tell(experiments, replace=True, retrain=True) print(f\"fit took {(time.time()-t1):.2f} seconds\") <p>Using this data we can now get a proposal for a next point to evaluate via the <code>sobo_strategy.ask(1)</code> method.</p> In\u00a0[\u00a0]: Copied! <pre>t1 = time.time()\nnew_candidate = sobo_strategy.ask(1)\nprint(f\"SOBO step took {(time.time()-t1):.2f} seconds\")\n</pre> t1 = time.time() new_candidate = sobo_strategy.ask(1) print(f\"SOBO step took {(time.time()-t1):.2f} seconds\") <p>This ask call now takes way longer, since first a GP model is fitted to the data, and the acquisition function EI is optimized to obtain the new proposed candidates. Note that the predictied yield and standard deviation, as well as desirability function value (the underlying value the optimizer sees) are provided in the new_candidate dataframe.</p> In\u00a0[\u00a0]: Copied! <pre>new_candidate\n</pre> new_candidate In\u00a0[\u00a0]: Copied! <pre>experimental_budget = 10\ni = 0\n# in case of smoke_test we don't run the actual optimization loop ...\ndone = False if not SMOKE_TEST else True\n\nwhile not done:\n    i += 1\n    t1 = time.time()\n    # ask for a new experiment\n    new_candidate = sobo_strategy.ask(1)\n    new_experiment = evaluate_experiments(domain, new_candidate)\n    sobo_strategy.tell(new_experiment)\n    print(f\"Iteration took {(time.time()-t1):.2f} seconds\")\n    # inform the strategy about the new experiment\n    # experiments = pd.concat([experiments,new_experiment],ignore_index=True)\n    if i &gt; experimental_budget:\n        done = True\n</pre> experimental_budget = 10 i = 0 # in case of smoke_test we don't run the actual optimization loop ... done = False if not SMOKE_TEST else True  while not done:     i += 1     t1 = time.time()     # ask for a new experiment     new_candidate = sobo_strategy.ask(1)     new_experiment = evaluate_experiments(domain, new_candidate)     sobo_strategy.tell(new_experiment)     print(f\"Iteration took {(time.time()-t1):.2f} seconds\")     # inform the strategy about the new experiment     # experiments = pd.concat([experiments,new_experiment],ignore_index=True)     if i &gt; experimental_budget:         done = True In\u00a0[\u00a0]: Copied! <pre># you have access to the experiments here\nsobo_strategy.experiments\n</pre> # you have access to the experiments here sobo_strategy.experiments In\u00a0[\u00a0]: Copied! <pre># quick plot of yield vs. Iteration\nsobo_strategy.experiments[\"Yield\"].plot()\n</pre> # quick plot of yield vs. Iteration sobo_strategy.experiments[\"Yield\"].plot()"},{"location":"reaction_optimization/#getting-started-by-example-optimization-of-reaction-conditions","title":"Getting started by Example: Optimization of Reaction Conditions\u00b6","text":"<p>In this example we take on a reaction condition optimization problem: Suppose you have some simple reaction where two ingredients $A$ and $B$ react to $C$ as</p> <p>$$ A + B  \\rightarrow C $$</p> <p>Our reactors can be temperature controlled, and we can use different solvents. Furthermore, we can dilute our reaction mixture by using a different solvent volume. parameters like the temperature or the solvent volume are continuous parameters, where we have to set our ranges $$ 0^{\\, \\circ} \\text{C}  \\, \\le T \\le \\,  60^{\\, \\circ} \\text{C}  $$</p> <p>$$ 20 \\, \\text{ml} \\le V_{solvent} \\le 90 \\,\\text{ml}  $$</p> <p>Parameters like the use of which solvent, where there's a choice of either this or that, are categorical parameters $$ \\text{Solvent} \\, S \\,  \\in  \\, \\{ \\text{MeOH, THF, Dioxane} \\} $$</p> <p>For now we only wish top optimize the Reaction yield, making this a single objective optimization problem.</p> <p>$$ \\max _{T,V,S} \\text{ReactionYield}(T,V,S) $$</p> <p>Below we'll see how to perform such an optimization using bofire, Utilizing a Single Objective Bayesian Optimization (SOBO) Strategy</p>"},{"location":"reaction_optimization/#setting-up-the-optimization-problem-as-a-reaction-domain","title":"Setting up the optimization problem as a Reaction Domain\u00b6","text":""},{"location":"reaction_optimization/#import-a-toy-reaction-to-play-around-with","title":"Import a toy Reaction to play around with\u00b6","text":"<p>We've prepared a reaction emulator, which you can use to emulate a real experiment below.</p>"},{"location":"reaction_optimization/#strategy-setup","title":"Strategy Setup\u00b6","text":"<p>a BO Strategy requires a choice of an acquisition function in order to evaluate the quality of new trial candidates.</p> <p>In this example we'll use the popular Expected Improvement (EI) acqf, which can evaluate the expectation value for obtaining a better function value compared to the current best value by utilizing the regression models' prediction of botht the function value as well as the variance at that point.</p>"},{"location":"reaction_optimization/#optimization-loop","title":"Optimization Loop\u00b6","text":"<p>With this <code>strategy.ask()</code> and <code>strategy.tell()</code> we can now do our optimization loop, where after each new proposal, the conditions obtained from <code>ask</code> are evaluated and added to the known datapoints via <code>tell</code>. This requires to refit the underling model in each step.</p>"},{"location":"reaction_optimization/#investigating-results","title":"investigating results\u00b6","text":""},{"location":"reaction_optimization/#exercises","title":"Exercises:\u00b6","text":"<ul> <li>further analyze of the experiments.<ul> <li>plot of Iteration vs. Y and best observed Y</li> <li>evaluation of final model w.r.t. Optimal conditions</li> <li>plot of final models' mean and variance as a function of the input parameters, perhaps different plots for each solvent, or in 3d as slices</li> </ul> </li> </ul>"},{"location":"ref-constraints/","title":"Constraints","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.constraint","title":"<code>constraint</code>","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.Constraint","title":"<code>Constraint</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Abstract base class to define constraints on the optimization space.</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>class Constraint(BaseModel):\n    \"\"\"Abstract base class to define constraints on the optimization space.\"\"\"\n\n    type: str\n\n    @abstractmethod\n    def is_fulfilled(\n        self,\n        experiments: pd.DataFrame,\n        tol: Optional[float] = 1e-6,\n    ) -&gt; pd.Series:\n        \"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to check constraint fulfillment.\n            tol (float, optional): tolerance parameter. A constraint is considered as not fulfilled if\n                the violation is larger than tol. Defaults to 0.\n\n        Returns:\n            bool: True if fulfilled else False\n\n        \"\"\"\n\n    @abstractmethod\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Numerically evaluates the constraint.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n        Returns:\n            pd.Series: Distance to reach constraint fulfillment.\n\n        \"\"\"\n\n    @abstractmethod\n    def jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Numerically evaluates the jacobian of the constraint\n        Args:\n            experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n        Returns:\n            pd.DataFrame: the i-th row contains the jacobian evaluated at the i-th experiment\n\n        \"\"\"\n\n    @abstractmethod\n    def validate_inputs(self, inputs: Inputs):\n        \"\"\"Validates that the features stored in Inputs are compatible with the constraint.\n\n        Args:\n            inputs (Inputs): Inputs to validate.\n\n        \"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.Constraint.__call__","title":"<code>__call__(experiments)</code>  <code>abstractmethod</code>","text":"<p>Numerically evaluates the constraint.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe to evaluate the constraint on.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Distance to reach constraint fulfillment.</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>@abstractmethod\ndef __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Numerically evaluates the constraint.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n    Returns:\n        pd.Series: Distance to reach constraint fulfillment.\n\n    \"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.Constraint.is_fulfilled","title":"<code>is_fulfilled(experiments, tol=1e-06)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe to check constraint fulfillment.</p> required <code>tol</code> <code>float</code> <p>tolerance parameter. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 0.</p> <code>1e-06</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>Series</code> <p>True if fulfilled else False</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>@abstractmethod\ndef is_fulfilled(\n    self,\n    experiments: pd.DataFrame,\n    tol: Optional[float] = 1e-6,\n) -&gt; pd.Series:\n    \"\"\"Abstract method to check if a constraint is fulfilled for all the rows of the provided dataframe.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to check constraint fulfillment.\n        tol (float, optional): tolerance parameter. A constraint is considered as not fulfilled if\n            the violation is larger than tol. Defaults to 0.\n\n    Returns:\n        bool: True if fulfilled else False\n\n    \"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.Constraint.jacobian","title":"<code>jacobian(experiments)</code>  <code>abstractmethod</code>","text":"<p>Numerically evaluates the jacobian of the constraint Args:     experiments (pd.DataFrame): Dataframe to evaluate the constraint on.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: the i-th row contains the jacobian evaluated at the i-th experiment</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>@abstractmethod\ndef jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Numerically evaluates the jacobian of the constraint\n    Args:\n        experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n    Returns:\n        pd.DataFrame: the i-th row contains the jacobian evaluated at the i-th experiment\n\n    \"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.Constraint.validate_inputs","title":"<code>validate_inputs(inputs)</code>  <code>abstractmethod</code>","text":"<p>Validates that the features stored in Inputs are compatible with the constraint.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Inputs</code> <p>Inputs to validate.</p> required Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>@abstractmethod\ndef validate_inputs(self, inputs: Inputs):\n    \"\"\"Validates that the features stored in Inputs are compatible with the constraint.\n\n    Args:\n        inputs (Inputs): Inputs to validate.\n\n    \"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.ConstraintError","title":"<code>ConstraintError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base Error for Constraints</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>class ConstraintError(Exception):\n    \"\"\"Base Error for Constraints\"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.ConstraintNotFulfilledError","title":"<code>ConstraintNotFulfilledError</code>","text":"<p>               Bases: <code>ConstraintError</code></p> <p>Raised when an constraint is not fulfilled.</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>class ConstraintNotFulfilledError(ConstraintError):\n    \"\"\"Raised when an constraint is not fulfilled.\"\"\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.constraint.IntrapointConstraint","title":"<code>IntrapointConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>An intrapoint constraint describes required relationships within a candidate when asking a strategy to return one or more candidates.</p> Source code in <code>bofire/data_models/constraints/constraint.py</code> <pre><code>class IntrapointConstraint(Constraint):\n    \"\"\"An intrapoint constraint describes required relationships within a candidate\n    when asking a strategy to return one or more candidates.\n    \"\"\"\n\n    type: str\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.interpoint","title":"<code>interpoint</code>","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.interpoint.InterpointConstraint","title":"<code>InterpointConstraint</code>","text":"<p>               Bases: <code>Constraint</code></p> <p>An interpoint constraint describes required relationships between individual candidates when asking a strategy for returning more than one candidate.</p> Source code in <code>bofire/data_models/constraints/interpoint.py</code> <pre><code>class InterpointConstraint(Constraint):\n    \"\"\"An interpoint constraint describes required relationships between individual\n    candidates when asking a strategy for returning more than one candidate.\n    \"\"\"\n\n    type: str\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.interpoint.InterpointEqualityConstraint","title":"<code>InterpointEqualityConstraint</code>","text":"<p>               Bases: <code>InterpointConstraint</code></p> <p>Constraint that forces that values of a certain feature of a set/batch of candidates should have the same value. Currently this is only implemented for ContinuousInput features.</p> <p>Attributes:</p> Name Type Description <code>feature(str)</code> <p>The constrained feature.</p> <code>multiplicity(int)</code> <p>The multiplicity of the constraint, stating how many values of the feature in the batch should have always the same value.</p> Source code in <code>bofire/data_models/constraints/interpoint.py</code> <pre><code>class InterpointEqualityConstraint(InterpointConstraint):\n    \"\"\"Constraint that forces that values of a certain feature of a set/batch of\n    candidates should have the same value. Currently this is only implemented for ContinuousInput features.\n\n    Attributes:\n        feature(str): The constrained feature.\n        multiplicity(int): The multiplicity of the constraint, stating how many\n            values of the feature in the batch should have always the same value.\n\n    \"\"\"\n\n    type: Literal[\"InterpointEqualityConstraint\"] = \"InterpointEqualityConstraint\"\n    feature: str\n    multiplicity: Optional[Annotated[int, Field(ge=2)]] = None\n\n    def validate_inputs(self, inputs: Inputs):\n        if self.feature not in inputs.get_keys(ContinuousInput):\n            raise ValueError(\n                f\"Feature {self.feature} is not a continuous input feature in the provided Inputs object.\",\n            )\n\n    def is_fulfilled(\n        self,\n        experiments: pd.DataFrame,\n        tol: Optional[float] = 1e-6,\n    ) -&gt; pd.Series:\n        multiplicity = self.multiplicity or len(experiments)\n        for i in range(math.ceil(len(experiments) / multiplicity)):\n            batch = experiments[self.feature].values[\n                i * multiplicity : min((i + 1) * multiplicity, len(experiments))\n            ]\n            if not np.allclose(batch, batch[0]):\n                return pd.Series([False])\n        return pd.Series([True])\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Numerically evaluates the constraint. Returns the distance to the constraint fulfillment\n        for each batch of size batch_size.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n        Returns:\n            pd.Series: Distance to reach constraint fulfillment.\n\n        \"\"\"\n        multiplicity = self.multiplicity or len(experiments)\n        n_batches = int(np.ceil(experiments.shape[0] / multiplicity))\n        feature_values = np.zeros(n_batches * multiplicity)\n        feature_values[: experiments.shape[0]] = experiments[self.feature].values\n        feature_values[experiments.shape[0] :] = feature_values[-multiplicity]\n        feature_values = feature_values.reshape(n_batches, multiplicity).T\n\n        batchwise_constraint_matrix = np.zeros(shape=(multiplicity - 1, multiplicity))\n        batchwise_constraint_matrix[:, 0] = 1.0\n        batchwise_constraint_matrix[:, 1:] = -np.eye(multiplicity - 1)\n\n        return pd.Series(\n            np.linalg.norm(batchwise_constraint_matrix @ feature_values, axis=0, ord=2)\n            ** 2,\n            index=[f\"batch_{i}\" for i in range(n_batches)],\n        )\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        raise NotImplementedError(\"Method `jacobian` currently not implemented.\")\n\n    def hessian(self, experiments: pd.DataFrame) -&gt; Dict[Union[str, int], pd.DataFrame]:\n        raise NotImplementedError(\"Method `hessian` currently not implemented.\")\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.interpoint.InterpointEqualityConstraint.__call__","title":"<code>__call__(experiments)</code>","text":"<p>Numerically evaluates the constraint. Returns the distance to the constraint fulfillment for each batch of size batch_size.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe to evaluate the constraint on.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Distance to reach constraint fulfillment.</p> Source code in <code>bofire/data_models/constraints/interpoint.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Numerically evaluates the constraint. Returns the distance to the constraint fulfillment\n    for each batch of size batch_size.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to evaluate the constraint on.\n\n    Returns:\n        pd.Series: Distance to reach constraint fulfillment.\n\n    \"\"\"\n    multiplicity = self.multiplicity or len(experiments)\n    n_batches = int(np.ceil(experiments.shape[0] / multiplicity))\n    feature_values = np.zeros(n_batches * multiplicity)\n    feature_values[: experiments.shape[0]] = experiments[self.feature].values\n    feature_values[experiments.shape[0] :] = feature_values[-multiplicity]\n    feature_values = feature_values.reshape(n_batches, multiplicity).T\n\n    batchwise_constraint_matrix = np.zeros(shape=(multiplicity - 1, multiplicity))\n    batchwise_constraint_matrix[:, 0] = 1.0\n    batchwise_constraint_matrix[:, 1:] = -np.eye(multiplicity - 1)\n\n    return pd.Series(\n        np.linalg.norm(batchwise_constraint_matrix @ feature_values, axis=0, ord=2)\n        ** 2,\n        index=[f\"batch_{i}\" for i in range(n_batches)],\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear","title":"<code>linear</code>","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearConstraint","title":"<code>LinearConstraint</code>","text":"<p>               Bases: <code>IntrapointConstraint</code></p> <p>Abstract base class for linear equality and inequality constraints.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> <code>coefficients</code> <code>list</code> <p>list of coefficients (float) of the constraint.</p> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint</p> Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>class LinearConstraint(IntrapointConstraint):\n    \"\"\"Abstract base class for linear equality and inequality constraints.\n\n    Attributes:\n        features (list): list of feature keys (str) on which the constraint works on.\n        coefficients (list): list of coefficients (float) of the constraint.\n        rhs (float): Right-hand side of the constraint\n\n    \"\"\"\n\n    type: Literal[\"LinearConstraint\"] = \"LinearConstraint\"\n\n    features: FeatureKeys\n    coefficients: Annotated[List[float], Field(min_length=2)]\n    rhs: float\n\n    @model_validator(mode=\"after\")\n    def validate_list_lengths(self):\n        \"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\"\n        if len(self.features) != len(self.coefficients):\n            raise ValueError(\n                f\"must provide same number of features and coefficients, got {len(self.features)} != {len(self.coefficients)}\",\n            )\n        return self\n\n    def validate_inputs(self, inputs: Inputs):\n        keys = inputs.get_keys([ContinuousInput, DiscreteInput])\n        for f in self.features:\n            if f not in keys:\n                raise ValueError(\n                    f\"Feature {f} is not a continuous input feature in the provided Inputs object.\",\n                )\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        return (\n            experiments[self.features] @ self.coefficients - self.rhs\n        ) / np.linalg.norm(np.array(self.coefficients))\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        return pd.DataFrame(\n            np.tile(\n                [\n                    np.array(self.coefficients)\n                    / np.linalg.norm(np.array(self.coefficients)),\n                ],\n                [experiments.shape[0], 1],\n            ),\n            columns=[f\"dg/d{name}\" for name in self.features],\n        )\n\n    def hessian(self, experiments: pd.DataFrame) -&gt; Dict[Union[int, str], float]:\n        return {idx: 0.0 for idx in range(experiments.shape[0])}\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearConstraint.validate_list_lengths","title":"<code>validate_list_lengths()</code>","text":"<p>Validate that length of the feature and coefficient lists have the same length.</p> Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_list_lengths(self):\n    \"\"\"Validate that length of the feature and coefficient lists have the same length.\"\"\"\n    if len(self.features) != len(self.coefficients):\n        raise ValueError(\n            f\"must provide same number of features and coefficients, got {len(self.features)} != {len(self.coefficients)}\",\n        )\n    return self\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearEqualityConstraint","title":"<code>LinearEqualityConstraint</code>","text":"<p>               Bases: <code>LinearConstraint</code>, <code>EqualityConstraint</code></p> <p>Linear equality constraint of the form <code>coefficients * x = rhs</code>.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> <code>coefficients</code> <code>list</code> <p>list of coefficients (float) of the constraint.</p> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint</p> Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>class LinearEqualityConstraint(LinearConstraint, EqualityConstraint):\n    \"\"\"Linear equality constraint of the form `coefficients * x = rhs`.\n\n    Attributes:\n        features (list): list of feature keys (str) on which the constraint works on.\n        coefficients (list): list of coefficients (float) of the constraint.\n        rhs (float): Right-hand side of the constraint\n\n    \"\"\"\n\n    type: Literal[\"LinearEqualityConstraint\"] = \"LinearEqualityConstraint\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearInequalityConstraint","title":"<code>LinearInequalityConstraint</code>","text":"<p>               Bases: <code>LinearConstraint</code>, <code>InequalityConstraint</code></p> <p>Linear inequality constraint of the form <code>coefficients * x &lt;= rhs</code>.</p> <p>To instantiate a constraint of the form <code>coefficients * x &gt;= rhs</code> multiply coefficients and rhs by -1, or use the classmethod <code>from_greater_equal</code>.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> <code>coefficients</code> <code>list</code> <p>list of coefficients (float) of the constraint.</p> <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint</p> Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>class LinearInequalityConstraint(LinearConstraint, InequalityConstraint):\n    \"\"\"Linear inequality constraint of the form `coefficients * x &lt;= rhs`.\n\n    To instantiate a constraint of the form `coefficients * x &gt;= rhs` multiply coefficients and rhs by -1, or\n    use the classmethod `from_greater_equal`.\n\n    Attributes:\n        features (list): list of feature keys (str) on which the constraint works on.\n        coefficients (list): list of coefficients (float) of the constraint.\n        rhs (float): Right-hand side of the constraint\n\n    \"\"\"\n\n    type: Literal[\"LinearInequalityConstraint\"] = \"LinearInequalityConstraint\"\n\n    def as_smaller_equal(self) -&gt; Tuple[List[str], List[float], float]:\n        \"\"\"Return attributes in the smaller equal convention\n\n        Returns:\n            Tuple[List[str], List[float], float]: features, coefficients, rhs\n\n        \"\"\"\n        return self.features, self.coefficients, self.rhs\n\n    def as_greater_equal(self) -&gt; Tuple[List[str], List[float], float]:\n        \"\"\"Return attributes in the greater equal convention\n\n        Returns:\n            Tuple[List[str], List[float], float]: features, coefficients, rhs\n\n        \"\"\"\n        return self.features, [-1.0 * c for c in self.coefficients], -1.0 * self.rhs\n\n    @classmethod\n    def from_greater_equal(\n        cls,\n        features: List[str],\n        coefficients: List[float],\n        rhs: float,\n    ):\n        \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &gt;= rhs`.\n\n        Args:\n            features (List[str]): List of feature keys.\n            coefficients (List[float]): List of coefficients.\n            rhs (float): Right-hand side of the constraint.\n\n        \"\"\"\n        return cls(\n            features=features,\n            coefficients=[-1.0 * c for c in coefficients],\n            rhs=-1.0 * rhs,\n        )\n\n    @classmethod\n    def from_smaller_equal(\n        cls,\n        features: List[str],\n        coefficients: List[float],\n        rhs: float,\n    ):\n        \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &lt;= rhs`.\n\n        Args:\n            features (List[str]): List of feature keys.\n            coefficients (List[float]): List of coefficients.\n            rhs (float): Right-hand side of the constraint.\n\n        \"\"\"\n        return cls(\n            features=features,\n            coefficients=coefficients,\n            rhs=rhs,\n        )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearInequalityConstraint.as_greater_equal","title":"<code>as_greater_equal()</code>","text":"<p>Return attributes in the greater equal convention</p> <p>Returns:</p> Type Description <code>Tuple[List[str], List[float], float]</code> <p>Tuple[List[str], List[float], float]: features, coefficients, rhs</p> Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>def as_greater_equal(self) -&gt; Tuple[List[str], List[float], float]:\n    \"\"\"Return attributes in the greater equal convention\n\n    Returns:\n        Tuple[List[str], List[float], float]: features, coefficients, rhs\n\n    \"\"\"\n    return self.features, [-1.0 * c for c in self.coefficients], -1.0 * self.rhs\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearInequalityConstraint.as_smaller_equal","title":"<code>as_smaller_equal()</code>","text":"<p>Return attributes in the smaller equal convention</p> <p>Returns:</p> Type Description <code>Tuple[List[str], List[float], float]</code> <p>Tuple[List[str], List[float], float]: features, coefficients, rhs</p> Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>def as_smaller_equal(self) -&gt; Tuple[List[str], List[float], float]:\n    \"\"\"Return attributes in the smaller equal convention\n\n    Returns:\n        Tuple[List[str], List[float], float]: features, coefficients, rhs\n\n    \"\"\"\n    return self.features, self.coefficients, self.rhs\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearInequalityConstraint.from_greater_equal","title":"<code>from_greater_equal(features, coefficients, rhs)</code>  <code>classmethod</code>","text":"<p>Class method to construct linear inequality constraint of the form <code>coefficients * x &gt;= rhs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>List of feature keys.</p> required <code>coefficients</code> <code>List[float]</code> <p>List of coefficients.</p> required <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> required Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>@classmethod\ndef from_greater_equal(\n    cls,\n    features: List[str],\n    coefficients: List[float],\n    rhs: float,\n):\n    \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &gt;= rhs`.\n\n    Args:\n        features (List[str]): List of feature keys.\n        coefficients (List[float]): List of coefficients.\n        rhs (float): Right-hand side of the constraint.\n\n    \"\"\"\n    return cls(\n        features=features,\n        coefficients=[-1.0 * c for c in coefficients],\n        rhs=-1.0 * rhs,\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.linear.LinearInequalityConstraint.from_smaller_equal","title":"<code>from_smaller_equal(features, coefficients, rhs)</code>  <code>classmethod</code>","text":"<p>Class method to construct linear inequality constraint of the form <code>coefficients * x &lt;= rhs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>List[str]</code> <p>List of feature keys.</p> required <code>coefficients</code> <code>List[float]</code> <p>List of coefficients.</p> required <code>rhs</code> <code>float</code> <p>Right-hand side of the constraint.</p> required Source code in <code>bofire/data_models/constraints/linear.py</code> <pre><code>@classmethod\ndef from_smaller_equal(\n    cls,\n    features: List[str],\n    coefficients: List[float],\n    rhs: float,\n):\n    \"\"\"Class method to construct linear inequality constraint of the form `coefficients * x &lt;= rhs`.\n\n    Args:\n        features (List[str]): List of feature keys.\n        coefficients (List[float]): List of coefficients.\n        rhs (float): Right-hand side of the constraint.\n\n    \"\"\"\n    return cls(\n        features=features,\n        coefficients=coefficients,\n        rhs=rhs,\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nchoosek","title":"<code>nchoosek</code>","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.nchoosek.NChooseKConstraint","title":"<code>NChooseKConstraint</code>","text":"<p>               Bases: <code>IntrapointConstraint</code></p> <p>NChooseK constraint that defines how many ingredients are allowed in a formulation.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List[str]</code> <p>List of feature keys to which the constraint applies.</p> <code>min_count</code> <code>int</code> <p>Minimal number of non-zero/active feature values.</p> <code>max_count</code> <code>int</code> <p>Maximum number of non-zero/active feature values.</p> <code>none_also_valid</code> <code>bool</code> <p>In case that min_count &gt; 0, this flag decides if zero active features are also allowed.</p> Source code in <code>bofire/data_models/constraints/nchoosek.py</code> <pre><code>class NChooseKConstraint(IntrapointConstraint):\n    \"\"\"NChooseK constraint that defines how many ingredients are allowed in a formulation.\n\n    Attributes:\n        features (List[str]): List of feature keys to which the constraint applies.\n        min_count (int): Minimal number of non-zero/active feature values.\n        max_count (int): Maximum number of non-zero/active feature values.\n        none_also_valid (bool): In case that min_count &gt; 0,\n            this flag decides if zero active features are also allowed.\n\n    \"\"\"\n\n    type: Literal[\"NChooseKConstraint\"] = \"NChooseKConstraint\"\n    features: FeatureKeys\n    min_count: int\n    max_count: int\n    none_also_valid: bool\n\n    def validate_inputs(self, inputs: Inputs):\n        keys = inputs.get_keys([ContinuousInput, DiscreteInput])\n        for f in self.features:\n            if f not in keys:\n                raise ValueError(\n                    f\"Feature {f} is not a continuous input feature in the provided Inputs object.\",\n                )\n            feature_ = inputs.get_by_key(f)\n            assert isinstance(\n                feature_, ContinuousInput\n            ), f\"Feature {f} is not a ContinuousInput.\"\n            if feature_.bounds[0] &lt; 0:\n                raise ValueError(\n                    f\"Feature {f} must have a lower bound of &gt;=0, but has {feature_.bounds[0]}\",\n                )\n\n    @model_validator(mode=\"after\")\n    def validate_counts(self):\n        \"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\"\n        if self.min_count &gt; len(self.features):\n            raise ValueError(\"min_count must be &lt;= # of features\")\n        if self.max_count &gt; len(self.features):\n            raise ValueError(\"max_count must be &lt;= # of features\")\n        if self.min_count &gt; self.max_count:\n            raise ValueError(\"min_values must be &lt;= max_values\")\n\n        return self\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Smooth relaxation of NChooseK constraint by countig the number of zeros in a candidate by a sum of\n        narrow gaussians centered at zero.\n\n        Args:\n            experiments (pd.DataFrame): Data to evaluate the constraint on.\n\n        Returns:\n            pd.Series containing the constraint violation for each experiment (row in experiments argument).\n\n        \"\"\"\n\n        def relu(x):\n            return np.maximum(0, x)\n\n        indices = np.array(\n            [i for i, name in enumerate(experiments.columns) if name in self.features],\n            dtype=np.int64,\n        )\n        experiments_tensor = np.array(experiments.to_numpy())\n\n        max_count_violation = np.zeros(experiments_tensor.shape[0])\n        min_count_violation = np.zeros(experiments_tensor.shape[0])\n\n        if self.max_count != len(self.features):\n            max_count_violation = relu(\n                -1 * narrow_gaussian(x=experiments_tensor[..., indices]).sum(axis=-1)\n                + (len(self.features) - self.max_count),\n            )\n\n        if self.min_count &gt; 0:\n            min_count_violation = relu(\n                narrow_gaussian(x=experiments_tensor[..., indices]).sum(axis=-1)\n                - (len(self.features) - self.min_count),\n            )\n\n        return pd.Series(max_count_violation + min_count_violation)\n\n    def is_fulfilled(self, experiments: pd.DataFrame, tol: float = 1e-6) -&gt; pd.Series:\n        \"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to evaluate constraint on.\n            tol (float,optional): tolerance parameter. A constraint is considered as not fulfilled\n                if the violation is larger than tol. Defaults to 1e-6.\n\n        Returns:\n            bool: True if fulfilled else False.\n\n        \"\"\"\n        cols = self.features\n        sums = (np.abs(experiments[cols]) &gt; tol).sum(axis=1)\n\n        lower = sums &gt;= self.min_count\n        upper = sums &lt;= self.max_count\n\n        if not self.none_also_valid:\n            # return lower.all() and upper.all()\n            return pd.Series(np.logical_and(lower, upper), index=experiments.index)\n        none = sums == 0\n        return pd.Series(\n            np.logical_or(none, np.logical_and(lower, upper)),\n            index=experiments.index,\n        )\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        raise NotImplementedError(\"Jacobian not implemented for NChooseK constraints.\")\n\n    def hessian(self, experiments: pd.DataFrame) -&gt; Dict[Union[str, int], pd.DataFrame]:\n        raise NotImplementedError(\"Hessian not implemented for NChooseK constraints.\")\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nchoosek.NChooseKConstraint.__call__","title":"<code>__call__(experiments)</code>","text":"<p>Smooth relaxation of NChooseK constraint by countig the number of zeros in a candidate by a sum of narrow gaussians centered at zero.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Data to evaluate the constraint on.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series containing the constraint violation for each experiment (row in experiments argument).</p> Source code in <code>bofire/data_models/constraints/nchoosek.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Smooth relaxation of NChooseK constraint by countig the number of zeros in a candidate by a sum of\n    narrow gaussians centered at zero.\n\n    Args:\n        experiments (pd.DataFrame): Data to evaluate the constraint on.\n\n    Returns:\n        pd.Series containing the constraint violation for each experiment (row in experiments argument).\n\n    \"\"\"\n\n    def relu(x):\n        return np.maximum(0, x)\n\n    indices = np.array(\n        [i for i, name in enumerate(experiments.columns) if name in self.features],\n        dtype=np.int64,\n    )\n    experiments_tensor = np.array(experiments.to_numpy())\n\n    max_count_violation = np.zeros(experiments_tensor.shape[0])\n    min_count_violation = np.zeros(experiments_tensor.shape[0])\n\n    if self.max_count != len(self.features):\n        max_count_violation = relu(\n            -1 * narrow_gaussian(x=experiments_tensor[..., indices]).sum(axis=-1)\n            + (len(self.features) - self.max_count),\n        )\n\n    if self.min_count &gt; 0:\n        min_count_violation = relu(\n            narrow_gaussian(x=experiments_tensor[..., indices]).sum(axis=-1)\n            - (len(self.features) - self.min_count),\n        )\n\n    return pd.Series(max_count_violation + min_count_violation)\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nchoosek.NChooseKConstraint.is_fulfilled","title":"<code>is_fulfilled(experiments, tol=1e-06)</code>","text":"<p>Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe to evaluate constraint on.</p> required <code>tol</code> <code>(float, optional)</code> <p>tolerance parameter. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 1e-6.</p> <code>1e-06</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>Series</code> <p>True if fulfilled else False.</p> Source code in <code>bofire/data_models/constraints/nchoosek.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame, tol: float = 1e-6) -&gt; pd.Series:\n    \"\"\"Check if the concurrency constraint is fulfilled for all the rows of the provided dataframe.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to evaluate constraint on.\n        tol (float,optional): tolerance parameter. A constraint is considered as not fulfilled\n            if the violation is larger than tol. Defaults to 1e-6.\n\n    Returns:\n        bool: True if fulfilled else False.\n\n    \"\"\"\n    cols = self.features\n    sums = (np.abs(experiments[cols]) &gt; tol).sum(axis=1)\n\n    lower = sums &gt;= self.min_count\n    upper = sums &lt;= self.max_count\n\n    if not self.none_also_valid:\n        # return lower.all() and upper.all()\n        return pd.Series(np.logical_and(lower, upper), index=experiments.index)\n    none = sums == 0\n    return pd.Series(\n        np.logical_or(none, np.logical_and(lower, upper)),\n        index=experiments.index,\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nchoosek.NChooseKConstraint.validate_counts","title":"<code>validate_counts()</code>","text":"<p>Validates if the minimum and maximum of allowed features are smaller than the overall number of features.</p> Source code in <code>bofire/data_models/constraints/nchoosek.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_counts(self):\n    \"\"\"Validates if the minimum and maximum of allowed features are smaller than the overall number of features.\"\"\"\n    if self.min_count &gt; len(self.features):\n        raise ValueError(\"min_count must be &lt;= # of features\")\n    if self.max_count &gt; len(self.features):\n        raise ValueError(\"max_count must be &lt;= # of features\")\n    if self.min_count &gt; self.max_count:\n        raise ValueError(\"min_values must be &lt;= max_values\")\n\n    return self\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nonlinear","title":"<code>nonlinear</code>","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.nonlinear.NonlinearConstraint","title":"<code>NonlinearConstraint</code>","text":"<p>               Bases: <code>IntrapointConstraint</code></p> <p>Base class for nonlinear equality and inequality constraints.</p> <p>Attributes:</p> Name Type Description <code>expression</code> <code>str</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> <code>jacobian_expression</code> <code>str</code> <p>Mathematical expression that that can be evaluated by <code>pandas.eval</code>.</p> <code>features</code> <code>list</code> <p>list of feature keys (str) on which the constraint works on.</p> Source code in <code>bofire/data_models/constraints/nonlinear.py</code> <pre><code>class NonlinearConstraint(IntrapointConstraint):\n    \"\"\"Base class for nonlinear equality and inequality constraints.\n\n    Attributes:\n        expression (str): Mathematical expression that can be evaluated by `pandas.eval`.\n        jacobian_expression (str): Mathematical expression that that can be evaluated by `pandas.eval`.\n        features (list): list of feature keys (str) on which the constraint works on.\n\n    \"\"\"\n\n    expression: Union[str, Callable]\n    features: Optional[FeatureKeys] = Field(default=None, validate_default=True)\n    jacobian_expression: Optional[Union[str, Callable]] = Field(\n        default=None, validate_default=True\n    )\n    hessian_expression: Optional[Union[str, Callable]] = Field(\n        default=None, validate_default=True\n    )\n\n    def validate_inputs(self, inputs: Inputs):\n        if self.features is not None:\n            keys = inputs.get_keys(ContinuousInput)\n            for f in self.features:\n                if f not in keys:\n                    raise ValueError(\n                        f\"Feature {f} is not a continuous input feature in the provided Inputs object.\",\n                    )\n\n    @field_validator(\"features\")\n    @classmethod\n    def set_features(cls, features, info) -&gt; Optional[FeatureKeys]:\n        if \"expression\" in info.data.keys():\n            if isinstance(info.data[\"expression\"], Callable):\n                if features is None:\n                    return list(inspect.getfullargspec(info.data[\"expression\"]).args)\n                else:\n                    raise ValueError(\n                        \"Features must be None if expression is a callable. They will be inferred from the callable.\",\n                    )\n\n        return features\n\n    @field_validator(\"jacobian_expression\")\n    @classmethod\n    def set_jacobian_expression(cls, jacobian_expression, info) -&gt; Union[str, Callable]:\n        if (\n            jacobian_expression is None\n            and \"features\" in info.data.keys()\n            and \"expression\" in info.data.keys()\n        ):\n            try:\n                import sympy\n            except ImportError as e:\n                warnings.warn(e.msg)\n                warnings.warn(\"please run `pip install sympy` for this functionality.\")\n                return jacobian_expression\n            if info.data[\"features\"] is not None:\n                if isinstance(info.data[\"expression\"], str):\n                    return (\n                        \"[\"\n                        + \", \".join(\n                            [\n                                str(sympy.S(info.data[\"expression\"]).diff(key))\n                                for key in info.data[\"features\"]\n                            ],\n                        )\n                        + \"]\"\n                    )\n\n        return jacobian_expression\n\n    @field_validator(\"hessian_expression\")\n    @classmethod\n    def set_hessian_expression(cls, hessian_expression, info) -&gt; Union[str, Callable]:\n        if (\n            hessian_expression is None\n            and \"features\" in info.data.keys()\n            and \"expression\" in info.data.keys()\n        ):\n            try:\n                import sympy\n            except ImportError as e:\n                warnings.warn(e.msg)\n                warnings.warn(\"please run `pip install sympy` for this functionality.\")\n                return hessian_expression\n            if info.data[\"features\"] is not None:\n                if isinstance(info.data[\"expression\"], str):\n                    return (\n                        \"[\"\n                        + \", \".join(\n                            [\n                                \"[\"\n                                + \", \".join(\n                                    [\n                                        str(\n                                            sympy.S(info.data[\"expression\"])\n                                            .diff(key1)\n                                            .diff(key2)\n                                        )\n                                        for key1 in info.data[\"features\"]\n                                    ]\n                                )\n                                + \"]\"\n                                for key2 in info.data[\"features\"]\n                            ]\n                        )\n                        + \"]\"\n                    )\n\n        return hessian_expression\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        if isinstance(self.expression, str):\n            return experiments.eval(self.expression)\n        elif isinstance(self.expression, Callable):\n            func_input = {\n                col: torch_tensor(experiments[col], requires_grad=False)\n                for col in experiments.columns\n            }\n            return pd.Series(self.expression(**func_input).cpu().numpy())\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        if self.jacobian_expression is not None:\n            if isinstance(self.jacobian_expression, str):\n                res = experiments.eval(self.jacobian_expression)\n                for i, col in enumerate(res):\n                    if not hasattr(col, \"__iter__\"):\n                        res[i] = pd.Series(np.repeat(col, experiments.shape[0]))\n\n                if self.features is not None:\n                    return pd.DataFrame(\n                        res,\n                        index=[\"dg/d\" + name for name in self.features],\n                    ).transpose()\n                return pd.DataFrame(\n                    res,\n                    index=[f\"dg/dx{i}\" for i in range(experiments.shape[1])],\n                ).transpose()\n            elif isinstance(self.jacobian_expression, Callable):\n                args = inspect.getfullargspec(self.jacobian_expression).args\n\n                func_input = {\n                    arg: torch_tensor(experiments[arg], requires_grad=False)\n                    for arg in args\n                }\n                result = self.jacobian_expression(**func_input)\n\n                return pd.DataFrame(\n                    np.array(\n                        [\n                            result[args.index(col)]\n                            if col in args\n                            else np.zeros(shape=(len(experiments)))\n                            for col in experiments.columns\n                        ]\n                    ),\n                    index=[\"dg/d\" + name for name in experiments.columns],\n                ).transpose()\n        elif isinstance(self.expression, Callable):\n            args = inspect.getfullargspec(self.expression).args\n\n            func_input = tuple(\n                [torch_tensor(experiments[arg], requires_grad=False) for arg in args]\n            )\n\n            result = torch_jacobian(self.expression, func_input)\n            result = [torch_diag(result[i]).cpu().numpy() for i in range(len(args))]\n\n            return pd.DataFrame(\n                np.array([result[args.index(col)] for col in args]),\n                index=[\"dg/d\" + name for name in args],\n            ).transpose()\n\n        raise ValueError(\n            \"The jacobian of a nonlinear constraint cannot be evaluated if jacobian_expression is None and expression is not Callable.\",\n        )\n\n    def hessian(self, experiments: pd.DataFrame) -&gt; Dict[Union[str, int], pd.DataFrame]:\n        \"\"\"\n        Computes a dict of dataframes where the key dimension is the index of the experiments dataframe\n        and the value is the hessian matrix of the constraint evaluated at the corresponding experiment.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe to evaluate the constraint Hessian on.\n\n        Returns:\n            Dict[pd.DataFrame]: Dictionary of dataframes where the key is the index of the experiments dataframe\n            and the value is the Hessian matrix of the constraint evaluated at the corresponding experiment.\n        \"\"\"\n        if self.hessian_expression is not None:\n            if isinstance(self.hessian_expression, str):\n                res = experiments.eval(self.hessian_expression)\n            else:\n                if not isinstance(self.hessian_expression, Callable):\n                    raise ValueError(\n                        \"The hessian_expression must be a string or a callable.\",\n                    )\n                args = inspect.getfullargspec(self.hessian_expression).args\n\n                func_input = {\n                    arg: torch_tensor(experiments[arg], requires_grad=False)\n                    for arg in args\n                }\n                res = self.hessian_expression(**func_input)\n            for i, _ in enumerate(res):\n                for j, entry in enumerate(res[i]):\n                    if not hasattr(entry, \"__iter__\"):\n                        res[i][j] = pd.Series(np.repeat(entry, experiments.shape[0]))\n            res = np.array(res)\n            names = self.features or [f\"x{i}\" for i in range(experiments.shape[1])]\n\n            return {\n                idx: pd.DataFrame(\n                    res[..., i],\n                    columns=[f\"d/d{name}\" for name in names],\n                    index=[f\"d/d{name}\" for name in names],\n                )\n                for i, idx in enumerate(experiments.index)\n            }\n\n        elif isinstance(self.expression, Callable):\n            args = inspect.getfullargspec(self.expression).args\n\n            func_input = {\n                idx: tuple(\n                    [\n                        torch_tensor(experiments[arg][idx], requires_grad=False)\n                        for arg in args\n                    ]\n                )\n                for idx in experiments.index\n            }\n\n            names = self.features or [f\"x{i}\" for i in range(experiments.shape[1])]\n            res = {\n                idx: pd.DataFrame(\n                    np.array(torch_hessian(self.expression, func_input[idx])),\n                    columns=[f\"d/d{name}\" for name in names],\n                    index=[f\"d/d{name}\" for name in names],\n                )\n                for idx in experiments.index\n            }\n            return res\n\n        raise ValueError(\n            \"The hessian of a nonlinear constraint cannot be evaluated if hessian_expression is None and expression is not Callable.\",\n        )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nonlinear.NonlinearConstraint.hessian","title":"<code>hessian(experiments)</code>","text":"<p>Computes a dict of dataframes where the key dimension is the index of the experiments dataframe and the value is the hessian matrix of the constraint evaluated at the corresponding experiment.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe to evaluate the constraint Hessian on.</p> required <p>Returns:</p> Type Description <code>Dict[Union[str, int], DataFrame]</code> <p>Dict[pd.DataFrame]: Dictionary of dataframes where the key is the index of the experiments dataframe</p> <code>Dict[Union[str, int], DataFrame]</code> <p>and the value is the Hessian matrix of the constraint evaluated at the corresponding experiment.</p> Source code in <code>bofire/data_models/constraints/nonlinear.py</code> <pre><code>def hessian(self, experiments: pd.DataFrame) -&gt; Dict[Union[str, int], pd.DataFrame]:\n    \"\"\"\n    Computes a dict of dataframes where the key dimension is the index of the experiments dataframe\n    and the value is the hessian matrix of the constraint evaluated at the corresponding experiment.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe to evaluate the constraint Hessian on.\n\n    Returns:\n        Dict[pd.DataFrame]: Dictionary of dataframes where the key is the index of the experiments dataframe\n        and the value is the Hessian matrix of the constraint evaluated at the corresponding experiment.\n    \"\"\"\n    if self.hessian_expression is not None:\n        if isinstance(self.hessian_expression, str):\n            res = experiments.eval(self.hessian_expression)\n        else:\n            if not isinstance(self.hessian_expression, Callable):\n                raise ValueError(\n                    \"The hessian_expression must be a string or a callable.\",\n                )\n            args = inspect.getfullargspec(self.hessian_expression).args\n\n            func_input = {\n                arg: torch_tensor(experiments[arg], requires_grad=False)\n                for arg in args\n            }\n            res = self.hessian_expression(**func_input)\n        for i, _ in enumerate(res):\n            for j, entry in enumerate(res[i]):\n                if not hasattr(entry, \"__iter__\"):\n                    res[i][j] = pd.Series(np.repeat(entry, experiments.shape[0]))\n        res = np.array(res)\n        names = self.features or [f\"x{i}\" for i in range(experiments.shape[1])]\n\n        return {\n            idx: pd.DataFrame(\n                res[..., i],\n                columns=[f\"d/d{name}\" for name in names],\n                index=[f\"d/d{name}\" for name in names],\n            )\n            for i, idx in enumerate(experiments.index)\n        }\n\n    elif isinstance(self.expression, Callable):\n        args = inspect.getfullargspec(self.expression).args\n\n        func_input = {\n            idx: tuple(\n                [\n                    torch_tensor(experiments[arg][idx], requires_grad=False)\n                    for arg in args\n                ]\n            )\n            for idx in experiments.index\n        }\n\n        names = self.features or [f\"x{i}\" for i in range(experiments.shape[1])]\n        res = {\n            idx: pd.DataFrame(\n                np.array(torch_hessian(self.expression, func_input[idx])),\n                columns=[f\"d/d{name}\" for name in names],\n                index=[f\"d/d{name}\" for name in names],\n            )\n            for idx in experiments.index\n        }\n        return res\n\n    raise ValueError(\n        \"The hessian of a nonlinear constraint cannot be evaluated if hessian_expression is None and expression is not Callable.\",\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nonlinear.NonlinearEqualityConstraint","title":"<code>NonlinearEqualityConstraint</code>","text":"<p>               Bases: <code>NonlinearConstraint</code>, <code>EqualityConstraint</code></p> <p>Nonlinear equality constraint of the form 'expression == 0'.</p> <p>Attributes:</p> Name Type Description <code>expression</code> <code>Union[str, Callable]</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> Source code in <code>bofire/data_models/constraints/nonlinear.py</code> <pre><code>class NonlinearEqualityConstraint(NonlinearConstraint, EqualityConstraint):\n    \"\"\"Nonlinear equality constraint of the form 'expression == 0'.\n\n    Attributes:\n        expression: Mathematical expression that can be evaluated by `pandas.eval`.\n\n    \"\"\"\n\n    type: Literal[\"NonlinearEqualityConstraint\"] = \"NonlinearEqualityConstraint\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.nonlinear.NonlinearInequalityConstraint","title":"<code>NonlinearInequalityConstraint</code>","text":"<p>               Bases: <code>NonlinearConstraint</code>, <code>InequalityConstraint</code></p> <p>Nonlinear inequality constraint of the form 'expression &lt;= 0'.</p> <p>Attributes:</p> Name Type Description <code>expression</code> <code>Union[str, Callable]</code> <p>Mathematical expression that can be evaluated by <code>pandas.eval</code>.</p> Source code in <code>bofire/data_models/constraints/nonlinear.py</code> <pre><code>class NonlinearInequalityConstraint(NonlinearConstraint, InequalityConstraint):\n    \"\"\"Nonlinear inequality constraint of the form 'expression &lt;= 0'.\n\n    Attributes:\n        expression: Mathematical expression that can be evaluated by `pandas.eval`.\n\n    \"\"\"\n\n    type: Literal[\"NonlinearInequalityConstraint\"] = \"NonlinearInequalityConstraint\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.product","title":"<code>product</code>","text":""},{"location":"ref-constraints/#bofire.data_models.constraints.product.ProductConstraint","title":"<code>ProductConstraint</code>","text":"<p>               Bases: <code>IntrapointConstraint</code></p> <p>Represents a product constraint of the form <code>sign * x1**e1 * x2**e2 * ... * xn**en</code>.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of the constraint.</p> <code>features</code> <code>FeatureKeys</code> <p>The keys of the features used in the constraint.</p> <code>exponents</code> <code>List[float]</code> <p>The exponents corresponding to each feature.</p> <code>rhs</code> <code>float</code> <p>The right-hand side value of the constraint.</p> <code>sign</code> <code>Literal[1, -1]</code> <p>The sign of the left hand side of the constraint. Defaults to 1.</p> Source code in <code>bofire/data_models/constraints/product.py</code> <pre><code>class ProductConstraint(IntrapointConstraint):\n    \"\"\"Represents a product constraint of the form `sign * x1**e1 * x2**e2 * ... * xn**en`.\n\n    Attributes:\n        type (str): The type of the constraint.\n        features (FeatureKeys): The keys of the features used in the constraint.\n        exponents (List[float]): The exponents corresponding to each feature.\n        rhs (float): The right-hand side value of the constraint.\n        sign (Literal[1, -1], optional): The sign of the left hand side of the constraint.\n            Defaults to 1.\n\n    \"\"\"\n\n    type: str\n    features: FeatureKeys\n    exponents: Annotated[List[float], Field(min_length=2)]\n    rhs: float\n    sign: Literal[1, -1] = 1\n\n    @model_validator(mode=\"after\")\n    def validate_list_lengths(self) -&gt; \"ProductConstraint\":\n        \"\"\"Validates that the number of features and exponents provided are the same.\n\n        Raises:\n            ValueError: If the number of features and exponents are not equal.\n\n        Returns:\n            ProductConstraint: The current instance of the class.\n\n        \"\"\"\n        if len(self.features) != len(self.exponents):\n            raise ValueError(\n                f\"must provide same number of features and exponents, got {len(self.features)} != {len(self.exponents)}\",\n            )\n        return self\n\n    def validate_inputs(self, inputs: Inputs):\n        keys = inputs.get_keys(ContinuousInput)\n        for f in self.features:\n            if f not in keys:\n                raise ValueError(\n                    f\"Feature {f} is not a continuous input feature in the provided Inputs object.\",\n                )\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Evaluates the constraint on the given experiments.\n\n        Args:\n            experiments (pd.DataFrame): The experiments to evaluate the constraint on.\n\n        Returns:\n            pd.Series: The distance to reach constraint fulfillment.\n\n        \"\"\"\n        return pd.Series(\n            self.sign\n            * np.prod(\n                np.power(experiments[self.features].values, np.array(self.exponents)),\n                axis=1,\n            )\n            - self.rhs,\n            index=experiments.index,\n        )\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        raise NotImplementedError(\n            \"Jacobian for product constraints is not yet implemented.\",\n        )\n\n    def hessian(self, experiments: pd.DataFrame) -&gt; List[pd.DataFrame]:\n        raise NotImplementedError(\n            \"Hessian for product constraints is not yet implemented.\",\n        )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.product.ProductConstraint.__call__","title":"<code>__call__(experiments)</code>","text":"<p>Evaluates the constraint on the given experiments.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>The experiments to evaluate the constraint on.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The distance to reach constraint fulfillment.</p> Source code in <code>bofire/data_models/constraints/product.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Evaluates the constraint on the given experiments.\n\n    Args:\n        experiments (pd.DataFrame): The experiments to evaluate the constraint on.\n\n    Returns:\n        pd.Series: The distance to reach constraint fulfillment.\n\n    \"\"\"\n    return pd.Series(\n        self.sign\n        * np.prod(\n            np.power(experiments[self.features].values, np.array(self.exponents)),\n            axis=1,\n        )\n        - self.rhs,\n        index=experiments.index,\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.product.ProductConstraint.validate_list_lengths","title":"<code>validate_list_lengths()</code>","text":"<p>Validates that the number of features and exponents provided are the same.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of features and exponents are not equal.</p> <p>Returns:</p> Name Type Description <code>ProductConstraint</code> <code>ProductConstraint</code> <p>The current instance of the class.</p> Source code in <code>bofire/data_models/constraints/product.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_list_lengths(self) -&gt; \"ProductConstraint\":\n    \"\"\"Validates that the number of features and exponents provided are the same.\n\n    Raises:\n        ValueError: If the number of features and exponents are not equal.\n\n    Returns:\n        ProductConstraint: The current instance of the class.\n\n    \"\"\"\n    if len(self.features) != len(self.exponents):\n        raise ValueError(\n            f\"must provide same number of features and exponents, got {len(self.features)} != {len(self.exponents)}\",\n        )\n    return self\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.product.ProductEqualityConstraint","title":"<code>ProductEqualityConstraint</code>","text":"<p>               Bases: <code>ProductConstraint</code>, <code>EqualityConstraint</code></p> <p>Represents a product constraint of the form <code>sign * x1**e1 * x2**e2 * ... * xn**en == rhs</code>.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of the constraint.</p> <code>features</code> <code>FeatureKeys</code> <p>The keys of the features used in the constraint.</p> <code>exponents</code> <code>List[float]</code> <p>The exponents corresponding to each feature.</p> <code>rhs</code> <code>float</code> <p>The right-hand side value of the constraint.</p> <code>sign</code> <code>Literal[1, -1]</code> <p>The sign of the left hand side of the constraint. Defaults to 1.</p> Source code in <code>bofire/data_models/constraints/product.py</code> <pre><code>class ProductEqualityConstraint(ProductConstraint, EqualityConstraint):\n    \"\"\"Represents a product constraint of the form `sign * x1**e1 * x2**e2 * ... * xn**en == rhs`.\n\n    Attributes:\n        type (str): The type of the constraint.\n        features (FeatureKeys): The keys of the features used in the constraint.\n        exponents (List[float]): The exponents corresponding to each feature.\n        rhs (float): The right-hand side value of the constraint.\n        sign (Literal[1, -1], optional): The sign of the left hand side of the constraint.\n            Defaults to 1.\n\n    \"\"\"\n\n    type: Literal[\"ProductEqualityConstraint\"] = \"ProductEqualityConstraint\"\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.constraints.product.ProductInequalityConstraint","title":"<code>ProductInequalityConstraint</code>","text":"<p>               Bases: <code>ProductConstraint</code>, <code>InequalityConstraint</code></p> <p>Represents a product constraint of the form <code>sign * x1**e1 * x2**e2 * ... * xn**en &lt;= rhs</code>.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>The type of the constraint.</p> <code>features</code> <code>FeatureKeys</code> <p>The keys of the features used in the constraint.</p> <code>exponents</code> <code>List[float]</code> <p>The exponents corresponding to each feature.</p> <code>rhs</code> <code>float</code> <p>The right-hand side value of the constraint.</p> <code>sign</code> <code>Literal[1, -1]</code> <p>The sign of the left hand side of the constraint. Defaults to 1.</p> Source code in <code>bofire/data_models/constraints/product.py</code> <pre><code>class ProductInequalityConstraint(ProductConstraint, InequalityConstraint):\n    \"\"\"Represents a product constraint of the form `sign * x1**e1 * x2**e2 * ... * xn**en &lt;= rhs`.\n\n    Attributes:\n        type (str): The type of the constraint.\n        features (FeatureKeys): The keys of the features used in the constraint.\n        exponents (List[float]): The exponents corresponding to each feature.\n        rhs (float): The right-hand side value of the constraint.\n        sign (Literal[1, -1], optional): The sign of the left hand side of the constraint.\n            Defaults to 1.\n\n    \"\"\"\n\n    type: Literal[\"ProductInequalityConstraint\"] = \"ProductInequalityConstraint\"\n</code></pre>"},{"location":"ref-domain/","title":"Domain","text":""},{"location":"ref-domain/#bofire.data_models.domain.constraints","title":"<code>constraints</code>","text":""},{"location":"ref-domain/#bofire.data_models.domain.constraints.Constraints","title":"<code>Constraints</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[C]</code></p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>class Constraints(BaseModel, Generic[C]):\n    type: Literal[\"Constraints\"] = \"Constraints\"\n    constraints: Sequence[C] = Field(default_factory=list)\n\n    def __iter__(self) -&gt; Iterator[C]:\n        return iter(self.constraints)\n\n    def __len__(self):\n        return len(self.constraints)\n\n    def __getitem__(self, i) -&gt; C:\n        return self.constraints[i]\n\n    def __add__(\n        self,\n        other: Union[Sequence[CIncludes], \"Constraints[CIncludes]\"],\n    ) -&gt; \"Constraints[Union[C, CIncludes]]\":\n        if isinstance(other, collections.abc.Sequence):\n            other_constraints = other\n        else:\n            other_constraints = other.constraints\n        constraints = list(chain(self.constraints, other_constraints))\n        return Constraints(constraints=constraints)\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Numerically evaluate all constraints\n\n        Args:\n            experiments (pd.DataFrame): data to evaluate the constraint on\n\n        Returns:\n            pd.DataFrame: Constraint evaluation for each of the constraints\n\n        \"\"\"\n        return pd.concat([c(experiments) for c in self.constraints], axis=1)\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; list:\n        \"\"\"Numerically evaluate the jacobians of all constraints\n\n        Args:\n            experiments (pd.DataFrame): data to evaluate the constraint jacobians on\n\n        Returns:\n            list: A list containing the jacobians as pd.DataFrames\n\n        \"\"\"\n        return [c.jacobian(experiments) for c in self.constraints]\n\n    def is_fulfilled(self, experiments: pd.DataFrame, tol: float = 1e-6) -&gt; pd.Series:\n        \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with data, the constraint validity should be tested on\n            tol (float, optional): tolerance parameter. A constraint is considered as not fulfilled if\n                the violation is larger than tol. Defaults to 0.\n\n        Returns:\n            Boolean: True if all constraints are fulfilled for all rows, false if not\n\n        \"\"\"\n        if len(self.constraints) == 0:\n            return pd.Series([True] * len(experiments), index=experiments.index)\n        return (\n            pd.concat(\n                [c.is_fulfilled(experiments, tol) for c in self.constraints],\n                axis=1,\n            )\n            .fillna(True)\n            .all(axis=1)\n        )\n\n    def get(\n        self,\n        includes: Union[Type[CIncludes], Sequence[Type[CIncludes]]] = Constraint,\n        excludes: Optional[Union[Type[CExcludes], List[Type[CExcludes]]]] = None,\n        exact: bool = False,\n    ) -&gt; \"Constraints[CIncludes]\":\n        \"\"\"Get constraints of the domain\n\n        Args:\n            includes: Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.\n            excludes: Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\n            exact: Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            Constraints: constraints in the domain fitting to the passed requirements.\n\n        \"\"\"\n        return Constraints(\n            constraints=filter_by_class(\n                self.constraints,\n                includes=includes,\n                excludes=excludes,\n                exact=exact,\n            ),\n        )\n\n    def get_reps_df(self):\n        \"\"\"Provides a tabular overwiev of all constraints within the domain\n\n        Returns:\n            pd.DataFrame: DataFrame listing all constraints of the domain with a description\n\n        \"\"\"\n        df = pd.DataFrame(\n            index=range(len(self.constraints)),\n            columns=[\"Type\", \"Description\"],\n            data={\n                \"Type\": [feat.__class__.__name__ for feat in self.get(Constraint)],\n                \"Description\": [\n                    constraint.__str__() for constraint in self.get(Constraint)\n                ],\n            },\n        )\n        return df\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.constraints.Constraints.__call__","title":"<code>__call__(experiments)</code>","text":"<p>Numerically evaluate all constraints</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>data to evaluate the constraint on</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Constraint evaluation for each of the constraints</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Numerically evaluate all constraints\n\n    Args:\n        experiments (pd.DataFrame): data to evaluate the constraint on\n\n    Returns:\n        pd.DataFrame: Constraint evaluation for each of the constraints\n\n    \"\"\"\n    return pd.concat([c(experiments) for c in self.constraints], axis=1)\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.constraints.Constraints.get","title":"<code>get(includes=Constraint, excludes=None, exact=False)</code>","text":"<p>Get constraints of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Type[CIncludes], Sequence[Type[CIncludes]]]</code> <p>Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.</p> <code>Constraint</code> <code>excludes</code> <code>Optional[Union[Type[CExcludes], List[Type[CExcludes]]]]</code> <p>Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Constraints</code> <code>Constraints[CIncludes]</code> <p>constraints in the domain fitting to the passed requirements.</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def get(\n    self,\n    includes: Union[Type[CIncludes], Sequence[Type[CIncludes]]] = Constraint,\n    excludes: Optional[Union[Type[CExcludes], List[Type[CExcludes]]]] = None,\n    exact: bool = False,\n) -&gt; \"Constraints[CIncludes]\":\n    \"\"\"Get constraints of the domain\n\n    Args:\n        includes: Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.\n        excludes: Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\n        exact: Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        Constraints: constraints in the domain fitting to the passed requirements.\n\n    \"\"\"\n    return Constraints(\n        constraints=filter_by_class(\n            self.constraints,\n            includes=includes,\n            excludes=excludes,\n            exact=exact,\n        ),\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.constraints.Constraints.get_reps_df","title":"<code>get_reps_df()</code>","text":"<p>Provides a tabular overwiev of all constraints within the domain</p> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame listing all constraints of the domain with a description</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def get_reps_df(self):\n    \"\"\"Provides a tabular overwiev of all constraints within the domain\n\n    Returns:\n        pd.DataFrame: DataFrame listing all constraints of the domain with a description\n\n    \"\"\"\n    df = pd.DataFrame(\n        index=range(len(self.constraints)),\n        columns=[\"Type\", \"Description\"],\n        data={\n            \"Type\": [feat.__class__.__name__ for feat in self.get(Constraint)],\n            \"Description\": [\n                constraint.__str__() for constraint in self.get(Constraint)\n            ],\n        },\n    )\n    return df\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.constraints.Constraints.is_fulfilled","title":"<code>is_fulfilled(experiments, tol=1e-06)</code>","text":"<p>Check if all constraints are fulfilled on all rows of the provided dataframe</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with data, the constraint validity should be tested on</p> required <code>tol</code> <code>float</code> <p>tolerance parameter. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 0.</p> <code>1e-06</code> <p>Returns:</p> Name Type Description <code>Boolean</code> <code>Series</code> <p>True if all constraints are fulfilled for all rows, false if not</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame, tol: float = 1e-6) -&gt; pd.Series:\n    \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with data, the constraint validity should be tested on\n        tol (float, optional): tolerance parameter. A constraint is considered as not fulfilled if\n            the violation is larger than tol. Defaults to 0.\n\n    Returns:\n        Boolean: True if all constraints are fulfilled for all rows, false if not\n\n    \"\"\"\n    if len(self.constraints) == 0:\n        return pd.Series([True] * len(experiments), index=experiments.index)\n    return (\n        pd.concat(\n            [c.is_fulfilled(experiments, tol) for c in self.constraints],\n            axis=1,\n        )\n        .fillna(True)\n        .all(axis=1)\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.constraints.Constraints.jacobian","title":"<code>jacobian(experiments)</code>","text":"<p>Numerically evaluate the jacobians of all constraints</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>data to evaluate the constraint jacobians on</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the jacobians as pd.DataFrames</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def jacobian(self, experiments: pd.DataFrame) -&gt; list:\n    \"\"\"Numerically evaluate the jacobians of all constraints\n\n    Args:\n        experiments (pd.DataFrame): data to evaluate the constraint jacobians on\n\n    Returns:\n        list: A list containing the jacobians as pd.DataFrames\n\n    \"\"\"\n    return [c.jacobian(experiments) for c in self.constraints]\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain","title":"<code>domain</code>","text":""},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain","title":"<code>Domain</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>class Domain(BaseModel):\n    type: Literal[\"Domain\"] = \"Domain\"\n\n    inputs: Inputs = Field(default_factory=lambda: Inputs())\n    outputs: Outputs = Field(default_factory=lambda: Outputs())\n    constraints: Constraints = Field(default_factory=lambda: Constraints())\n\n    \"\"\"Representation of the optimization problem/domain\n\n    Attributes:\n        inputs (List[Input], optional): List of input features. Defaults to [].\n        outputs (List[Output], optional): List of output features. Defaults to [].\n        constraints (List[Constraint], optional): List of constraints. Defaults to [].\n    \"\"\"\n\n    @classmethod\n    def from_lists(\n        cls,\n        inputs: Optional[Sequence[AnyInput]] = None,\n        outputs: Optional[Sequence[AnyOutput]] = None,\n        constraints: Optional[Sequence[AnyConstraint]] = None,\n    ):\n        inputs = [] if inputs is None else inputs\n        outputs = [] if outputs is None else outputs\n        constraints = [] if constraints is None else constraints\n        return cls(\n            inputs=Inputs(features=inputs),\n            outputs=Outputs(features=outputs),\n            constraints=Constraints(constraints=constraints),\n        )\n\n    @field_validator(\"inputs\", mode=\"before\")\n    @classmethod\n    def validate_inputs_list(cls, v):\n        if isinstance(v, collections.abc.Sequence):\n            v = Inputs(features=v)\n            return v\n        if isinstance_or_union(v, AnyInput):\n            return Inputs(features=[v])\n        return v\n\n    @field_validator(\"outputs\", mode=\"before\")\n    @classmethod\n    def validate_outputs_list(cls, v):\n        if isinstance(v, collections.abc.Sequence):\n            return Outputs(features=v)\n        if isinstance_or_union(v, AnyOutput):\n            return Outputs(features=[v])\n        return v\n\n    @field_validator(\"constraints\", mode=\"before\")\n    @classmethod\n    def validate_constraints_list(cls, v):\n        if isinstance(v, list):\n            return Constraints(constraints=v)\n        if isinstance_or_union(v, AnyConstraint):\n            return Constraints(constraints=[v])\n        return v\n\n    @model_validator(mode=\"after\")\n    def validate_unique_feature_keys(self):\n        \"\"\"Validates if provided input and output feature keys are unique\n\n        Args:\n            v (Outputs): List of all output features of the domain.\n            value (Dict[str, Inputs]): Dict containing a list of input features as single entry.\n\n        Raises:\n            ValueError: Feature keys are not unique.\n\n        Returns:\n            Outputs: Keeps output features as given.\n\n        \"\"\"\n        keys = self.outputs.get_keys() + self.inputs.get_keys()\n        if len(set(keys)) != len(keys):\n            raise ValueError(\"Feature keys are not unique\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_constraints(self):\n        \"\"\"Validate that the constraints defined in the domain fit to the input features.\n\n        Args:\n            v (List[Constraint]): List of constraints or empty if no constraints are defined\n            values (List[Input]): List of input features of the domain\n\n        Raises:\n            ValueError: Feature key in constraint is unknown.\n\n        Returns:\n            List[Constraint]: List of constraints defined for the domain\n\n        \"\"\"\n        for c in self.constraints.get():\n            c.validate_inputs(self.inputs)\n        return self\n\n    # TODO: tidy this up\n    def get_nchoosek_combinations(self, exhaustive: bool = False):\n        \"\"\"Get all possible NChooseK combinations\n\n        Args:\n            exhaustive (bool, optional): if True all combinations are returned. Defaults to False.\n\n        Returns:\n            Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination.\n                unused_features_list is a list of lists containing features unused in each NChooseK combination.\n\n        \"\"\"\n        if len(self.constraints.get(NChooseKConstraint)) == 0:\n            used_continuous_features = self.inputs.get_keys(ContinuousInput)\n            return used_continuous_features, []\n\n        used_features_list_all = []\n\n        # loops through each NChooseK constraint\n        for con in self.constraints.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            used_features_list = []\n\n            if exhaustive:\n                for n in range(con.min_count, con.max_count + 1):\n                    used_features_list.extend(itertools.combinations(con.features, n))\n\n                if con.none_also_valid:\n                    used_features_list.append(())\n            else:\n                used_features_list.extend(\n                    itertools.combinations(con.features, con.max_count),\n                )\n\n            used_features_list_all.append(used_features_list)\n\n        used_features_list_all = list(\n            itertools.product(*used_features_list_all),\n        )  # product between NChooseK constraints\n\n        # format into a list of used features\n        used_features_list_formatted = []\n        for used_features_list in used_features_list_all:\n            used_features_list_flattened = [\n                item for sublist in used_features_list for item in sublist\n            ]\n            used_features_list_formatted.append(list(set(used_features_list_flattened)))\n\n        # sort lists\n        used_features_list_sorted = []\n        for used_features in used_features_list_formatted:\n            used_features_list_sorted.append(sorted(used_features))\n\n        # drop duplicates\n        used_features_list_no_dup = []\n        for used_features in used_features_list_sorted:\n            if used_features not in used_features_list_no_dup:\n                used_features_list_no_dup.append(used_features)\n\n        # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\")\n\n        # remove combinations not fulfilling constraints\n        used_features_list_final = []\n        for combo in used_features_list_no_dup:\n            fulfil_constraints = []  # list of bools tracking if constraints are fulfilled\n            for con in self.constraints.get(NChooseKConstraint):\n                assert isinstance(con, NChooseKConstraint)\n                count = 0  # count of features in combo that are in con.features\n                for f in combo:\n                    if f in con.features:\n                        count += 1\n                if (\n                    count &gt;= con.min_count\n                    and count &lt;= con.max_count\n                    or count == 0\n                    and con.none_also_valid\n                ):\n                    fulfil_constraints.append(True)\n                else:\n                    fulfil_constraints.append(False)\n            if np.all(fulfil_constraints):\n                used_features_list_final.append(combo)\n\n        # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\")\n\n        # features unused\n        features_in_cc = []\n        for con in self.constraints.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            features_in_cc.extend(con.features)\n        features_in_cc = list(set(features_in_cc))\n        features_in_cc.sort()\n        unused_features_list = []\n        for used_features in used_features_list_final:\n            unused_features_list.append(\n                [f_key for f_key in features_in_cc if f_key not in used_features],\n            )\n\n        # postprocess\n        # used_features_list_final2 = []\n        # unused_features_list2 = []\n        # for used, unused in zip(used_features_list_final,unused_features_list):\n        #     if len(used) == 3:\n        #         used_features_list_final2.append(used), unused_features_list2.append(unused)\n\n        return used_features_list_final, unused_features_list\n\n    def coerce_invalids(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Coerces all invalid output measurements to np.nan\n\n        Args:\n            experiments (pd.DataFrame): Dataframe containing experimental data\n\n        Returns:\n            pd.DataFrame: coerced dataframe\n\n        \"\"\"\n        # coerce invalid to nan\n        for feat in self.outputs.get_keys(Output):\n            experiments.loc[experiments[f\"valid_{feat}\"] == 0, feat] = np.nan\n        return experiments\n\n    def aggregate_by_duplicates(\n        self,\n        experiments: pd.DataFrame,\n        prec: int,\n        delimiter: str = \"-\",\n        method: Literal[\"mean\", \"median\"] = \"mean\",\n    ) -&gt; Tuple[pd.DataFrame, list]:\n        \"\"\"Aggregate the dataframe by duplicate experiments\n\n        Duplicates are identified based on the experiments with the same input\n        features. Continuous input features are rounded before identifying the\n        duplicates. Aggregation is performed by taking the average of the\n        involved output features.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe containing experimental data\n            prec (int): Precision of the rounding of the continuous input features\n            delimiter (str, optional): Delimiter used when combining the orig.\n                labcodes to a new one. Defaults to \"-\".\n            method (Literal[\"mean\", \"median\"], optional): Which aggregation\n                method to use. Defaults to \"mean\".\n\n        Returns:\n            Tuple[pd.DataFrame, list]: Dataframe holding the aggregated\n                experiments, list of lists holding the labcodes of the duplicates\n\n        \"\"\"\n        # prepare the parent frame\n        if method not in [\"mean\", \"median\"]:\n            raise ValueError(f\"Unknown aggregation type provided: {method}\")\n\n        preprocessed = self.outputs.preprocess_experiments_any_valid_output(experiments)\n        assert preprocessed is not None\n        experiments = preprocessed.copy()\n        if \"labcode\" not in experiments.columns:\n            experiments[\"labcode\"] = [\n                str(i + 1).zfill(int(np.ceil(np.log10(experiments.shape[0]))))\n                for i in range(experiments.shape[0])\n            ]\n\n        # round it if continuous inputs are present\n        if len(self.inputs.get(ContinuousInput)) &gt; 0:\n            experiments[self.inputs.get_keys(ContinuousInput)] = experiments[\n                self.inputs.get_keys(ContinuousInput)\n            ].round(prec)\n\n        # coerce invalid to nan\n        experiments = self.coerce_invalids(experiments)\n\n        # group and aggregate\n        agg: Dict[str, Any] = {\n            feat: method for feat in self.outputs.get_keys(ContinuousOutput)\n        }\n        agg[\"labcode\"] = lambda x: delimiter.join(sorted(x.tolist()))\n        for feat in self.outputs.get_keys(Output):\n            agg[f\"valid_{feat}\"] = lambda x: 1\n\n        grouped = experiments.groupby(self.inputs.get_keys(Input))\n        duplicated_labcodes = [\n            sorted(group.labcode.to_numpy().tolist())\n            for _, group in grouped\n            if group.shape[0] &gt; 1\n        ]\n\n        experiments = grouped.aggregate(agg).reset_index(drop=False)\n        for feat in self.outputs.get_keys(Output):\n            experiments.loc[experiments[feat].isna(), f\"valid_{feat}\"] = 0\n\n        experiments = experiments.sort_values(by=\"labcode\")\n        experiments = experiments.reset_index(drop=True)\n        return experiments, sorted(duplicated_labcodes)\n\n    def validate_experiments(\n        self,\n        experiments: pd.DataFrame,\n        strict: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Checks the experimental data on validity\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n            strict (bool, optional): Boolean to distinguish if the occurrence of\n                fixed features in the dataset should be considered or not.\n                Defaults to False.\n\n        Raises:\n            ValueError: empty dataframe\n            ValueError: the column for a specific feature is missing the provided data\n            ValueError: there are labcodes with null value\n            ValueError: there are labcodes with nan value\n            ValueError: labcodes are not unique\n            ValueError: the provided columns do no match to the defined domain\n            ValueError: the provided columns do no match to the defined domain\n            ValueError: Input with null values\n            ValueError: Input with nan values\n\n        Returns:\n            pd.DataFrame: The provided dataframe with experimental data\n\n        \"\"\"\n        if len(experiments) == 0:\n            raise ValueError(\"no experiments provided (empty dataframe)\")\n\n        # we allow here for a column named labcode used to identify experiments\n        if \"labcode\" in experiments.columns:\n            # test that labcodes are not na\n            if experiments.labcode.isnull().to_numpy().any():\n                raise ValueError(\"there are labcodes with null value\")\n            if experiments.labcode.isna().to_numpy().any():\n                raise ValueError(\"there are labcodes with nan value\")\n            # test that labcodes are distinct\n            if (\n                len(set(experiments.labcode.to_numpy().tolist()))\n                != experiments.shape[0]\n            ):\n                raise ValueError(\"labcodes are not unique\")\n\n        # run the individual validators\n        experiments = self.inputs.validate_experiments(\n            experiments=experiments,\n            strict=strict,\n        )\n        experiments = self.outputs.validate_experiments(experiments=experiments)\n        return experiments\n\n    def describe_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n\n        Returns:\n            pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n\n        \"\"\"\n        data = {}\n        for feat in self.outputs.get_keys(Output):\n            data[feat] = [\n                experiments.loc[experiments[feat].notna()].shape[0],\n                experiments.loc[experiments[feat].notna(), \"valid_%s\" % feat].sum(),\n            ]\n        preprocessed = self.outputs.preprocess_experiments_all_valid_outputs(\n            experiments,\n        )\n        assert preprocessed is not None\n        data[\"all\"] = [\n            experiments.shape[0],\n            preprocessed.shape[0],\n        ]\n        return pd.DataFrame.from_dict(\n            data,\n            orient=\"index\",\n            columns=[\"measured\", \"valid\"],\n        )\n\n    def validate_candidates(\n        self,\n        candidates: pd.DataFrame,\n        only_inputs: bool = False,\n        tol: float = 1e-5,\n        raise_validation_error: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Method to check the validty of proposed candidates\n\n        Args:\n            candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates)\n            only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False.\n            tol (float,optional): tolerance parameter for constraints. A constraint is considered as not fulfilled if the violation\n                is larger than tol. Defaults to 1e-6.\n            raise_validation_error (bool, optional): If true an error will be raised if candidates violate constraints,\n                otherwise only a warning will be displayed. Defaults to True.\n\n        Raises:\n            ValueError: when a column is missing for a defined input feature\n            ValueError: when a column is missing for a defined output feature\n            ValueError: when a non-numerical value is proposed\n            ValueError: when an additional column is found\n            ConstraintNotFulfilledError: when the constraints are not fulfilled and `raise_validation_error = True`\n\n        Returns:\n            pd.DataFrame: dataframe with suggested experiments (candidates)\n\n        \"\"\"\n        # check that each input feature has a col and is valid in itself\n        assert isinstance(self.inputs, Inputs)\n        candidates = self.inputs.validate_candidates(candidates)\n        # check if all constraints are fulfilled\n        if not self.constraints.is_fulfilled(candidates, tol=tol).all():\n            if raise_validation_error:\n                raise ConstraintNotFulfilledError(\n                    f\"Constraints not fulfilled: {candidates}\",\n                )\n            warnings.warn(\"Not all constraints are fulfilled.\")\n        # for each continuous output feature with an attached objective object\n        if not only_inputs:\n            assert isinstance(self.outputs, Outputs)\n            candidates = self.outputs.validate_candidates(candidates=candidates)\n        return candidates\n\n    @property\n    def experiment_column_names(self):\n        \"\"\"The columns in the experimental dataframe\n\n        Returns:\n            List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys)\n\n        \"\"\"\n        return (self.inputs + self.outputs).get_keys() + [\n            f\"valid_{output_feature_key}\"\n            for output_feature_key in self.outputs.get_keys(Output)\n        ]\n\n    @property\n    def candidate_column_names(self):\n        \"\"\"The columns in the candidate dataframe\n\n        Returns:\n            List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)\n\n        \"\"\"\n        assert isinstance(self.outputs, Outputs)\n        return (\n            self.inputs.get_keys(Input)\n            + [\n                f\"{output_feature_key}_pred\"\n                for output_feature_key in self.outputs.get_keys_by_objective(Objective)\n            ]\n            + [\n                f\"{output_feature_key}_sd\"\n                for output_feature_key in self.outputs.get_keys_by_objective(Objective)\n            ]\n            + [\n                f\"{output_feature_key}_des\"\n                for output_feature_key in self.outputs.get_keys_by_objective(Objective)\n            ]\n        )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.candidate_column_names","title":"<code>candidate_column_names</code>  <code>property</code>","text":"<p>The columns in the candidate dataframe</p> <p>Returns:</p> Type Description <p>List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)</p>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.constraints","title":"<code>constraints = Field(default_factory=lambda: Constraints())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Representation of the optimization problem/domain</p> <p>Attributes:</p> Name Type Description <code>inputs</code> <code>List[Input]</code> <p>List of input features. Defaults to [].</p> <code>outputs</code> <code>List[Output]</code> <p>List of output features. Defaults to [].</p> <code>constraints</code> <code>List[Constraint]</code> <p>List of constraints. Defaults to [].</p>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.experiment_column_names","title":"<code>experiment_column_names</code>  <code>property</code>","text":"<p>The columns in the experimental dataframe</p> <p>Returns:</p> Type Description <p>List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys)</p>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.aggregate_by_duplicates","title":"<code>aggregate_by_duplicates(experiments, prec, delimiter='-', method='mean')</code>","text":"<p>Aggregate the dataframe by duplicate experiments</p> <p>Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe containing experimental data</p> required <code>prec</code> <code>int</code> <p>Precision of the rounding of the continuous input features</p> required <code>delimiter</code> <code>str</code> <p>Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\".</p> <code>'-'</code> <code>method</code> <code>Literal['mean', 'median']</code> <p>Which aggregation method to use. Defaults to \"mean\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, list]</code> <p>Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def aggregate_by_duplicates(\n    self,\n    experiments: pd.DataFrame,\n    prec: int,\n    delimiter: str = \"-\",\n    method: Literal[\"mean\", \"median\"] = \"mean\",\n) -&gt; Tuple[pd.DataFrame, list]:\n    \"\"\"Aggregate the dataframe by duplicate experiments\n\n    Duplicates are identified based on the experiments with the same input\n    features. Continuous input features are rounded before identifying the\n    duplicates. Aggregation is performed by taking the average of the\n    involved output features.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe containing experimental data\n        prec (int): Precision of the rounding of the continuous input features\n        delimiter (str, optional): Delimiter used when combining the orig.\n            labcodes to a new one. Defaults to \"-\".\n        method (Literal[\"mean\", \"median\"], optional): Which aggregation\n            method to use. Defaults to \"mean\".\n\n    Returns:\n        Tuple[pd.DataFrame, list]: Dataframe holding the aggregated\n            experiments, list of lists holding the labcodes of the duplicates\n\n    \"\"\"\n    # prepare the parent frame\n    if method not in [\"mean\", \"median\"]:\n        raise ValueError(f\"Unknown aggregation type provided: {method}\")\n\n    preprocessed = self.outputs.preprocess_experiments_any_valid_output(experiments)\n    assert preprocessed is not None\n    experiments = preprocessed.copy()\n    if \"labcode\" not in experiments.columns:\n        experiments[\"labcode\"] = [\n            str(i + 1).zfill(int(np.ceil(np.log10(experiments.shape[0]))))\n            for i in range(experiments.shape[0])\n        ]\n\n    # round it if continuous inputs are present\n    if len(self.inputs.get(ContinuousInput)) &gt; 0:\n        experiments[self.inputs.get_keys(ContinuousInput)] = experiments[\n            self.inputs.get_keys(ContinuousInput)\n        ].round(prec)\n\n    # coerce invalid to nan\n    experiments = self.coerce_invalids(experiments)\n\n    # group and aggregate\n    agg: Dict[str, Any] = {\n        feat: method for feat in self.outputs.get_keys(ContinuousOutput)\n    }\n    agg[\"labcode\"] = lambda x: delimiter.join(sorted(x.tolist()))\n    for feat in self.outputs.get_keys(Output):\n        agg[f\"valid_{feat}\"] = lambda x: 1\n\n    grouped = experiments.groupby(self.inputs.get_keys(Input))\n    duplicated_labcodes = [\n        sorted(group.labcode.to_numpy().tolist())\n        for _, group in grouped\n        if group.shape[0] &gt; 1\n    ]\n\n    experiments = grouped.aggregate(agg).reset_index(drop=False)\n    for feat in self.outputs.get_keys(Output):\n        experiments.loc[experiments[feat].isna(), f\"valid_{feat}\"] = 0\n\n    experiments = experiments.sort_values(by=\"labcode\")\n    experiments = experiments.reset_index(drop=True)\n    return experiments, sorted(duplicated_labcodes)\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.coerce_invalids","title":"<code>coerce_invalids(experiments)</code>","text":"<p>Coerces all invalid output measurements to np.nan</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe containing experimental data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: coerced dataframe</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def coerce_invalids(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Coerces all invalid output measurements to np.nan\n\n    Args:\n        experiments (pd.DataFrame): Dataframe containing experimental data\n\n    Returns:\n        pd.DataFrame: coerced dataframe\n\n    \"\"\"\n    # coerce invalid to nan\n    for feat in self.outputs.get_keys(Output):\n        experiments.loc[experiments[f\"valid_{feat}\"] == 0, feat] = np.nan\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.describe_experiments","title":"<code>describe_experiments(experiments)</code>","text":"<p>Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def describe_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n\n    Returns:\n        pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n\n    \"\"\"\n    data = {}\n    for feat in self.outputs.get_keys(Output):\n        data[feat] = [\n            experiments.loc[experiments[feat].notna()].shape[0],\n            experiments.loc[experiments[feat].notna(), \"valid_%s\" % feat].sum(),\n        ]\n    preprocessed = self.outputs.preprocess_experiments_all_valid_outputs(\n        experiments,\n    )\n    assert preprocessed is not None\n    data[\"all\"] = [\n        experiments.shape[0],\n        preprocessed.shape[0],\n    ]\n    return pd.DataFrame.from_dict(\n        data,\n        orient=\"index\",\n        columns=[\"measured\", \"valid\"],\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.get_nchoosek_combinations","title":"<code>get_nchoosek_combinations(exhaustive=False)</code>","text":"<p>Get all possible NChooseK combinations</p> <p>Parameters:</p> Name Type Description Default <code>exhaustive</code> <code>bool</code> <p>if True all combinations are returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>(used_features_list, unused_features_list)</code> <p>used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination.</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def get_nchoosek_combinations(self, exhaustive: bool = False):\n    \"\"\"Get all possible NChooseK combinations\n\n    Args:\n        exhaustive (bool, optional): if True all combinations are returned. Defaults to False.\n\n    Returns:\n        Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination.\n            unused_features_list is a list of lists containing features unused in each NChooseK combination.\n\n    \"\"\"\n    if len(self.constraints.get(NChooseKConstraint)) == 0:\n        used_continuous_features = self.inputs.get_keys(ContinuousInput)\n        return used_continuous_features, []\n\n    used_features_list_all = []\n\n    # loops through each NChooseK constraint\n    for con in self.constraints.get(NChooseKConstraint):\n        assert isinstance(con, NChooseKConstraint)\n        used_features_list = []\n\n        if exhaustive:\n            for n in range(con.min_count, con.max_count + 1):\n                used_features_list.extend(itertools.combinations(con.features, n))\n\n            if con.none_also_valid:\n                used_features_list.append(())\n        else:\n            used_features_list.extend(\n                itertools.combinations(con.features, con.max_count),\n            )\n\n        used_features_list_all.append(used_features_list)\n\n    used_features_list_all = list(\n        itertools.product(*used_features_list_all),\n    )  # product between NChooseK constraints\n\n    # format into a list of used features\n    used_features_list_formatted = []\n    for used_features_list in used_features_list_all:\n        used_features_list_flattened = [\n            item for sublist in used_features_list for item in sublist\n        ]\n        used_features_list_formatted.append(list(set(used_features_list_flattened)))\n\n    # sort lists\n    used_features_list_sorted = []\n    for used_features in used_features_list_formatted:\n        used_features_list_sorted.append(sorted(used_features))\n\n    # drop duplicates\n    used_features_list_no_dup = []\n    for used_features in used_features_list_sorted:\n        if used_features not in used_features_list_no_dup:\n            used_features_list_no_dup.append(used_features)\n\n    # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\")\n\n    # remove combinations not fulfilling constraints\n    used_features_list_final = []\n    for combo in used_features_list_no_dup:\n        fulfil_constraints = []  # list of bools tracking if constraints are fulfilled\n        for con in self.constraints.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            count = 0  # count of features in combo that are in con.features\n            for f in combo:\n                if f in con.features:\n                    count += 1\n            if (\n                count &gt;= con.min_count\n                and count &lt;= con.max_count\n                or count == 0\n                and con.none_also_valid\n            ):\n                fulfil_constraints.append(True)\n            else:\n                fulfil_constraints.append(False)\n        if np.all(fulfil_constraints):\n            used_features_list_final.append(combo)\n\n    # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\")\n\n    # features unused\n    features_in_cc = []\n    for con in self.constraints.get(NChooseKConstraint):\n        assert isinstance(con, NChooseKConstraint)\n        features_in_cc.extend(con.features)\n    features_in_cc = list(set(features_in_cc))\n    features_in_cc.sort()\n    unused_features_list = []\n    for used_features in used_features_list_final:\n        unused_features_list.append(\n            [f_key for f_key in features_in_cc if f_key not in used_features],\n        )\n\n    # postprocess\n    # used_features_list_final2 = []\n    # unused_features_list2 = []\n    # for used, unused in zip(used_features_list_final,unused_features_list):\n    #     if len(used) == 3:\n    #         used_features_list_final2.append(used), unused_features_list2.append(unused)\n\n    return used_features_list_final, unused_features_list\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_candidates","title":"<code>validate_candidates(candidates, only_inputs=False, tol=1e-05, raise_validation_error=True)</code>","text":"<p>Method to check the validty of proposed candidates</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>DataFrame</code> <p>Dataframe with suggested new experiments (candidates)</p> required <code>only_inputs</code> <code>(bool, optional)</code> <p>If True, only the input columns are validated. Defaults to False.</p> <code>False</code> <code>tol</code> <code>(float, optional)</code> <p>tolerance parameter for constraints. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 1e-6.</p> <code>1e-05</code> <code>raise_validation_error</code> <code>bool</code> <p>If true an error will be raised if candidates violate constraints, otherwise only a warning will be displayed. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when a column is missing for a defined input feature</p> <code>ValueError</code> <p>when a column is missing for a defined output feature</p> <code>ValueError</code> <p>when a non-numerical value is proposed</p> <code>ValueError</code> <p>when an additional column is found</p> <code>ConstraintNotFulfilledError</code> <p>when the constraints are not fulfilled and <code>raise_validation_error = True</code></p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: dataframe with suggested experiments (candidates)</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def validate_candidates(\n    self,\n    candidates: pd.DataFrame,\n    only_inputs: bool = False,\n    tol: float = 1e-5,\n    raise_validation_error: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Method to check the validty of proposed candidates\n\n    Args:\n        candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates)\n        only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False.\n        tol (float,optional): tolerance parameter for constraints. A constraint is considered as not fulfilled if the violation\n            is larger than tol. Defaults to 1e-6.\n        raise_validation_error (bool, optional): If true an error will be raised if candidates violate constraints,\n            otherwise only a warning will be displayed. Defaults to True.\n\n    Raises:\n        ValueError: when a column is missing for a defined input feature\n        ValueError: when a column is missing for a defined output feature\n        ValueError: when a non-numerical value is proposed\n        ValueError: when an additional column is found\n        ConstraintNotFulfilledError: when the constraints are not fulfilled and `raise_validation_error = True`\n\n    Returns:\n        pd.DataFrame: dataframe with suggested experiments (candidates)\n\n    \"\"\"\n    # check that each input feature has a col and is valid in itself\n    assert isinstance(self.inputs, Inputs)\n    candidates = self.inputs.validate_candidates(candidates)\n    # check if all constraints are fulfilled\n    if not self.constraints.is_fulfilled(candidates, tol=tol).all():\n        if raise_validation_error:\n            raise ConstraintNotFulfilledError(\n                f\"Constraints not fulfilled: {candidates}\",\n            )\n        warnings.warn(\"Not all constraints are fulfilled.\")\n    # for each continuous output feature with an attached objective object\n    if not only_inputs:\n        assert isinstance(self.outputs, Outputs)\n        candidates = self.outputs.validate_candidates(candidates=candidates)\n    return candidates\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_constraints","title":"<code>validate_constraints()</code>","text":"<p>Validate that the constraints defined in the domain fit to the input features.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[Constraint]</code> <p>List of constraints or empty if no constraints are defined</p> required <code>values</code> <code>List[Input]</code> <p>List of input features of the domain</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Feature key in constraint is unknown.</p> <p>Returns:</p> Type Description <p>List[Constraint]: List of constraints defined for the domain</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_constraints(self):\n    \"\"\"Validate that the constraints defined in the domain fit to the input features.\n\n    Args:\n        v (List[Constraint]): List of constraints or empty if no constraints are defined\n        values (List[Input]): List of input features of the domain\n\n    Raises:\n        ValueError: Feature key in constraint is unknown.\n\n    Returns:\n        List[Constraint]: List of constraints defined for the domain\n\n    \"\"\"\n    for c in self.constraints.get():\n        c.validate_inputs(self.inputs)\n    return self\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_experiments","title":"<code>validate_experiments(experiments, strict=False)</code>","text":"<p>Checks the experimental data on validity</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>empty dataframe</p> <code>ValueError</code> <p>the column for a specific feature is missing the provided data</p> <code>ValueError</code> <p>there are labcodes with null value</p> <code>ValueError</code> <p>there are labcodes with nan value</p> <code>ValueError</code> <p>labcodes are not unique</p> <code>ValueError</code> <p>the provided columns do no match to the defined domain</p> <code>ValueError</code> <p>the provided columns do no match to the defined domain</p> <code>ValueError</code> <p>Input with null values</p> <code>ValueError</code> <p>Input with nan values</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The provided dataframe with experimental data</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def validate_experiments(\n    self,\n    experiments: pd.DataFrame,\n    strict: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Checks the experimental data on validity\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n        strict (bool, optional): Boolean to distinguish if the occurrence of\n            fixed features in the dataset should be considered or not.\n            Defaults to False.\n\n    Raises:\n        ValueError: empty dataframe\n        ValueError: the column for a specific feature is missing the provided data\n        ValueError: there are labcodes with null value\n        ValueError: there are labcodes with nan value\n        ValueError: labcodes are not unique\n        ValueError: the provided columns do no match to the defined domain\n        ValueError: the provided columns do no match to the defined domain\n        ValueError: Input with null values\n        ValueError: Input with nan values\n\n    Returns:\n        pd.DataFrame: The provided dataframe with experimental data\n\n    \"\"\"\n    if len(experiments) == 0:\n        raise ValueError(\"no experiments provided (empty dataframe)\")\n\n    # we allow here for a column named labcode used to identify experiments\n    if \"labcode\" in experiments.columns:\n        # test that labcodes are not na\n        if experiments.labcode.isnull().to_numpy().any():\n            raise ValueError(\"there are labcodes with null value\")\n        if experiments.labcode.isna().to_numpy().any():\n            raise ValueError(\"there are labcodes with nan value\")\n        # test that labcodes are distinct\n        if (\n            len(set(experiments.labcode.to_numpy().tolist()))\n            != experiments.shape[0]\n        ):\n            raise ValueError(\"labcodes are not unique\")\n\n    # run the individual validators\n    experiments = self.inputs.validate_experiments(\n        experiments=experiments,\n        strict=strict,\n    )\n    experiments = self.outputs.validate_experiments(experiments=experiments)\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_unique_feature_keys","title":"<code>validate_unique_feature_keys()</code>","text":"<p>Validates if provided input and output feature keys are unique</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Outputs</code> <p>List of all output features of the domain.</p> required <code>value</code> <code>Dict[str, Inputs]</code> <p>Dict containing a list of input features as single entry.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Feature keys are not unique.</p> <p>Returns:</p> Name Type Description <code>Outputs</code> <p>Keeps output features as given.</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_unique_feature_keys(self):\n    \"\"\"Validates if provided input and output feature keys are unique\n\n    Args:\n        v (Outputs): List of all output features of the domain.\n        value (Dict[str, Inputs]): Dict containing a list of input features as single entry.\n\n    Raises:\n        ValueError: Feature keys are not unique.\n\n    Returns:\n        Outputs: Keeps output features as given.\n\n    \"\"\"\n    keys = self.outputs.get_keys() + self.inputs.get_keys()\n    if len(set(keys)) != len(keys):\n        raise ValueError(\"Feature keys are not unique\")\n    return self\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features","title":"<code>features</code>","text":""},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs","title":"<code>Inputs</code>","text":"<p>               Bases: <code>_BaseFeatures[AnyInput]</code></p> <p>Container of input features, only input features are allowed.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List(Inputs</code> <p>list of the features.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>class Inputs(_BaseFeatures[AnyInput]):\n    \"\"\"Container of input features, only input features are allowed.\n\n    Attributes:\n        features (List(Inputs)): list of the features.\n\n    \"\"\"\n\n    type: Literal[\"Inputs\"] = \"Inputs\"  # type: ignore\n\n    @field_validator(\"features\")\n    @classmethod\n    def validate_only_one_task_input(cls, features: Sequence[AnyInput]):\n        filtered = filter_by_class(\n            features,\n            includes=TaskInput,\n            excludes=None,\n            exact=False,\n        )\n        if len(filtered) &gt; 1:\n            raise ValueError(f\"Only one `TaskInput` is allowed, got {len(filtered)}.\")\n        return features\n\n    def get_fixed(self) -&gt; Inputs:\n        \"\"\"Gets all features in `self` that are fixed and returns them as new\n        `Inputs` object.\n\n        Returns:\n            Inputs: Input features object containing only fixed features.\n\n        \"\"\"\n        return Inputs(features=[feat for feat in self if feat.is_fixed()])\n\n    def get_free(self) -&gt; Inputs:\n        \"\"\"Gets all features in `self` that are not fixed and returns them as\n        new `Inputs` object.\n\n        Returns:\n            Inputs: Input features object containing only non-fixed features.\n\n        \"\"\"\n        return Inputs(features=[feat for feat in self if not feat.is_fixed()])\n\n    @validate_call\n    def sample(\n        self,\n        n: int = 1,\n        method: SamplingMethodEnum = SamplingMethodEnum.UNIFORM,\n        seed: Optional[int] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Draw sobol samples\n\n        Args:\n            n (int, optional): Number of samples, has to be larger than 0.\n                Defaults to 1.\n            method (SamplingMethodEnum, optional): Method to use, implemented\n                methods are `UNIFORM`, `SOBOL` and `LHS`. Defaults to `UNIFORM`.\n            reference_value\n            seed (int, optional): random seed. Defaults to None.\n\n        Returns:\n            pd.DataFrame: Dataframe containing the samples.\n\n        \"\"\"\n        if len(self) == 0:\n            return pd.DataFrame()\n\n        if method == SamplingMethodEnum.UNIFORM:\n            # we cannot just propagate the provided seed to the sample methods\n            # as they would then sample always the same value if the bounds\n            # are the same for a feature.\n            rng = np.random.default_rng(seed=seed)\n            return self.validate_candidates(\n                pd.concat(\n                    [\n                        feat.sample(n, seed=int(rng.integers(1, 1000000)))\n                        for feat in self.get(Input)\n                    ],\n                    axis=1,\n                ),\n            )\n\n        free_features = self.get_free()\n        if method == SamplingMethodEnum.SOBOL:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                X = Sobol(len(free_features), seed=seed).random(n)\n        else:\n            X = LatinHypercube(len(free_features), seed=seed).random(n)\n\n        res = []\n        for i, feat in enumerate(free_features):\n            if isinstance(feat, ContinuousInput):\n                x = feat.from_unit_range(X[:, i])\n            elif isinstance(feat, (DiscreteInput, CategoricalInput)):\n                levels = (\n                    feat.values\n                    if isinstance(feat, DiscreteInput)\n                    else feat.get_allowed_categories()\n                )\n                bins = np.linspace(0, 1, len(levels) + 1)\n                idx = np.digitize(X[:, i], bins) - 1\n                x = np.array(levels)[idx]\n            else:\n                raise ValueError(\n                    f\"Unknown input feature with key {feat.key} of type {feat.type}\",\n                )\n            res.append(pd.Series(x, name=feat.key))\n\n        samples = pd.concat(res, axis=1)\n\n        for feat in self.get_fixed():\n            samples[feat.key] = feat.fixed_value()[0]  # type: ignore\n\n        return self.validate_candidates(samples)[self.get_keys(Input)]\n\n    def validate_candidates(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Validate a pandas dataframe with input feature values.\n\n        Args:\n            candidates (pd.Dataframe): Inputs to validate.\n\n        Raises:\n            ValueError: Raises a Valueerror if a feature based validation raises an exception.\n\n        Returns:\n            pd.Dataframe: Validated dataframe\n\n        \"\"\"\n        for feature in self:\n            if feature.key not in candidates:\n                raise ValueError(f\"no col for input feature `{feature.key}`\")\n            candidates[feature.key] = feature.validate_candidental(\n                candidates[feature.key],\n            )\n        if candidates[self.get_keys()].isnull().to_numpy().any():\n            raise ValueError(\"there are null values\")\n        if candidates[self.get_keys()].isna().to_numpy().any():\n            raise ValueError(\"there are na values\")\n        return candidates\n\n    def validate_experiments(\n        self,\n        experiments: pd.DataFrame,\n        strict=False,\n    ) -&gt; pd.DataFrame:\n        for feature in self:\n            if feature.key not in experiments:\n                raise ValueError(f\"no col for input feature `{feature.key}`\")\n            experiments[feature.key] = feature.validate_experimental(\n                experiments[feature.key],\n                strict=strict,\n            )\n        if experiments[self.get_keys()].isnull().to_numpy().any():\n            raise ValueError(\"there are null values\")\n        if experiments[self.get_keys()].isna().to_numpy().any():\n            raise ValueError(\"there are na values\")\n        return experiments\n\n    def get_categorical_combinations(\n        self,\n        include: Union[Type, List[Type]] = Input,\n        exclude: Union[Type, List[Type]] = None,  # type: ignore\n    ):\n        \"\"\"Get a list of tuples pairing the feature keys with a list of valid categories\n\n        Args:\n            include (Feature, optional): Features to be included. Defaults to Input.\n            exclude (Feature, optional): Features to be excluded, e.g. subclasses\n                of the included features. Defaults to None.\n\n        Returns:\n            List[(str, List[str])]: Returns a list of tuples pairing the feature\n                keys with a list of valid categories (str)\n\n        \"\"\"\n        features = [\n            f\n            for f in self.get(includes=include, excludes=exclude)\n            if (isinstance(f, CategoricalInput) and not f.is_fixed())\n        ]\n        list_of_lists = [\n            [(f.key, cat) for cat in f.get_allowed_categories()] for f in features\n        ]\n\n        discretes = [\n            f\n            for f in self.get(includes=include, excludes=exclude)\n            if (isinstance(f, DiscreteInput) and not f.is_fixed())\n        ]\n\n        list_of_lists_2 = [[(d.key, v) for v in d.values] for d in discretes]\n\n        list_of_lists = list_of_lists + list_of_lists_2\n\n        return list(itertools.product(*list_of_lists))\n\n    # transformation related methods\n    def _get_transform_info(\n        self,\n        specs: InputTransformSpecs,\n    ) -&gt; Tuple[Dict[str, Tuple[int]], Dict[str, Tuple[str]]]:\n        \"\"\"Generates two dictionaries. The first one specifies which key is mapped to\n        which column indices when applying `transform`. The second one specifies\n        which key is mapped to which transformed keys.\n\n        Args:\n            specs (InputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n\n        Returns:\n            Dict[str, Tuple[int]]: Dictionary mapping feature keys to column indices.\n            Dict[str, Tuple[str]]: Dictionary mapping feature keys to transformed feature\n                keys.\n\n        \"\"\"\n        self._validate_transform_specs(specs)\n        features2idx = {}\n        features2names = {}\n        counter = 0\n        for _, feat in enumerate(self.get()):\n            if feat.key not in specs.keys():\n                features2idx[feat.key] = (counter,)\n                features2names[feat.key] = (feat.key,)\n                counter += 1\n            elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n                assert isinstance(feat, CategoricalInput)\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(feat.categories))) + counter).tolist(),\n                )\n                features2names[feat.key] = tuple(\n                    [get_encoded_name(feat.key, c) for c in feat.categories],\n                )\n                counter += len(feat.categories)\n            elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n                features2idx[feat.key] = (counter,)\n                features2names[feat.key] = (feat.key,)\n                counter += 1\n            elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n                assert isinstance(feat, CategoricalInput)\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(feat.categories) - 1)) + counter).tolist(),\n                )\n                features2names[feat.key] = tuple(\n                    [get_encoded_name(feat.key, c) for c in feat.categories[1:]],\n                )\n                counter += len(feat.categories) - 1\n            elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n                assert isinstance(feat, CategoricalDescriptorInput)\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(feat.descriptors))) + counter).tolist(),\n                )\n                features2names[feat.key] = tuple(\n                    [get_encoded_name(feat.key, d) for d in feat.descriptors],\n                )\n                counter += len(feat.descriptors)\n            elif isinstance(specs[feat.key], MolFeatures):\n                assert isinstance(feat, MolecularInput)\n                descriptor_names = specs[feat.key].get_descriptor_names()  # type: ignore\n                features2idx[feat.key] = tuple(\n                    (np.array(range(len(descriptor_names))) + counter).tolist(),\n                )\n                features2names[feat.key] = tuple(\n                    [get_encoded_name(feat.key, d) for d in descriptor_names],\n                )\n                counter += len(descriptor_names)\n        return features2idx, features2names\n\n    def transform(\n        self,\n        experiments: pd.DataFrame,\n        specs: InputTransformSpecs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Transform a dataframe to the representation specified in `specs`.\n\n        Currently only input categoricals are supported.\n\n        Args:\n            experiments (pd.DataFrame): Data dataframe to be transformed.\n            specs (InputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n\n        Returns:\n            pd.DataFrame: Transformed dataframe. Only input features are included.\n\n        \"\"\"\n        # TODO: clean this up and move it into the individual classes\n        specs = self._validate_transform_specs(specs)\n        transformed = []\n        for feat in self.get():\n            s = experiments[feat.key]\n            if feat.key not in specs.keys():\n                transformed.append(s)\n            elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.to_onehot_encoding(s))\n            elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.to_ordinal_encoding(s))\n            elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.to_dummy_encoding(s))\n            elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n                assert isinstance(feat, CategoricalDescriptorInput)\n                transformed.append(feat.to_descriptor_encoding(s))\n            elif isinstance(specs[feat.key], MolFeatures):\n                assert isinstance(feat, MolecularInput)\n                transformed.append(feat.to_descriptor_encoding(specs[feat.key], s))  # type: ignore\n        return pd.concat(transformed, axis=1)\n\n    def inverse_transform(\n        self,\n        experiments: pd.DataFrame,\n        specs: InputTransformSpecs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Transform a dataframe back to the original representations.\n\n        The original applied transformation has to be provided via the specs dictionary.\n        Currently only input categoricals are supported.\n\n        Args:\n            experiments (pd.DataFrame): Transformed data dataframe.\n            specs (InputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n\n        Returns:\n            pd.DataFrame: Back transformed dataframe. Only input features are included.\n\n        \"\"\"\n        # TODO: clean this up and move it into the individual classes\n        self._validate_transform_specs(specs=specs)\n        transformed = []\n        for feat in self.get():\n            if isinstance(feat, DiscreteInput):\n                transformed.append(feat.from_continuous(experiments))\n            elif feat.key not in specs.keys():\n                transformed.append(experiments[feat.key])\n            elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.from_onehot_encoding(experiments))\n            elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(\n                    feat.from_ordinal_encoding(experiments[feat.key].astype(int)),\n                )\n            elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n                assert isinstance(feat, CategoricalInput)\n                transformed.append(feat.from_dummy_encoding(experiments))\n            elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n                assert isinstance(feat, CategoricalDescriptorInput)\n                transformed.append(feat.from_descriptor_encoding(experiments))\n            elif isinstance(specs[feat.key], MolFeatures):\n                assert isinstance(feat, CategoricalMolecularInput)\n                transformed.append(\n                    feat.from_descriptor_encoding(specs[feat.key], experiments),  # type: ignore\n                )\n\n        return pd.concat(transformed, axis=1)\n\n    def _validate_transform_specs(\n        self,\n        specs: InputTransformSpecs,\n    ) -&gt; InputTransformSpecs:\n        \"\"\"Checks the validity of the transform specs .\n\n        Args:\n            specs (InputTransformSpecs): Transform specs to be validated.\n\n        \"\"\"\n        # first check that the keys in the specs dict are correct also correct feature keys\n        # next check that all values are of type CategoricalEncodingEnum or MolFeatures\n        for key, value in specs.items():\n            try:\n                feat = self.get_by_key(key)\n            except KeyError:\n                raise ValueError(\n                    f\"Unknown feature with key {key} specified in transform specs.\",\n                )\n            # TODO\n            # this is ugly, on the long run we have to get rid of the transform enums\n            # and replace them with classes, then the following lines collapse into just two\n            assert isinstance(feat, Input)\n            enums = [t for t in feat.valid_transform_types() if isinstance(t, Enum)]\n            no_enums = [\n                t for t in feat.valid_transform_types() if not isinstance(t, Enum)\n            ]\n            if isinstance(value, Enum):\n                if value not in enums:\n                    raise ValueError(\n                        f\"Forbidden transform type for feature with key {key}\",\n                    )\n            else:\n                if len(no_enums) == 0:\n                    raise ValueError(\n                        f\"Forbidden transform type for feature with key {key}\",\n                    )\n                if not isinstance(value, tuple(no_enums)):  # type: ignore\n                    raise ValueError(\n                        f\"Forbidden transform type for feature with key {key}\",\n                    )\n\n        return specs\n\n    def get_bounds(\n        self,\n        specs: InputTransformSpecs,\n        experiments: Optional[pd.DataFrame] = None,\n        reference_experiment: Optional[pd.Series] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Returns the boundaries of the optimization problem based on the transformations\n        defined in the  `specs` dictionary.\n\n        Args:\n            specs (InputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n            experiments (Optional[pd.DataFrame], optional): Dataframe with input features.\n                If provided the real feature bounds are returned based on both the opt.\n                feature bounds and the extreme points in the dataframe. Defaults to None,\n            reference_experiment (Optional[pd.Serues], optional): If a reference experiment provided,\n            then the local bounds based on a local search region are provided as reference to the\n                reference experiment. Currently only supported for continuous inputs.\n                For more details, it is referred to https://www.merl.com/publications/docs/TR2023-057.pdf. Defaults to None.\n\n        Raises:\n            ValueError: If a feature type is not known.\n            ValueError: If no transformation is provided for a categorical feature.\n\n        Returns:\n            Tuple[List[float], List[float]]: list with lower bounds, list with upper bounds.\n\n        \"\"\"\n        if reference_experiment is not None and experiments is not None:\n            raise ValueError(\n                \"Only one can be used, `reference_experiments` or `experiments`.\",\n            )\n\n        self._validate_transform_specs(specs=specs)\n\n        lower = []\n        upper = []\n\n        for feat in self.get():\n            assert isinstance(feat, Input)\n            lo, up = feat.get_bounds(\n                transform_type=specs.get(feat.key),  # type: ignore\n                values=experiments[feat.key] if experiments is not None else None,  # type: ignore\n                reference_value=(\n                    reference_experiment[feat.key]\n                    if reference_experiment is not None\n                    else None\n                ),\n            )\n            lower += lo\n            upper += up\n        return lower, upper\n\n    def get_feature_indices(\n        self,\n        specs: InputTransformSpecs,\n        feature_keys: List[str],\n    ) -&gt; List[int]:\n        \"\"\"Returns a list of indices of the given feature key list.\n\n        Args:\n            specs (InputTransformSpecs): Dictionary specifying which\n                input feature is transformed by which encoder.\n            feature_keys (List[str]): List of feature keys.\n\n        Returns:\n            List[int]: The list of indices.\n\n        \"\"\"\n        features2idx, _ = self._get_transform_info(specs)\n        return sorted(\n            itertools.chain.from_iterable(\n                [features2idx[feat] for feat in feature_keys]\n            ),\n        )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.get_bounds","title":"<code>get_bounds(specs, experiments=None, reference_experiment=None)</code>","text":"<p>Returns the boundaries of the optimization problem based on the transformations defined in the  <code>specs</code> dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>InputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <code>experiments</code> <code>Optional[DataFrame]</code> <p>Dataframe with input features. If provided the real feature bounds are returned based on both the opt. feature bounds and the extreme points in the dataframe. Defaults to None,</p> <code>None</code> <code>reference_experiment</code> <code>Optional[Serues]</code> <p>If a reference experiment provided,</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a feature type is not known.</p> <code>ValueError</code> <p>If no transformation is provided for a categorical feature.</p> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>Tuple[List[float], List[float]]: list with lower bounds, list with upper bounds.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_bounds(\n    self,\n    specs: InputTransformSpecs,\n    experiments: Optional[pd.DataFrame] = None,\n    reference_experiment: Optional[pd.Series] = None,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Returns the boundaries of the optimization problem based on the transformations\n    defined in the  `specs` dictionary.\n\n    Args:\n        specs (InputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n        experiments (Optional[pd.DataFrame], optional): Dataframe with input features.\n            If provided the real feature bounds are returned based on both the opt.\n            feature bounds and the extreme points in the dataframe. Defaults to None,\n        reference_experiment (Optional[pd.Serues], optional): If a reference experiment provided,\n        then the local bounds based on a local search region are provided as reference to the\n            reference experiment. Currently only supported for continuous inputs.\n            For more details, it is referred to https://www.merl.com/publications/docs/TR2023-057.pdf. Defaults to None.\n\n    Raises:\n        ValueError: If a feature type is not known.\n        ValueError: If no transformation is provided for a categorical feature.\n\n    Returns:\n        Tuple[List[float], List[float]]: list with lower bounds, list with upper bounds.\n\n    \"\"\"\n    if reference_experiment is not None and experiments is not None:\n        raise ValueError(\n            \"Only one can be used, `reference_experiments` or `experiments`.\",\n        )\n\n    self._validate_transform_specs(specs=specs)\n\n    lower = []\n    upper = []\n\n    for feat in self.get():\n        assert isinstance(feat, Input)\n        lo, up = feat.get_bounds(\n            transform_type=specs.get(feat.key),  # type: ignore\n            values=experiments[feat.key] if experiments is not None else None,  # type: ignore\n            reference_value=(\n                reference_experiment[feat.key]\n                if reference_experiment is not None\n                else None\n            ),\n        )\n        lower += lo\n        upper += up\n    return lower, upper\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.get_categorical_combinations","title":"<code>get_categorical_combinations(include=Input, exclude=None)</code>","text":"<p>Get a list of tuples pairing the feature keys with a list of valid categories</p> <p>Parameters:</p> Name Type Description Default <code>include</code> <code>Feature</code> <p>Features to be included. Defaults to Input.</p> <code>Input</code> <code>exclude</code> <code>Feature</code> <p>Features to be excluded, e.g. subclasses of the included features. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>List[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str)</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_categorical_combinations(\n    self,\n    include: Union[Type, List[Type]] = Input,\n    exclude: Union[Type, List[Type]] = None,  # type: ignore\n):\n    \"\"\"Get a list of tuples pairing the feature keys with a list of valid categories\n\n    Args:\n        include (Feature, optional): Features to be included. Defaults to Input.\n        exclude (Feature, optional): Features to be excluded, e.g. subclasses\n            of the included features. Defaults to None.\n\n    Returns:\n        List[(str, List[str])]: Returns a list of tuples pairing the feature\n            keys with a list of valid categories (str)\n\n    \"\"\"\n    features = [\n        f\n        for f in self.get(includes=include, excludes=exclude)\n        if (isinstance(f, CategoricalInput) and not f.is_fixed())\n    ]\n    list_of_lists = [\n        [(f.key, cat) for cat in f.get_allowed_categories()] for f in features\n    ]\n\n    discretes = [\n        f\n        for f in self.get(includes=include, excludes=exclude)\n        if (isinstance(f, DiscreteInput) and not f.is_fixed())\n    ]\n\n    list_of_lists_2 = [[(d.key, v) for v in d.values] for d in discretes]\n\n    list_of_lists = list_of_lists + list_of_lists_2\n\n    return list(itertools.product(*list_of_lists))\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.get_feature_indices","title":"<code>get_feature_indices(specs, feature_keys)</code>","text":"<p>Returns a list of indices of the given feature key list.</p> <p>Parameters:</p> Name Type Description Default <code>specs</code> <code>InputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <code>feature_keys</code> <code>List[str]</code> <p>List of feature keys.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: The list of indices.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_feature_indices(\n    self,\n    specs: InputTransformSpecs,\n    feature_keys: List[str],\n) -&gt; List[int]:\n    \"\"\"Returns a list of indices of the given feature key list.\n\n    Args:\n        specs (InputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n        feature_keys (List[str]): List of feature keys.\n\n    Returns:\n        List[int]: The list of indices.\n\n    \"\"\"\n    features2idx, _ = self._get_transform_info(specs)\n    return sorted(\n        itertools.chain.from_iterable(\n            [features2idx[feat] for feat in feature_keys]\n        ),\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.get_fixed","title":"<code>get_fixed()</code>","text":"<p>Gets all features in <code>self</code> that are fixed and returns them as new <code>Inputs</code> object.</p> <p>Returns:</p> Name Type Description <code>Inputs</code> <code>Inputs</code> <p>Input features object containing only fixed features.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_fixed(self) -&gt; Inputs:\n    \"\"\"Gets all features in `self` that are fixed and returns them as new\n    `Inputs` object.\n\n    Returns:\n        Inputs: Input features object containing only fixed features.\n\n    \"\"\"\n    return Inputs(features=[feat for feat in self if feat.is_fixed()])\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.get_free","title":"<code>get_free()</code>","text":"<p>Gets all features in <code>self</code> that are not fixed and returns them as new <code>Inputs</code> object.</p> <p>Returns:</p> Name Type Description <code>Inputs</code> <code>Inputs</code> <p>Input features object containing only non-fixed features.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_free(self) -&gt; Inputs:\n    \"\"\"Gets all features in `self` that are not fixed and returns them as\n    new `Inputs` object.\n\n    Returns:\n        Inputs: Input features object containing only non-fixed features.\n\n    \"\"\"\n    return Inputs(features=[feat for feat in self if not feat.is_fixed()])\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.inverse_transform","title":"<code>inverse_transform(experiments, specs)</code>","text":"<p>Transform a dataframe back to the original representations.</p> <p>The original applied transformation has to be provided via the specs dictionary. Currently only input categoricals are supported.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Transformed data dataframe.</p> required <code>specs</code> <code>InputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Back transformed dataframe. Only input features are included.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def inverse_transform(\n    self,\n    experiments: pd.DataFrame,\n    specs: InputTransformSpecs,\n) -&gt; pd.DataFrame:\n    \"\"\"Transform a dataframe back to the original representations.\n\n    The original applied transformation has to be provided via the specs dictionary.\n    Currently only input categoricals are supported.\n\n    Args:\n        experiments (pd.DataFrame): Transformed data dataframe.\n        specs (InputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n\n    Returns:\n        pd.DataFrame: Back transformed dataframe. Only input features are included.\n\n    \"\"\"\n    # TODO: clean this up and move it into the individual classes\n    self._validate_transform_specs(specs=specs)\n    transformed = []\n    for feat in self.get():\n        if isinstance(feat, DiscreteInput):\n            transformed.append(feat.from_continuous(experiments))\n        elif feat.key not in specs.keys():\n            transformed.append(experiments[feat.key])\n        elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.from_onehot_encoding(experiments))\n        elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(\n                feat.from_ordinal_encoding(experiments[feat.key].astype(int)),\n            )\n        elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.from_dummy_encoding(experiments))\n        elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n            assert isinstance(feat, CategoricalDescriptorInput)\n            transformed.append(feat.from_descriptor_encoding(experiments))\n        elif isinstance(specs[feat.key], MolFeatures):\n            assert isinstance(feat, CategoricalMolecularInput)\n            transformed.append(\n                feat.from_descriptor_encoding(specs[feat.key], experiments),  # type: ignore\n            )\n\n    return pd.concat(transformed, axis=1)\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.sample","title":"<code>sample(n=1, method=SamplingMethodEnum.UNIFORM, seed=None)</code>","text":"<p>Draw sobol samples</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples, has to be larger than 0. Defaults to 1.</p> <code>1</code> <code>method</code> <code>SamplingMethodEnum</code> <p>Method to use, implemented methods are <code>UNIFORM</code>, <code>SOBOL</code> and <code>LHS</code>. Defaults to <code>UNIFORM</code>.</p> <code>UNIFORM</code> <code>seed</code> <code>int</code> <p>random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe containing the samples.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>@validate_call\ndef sample(\n    self,\n    n: int = 1,\n    method: SamplingMethodEnum = SamplingMethodEnum.UNIFORM,\n    seed: Optional[int] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Draw sobol samples\n\n    Args:\n        n (int, optional): Number of samples, has to be larger than 0.\n            Defaults to 1.\n        method (SamplingMethodEnum, optional): Method to use, implemented\n            methods are `UNIFORM`, `SOBOL` and `LHS`. Defaults to `UNIFORM`.\n        reference_value\n        seed (int, optional): random seed. Defaults to None.\n\n    Returns:\n        pd.DataFrame: Dataframe containing the samples.\n\n    \"\"\"\n    if len(self) == 0:\n        return pd.DataFrame()\n\n    if method == SamplingMethodEnum.UNIFORM:\n        # we cannot just propagate the provided seed to the sample methods\n        # as they would then sample always the same value if the bounds\n        # are the same for a feature.\n        rng = np.random.default_rng(seed=seed)\n        return self.validate_candidates(\n            pd.concat(\n                [\n                    feat.sample(n, seed=int(rng.integers(1, 1000000)))\n                    for feat in self.get(Input)\n                ],\n                axis=1,\n            ),\n        )\n\n    free_features = self.get_free()\n    if method == SamplingMethodEnum.SOBOL:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            X = Sobol(len(free_features), seed=seed).random(n)\n    else:\n        X = LatinHypercube(len(free_features), seed=seed).random(n)\n\n    res = []\n    for i, feat in enumerate(free_features):\n        if isinstance(feat, ContinuousInput):\n            x = feat.from_unit_range(X[:, i])\n        elif isinstance(feat, (DiscreteInput, CategoricalInput)):\n            levels = (\n                feat.values\n                if isinstance(feat, DiscreteInput)\n                else feat.get_allowed_categories()\n            )\n            bins = np.linspace(0, 1, len(levels) + 1)\n            idx = np.digitize(X[:, i], bins) - 1\n            x = np.array(levels)[idx]\n        else:\n            raise ValueError(\n                f\"Unknown input feature with key {feat.key} of type {feat.type}\",\n            )\n        res.append(pd.Series(x, name=feat.key))\n\n    samples = pd.concat(res, axis=1)\n\n    for feat in self.get_fixed():\n        samples[feat.key] = feat.fixed_value()[0]  # type: ignore\n\n    return self.validate_candidates(samples)[self.get_keys(Input)]\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.transform","title":"<code>transform(experiments, specs)</code>","text":"<p>Transform a dataframe to the representation specified in <code>specs</code>.</p> <p>Currently only input categoricals are supported.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Data dataframe to be transformed.</p> required <code>specs</code> <code>InputTransformSpecs</code> <p>Dictionary specifying which input feature is transformed by which encoder.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Transformed dataframe. Only input features are included.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def transform(\n    self,\n    experiments: pd.DataFrame,\n    specs: InputTransformSpecs,\n) -&gt; pd.DataFrame:\n    \"\"\"Transform a dataframe to the representation specified in `specs`.\n\n    Currently only input categoricals are supported.\n\n    Args:\n        experiments (pd.DataFrame): Data dataframe to be transformed.\n        specs (InputTransformSpecs): Dictionary specifying which\n            input feature is transformed by which encoder.\n\n    Returns:\n        pd.DataFrame: Transformed dataframe. Only input features are included.\n\n    \"\"\"\n    # TODO: clean this up and move it into the individual classes\n    specs = self._validate_transform_specs(specs)\n    transformed = []\n    for feat in self.get():\n        s = experiments[feat.key]\n        if feat.key not in specs.keys():\n            transformed.append(s)\n        elif specs[feat.key] == CategoricalEncodingEnum.ONE_HOT:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.to_onehot_encoding(s))\n        elif specs[feat.key] == CategoricalEncodingEnum.ORDINAL:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.to_ordinal_encoding(s))\n        elif specs[feat.key] == CategoricalEncodingEnum.DUMMY:\n            assert isinstance(feat, CategoricalInput)\n            transformed.append(feat.to_dummy_encoding(s))\n        elif specs[feat.key] == CategoricalEncodingEnum.DESCRIPTOR:\n            assert isinstance(feat, CategoricalDescriptorInput)\n            transformed.append(feat.to_descriptor_encoding(s))\n        elif isinstance(specs[feat.key], MolFeatures):\n            assert isinstance(feat, MolecularInput)\n            transformed.append(feat.to_descriptor_encoding(specs[feat.key], s))  # type: ignore\n    return pd.concat(transformed, axis=1)\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Inputs.validate_candidates","title":"<code>validate_candidates(candidates)</code>","text":"<p>Validate a pandas dataframe with input feature values.</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>Dataframe</code> <p>Inputs to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises a Valueerror if a feature based validation raises an exception.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: Validated dataframe</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def validate_candidates(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Validate a pandas dataframe with input feature values.\n\n    Args:\n        candidates (pd.Dataframe): Inputs to validate.\n\n    Raises:\n        ValueError: Raises a Valueerror if a feature based validation raises an exception.\n\n    Returns:\n        pd.Dataframe: Validated dataframe\n\n    \"\"\"\n    for feature in self:\n        if feature.key not in candidates:\n            raise ValueError(f\"no col for input feature `{feature.key}`\")\n        candidates[feature.key] = feature.validate_candidental(\n            candidates[feature.key],\n        )\n    if candidates[self.get_keys()].isnull().to_numpy().any():\n        raise ValueError(\"there are null values\")\n    if candidates[self.get_keys()].isna().to_numpy().any():\n        raise ValueError(\"there are na values\")\n    return candidates\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs","title":"<code>Outputs</code>","text":"<p>               Bases: <code>_BaseFeatures[AnyOutput]</code></p> <p>Container of output features, only output features are allowed.</p> <p>Attributes:</p> Name Type Description <code>features</code> <code>List(Outputs</code> <p>list of the features.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>class Outputs(_BaseFeatures[AnyOutput]):\n    \"\"\"Container of output features, only output features are allowed.\n\n    Attributes:\n        features (List(Outputs)): list of the features.\n\n    \"\"\"\n\n    type: Literal[\"Outputs\"] = \"Outputs\"  # type: ignore\n\n    def get_by_objective(\n        self,\n        includes: Union[\n            List[Type[AbstractObjective]],\n            Type[AbstractObjective],\n            Type[Objective],\n        ] = Objective,\n        excludes: Union[\n            List[Type[AbstractObjective]],\n            Type[AbstractObjective],\n            None,\n        ] = None,\n        exact: bool = False,\n    ) -&gt; Outputs:\n        \"\"\"Get output features filtered by the type of the attached objective.\n\n        Args:\n            includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n                to be returned. Defaults to Objective.\n            excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[AnyOutput]: List of output features fitting to the passed requirements.\n\n        \"\"\"\n        if len(self.features) == 0:\n            return Outputs(features=[])\n        return Outputs(\n            features=sorted(\n                filter_by_attribute(\n                    self.get([ContinuousOutput, CategoricalOutput]).features,\n                    lambda of: of.objective,\n                    includes,\n                    excludes,\n                    exact,\n                ),\n            ),\n        )\n\n    def get_keys_by_objective(\n        self,\n        includes: Union[\n            List[Type[AbstractObjective]],\n            Type[AbstractObjective],\n            Type[Objective],\n        ] = Objective,\n        excludes: Union[\n            List[Type[AbstractObjective]],\n            Type[AbstractObjective],\n            None,\n        ] = None,\n        exact: bool = False,\n    ) -&gt; List[str]:\n        \"\"\"Get keys of output features filtered by the type of the attached objective.\n\n        Args:\n            includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n                to be returned. Defaults to Objective.\n            excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n            exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            List[str]: List of output feature keys fitting to the passed requirements.\n\n        \"\"\"\n        return [f.key for f in self.get_by_objective(includes, excludes, exact)]\n\n    def __call__(\n        self,\n        experiments: pd.DataFrame,\n        experiments_adapt: Optional[pd.DataFrame] = None,\n        predictions: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Evaluate the objective for every feature.\n\n        Args:\n            experiments (pd.DataFrame): Experiments for which the objectives\n                should be evaluated.\n            experiments_adapt (pd.DataFrame, optional): Experimental values\n                which are used to update the objective parameters on the fly.\n                This is for example needed when a `MovingMaximizeSigmoidObjective`\n                is used as this depends on the best experimental value achieved\n                so far. For this reason `experiments_adapt` has to be provided\n                if `predictions=True` ie. that the objectives of candidates\n                are evaluated. Defaults to None.\n            predictions (bool, optional): If True use the prediction columns in\n                the dataframe to calc the desirabilities `f\"{feat.key}_pred`,\n                furthermore `experiments_adapt` has to be provided.\n\n        Returns:\n            pd.DataFrame: Objective values for the experiments of interest.\n\n        \"\"\"\n        if predictions and experiments_adapt is None:\n            raise ValueError(\n                \"If predictions are used, `experiments_adapt` has to be provided.\",\n            )\n        else:\n            experiments_adapt = (\n                experiments if experiments_adapt is None else experiments_adapt\n            )\n\n        desis = pd.concat(\n            [\n                feat(\n                    experiments[f\"{feat.key}_pred\" if predictions else feat.key],\n                    experiments_adapt[feat.key].dropna(),  # type: ignore\n                )\n                for feat in self.features\n                if feat.objective is not None\n                and not isinstance(feat, CategoricalOutput)\n            ]\n            + [\n                (\n                    pd.Series(  # type: ignore\n                        data=feat(\n                            experiments.filter(regex=f\"{feat.key}(.*)_prob\"),  # type: ignore\n                            experiments.filter(regex=f\"{feat.key}(.*)_prob\"),  # type: ignore\n                        ),\n                        name=f\"{feat.key}_pred\",\n                    )\n                    if predictions\n                    else experiments[feat.key]\n                )\n                for feat in self.features\n                if feat.objective is not None and isinstance(feat, CategoricalOutput)\n            ],\n            axis=1,\n        )\n        return desis.rename(\n            {\n                f\"{feat.key}_pred\" if predictions else feat.key: f\"{feat.key}_des\"\n                for feat in self.features\n                if feat.objective is not None\n            },\n            axis=1,\n        )\n\n    def add_valid_columns(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Add the `valid_{feature.key}` columns to the experiments dataframe,\n        in case that they are not present.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe holding the experiments.\n\n        Returns:\n            pd.DataFrame: Dataframe holding the experiments.\n\n        \"\"\"\n        valid_keys = [\n            f\"valid_{output_feature_key}\" for output_feature_key in self.get_keys()\n        ]\n        for valid_key in valid_keys:\n            if valid_key not in experiments:\n                experiments[valid_key] = True\n            else:\n                try:\n                    experiments[valid_key] = (\n                        experiments[valid_key].astype(int).astype(bool)\n                    )\n                except ValueError:\n                    raise ValueError(f\"Column {valid_key} cannot casted to dtype bool.\")\n        return experiments\n\n    def validate_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        for feat in self.get():\n            if feat.key not in experiments:\n                raise ValueError(f\"no col for input feature `{feat.key}`\")\n            experiments[feat.key] = feat.validate_experimental(experiments[feat.key])\n        experiments = self.add_valid_columns(experiments=experiments)\n        return experiments\n\n    def validate_candidates(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:\n        # for each continuous output feature with an attached objective object\n        continuous_cols = list(\n            itertools.chain.from_iterable(\n                [\n                    [f\"{feat.key}_pred\", f\"{feat.key}_sd\", f\"{feat.key}_des\"]\n                    for feat in self.get_by_objective(\n                        includes=Objective,\n                        excludes=ConstrainedCategoricalObjective,\n                    )\n                ]\n                + [\n                    [f\"{key}_pred\", f\"{key}_sd\"]\n                    for key in self.get_keys_by_objective(\n                        excludes=Objective,\n                        includes=None,  # type: ignore\n                    )\n                ],\n            ),\n        )\n        # check that pred, sd, and des cols are specified and numerical\n        for col in continuous_cols:\n            if col not in candidates:\n                raise ValueError(f\"missing column {col}\")\n            try:\n                candidates[col] = pd.to_numeric(candidates[col], errors=\"raise\").astype(\n                    \"float64\",\n                )\n            except ValueError:\n                raise ValueError(f\"Not all values of column `{col}` are numerical.\")\n            if candidates[col].isnull().to_numpy().any():\n                raise ValueError(f\"Nan values are present in {col}.\")\n        # Looping over features allows to check categories objective wise\n        for feat in self.get(CategoricalOutput):\n            cols = [f\"{feat.key}_pred\", f\"{feat.key}_des\"]\n            for col in cols:\n                if col not in candidates:\n                    raise ValueError(f\"missing column {col}\")\n                if col == f\"{feat.key}_pred\":\n                    feat.validate_experimental(candidates[col])\n                # Check sd and desirability\n                elif candidates[col].isnull().to_numpy().any():\n                    raise ValueError(f\"Nan values are present in {col}.\")\n        return candidates\n\n    def preprocess_experiments_one_valid_output(\n        self,\n        output_feature_key: str,\n        experiments: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n            output_feature_key (str): The feature based on which non-valid entries rows are removed\n\n        Returns:\n            pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included\n\n        \"\"\"\n        clean_exp = experiments.loc[\n            (experiments[\"valid_%s\" % output_feature_key] == 1)\n            &amp; (experiments[output_feature_key].notna())\n        ]\n\n        return clean_exp\n\n    def preprocess_experiments_all_valid_outputs(\n        self,\n        experiments: pd.DataFrame,\n        output_feature_keys: Optional[List] = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Method to get a dataframe where non-valid entries of all output feature are removed\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n            output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None.\n\n        Returns:\n            pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included\n\n        \"\"\"\n        if (output_feature_keys is None) or (len(output_feature_keys) == 0):\n            output_feature_keys = self.get_keys(Output)\n\n        clean_exp = experiments.query(\n            \" &amp; \".join([\"(`valid_%s` &gt; 0)\" % key for key in output_feature_keys]),\n        )\n        clean_exp = clean_exp.dropna(subset=output_feature_keys)\n\n        return clean_exp\n\n    def preprocess_experiments_any_valid_output(\n        self,\n        experiments: pd.DataFrame,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Method to get a dataframe where at least one output feature has a valid entry\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n\n        Returns:\n            pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry\n\n        \"\"\"\n        output_feature_keys = self.get_keys(Output)\n\n        # clean_exp = experiments.query(\" or \".join([\"(valid_%s &gt; 0)\" % key for key in output_feature_keys]))\n        # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys]))\n\n        assert experiments is not None\n        clean_exp = experiments.query(\n            \" or \".join(\n                [\n                    \"((`valid_%s` &gt;0) &amp; `%s`.notna())\" % (key, key)\n                    for key in output_feature_keys\n                ],\n            ),\n        )\n        return clean_exp\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.__call__","title":"<code>__call__(experiments, experiments_adapt=None, predictions=False)</code>","text":"<p>Evaluate the objective for every feature.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Experiments for which the objectives should be evaluated.</p> required <code>experiments_adapt</code> <code>DataFrame</code> <p>Experimental values which are used to update the objective parameters on the fly. This is for example needed when a <code>MovingMaximizeSigmoidObjective</code> is used as this depends on the best experimental value achieved so far. For this reason <code>experiments_adapt</code> has to be provided if <code>predictions=True</code> ie. that the objectives of candidates are evaluated. Defaults to None.</p> <code>None</code> <code>predictions</code> <code>bool</code> <p>If True use the prediction columns in the dataframe to calc the desirabilities <code>f\"{feat.key}_pred</code>, furthermore <code>experiments_adapt</code> has to be provided.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Objective values for the experiments of interest.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def __call__(\n    self,\n    experiments: pd.DataFrame,\n    experiments_adapt: Optional[pd.DataFrame] = None,\n    predictions: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Evaluate the objective for every feature.\n\n    Args:\n        experiments (pd.DataFrame): Experiments for which the objectives\n            should be evaluated.\n        experiments_adapt (pd.DataFrame, optional): Experimental values\n            which are used to update the objective parameters on the fly.\n            This is for example needed when a `MovingMaximizeSigmoidObjective`\n            is used as this depends on the best experimental value achieved\n            so far. For this reason `experiments_adapt` has to be provided\n            if `predictions=True` ie. that the objectives of candidates\n            are evaluated. Defaults to None.\n        predictions (bool, optional): If True use the prediction columns in\n            the dataframe to calc the desirabilities `f\"{feat.key}_pred`,\n            furthermore `experiments_adapt` has to be provided.\n\n    Returns:\n        pd.DataFrame: Objective values for the experiments of interest.\n\n    \"\"\"\n    if predictions and experiments_adapt is None:\n        raise ValueError(\n            \"If predictions are used, `experiments_adapt` has to be provided.\",\n        )\n    else:\n        experiments_adapt = (\n            experiments if experiments_adapt is None else experiments_adapt\n        )\n\n    desis = pd.concat(\n        [\n            feat(\n                experiments[f\"{feat.key}_pred\" if predictions else feat.key],\n                experiments_adapt[feat.key].dropna(),  # type: ignore\n            )\n            for feat in self.features\n            if feat.objective is not None\n            and not isinstance(feat, CategoricalOutput)\n        ]\n        + [\n            (\n                pd.Series(  # type: ignore\n                    data=feat(\n                        experiments.filter(regex=f\"{feat.key}(.*)_prob\"),  # type: ignore\n                        experiments.filter(regex=f\"{feat.key}(.*)_prob\"),  # type: ignore\n                    ),\n                    name=f\"{feat.key}_pred\",\n                )\n                if predictions\n                else experiments[feat.key]\n            )\n            for feat in self.features\n            if feat.objective is not None and isinstance(feat, CategoricalOutput)\n        ],\n        axis=1,\n    )\n    return desis.rename(\n        {\n            f\"{feat.key}_pred\" if predictions else feat.key: f\"{feat.key}_des\"\n            for feat in self.features\n            if feat.objective is not None\n        },\n        axis=1,\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.add_valid_columns","title":"<code>add_valid_columns(experiments)</code>","text":"<p>Add the <code>valid_{feature.key}</code> columns to the experiments dataframe, in case that they are not present.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe holding the experiments.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe holding the experiments.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def add_valid_columns(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Add the `valid_{feature.key}` columns to the experiments dataframe,\n    in case that they are not present.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe holding the experiments.\n\n    Returns:\n        pd.DataFrame: Dataframe holding the experiments.\n\n    \"\"\"\n    valid_keys = [\n        f\"valid_{output_feature_key}\" for output_feature_key in self.get_keys()\n    ]\n    for valid_key in valid_keys:\n        if valid_key not in experiments:\n            experiments[valid_key] = True\n        else:\n            try:\n                experiments[valid_key] = (\n                    experiments[valid_key].astype(int).astype(bool)\n                )\n            except ValueError:\n                raise ValueError(f\"Column {valid_key} cannot casted to dtype bool.\")\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.get_by_objective","title":"<code>get_by_objective(includes=Objective, excludes=None, exact=False)</code>","text":"<p>Get output features filtered by the type of the attached objective.</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[List[TObjective], TObjective]</code> <p>Objective class or list of objective classes to be returned. Defaults to Objective.</p> <code>Objective</code> <code>excludes</code> <code>Union[List[TObjective], TObjective, None]</code> <p>Objective class or list of specific objective classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Outputs</code> <p>List[AnyOutput]: List of output features fitting to the passed requirements.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_by_objective(\n    self,\n    includes: Union[\n        List[Type[AbstractObjective]],\n        Type[AbstractObjective],\n        Type[Objective],\n    ] = Objective,\n    excludes: Union[\n        List[Type[AbstractObjective]],\n        Type[AbstractObjective],\n        None,\n    ] = None,\n    exact: bool = False,\n) -&gt; Outputs:\n    \"\"\"Get output features filtered by the type of the attached objective.\n\n    Args:\n        includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n            to be returned. Defaults to Objective.\n        excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[AnyOutput]: List of output features fitting to the passed requirements.\n\n    \"\"\"\n    if len(self.features) == 0:\n        return Outputs(features=[])\n    return Outputs(\n        features=sorted(\n            filter_by_attribute(\n                self.get([ContinuousOutput, CategoricalOutput]).features,\n                lambda of: of.objective,\n                includes,\n                excludes,\n                exact,\n            ),\n        ),\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.get_keys_by_objective","title":"<code>get_keys_by_objective(includes=Objective, excludes=None, exact=False)</code>","text":"<p>Get keys of output features filtered by the type of the attached objective.</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[List[TObjective], TObjective]</code> <p>Objective class or list of objective classes to be returned. Defaults to Objective.</p> <code>Objective</code> <code>excludes</code> <code>Union[List[TObjective], TObjective, None]</code> <p>Objective class or list of specific objective classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of output feature keys fitting to the passed requirements.</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def get_keys_by_objective(\n    self,\n    includes: Union[\n        List[Type[AbstractObjective]],\n        Type[AbstractObjective],\n        Type[Objective],\n    ] = Objective,\n    excludes: Union[\n        List[Type[AbstractObjective]],\n        Type[AbstractObjective],\n        None,\n    ] = None,\n    exact: bool = False,\n) -&gt; List[str]:\n    \"\"\"Get keys of output features filtered by the type of the attached objective.\n\n    Args:\n        includes (Union[List[TObjective], TObjective], optional): Objective class or list of objective classes\n            to be returned. Defaults to Objective.\n        excludes (Union[List[TObjective], TObjective, None], optional): Objective class or list of specific objective classes to be excluded from the return. Defaults to None.\n        exact (bool, optional): Boolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        List[str]: List of output feature keys fitting to the passed requirements.\n\n    \"\"\"\n    return [f.key for f in self.get_by_objective(includes, excludes, exact)]\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.preprocess_experiments_all_valid_outputs","title":"<code>preprocess_experiments_all_valid_outputs(experiments, output_feature_keys=None)</code>","text":"<p>Method to get a dataframe where non-valid entries of all output feature are removed</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <code>output_feature_keys</code> <code>Optional[List]</code> <p>List of output feature keys which should be considered for removal of invalid values. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def preprocess_experiments_all_valid_outputs(\n    self,\n    experiments: pd.DataFrame,\n    output_feature_keys: Optional[List] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Method to get a dataframe where non-valid entries of all output feature are removed\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n        output_feature_keys (Optional[List], optional): List of output feature keys which should be considered for removal of invalid values. Defaults to None.\n\n    Returns:\n        pd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included\n\n    \"\"\"\n    if (output_feature_keys is None) or (len(output_feature_keys) == 0):\n        output_feature_keys = self.get_keys(Output)\n\n    clean_exp = experiments.query(\n        \" &amp; \".join([\"(`valid_%s` &gt; 0)\" % key for key in output_feature_keys]),\n    )\n    clean_exp = clean_exp.dropna(subset=output_feature_keys)\n\n    return clean_exp\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.preprocess_experiments_any_valid_output","title":"<code>preprocess_experiments_any_valid_output(experiments)</code>","text":"<p>Method to get a dataframe where at least one output feature has a valid entry</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def preprocess_experiments_any_valid_output(\n    self,\n    experiments: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Method to get a dataframe where at least one output feature has a valid entry\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n\n    Returns:\n        pd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry\n\n    \"\"\"\n    output_feature_keys = self.get_keys(Output)\n\n    # clean_exp = experiments.query(\" or \".join([\"(valid_%s &gt; 0)\" % key for key in output_feature_keys]))\n    # clean_exp = clean_exp.query(\" or \".join([\"%s.notna()\" % key for key in output_feature_keys]))\n\n    assert experiments is not None\n    clean_exp = experiments.query(\n        \" or \".join(\n            [\n                \"((`valid_%s` &gt;0) &amp; `%s`.notna())\" % (key, key)\n                for key in output_feature_keys\n            ],\n        ),\n    )\n    return clean_exp\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.features.Outputs.preprocess_experiments_one_valid_output","title":"<code>preprocess_experiments_one_valid_output(output_feature_key, experiments)</code>","text":"<p>Method to get a dataframe where non-valid entries of the provided output feature are removed</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <code>output_feature_key</code> <code>str</code> <p>The feature based on which non-valid entries rows are removed</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included</p> Source code in <code>bofire/data_models/domain/features.py</code> <pre><code>def preprocess_experiments_one_valid_output(\n    self,\n    output_feature_key: str,\n    experiments: pd.DataFrame,\n) -&gt; pd.DataFrame:\n    \"\"\"Method to get a dataframe where non-valid entries of the provided output feature are removed\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n        output_feature_key (str): The feature based on which non-valid entries rows are removed\n\n    Returns:\n        pd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included\n\n    \"\"\"\n    clean_exp = experiments.loc[\n        (experiments[\"valid_%s\" % output_feature_key] == 1)\n        &amp; (experiments[output_feature_key].notna())\n    ]\n\n    return clean_exp\n</code></pre>"},{"location":"ref-features/","title":"Features","text":""},{"location":"ref-features/#bofire.data_models.features.categorical","title":"<code>categorical</code>","text":""},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput","title":"<code>CategoricalInput</code>","text":"<p>               Bases: <code>Input</code></p> <p>Base class for all categorical input features.</p> <p>Attributes:</p> Name Type Description <code>categories</code> <code>List[str]</code> <p>Names of the categories.</p> <code>allowed</code> <code>List[bool]</code> <p>List of bools indicating if a category is allowed within the optimization.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>class CategoricalInput(Input):\n    \"\"\"Base class for all categorical input features.\n\n    Attributes:\n        categories (List[str]): Names of the categories.\n        allowed (List[bool]): List of bools indicating if a category is allowed within the optimization.\n\n    \"\"\"\n\n    type: Literal[\"CategoricalInput\"] = \"CategoricalInput\"  # type: ignore\n    # order_id: ClassVar[int] = 5\n    order_id: ClassVar[int] = 7\n\n    categories: CategoryVals\n    allowed: Optional[Annotated[List[bool], Field(min_length=2)]] = Field(\n        default=None,\n        validate_default=True,\n    )\n\n    @field_validator(\"allowed\")\n    @classmethod\n    def generate_allowed(cls, allowed, info):\n        \"\"\"Generates the list of allowed categories if not provided.\"\"\"\n        if allowed is None and \"categories\" in info.data.keys():\n            return [True for _ in range(len(info.data[\"categories\"]))]\n        return allowed\n\n    @model_validator(mode=\"after\")\n    def validate_categories_fitting_allowed(self):\n        if len(self.allowed) != len(self.categories):  # type: ignore\n            raise ValueError(\"allowed must have same length as categories\")\n        if sum(self.allowed) == 0:  # type: ignore\n            raise ValueError(\"no category is allowed\")\n        return self\n\n    @staticmethod\n    def valid_transform_types() -&gt; List[CategoricalEncodingEnum]:  # type: ignore\n        return [\n            CategoricalEncodingEnum.ONE_HOT,\n            CategoricalEncodingEnum.DUMMY,\n            CategoricalEncodingEnum.ORDINAL,\n        ]\n\n    def is_fixed(self) -&gt; bool:\n        \"\"\"Returns True if there is only one allowed category.\n\n        Returns:\n            [bool]: True if there is only one allowed category\n\n        \"\"\"\n        if self.allowed is None:\n            return False\n        return sum(self.allowed) == 1\n\n    def fixed_value(\n        self,\n        transform_type: Optional[TTransform] = None,\n    ) -&gt; Union[List[str], List[float], None]:\n        \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n        Returns:\n            List[str]: List of categories or None\n\n        \"\"\"\n        if self.is_fixed():\n            val = self.get_allowed_categories()[0]\n            if transform_type is None:\n                return [val]\n            if transform_type == CategoricalEncodingEnum.ONE_HOT:\n                return self.to_onehot_encoding(pd.Series([val])).values[0].tolist()\n            if transform_type == CategoricalEncodingEnum.DUMMY:\n                return self.to_dummy_encoding(pd.Series([val])).values[0].tolist()\n            if transform_type == CategoricalEncodingEnum.ORDINAL:\n                return self.to_ordinal_encoding(pd.Series([val])).tolist()\n            raise ValueError(\n                f\"Unkwon transform type {transform_type} for categorical input {self.key}\",\n            )\n        return None\n\n    def get_allowed_categories(self):\n        \"\"\"Returns the allowed categories.\n\n        Returns:\n            list of str: The allowed categories\n\n        \"\"\"\n        if self.allowed is None:\n            return []\n        return [c for c, a in zip(self.categories, self.allowed) if a]\n\n    def validate_experimental(\n        self,\n        values: pd.Series,\n        strict: bool = False,\n    ) -&gt; pd.Series:\n        \"\"\"Method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\n\n        Raises:\n            ValueError: when an entry is not in the list of allowed categories\n            ValueError: when there is no variation in a feature provided by the experimental data\n\n        Returns:\n            pd.Series: A dataFrame with experiments\n\n        \"\"\"\n        values = values.map(str)\n        if sum(values.isin(self.categories)) != len(values):\n            raise ValueError(\n                f\"invalid values for `{self.key}`, allowed are: `{self.categories}`\",\n            )\n        if strict:\n            possible_categories = self.get_possible_categories(values)\n            if len(possible_categories) != len(self.categories):\n                raise ValueError(\n                    f\"Categories {list(set(self.categories)-set(possible_categories))} of feature {self.key} not used. Remove them.\",\n                )\n        return values\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Method to validate the suggested candidates\n\n        Args:\n            values (pd.Series): A dataFrame with candidates\n\n        Raises:\n            ValueError: when not all values for a feature are one of the allowed categories\n\n        Returns:\n            pd.Series: The passed dataFrame with candidates\n\n        \"\"\"\n        values = values.map(str)\n        if sum(values.isin(self.get_allowed_categories())) != len(values):\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are a valid allowed category from {self.get_allowed_categories()}\",\n            )\n        return values\n\n    def get_forbidden_categories(self):\n        \"\"\"Returns the non-allowed categories\n\n        Returns:\n            List[str]: List of the non-allowed categories\n\n        \"\"\"\n        return list(set(self.categories) - set(self.get_allowed_categories()))\n\n    def get_possible_categories(self, values: pd.Series) -&gt; list:\n        \"\"\"Return the superset of categories that have been used in the experimental dataset and\n        that can be used in the optimization\n\n        Args:\n            values (pd.Series): Series with the values for this feature\n\n        Returns:\n            list: list of possible categories\n\n        \"\"\"\n        return sorted(set(list(set(values.tolist())) + self.get_allowed_categories()))\n\n    def to_onehot_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"Converts values to a one-hot encoding.\n\n        Args:\n            values (pd.Series): Series to be transformed.\n\n        Returns:\n            pd.DataFrame: One-hot transformed data frame.\n\n        \"\"\"\n        return pd.DataFrame(\n            {get_encoded_name(self.key, c): values == c for c in self.categories},\n            dtype=float,\n            index=values.index,\n        )\n\n    def from_onehot_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Converts values back from one-hot encoding.\n\n        Args:\n            values (pd.DataFrame): One-hot encoded values.\n\n        Raises:\n            ValueError: If one-hot columns not present in `values`.\n\n        Returns:\n            pd.Series: Series with categorical values.\n\n        \"\"\"\n        cat_cols = [get_encoded_name(self.key, c) for c in self.categories]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\",\n            )\n        s = values[cat_cols].idxmax(1).str[(len(self.key) + 1) :]\n        s.name = self.key\n        return s\n\n    def to_dummy_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"Converts values to a dummy-hot encoding, dropping the first categorical level.\n\n        Args:\n            values (pd.Series): Series to be transformed.\n\n        Returns:\n            pd.DataFrame: Dummy-hot transformed data frame.\n\n        \"\"\"\n        return pd.DataFrame(\n            {get_encoded_name(self.key, c): values == c for c in self.categories[1:]},\n            dtype=float,\n            index=values.index,\n        )\n\n    def from_dummy_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Convert points back from dummy encoding.\n\n        Args:\n            values (pd.DataFrame): Dummy-hot encoded values.\n\n        Raises:\n            ValueError: If one-hot columns not present in `values`.\n\n        Returns:\n            pd.Series: Series with categorical values.\n\n        \"\"\"\n        cat_cols = [get_encoded_name(self.key, c) for c in self.categories]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols[1:]]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols[1:]}.\",\n            )\n        values = values.copy()\n        values[cat_cols[0]] = 1 - values[cat_cols[1:]].sum(axis=1)\n        s = values[cat_cols].idxmax(1).str[(len(self.key) + 1) :]\n        s.name = self.key\n        return s\n\n    def to_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Converts values to an ordinal integer based encoding.\n\n        Args:\n            values (pd.Series): Series to be transformed.\n\n        Returns:\n            pd.Series: Ordinal encoded values.\n\n        \"\"\"\n        enc = pd.Series(range(len(self.categories)), index=list(self.categories))\n        s = enc[values]\n        s.index = values.index\n        s.name = self.key\n        return s\n\n    def from_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Convertes values back from ordinal encoding.\n\n        Args:\n            values (pd.Series): Ordinal encoded series.\n\n        Returns:\n            pd.Series: Series with categorical values.\n\n        \"\"\"\n        enc = np.array(self.categories)\n        return pd.Series(enc[values], index=values.index, name=self.key)\n\n    def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n        \"\"\"Draw random samples from the feature.\n\n        Args:\n            n (int): number of samples.\n            seed (int, optional): random seed. Defaults to None.\n\n        Returns:\n            pd.Series: drawn samples.\n\n        \"\"\"\n        return pd.Series(\n            name=self.key,\n            data=np.random.default_rng(seed=seed).choice(\n                self.get_allowed_categories(),\n                n,\n            ),\n        )\n\n    def get_bounds(  # type: ignore\n        self,\n        transform_type: TTransform,\n        values: Optional[pd.Series] = None,\n        reference_value: Optional[str] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        assert isinstance(transform_type, CategoricalEncodingEnum)\n        if transform_type == CategoricalEncodingEnum.ORDINAL:\n            return [0], [len(self.categories) - 1]\n        if transform_type == CategoricalEncodingEnum.ONE_HOT:\n            # in the case that values are None, we return the bounds\n            # based on the optimization bounds, else we return the true\n            # bounds as this is for model fitting.\n            if values is None:\n                lower = [0.0 for _ in self.categories]\n                upper = [\n                    1.0 if self.allowed[i] is True else 0.0  # type: ignore\n                    for i, _ in enumerate(self.categories)\n                ]\n            else:\n                lower = [0.0 for _ in self.categories]\n                upper = [1.0 for _ in self.categories]\n            return lower, upper\n        if transform_type == CategoricalEncodingEnum.DUMMY:\n            lower = [0.0 for _ in range(len(self.categories) - 1)]\n            upper = [1.0 for _ in range(len(self.categories) - 1)]\n            return lower, upper\n        if transform_type == CategoricalEncodingEnum.DESCRIPTOR:\n            raise ValueError(\n                f\"Invalid descriptor transform for categorical {self.key}.\",\n            )\n        raise ValueError(\n            f\"Invalid transform_type {transform_type} provided for categorical {self.key}.\",\n        )\n\n    def __str__(self) -&gt; str:\n        \"\"\"Returns the number of categories as str\n\n        Returns:\n            str: Number of categories\n\n        \"\"\"\n        return f\"{len(self.categories)} categories\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.__str__","title":"<code>__str__()</code>","text":"<p>Returns the number of categories as str</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Number of categories</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Returns the number of categories as str\n\n    Returns:\n        str: Number of categories\n\n    \"\"\"\n    return f\"{len(self.categories)} categories\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.fixed_value","title":"<code>fixed_value(transform_type=None)</code>","text":"<p>Returns the categories to which the feature is fixed, None if the feature is not fixed</p> <p>Returns:</p> Type Description <code>Union[List[str], List[float], None]</code> <p>List[str]: List of categories or None</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def fixed_value(\n    self,\n    transform_type: Optional[TTransform] = None,\n) -&gt; Union[List[str], List[float], None]:\n    \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n    Returns:\n        List[str]: List of categories or None\n\n    \"\"\"\n    if self.is_fixed():\n        val = self.get_allowed_categories()[0]\n        if transform_type is None:\n            return [val]\n        if transform_type == CategoricalEncodingEnum.ONE_HOT:\n            return self.to_onehot_encoding(pd.Series([val])).values[0].tolist()\n        if transform_type == CategoricalEncodingEnum.DUMMY:\n            return self.to_dummy_encoding(pd.Series([val])).values[0].tolist()\n        if transform_type == CategoricalEncodingEnum.ORDINAL:\n            return self.to_ordinal_encoding(pd.Series([val])).tolist()\n        raise ValueError(\n            f\"Unkwon transform type {transform_type} for categorical input {self.key}\",\n        )\n    return None\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.from_dummy_encoding","title":"<code>from_dummy_encoding(values)</code>","text":"<p>Convert points back from dummy encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>DataFrame</code> <p>Dummy-hot encoded values.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If one-hot columns not present in <code>values</code>.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with categorical values.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def from_dummy_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Convert points back from dummy encoding.\n\n    Args:\n        values (pd.DataFrame): Dummy-hot encoded values.\n\n    Raises:\n        ValueError: If one-hot columns not present in `values`.\n\n    Returns:\n        pd.Series: Series with categorical values.\n\n    \"\"\"\n    cat_cols = [get_encoded_name(self.key, c) for c in self.categories]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols[1:]]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols[1:]}.\",\n        )\n    values = values.copy()\n    values[cat_cols[0]] = 1 - values[cat_cols[1:]].sum(axis=1)\n    s = values[cat_cols].idxmax(1).str[(len(self.key) + 1) :]\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.from_onehot_encoding","title":"<code>from_onehot_encoding(values)</code>","text":"<p>Converts values back from one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>DataFrame</code> <p>One-hot encoded values.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If one-hot columns not present in <code>values</code>.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with categorical values.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def from_onehot_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Converts values back from one-hot encoding.\n\n    Args:\n        values (pd.DataFrame): One-hot encoded values.\n\n    Raises:\n        ValueError: If one-hot columns not present in `values`.\n\n    Returns:\n        pd.Series: Series with categorical values.\n\n    \"\"\"\n    cat_cols = [get_encoded_name(self.key, c) for c in self.categories]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\",\n        )\n    s = values[cat_cols].idxmax(1).str[(len(self.key) + 1) :]\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.from_ordinal_encoding","title":"<code>from_ordinal_encoding(values)</code>","text":"<p>Convertes values back from ordinal encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Ordinal encoded series.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with categorical values.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def from_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Convertes values back from ordinal encoding.\n\n    Args:\n        values (pd.Series): Ordinal encoded series.\n\n    Returns:\n        pd.Series: Series with categorical values.\n\n    \"\"\"\n    enc = np.array(self.categories)\n    return pd.Series(enc[values], index=values.index, name=self.key)\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.generate_allowed","title":"<code>generate_allowed(allowed, info)</code>  <code>classmethod</code>","text":"<p>Generates the list of allowed categories if not provided.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>@field_validator(\"allowed\")\n@classmethod\ndef generate_allowed(cls, allowed, info):\n    \"\"\"Generates the list of allowed categories if not provided.\"\"\"\n    if allowed is None and \"categories\" in info.data.keys():\n        return [True for _ in range(len(info.data[\"categories\"]))]\n    return allowed\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.get_allowed_categories","title":"<code>get_allowed_categories()</code>","text":"<p>Returns the allowed categories.</p> <p>Returns:</p> Type Description <p>list of str: The allowed categories</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def get_allowed_categories(self):\n    \"\"\"Returns the allowed categories.\n\n    Returns:\n        list of str: The allowed categories\n\n    \"\"\"\n    if self.allowed is None:\n        return []\n    return [c for c, a in zip(self.categories, self.allowed) if a]\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.get_forbidden_categories","title":"<code>get_forbidden_categories()</code>","text":"<p>Returns the non-allowed categories</p> <p>Returns:</p> Type Description <p>List[str]: List of the non-allowed categories</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def get_forbidden_categories(self):\n    \"\"\"Returns the non-allowed categories\n\n    Returns:\n        List[str]: List of the non-allowed categories\n\n    \"\"\"\n    return list(set(self.categories) - set(self.get_allowed_categories()))\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.get_possible_categories","title":"<code>get_possible_categories(values)</code>","text":"<p>Return the superset of categories that have been used in the experimental dataset and that can be used in the optimization</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Series with the values for this feature</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>list of possible categories</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def get_possible_categories(self, values: pd.Series) -&gt; list:\n    \"\"\"Return the superset of categories that have been used in the experimental dataset and\n    that can be used in the optimization\n\n    Args:\n        values (pd.Series): Series with the values for this feature\n\n    Returns:\n        list: list of possible categories\n\n    \"\"\"\n    return sorted(set(list(set(values.tolist())) + self.get_allowed_categories()))\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.is_fixed","title":"<code>is_fixed()</code>","text":"<p>Returns True if there is only one allowed category.</p> <p>Returns:</p> Type Description <code>bool</code> <p>[bool]: True if there is only one allowed category</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def is_fixed(self) -&gt; bool:\n    \"\"\"Returns True if there is only one allowed category.\n\n    Returns:\n        [bool]: True if there is only one allowed category\n\n    \"\"\"\n    if self.allowed is None:\n        return False\n    return sum(self.allowed) == 1\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.sample","title":"<code>sample(n, seed=None)</code>","text":"<p>Draw random samples from the feature.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples.</p> required <code>seed</code> <code>int</code> <p>random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: drawn samples.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n    \"\"\"Draw random samples from the feature.\n\n    Args:\n        n (int): number of samples.\n        seed (int, optional): random seed. Defaults to None.\n\n    Returns:\n        pd.Series: drawn samples.\n\n    \"\"\"\n    return pd.Series(\n        name=self.key,\n        data=np.random.default_rng(seed=seed).choice(\n            self.get_allowed_categories(),\n            n,\n        ),\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.to_dummy_encoding","title":"<code>to_dummy_encoding(values)</code>","text":"<p>Converts values to a dummy-hot encoding, dropping the first categorical level.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Series to be transformed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dummy-hot transformed data frame.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def to_dummy_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Converts values to a dummy-hot encoding, dropping the first categorical level.\n\n    Args:\n        values (pd.Series): Series to be transformed.\n\n    Returns:\n        pd.DataFrame: Dummy-hot transformed data frame.\n\n    \"\"\"\n    return pd.DataFrame(\n        {get_encoded_name(self.key, c): values == c for c in self.categories[1:]},\n        dtype=float,\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.to_onehot_encoding","title":"<code>to_onehot_encoding(values)</code>","text":"<p>Converts values to a one-hot encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Series to be transformed.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: One-hot transformed data frame.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def to_onehot_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Converts values to a one-hot encoding.\n\n    Args:\n        values (pd.Series): Series to be transformed.\n\n    Returns:\n        pd.DataFrame: One-hot transformed data frame.\n\n    \"\"\"\n    return pd.DataFrame(\n        {get_encoded_name(self.key, c): values == c for c in self.categories},\n        dtype=float,\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.to_ordinal_encoding","title":"<code>to_ordinal_encoding(values)</code>","text":"<p>Converts values to an ordinal integer based encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Series to be transformed.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Ordinal encoded values.</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def to_ordinal_encoding(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Converts values to an ordinal integer based encoding.\n\n    Args:\n        values (pd.Series): Series to be transformed.\n\n    Returns:\n        pd.Series: Ordinal encoded values.\n\n    \"\"\"\n    enc = pd.Series(range(len(self.categories)), index=list(self.categories))\n    s = enc[values]\n    s.index = values.index\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.validate_candidental","title":"<code>validate_candidental(values)</code>","text":"<p>Method to validate the suggested candidates</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with candidates</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when not all values for a feature are one of the allowed categories</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The passed dataFrame with candidates</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Method to validate the suggested candidates\n\n    Args:\n        values (pd.Series): A dataFrame with candidates\n\n    Raises:\n        ValueError: when not all values for a feature are one of the allowed categories\n\n    Returns:\n        pd.Series: The passed dataFrame with candidates\n\n    \"\"\"\n    values = values.map(str)\n    if sum(values.isin(self.get_allowed_categories())) != len(values):\n        raise ValueError(\n            f\"not all values of input feature `{self.key}` are a valid allowed category from {self.get_allowed_categories()}\",\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalInput.validate_experimental","title":"<code>validate_experimental(values, strict=False)</code>","text":"<p>Method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when an entry is not in the list of allowed categories</p> <code>ValueError</code> <p>when there is no variation in a feature provided by the experimental data</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A dataFrame with experiments</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>def validate_experimental(\n    self,\n    values: pd.Series,\n    strict: bool = False,\n) -&gt; pd.Series:\n    \"\"\"Method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\n\n    Raises:\n        ValueError: when an entry is not in the list of allowed categories\n        ValueError: when there is no variation in a feature provided by the experimental data\n\n    Returns:\n        pd.Series: A dataFrame with experiments\n\n    \"\"\"\n    values = values.map(str)\n    if sum(values.isin(self.categories)) != len(values):\n        raise ValueError(\n            f\"invalid values for `{self.key}`, allowed are: `{self.categories}`\",\n        )\n    if strict:\n        possible_categories = self.get_possible_categories(values)\n        if len(possible_categories) != len(self.categories):\n            raise ValueError(\n                f\"Categories {list(set(self.categories)-set(possible_categories))} of feature {self.key} not used. Remove them.\",\n            )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalOutput","title":"<code>CategoricalOutput</code>","text":"<p>               Bases: <code>Output</code></p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>class CategoricalOutput(Output):\n    type: Literal[\"CategoricalOutput\"] = \"CategoricalOutput\"  # type: ignore\n    order_id: ClassVar[int] = 10\n\n    categories: CategoryVals\n    objective: AnyCategoricalObjective\n\n    @model_validator(mode=\"after\")\n    def validate_objective_categories(self):\n        \"\"\"Validates that objective categories match the output categories\n\n        Raises:\n            ValueError: when categories do not match objective categories\n\n        Returns:\n            self\n\n        \"\"\"\n        if self.objective.categories != self.categories:\n            raise ValueError(\"categories must match to objective categories\")\n        return self\n\n    def __call__(self, values: pd.Series, values_adapt: pd.Series) -&gt; pd.Series:  # type: ignore\n        if self.objective is None:\n            return pd.Series(\n                data=[np.nan for _ in range(len(values))],\n                index=values.index,\n                name=values.name,\n            )\n        return self.objective(values, values_adapt)  # type: ignore\n\n    def validate_experimental(self, values: pd.Series) -&gt; pd.Series:\n        values = values.map(str)\n        if sum(values.isin(self.categories)) != len(values):\n            raise ValueError(\n                f\"invalid values for `{self.key}`, allowed are: `{self.categories}`\",\n            )\n        return values\n\n    def __str__(self) -&gt; str:\n        return \"CategoricalOutputFeature\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.categorical.CategoricalOutput.validate_objective_categories","title":"<code>validate_objective_categories()</code>","text":"<p>Validates that objective categories match the output categories</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>when categories do not match objective categories</p> <p>Returns:</p> Type Description <p>self</p> Source code in <code>bofire/data_models/features/categorical.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_objective_categories(self):\n    \"\"\"Validates that objective categories match the output categories\n\n    Raises:\n        ValueError: when categories do not match objective categories\n\n    Returns:\n        self\n\n    \"\"\"\n    if self.objective.categories != self.categories:\n        raise ValueError(\"categories must match to objective categories\")\n    return self\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.continuous","title":"<code>continuous</code>","text":""},{"location":"ref-features/#bofire.data_models.features.continuous.ContinuousInput","title":"<code>ContinuousInput</code>","text":"<p>               Bases: <code>NumericalInput</code></p> <p>Base class for all continuous input features.</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>Tuple[float, float]</code> <p>A tuple that stores the lower and upper bound of the feature.</p> <code>stepsize</code> <code>float</code> <p>Float indicating the allowed stepsize between lower and upper. Defaults to None.</p> <code>local_relative_bounds</code> <code>Tuple[float, float]</code> <p>A tuple that stores the lower and upper bounds relative to a reference value. Defaults to None.</p> Source code in <code>bofire/data_models/features/continuous.py</code> <pre><code>class ContinuousInput(NumericalInput):\n    \"\"\"Base class for all continuous input features.\n\n    Attributes:\n        bounds (Tuple[float, float]): A tuple that stores the lower and upper bound of the feature.\n        stepsize (float, optional): Float indicating the allowed stepsize between lower and upper. Defaults to None.\n        local_relative_bounds (Tuple[float, float], optional): A tuple that stores the lower and upper bounds relative to a reference value.\n            Defaults to None.\n\n    \"\"\"\n\n    type: Literal[\"ContinuousInput\"] = \"ContinuousInput\"  # type: ignore\n    order_id: ClassVar[int] = 1\n\n    bounds: Bounds\n    local_relative_bounds: Optional[\n        Annotated[List[Annotated[float, Field(gt=0)]], Field(min_items=2, max_items=2)]  # type: ignore\n    ] = None\n    stepsize: Optional[float] = None\n\n    @property\n    def lower_bound(self) -&gt; float:\n        return self.bounds[0]\n\n    @property\n    def upper_bound(self) -&gt; float:\n        return self.bounds[1]\n\n    @model_validator(mode=\"after\")\n    def validate_step_size(self):\n        if self.stepsize is None:\n            return self\n        lower, upper = self.bounds\n        if lower == upper and self.stepsize is not None:\n            raise ValueError(\n                \"Stepsize cannot be provided for a fixed continuous input.\",\n            )\n        range = upper - lower\n        if np.arange(lower, upper + self.stepsize, self.stepsize)[-1] != upper:\n            raise ValueError(\n                f\"Stepsize of {self.stepsize} does not match the provided interval [{lower},{upper}].\",\n            )\n        if range // self.stepsize == 1:\n            raise ValueError(\"Stepsize is too big, only one value allowed.\")\n        return self\n\n    def round(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Round values to the stepsize of the feature. If no stepsize is provided return the\n        provided values.\n\n        Args:\n            values (pd.Series): The values that should be rounded.\n\n        Returns:\n            pd.Series: The rounded values\n\n        \"\"\"\n        if self.stepsize is None:\n            return values\n        self.validate_candidental(values=values)\n        allowed_values = np.arange(\n            self.lower_bound,\n            self.upper_bound + self.stepsize,\n            self.stepsize,\n        )\n        idx = abs(values.values.reshape([len(values), 1]) - allowed_values).argmin(  # type: ignore\n            axis=1,\n        )\n        return pd.Series(\n            data=self.lower_bound + idx * self.stepsize,\n            index=values.index,\n        )\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Method to validate the suggested candidates\n\n        Args:\n            values (pd.Series): A dataFrame with candidates\n\n        Raises:\n            ValueError: when non numerical values are passed\n            ValueError: when values are larger than the upper bound of the feature\n            ValueError: when values are lower than the lower bound of the feature\n\n        Returns:\n            pd.Series: The passed dataFrame with candidates\n\n        \"\"\"\n        noise = 10e-6\n        values = super().validate_candidental(values)\n        if (values &lt; self.lower_bound - noise).any():\n            raise ValueError(\n                f\"not all values of input feature `{self.key}`are larger than lower bound `{self.lower_bound}` \",\n            )\n        if (values &gt; self.upper_bound + noise).any():\n            raise ValueError(\n                f\"not all values of input feature `{self.key}`are smaller than upper bound `{self.upper_bound}` \",\n            )\n        return values\n\n    def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n        \"\"\"Draw random samples from the feature.\n\n        Args:\n            n (int): number of samples.\n            seed (int, optional): random seed. Defaults to None.\n\n        Returns:\n            pd.Series: drawn samples.\n\n        \"\"\"\n        return pd.Series(\n            name=self.key,\n            data=np.random.default_rng(seed=seed).uniform(\n                self.lower_bound,\n                self.upper_bound,\n                n,\n            ),\n        )\n\n    def get_bounds(  # type: ignore\n        self,\n        transform_type: Optional[TTransform] = None,\n        values: Optional[pd.Series] = None,\n        reference_value: Optional[float] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        assert transform_type is None\n        if reference_value is not None and values is not None:\n            raise ValueError(\"Only one can be used, `local_value` or `values`.\")\n\n        if values is None:\n            if reference_value is None or self.is_fixed():\n                return [self.lower_bound], [self.upper_bound]\n\n            local_relative_bounds = self.local_relative_bounds or (\n                math.inf,\n                math.inf,\n            )\n\n            return [\n                max(\n                    reference_value - local_relative_bounds[0],\n                    self.lower_bound,\n                ),\n            ], [\n                min(\n                    reference_value + local_relative_bounds[1],\n                    self.upper_bound,\n                ),\n            ]\n\n        lower = min(self.lower_bound, values.min())\n        upper = max(self.upper_bound, values.max())\n        return [lower], [upper]\n\n    def __str__(self) -&gt; str:\n        \"\"\"Method to return a string of lower and upper bound\n\n        Returns:\n            str: String of a list with lower and upper bound\n\n        \"\"\"\n        return f\"[{self.lower_bound},{self.upper_bound}]\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.continuous.ContinuousInput.__str__","title":"<code>__str__()</code>","text":"<p>Method to return a string of lower and upper bound</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String of a list with lower and upper bound</p> Source code in <code>bofire/data_models/features/continuous.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Method to return a string of lower and upper bound\n\n    Returns:\n        str: String of a list with lower and upper bound\n\n    \"\"\"\n    return f\"[{self.lower_bound},{self.upper_bound}]\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.continuous.ContinuousInput.round","title":"<code>round(values)</code>","text":"<p>Round values to the stepsize of the feature. If no stepsize is provided return the provided values.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>The values that should be rounded.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The rounded values</p> Source code in <code>bofire/data_models/features/continuous.py</code> <pre><code>def round(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Round values to the stepsize of the feature. If no stepsize is provided return the\n    provided values.\n\n    Args:\n        values (pd.Series): The values that should be rounded.\n\n    Returns:\n        pd.Series: The rounded values\n\n    \"\"\"\n    if self.stepsize is None:\n        return values\n    self.validate_candidental(values=values)\n    allowed_values = np.arange(\n        self.lower_bound,\n        self.upper_bound + self.stepsize,\n        self.stepsize,\n    )\n    idx = abs(values.values.reshape([len(values), 1]) - allowed_values).argmin(  # type: ignore\n        axis=1,\n    )\n    return pd.Series(\n        data=self.lower_bound + idx * self.stepsize,\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.continuous.ContinuousInput.sample","title":"<code>sample(n, seed=None)</code>","text":"<p>Draw random samples from the feature.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples.</p> required <code>seed</code> <code>int</code> <p>random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: drawn samples.</p> Source code in <code>bofire/data_models/features/continuous.py</code> <pre><code>def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n    \"\"\"Draw random samples from the feature.\n\n    Args:\n        n (int): number of samples.\n        seed (int, optional): random seed. Defaults to None.\n\n    Returns:\n        pd.Series: drawn samples.\n\n    \"\"\"\n    return pd.Series(\n        name=self.key,\n        data=np.random.default_rng(seed=seed).uniform(\n            self.lower_bound,\n            self.upper_bound,\n            n,\n        ),\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.continuous.ContinuousInput.validate_candidental","title":"<code>validate_candidental(values)</code>","text":"<p>Method to validate the suggested candidates</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with candidates</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when non numerical values are passed</p> <code>ValueError</code> <p>when values are larger than the upper bound of the feature</p> <code>ValueError</code> <p>when values are lower than the lower bound of the feature</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The passed dataFrame with candidates</p> Source code in <code>bofire/data_models/features/continuous.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Method to validate the suggested candidates\n\n    Args:\n        values (pd.Series): A dataFrame with candidates\n\n    Raises:\n        ValueError: when non numerical values are passed\n        ValueError: when values are larger than the upper bound of the feature\n        ValueError: when values are lower than the lower bound of the feature\n\n    Returns:\n        pd.Series: The passed dataFrame with candidates\n\n    \"\"\"\n    noise = 10e-6\n    values = super().validate_candidental(values)\n    if (values &lt; self.lower_bound - noise).any():\n        raise ValueError(\n            f\"not all values of input feature `{self.key}`are larger than lower bound `{self.lower_bound}` \",\n        )\n    if (values &gt; self.upper_bound + noise).any():\n        raise ValueError(\n            f\"not all values of input feature `{self.key}`are smaller than upper bound `{self.upper_bound}` \",\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.continuous.ContinuousOutput","title":"<code>ContinuousOutput</code>","text":"<p>               Bases: <code>Output</code></p> <p>The base class for a continuous output feature</p> <p>Attributes:</p> Name Type Description <code>objective</code> <code>objective</code> <p>objective of the feature indicating in which direction it should be optimized. Defaults to <code>MaximizeObjective</code>.</p> Source code in <code>bofire/data_models/features/continuous.py</code> <pre><code>class ContinuousOutput(Output):\n    \"\"\"The base class for a continuous output feature\n\n    Attributes:\n        objective (objective, optional): objective of the feature indicating in which direction it should be optimized. Defaults to `MaximizeObjective`.\n\n    \"\"\"\n\n    type: Literal[\"ContinuousOutput\"] = \"ContinuousOutput\"  # type: ignore\n    order_id: ClassVar[int] = 9\n    unit: Optional[str] = None\n\n    objective: Optional[AnyObjective] = Field(\n        default_factory=lambda: MaximizeObjective(w=1.0),\n    )\n\n    def __call__(self, values: pd.Series, values_adapt: pd.Series) -&gt; pd.Series:  # type: ignore\n        if self.objective is None:\n            return pd.Series(\n                data=[np.nan for _ in range(len(values))],\n                index=values.index,\n                name=values.name,\n            )\n        return self.objective(values, values_adapt)  # type: ignore\n\n    def validate_experimental(self, values: pd.Series) -&gt; pd.Series:\n        try:\n            values = pd.to_numeric(values, errors=\"raise\").astype(\"float64\")\n        except ValueError:\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are numerical\",\n            )\n        return values\n\n    def __str__(self) -&gt; str:\n        return \"ContinuousOutputFeature\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor","title":"<code>descriptor</code>","text":""},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput","title":"<code>CategoricalDescriptorInput</code>","text":"<p>               Bases: <code>CategoricalInput</code></p> <p>Class for categorical input features with descriptors</p> <p>Attributes:</p> Name Type Description <code>categories</code> <code>List[str]</code> <p>Names of the categories.</p> <code>allowed</code> <code>List[bool]</code> <p>List of bools indicating if a category is allowed within the optimization.</p> <code>descriptors</code> <code>List[str]</code> <p>List of strings representing the names of the descriptors.</p> <code>values</code> <code>List[List[float]]</code> <p>List of lists representing the descriptor values.</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>class CategoricalDescriptorInput(CategoricalInput):\n    \"\"\"Class for categorical input features with descriptors\n\n    Attributes:\n        categories (List[str]): Names of the categories.\n        allowed (List[bool]): List of bools indicating if a category is allowed within the optimization.\n        descriptors (List[str]): List of strings representing the names of the descriptors.\n        values (List[List[float]]): List of lists representing the descriptor values.\n\n    \"\"\"\n\n    type: Literal[\"CategoricalDescriptorInput\"] = \"CategoricalDescriptorInput\"\n    order_id: ClassVar[int] = 6\n\n    descriptors: Descriptors\n    values: Annotated[\n        List[List[float]],\n        Field(min_length=1),\n    ]\n\n    @field_validator(\"values\")\n    @classmethod\n    def validate_values(cls, v, info):\n        \"\"\"Validates the compatibility of passed values for the descriptors and the defined categories\n\n        Args:\n            v (List[List[float]]): Nested list with descriptor values\n            values (Dict): Dictionary with attributes\n\n        Raises:\n            ValueError: when values have different length than categories\n            ValueError: when rows in values have different length than descriptors\n            ValueError: when a descriptor shows no variance in the data\n\n        Returns:\n            List[List[float]]: Nested list with descriptor values\n\n        \"\"\"\n        if len(v) != len(info.data[\"categories\"]):\n            raise ValueError(\"values must have same length as categories\")\n        for row in v:\n            if len(row) != len(info.data[\"descriptors\"]):\n                raise ValueError(\"rows in values must have same length as descriptors\")\n        a = np.array(v)\n        for i, d in enumerate(info.data[\"descriptors\"]):\n            if len(set(a[:, i])) == 1:\n                raise ValueError(f\"No variation for descriptor {d}.\")\n        return v\n\n    @staticmethod\n    def valid_transform_types() -&gt; List[CategoricalEncodingEnum]:\n        return [\n            CategoricalEncodingEnum.ONE_HOT,\n            CategoricalEncodingEnum.DUMMY,\n            CategoricalEncodingEnum.ORDINAL,\n            CategoricalEncodingEnum.DESCRIPTOR,\n        ]\n\n    def to_df(self):\n        \"\"\"Tabular overview of the feature as DataFrame\n\n        Returns:\n            pd.DataFrame: tabular overview of the feature as DataFrame\n\n        \"\"\"\n        data = dict(zip(self.categories, self.values))\n        return pd.DataFrame.from_dict(data, orient=\"index\", columns=self.descriptors)\n\n    def fixed_value(\n        self,\n        transform_type: Optional[TTransform] = None,\n    ) -&gt; Union[List[str], List[float], None]:\n        \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n        Returns:\n            List[str]: List of categories or None\n\n        \"\"\"\n        if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n            return super().fixed_value(transform_type)\n        val = self.get_allowed_categories()[0]\n        return self.to_descriptor_encoding(pd.Series([val])).values[0].tolist()\n\n    def get_bounds(\n        self,\n        transform_type: TTransform,\n        values: Optional[pd.Series] = None,\n        reference_value: Optional[str] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n            return super().get_bounds(transform_type, values)\n        # in case that values is None, we return the optimization bounds\n        # else we return the complete bounds\n        if values is None:\n            df = self.to_df().loc[self.get_allowed_categories()]\n        else:\n            df = self.to_df()\n        lower = df.min().values.tolist()\n        upper = df.max().values.tolist()\n        return lower, upper\n\n    def validate_experimental(\n        self,\n        values: pd.Series,\n        strict: bool = False,\n    ) -&gt; pd.Series:\n        \"\"\"Method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\n\n        Raises:\n            ValueError: when an entry is not in the list of allowed categories\n            ValueError: when there is no variation in a feature provided by the experimental data\n            ValueError: when no variation is present or planned for a given descriptor\n\n        Returns:\n            pd.Series: A dataFrame with experiments\n\n        \"\"\"\n        values = super().validate_experimental(values, strict)\n        if strict:\n            lower, upper = self.get_bounds(\n                transform_type=CategoricalEncodingEnum.DESCRIPTOR,\n                values=values,\n            )\n            for i, desc in enumerate(self.descriptors):\n                if lower[i] == upper[i]:\n                    raise ValueError(\n                        f\"No variation present or planned for descriptor {desc} for feature {self.key}. Remove the descriptor.\",\n                    )\n        return values\n\n    @classmethod\n    def from_df(cls, key: str, df: pd.DataFrame):\n        \"\"\"Creates a feature from a dataframe\n\n        Args:\n            key (str): The name of the feature\n            df (pd.DataFrame): Categories as rows and descriptors as columns\n\n        Returns:\n            _type_: _description_\n\n        \"\"\"\n        return cls(\n            key=key,\n            categories=list(df.index),\n            allowed=[True for _ in range(len(df))],\n            descriptors=list(df.columns),\n            values=df.values.tolist(),\n        )\n\n    def to_descriptor_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n        \"\"\"Converts values to descriptor encoding.\n\n        Args:\n            values (pd.Series): Values to transform.\n\n        Returns:\n            pd.DataFrame: Descriptor encoded dataframe.\n\n        \"\"\"\n        return pd.DataFrame(\n            data=values.map(dict(zip(self.categories, self.values))).values.tolist(),\n            columns=[get_encoded_name(self.key, d) for d in self.descriptors],\n            index=values.index,\n        )\n\n    def from_descriptor_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Converts values back from descriptor encoding.\n\n        Args:\n            values (pd.DataFrame): Descriptor encoded dataframe.\n\n        Raises:\n            ValueError: If descriptor columns not found in the dataframe.\n\n        Returns:\n            pd.Series: Series with categorical values.\n\n        \"\"\"\n        cat_cols = [get_encoded_name(self.key, d) for d in self.descriptors]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\",\n            )\n        s = pd.DataFrame(\n            data=np.sqrt(\n                np.sum(\n                    (\n                        values[cat_cols].to_numpy()[:, np.newaxis, :]\n                        - self.to_df().iloc[self.allowed].to_numpy()\n                    )\n                    ** 2,\n                    axis=2,\n                ),\n            ),\n            columns=self.get_allowed_categories(),\n            index=values.index,\n        ).idxmin(1)\n        s.name = self.key\n        return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.fixed_value","title":"<code>fixed_value(transform_type=None)</code>","text":"<p>Returns the categories to which the feature is fixed, None if the feature is not fixed</p> <p>Returns:</p> Type Description <code>Union[List[str], List[float], None]</code> <p>List[str]: List of categories or None</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>def fixed_value(\n    self,\n    transform_type: Optional[TTransform] = None,\n) -&gt; Union[List[str], List[float], None]:\n    \"\"\"Returns the categories to which the feature is fixed, None if the feature is not fixed\n\n    Returns:\n        List[str]: List of categories or None\n\n    \"\"\"\n    if transform_type != CategoricalEncodingEnum.DESCRIPTOR:\n        return super().fixed_value(transform_type)\n    val = self.get_allowed_categories()[0]\n    return self.to_descriptor_encoding(pd.Series([val])).values[0].tolist()\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.from_descriptor_encoding","title":"<code>from_descriptor_encoding(values)</code>","text":"<p>Converts values back from descriptor encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>DataFrame</code> <p>Descriptor encoded dataframe.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If descriptor columns not found in the dataframe.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with categorical values.</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>def from_descriptor_encoding(self, values: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Converts values back from descriptor encoding.\n\n    Args:\n        values (pd.DataFrame): Descriptor encoded dataframe.\n\n    Raises:\n        ValueError: If descriptor columns not found in the dataframe.\n\n    Returns:\n        pd.Series: Series with categorical values.\n\n    \"\"\"\n    cat_cols = [get_encoded_name(self.key, d) for d in self.descriptors]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\",\n        )\n    s = pd.DataFrame(\n        data=np.sqrt(\n            np.sum(\n                (\n                    values[cat_cols].to_numpy()[:, np.newaxis, :]\n                    - self.to_df().iloc[self.allowed].to_numpy()\n                )\n                ** 2,\n                axis=2,\n            ),\n        ),\n        columns=self.get_allowed_categories(),\n        index=values.index,\n    ).idxmin(1)\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.from_df","title":"<code>from_df(key, df)</code>  <code>classmethod</code>","text":"<p>Creates a feature from a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the feature</p> required <code>df</code> <code>DataFrame</code> <p>Categories as rows and descriptors as columns</p> required <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>@classmethod\ndef from_df(cls, key: str, df: pd.DataFrame):\n    \"\"\"Creates a feature from a dataframe\n\n    Args:\n        key (str): The name of the feature\n        df (pd.DataFrame): Categories as rows and descriptors as columns\n\n    Returns:\n        _type_: _description_\n\n    \"\"\"\n    return cls(\n        key=key,\n        categories=list(df.index),\n        allowed=[True for _ in range(len(df))],\n        descriptors=list(df.columns),\n        values=df.values.tolist(),\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.to_descriptor_encoding","title":"<code>to_descriptor_encoding(values)</code>","text":"<p>Converts values to descriptor encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Values to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Descriptor encoded dataframe.</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>def to_descriptor_encoding(self, values: pd.Series) -&gt; pd.DataFrame:\n    \"\"\"Converts values to descriptor encoding.\n\n    Args:\n        values (pd.Series): Values to transform.\n\n    Returns:\n        pd.DataFrame: Descriptor encoded dataframe.\n\n    \"\"\"\n    return pd.DataFrame(\n        data=values.map(dict(zip(self.categories, self.values))).values.tolist(),\n        columns=[get_encoded_name(self.key, d) for d in self.descriptors],\n        index=values.index,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.to_df","title":"<code>to_df()</code>","text":"<p>Tabular overview of the feature as DataFrame</p> <p>Returns:</p> Type Description <p>pd.DataFrame: tabular overview of the feature as DataFrame</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>def to_df(self):\n    \"\"\"Tabular overview of the feature as DataFrame\n\n    Returns:\n        pd.DataFrame: tabular overview of the feature as DataFrame\n\n    \"\"\"\n    data = dict(zip(self.categories, self.values))\n    return pd.DataFrame.from_dict(data, orient=\"index\", columns=self.descriptors)\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.validate_experimental","title":"<code>validate_experimental(values, strict=False)</code>","text":"<p>Method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when an entry is not in the list of allowed categories</p> <code>ValueError</code> <p>when there is no variation in a feature provided by the experimental data</p> <code>ValueError</code> <p>when no variation is present or planned for a given descriptor</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A dataFrame with experiments</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>def validate_experimental(\n    self,\n    values: pd.Series,\n    strict: bool = False,\n) -&gt; pd.Series:\n    \"\"\"Method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\n\n    Raises:\n        ValueError: when an entry is not in the list of allowed categories\n        ValueError: when there is no variation in a feature provided by the experimental data\n        ValueError: when no variation is present or planned for a given descriptor\n\n    Returns:\n        pd.Series: A dataFrame with experiments\n\n    \"\"\"\n    values = super().validate_experimental(values, strict)\n    if strict:\n        lower, upper = self.get_bounds(\n            transform_type=CategoricalEncodingEnum.DESCRIPTOR,\n            values=values,\n        )\n        for i, desc in enumerate(self.descriptors):\n            if lower[i] == upper[i]:\n                raise ValueError(\n                    f\"No variation present or planned for descriptor {desc} for feature {self.key}. Remove the descriptor.\",\n                )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.CategoricalDescriptorInput.validate_values","title":"<code>validate_values(v, info)</code>  <code>classmethod</code>","text":"<p>Validates the compatibility of passed values for the descriptors and the defined categories</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[List[float]]</code> <p>Nested list with descriptor values</p> required <code>values</code> <code>Dict</code> <p>Dictionary with attributes</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when values have different length than categories</p> <code>ValueError</code> <p>when rows in values have different length than descriptors</p> <code>ValueError</code> <p>when a descriptor shows no variance in the data</p> <p>Returns:</p> Type Description <p>List[List[float]]: Nested list with descriptor values</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>@field_validator(\"values\")\n@classmethod\ndef validate_values(cls, v, info):\n    \"\"\"Validates the compatibility of passed values for the descriptors and the defined categories\n\n    Args:\n        v (List[List[float]]): Nested list with descriptor values\n        values (Dict): Dictionary with attributes\n\n    Raises:\n        ValueError: when values have different length than categories\n        ValueError: when rows in values have different length than descriptors\n        ValueError: when a descriptor shows no variance in the data\n\n    Returns:\n        List[List[float]]: Nested list with descriptor values\n\n    \"\"\"\n    if len(v) != len(info.data[\"categories\"]):\n        raise ValueError(\"values must have same length as categories\")\n    for row in v:\n        if len(row) != len(info.data[\"descriptors\"]):\n            raise ValueError(\"rows in values must have same length as descriptors\")\n    a = np.array(v)\n    for i, d in enumerate(info.data[\"descriptors\"]):\n        if len(set(a[:, i])) == 1:\n            raise ValueError(f\"No variation for descriptor {d}.\")\n    return v\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.ContinuousDescriptorInput","title":"<code>ContinuousDescriptorInput</code>","text":"<p>               Bases: <code>ContinuousInput</code></p> <p>Class for continuous input features with descriptors</p> <p>Attributes:</p> Name Type Description <code>lower_bound</code> <code>float</code> <p>Lower bound of the feature in the optimization.</p> <code>upper_bound</code> <code>float</code> <p>Upper bound of the feature in the optimization.</p> <code>descriptors</code> <code>List[str]</code> <p>Names of the descriptors.</p> <code>values</code> <code>List[float]</code> <p>Values of the descriptors.</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>class ContinuousDescriptorInput(ContinuousInput):\n    \"\"\"Class for continuous input features with descriptors\n\n    Attributes:\n        lower_bound (float): Lower bound of the feature in the optimization.\n        upper_bound (float): Upper bound of the feature in the optimization.\n        descriptors (List[str]): Names of the descriptors.\n        values (List[float]): Values of the descriptors.\n\n    \"\"\"\n\n    type: Literal[\"ContinuousDescriptorInput\"] = \"ContinuousDescriptorInput\"\n    order_id: ClassVar[int] = 2\n\n    descriptors: Descriptors\n    values: DiscreteVals\n\n    @model_validator(mode=\"after\")\n    def validate_list_lengths(self):\n        \"\"\"Compares the length of the defined descriptors list with the provided values\n\n        Args:\n            values (Dict): Dictionary with all attributes\n\n        Raises:\n            ValueError: when the number of descriptors does not math the number of provided values\n\n        Returns:\n            Dict: Dict with the attributes\n\n        \"\"\"\n        if len(self.descriptors) != len(self.values):\n            raise ValueError(\n                'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}',\n            )\n        return self\n\n    def to_df(self) -&gt; pd.DataFrame:\n        \"\"\"Tabular overview of the feature as DataFrame\n\n        Returns:\n            pd.DataFrame: tabular overview of the feature as DataFrame\n\n        \"\"\"\n        return pd.DataFrame(\n            data=[self.values],\n            index=[self.key],\n            columns=self.descriptors,\n        )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.ContinuousDescriptorInput.to_df","title":"<code>to_df()</code>","text":"<p>Tabular overview of the feature as DataFrame</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: tabular overview of the feature as DataFrame</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>def to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Tabular overview of the feature as DataFrame\n\n    Returns:\n        pd.DataFrame: tabular overview of the feature as DataFrame\n\n    \"\"\"\n    return pd.DataFrame(\n        data=[self.values],\n        index=[self.key],\n        columns=self.descriptors,\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.descriptor.ContinuousDescriptorInput.validate_list_lengths","title":"<code>validate_list_lengths()</code>","text":"<p>Compares the length of the defined descriptors list with the provided values</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>Dictionary with all attributes</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when the number of descriptors does not math the number of provided values</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>Dict with the attributes</p> Source code in <code>bofire/data_models/features/descriptor.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_list_lengths(self):\n    \"\"\"Compares the length of the defined descriptors list with the provided values\n\n    Args:\n        values (Dict): Dictionary with all attributes\n\n    Raises:\n        ValueError: when the number of descriptors does not math the number of provided values\n\n    Returns:\n        Dict: Dict with the attributes\n\n    \"\"\"\n    if len(self.descriptors) != len(self.values):\n        raise ValueError(\n            'must provide same number of descriptors and values, got {len(values[\"descriptors\"])} != {len(values[\"values\"])}',\n        )\n    return self\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.discrete","title":"<code>discrete</code>","text":""},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput","title":"<code>DiscreteInput</code>","text":"<p>               Bases: <code>NumericalInput</code></p> <p>Feature with discretized ordinal values allowed in the optimization.</p> <p>Attributes:</p> Name Type Description <code>key(str)</code> <p>key of the feature.</p> <code>values(List[float])</code> <p>the discretized allowed values during the optimization.</p> Source code in <code>bofire/data_models/features/discrete.py</code> <pre><code>class DiscreteInput(NumericalInput):\n    \"\"\"Feature with discretized ordinal values allowed in the optimization.\n\n    Attributes:\n        key(str): key of the feature.\n        values(List[float]): the discretized allowed values during the optimization.\n\n    \"\"\"\n\n    type: Literal[\"DiscreteInput\"] = \"DiscreteInput\"\n    order_id: ClassVar[int] = 3\n\n    values: DiscreteVals\n    rtol: float = 1e-7\n\n    @field_validator(\"values\")\n    @classmethod\n    def validate_values_unique(cls, values):\n        \"\"\"Validates that provided values are unique.\n\n        Args:\n            values (List[float]): List of values\n\n        Raises:\n            ValueError: when values are non-unique.\n            ValueError: when values contains only one entry.\n            ValueError: when values is empty.\n\n        Returns:\n            List[values]: Sorted list of values\n\n        \"\"\"\n        if len(values) != len(set(values)):\n            raise ValueError(\"Discrete values must be unique\")\n        if len(values) == 1:\n            raise ValueError(\n                \"Fixed discrete inputs are not supported. Please use a fixed continuous input.\",\n            )\n        if len(values) == 0:\n            raise ValueError(\"No values defined.\")\n        return sorted(values)\n\n    @property\n    def lower_bound(self) -&gt; float:\n        \"\"\"Lower bound of the set of allowed values\"\"\"\n        return min(self.values)\n\n    @property\n    def upper_bound(self) -&gt; float:\n        \"\"\"Upper bound of the set of allowed values\"\"\"\n        return max(self.values)\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Method to validate the provided candidates.\n\n        Args:\n            values (pd.Series): suggested candidates for the feature\n\n        Raises:\n            ValueError: Raises error when one of the provided values is not contained in the list of allowed values.\n\n        Returns:\n            pd.Series: _uggested candidates for the feature\n\n        \"\"\"\n        values = super().validate_candidental(values)\n        candidates_close_to_allowed_values = (\n            np.array(\n                [\n                    np.array(\n                        [np.isclose(x, y, rtol=self.rtol) for x in self.values]\n                    ).any()\n                    for y in values.to_numpy()\n                ]\n            )\n        ).all()\n        if not candidates_close_to_allowed_values:\n            raise ValueError(\n                f\"Not allowed values in candidates for feature {self.key}.\",\n            )\n        return values\n\n    def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n        \"\"\"Draw random samples from the feature.\n\n        Args:\n            n (int): number of samples.\n            seed (int, optional): random seed. Defaults to None.\n\n        Returns:\n            pd.Series: drawn samples.\n\n        \"\"\"\n        return pd.Series(\n            name=self.key,\n            data=np.random.default_rng(seed=seed).choice(self.values, n),\n        )\n\n    def from_continuous(self, values: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Rounds continuous values to the closest discrete ones.\n\n        Args:\n            values (pd.DataFrame): Dataframe with continuous entries.\n\n        Returns:\n            pd.Series: Series with discrete values.\n\n        \"\"\"\n        s = pd.DataFrame(\n            data=np.abs(\n                values[self.key].to_numpy()[:, np.newaxis] - np.array(self.values),\n            ),\n            columns=self.values,\n            index=values.index,\n        ).idxmin(1)\n        s.name = self.key\n        return s\n\n    def get_bounds(\n        self,\n        transform_type: Optional[TTransform] = None,\n        values: Optional[pd.Series] = None,\n        reference_value: Optional[float] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        assert transform_type is None\n        if values is None:\n            return [self.lower_bound], [self.upper_bound]\n        lower = min(self.lower_bound, values.min())\n        upper = max(self.upper_bound, values.max())\n        return [lower], [upper]\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput.lower_bound","title":"<code>lower_bound</code>  <code>property</code>","text":"<p>Lower bound of the set of allowed values</p>"},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput.upper_bound","title":"<code>upper_bound</code>  <code>property</code>","text":"<p>Upper bound of the set of allowed values</p>"},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput.from_continuous","title":"<code>from_continuous(values)</code>","text":"<p>Rounds continuous values to the closest discrete ones.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>DataFrame</code> <p>Dataframe with continuous entries.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with discrete values.</p> Source code in <code>bofire/data_models/features/discrete.py</code> <pre><code>def from_continuous(self, values: pd.DataFrame) -&gt; pd.Series:\n    \"\"\"Rounds continuous values to the closest discrete ones.\n\n    Args:\n        values (pd.DataFrame): Dataframe with continuous entries.\n\n    Returns:\n        pd.Series: Series with discrete values.\n\n    \"\"\"\n    s = pd.DataFrame(\n        data=np.abs(\n            values[self.key].to_numpy()[:, np.newaxis] - np.array(self.values),\n        ),\n        columns=self.values,\n        index=values.index,\n    ).idxmin(1)\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput.sample","title":"<code>sample(n, seed=None)</code>","text":"<p>Draw random samples from the feature.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>number of samples.</p> required <code>seed</code> <code>int</code> <p>random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: drawn samples.</p> Source code in <code>bofire/data_models/features/discrete.py</code> <pre><code>def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n    \"\"\"Draw random samples from the feature.\n\n    Args:\n        n (int): number of samples.\n        seed (int, optional): random seed. Defaults to None.\n\n    Returns:\n        pd.Series: drawn samples.\n\n    \"\"\"\n    return pd.Series(\n        name=self.key,\n        data=np.random.default_rng(seed=seed).choice(self.values, n),\n    )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput.validate_candidental","title":"<code>validate_candidental(values)</code>","text":"<p>Method to validate the provided candidates.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>suggested candidates for the feature</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises error when one of the provided values is not contained in the list of allowed values.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: _uggested candidates for the feature</p> Source code in <code>bofire/data_models/features/discrete.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Method to validate the provided candidates.\n\n    Args:\n        values (pd.Series): suggested candidates for the feature\n\n    Raises:\n        ValueError: Raises error when one of the provided values is not contained in the list of allowed values.\n\n    Returns:\n        pd.Series: _uggested candidates for the feature\n\n    \"\"\"\n    values = super().validate_candidental(values)\n    candidates_close_to_allowed_values = (\n        np.array(\n            [\n                np.array(\n                    [np.isclose(x, y, rtol=self.rtol) for x in self.values]\n                ).any()\n                for y in values.to_numpy()\n            ]\n        )\n    ).all()\n    if not candidates_close_to_allowed_values:\n        raise ValueError(\n            f\"Not allowed values in candidates for feature {self.key}.\",\n        )\n    return values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.discrete.DiscreteInput.validate_values_unique","title":"<code>validate_values_unique(values)</code>  <code>classmethod</code>","text":"<p>Validates that provided values are unique.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[float]</code> <p>List of values</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when values are non-unique.</p> <code>ValueError</code> <p>when values contains only one entry.</p> <code>ValueError</code> <p>when values is empty.</p> <p>Returns:</p> Type Description <p>List[values]: Sorted list of values</p> Source code in <code>bofire/data_models/features/discrete.py</code> <pre><code>@field_validator(\"values\")\n@classmethod\ndef validate_values_unique(cls, values):\n    \"\"\"Validates that provided values are unique.\n\n    Args:\n        values (List[float]): List of values\n\n    Raises:\n        ValueError: when values are non-unique.\n        ValueError: when values contains only one entry.\n        ValueError: when values is empty.\n\n    Returns:\n        List[values]: Sorted list of values\n\n    \"\"\"\n    if len(values) != len(set(values)):\n        raise ValueError(\"Discrete values must be unique\")\n    if len(values) == 1:\n        raise ValueError(\n            \"Fixed discrete inputs are not supported. Please use a fixed continuous input.\",\n        )\n    if len(values) == 0:\n        raise ValueError(\"No values defined.\")\n    return sorted(values)\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature","title":"<code>feature</code>","text":""},{"location":"ref-features/#bofire.data_models.features.feature.Feature","title":"<code>Feature</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base class for all features.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>class Feature(BaseModel):\n    \"\"\"The base class for all features.\"\"\"\n\n    type: str\n    key: str\n    order_id: ClassVar[int] = -1\n\n    def __lt__(self, other) -&gt; bool:\n        \"\"\"Method to compare two models to get them in the desired order.\n        Return True if other is larger than self, else False. (see FEATURE_ORDER)\n\n        Args:\n            other: The other class to compare to self\n\n        Returns:\n            bool: True if the other class is larger than self, else False\n\n        \"\"\"\n        order_self = self.order_id\n        order_other = other.order_id\n        if order_self == order_other:\n            return self.key &lt; other.key\n        return order_self &lt; order_other\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Feature.__lt__","title":"<code>__lt__(other)</code>","text":"<p>Method to compare two models to get them in the desired order. Return True if other is larger than self, else False. (see FEATURE_ORDER)</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>The other class to compare to self</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the other class is larger than self, else False</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>def __lt__(self, other) -&gt; bool:\n    \"\"\"Method to compare two models to get them in the desired order.\n    Return True if other is larger than self, else False. (see FEATURE_ORDER)\n\n    Args:\n        other: The other class to compare to self\n\n    Returns:\n        bool: True if the other class is larger than self, else False\n\n    \"\"\"\n    order_self = self.order_id\n    order_other = other.order_id\n    if order_self == order_other:\n        return self.key &lt; other.key\n    return order_self &lt; order_other\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input","title":"<code>Input</code>","text":"<p>               Bases: <code>Feature</code></p> <p>Base class for all input features.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>class Input(Feature):\n    \"\"\"Base class for all input features.\"\"\"\n\n    @staticmethod\n    @abstractmethod\n    def valid_transform_types() -&gt; List[Union[CategoricalEncodingEnum, AnyMolFeatures]]:\n        pass\n\n    @abstractmethod\n    def is_fixed(self) -&gt; bool:\n        \"\"\"Indicates if a variable is set to a fixed value.\n\n        Returns:\n            bool: True if fixed, els False.\n\n        \"\"\"\n\n    @abstractmethod\n    def fixed_value(\n        self,\n        transform_type: Optional[TTransform] = None,\n    ) -&gt; Union[None, List[str], List[float]]:\n        \"\"\"Method to return the fixed value in case of a fixed feature.\n\n        Returns:\n            Union[None,str,float]: None in case the feature is not fixed, else the fixed value.\n\n        \"\"\"\n\n    @abstractmethod\n    def validate_experimental(\n        self,\n        values: pd.Series,\n        strict: bool = False,\n    ) -&gt; pd.Series:\n        \"\"\"Abstract method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\n\n        Returns:\n            pd.Series: The passed dataFrame with experiments\n\n        \"\"\"\n\n    @abstractmethod\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Abstract method to validate the suggested candidates\n\n        Args:\n            values (pd.Series): A dataFrame with candidates\n\n        Returns:\n            pd.Series: The passed dataFrame with candidates\n\n        \"\"\"\n\n    @abstractmethod\n    def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n        \"\"\"Sample a series of allowed values.\n\n        Args:\n            n (int): Number of samples\n            seed (int, optional): random seed. Defaults to None.\n\n        Returns:\n            pd.Series: Sampled values.\n\n        \"\"\"\n\n    @abstractmethod\n    def get_bounds(\n        self,\n        transform_type: Optional[TTransform] = None,\n        values: Optional[pd.Series] = None,\n        reference_value: Optional[Union[float, str]] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Returns the bounds of an input feature depending on the requested transform type.\n\n        Args:\n            transform_type (Optional[TTransform], optional): The requested transform type. Defaults to None.\n            values (Optional[pd.Series], optional): If values are provided the bounds are returned taking\n                the most extreme values for the feature into account. Defaults to None.\n            reference_value (Optional[float], optional): If a reference value is provided, then the local bounds based\n                on a local search region are provided. Currently only supported for continuous inputs. For more\n                details, it is referred to https://www.merl.com/publications/docs/TR2023-057.pdf.\n\n        Returns:\n            Tuple[List[float], List[float]]: List of lower bound values, list of upper bound values.\n\n        \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input.fixed_value","title":"<code>fixed_value(transform_type=None)</code>  <code>abstractmethod</code>","text":"<p>Method to return the fixed value in case of a fixed feature.</p> <p>Returns:</p> Type Description <code>Union[None, List[str], List[float]]</code> <p>Union[None,str,float]: None in case the feature is not fixed, else the fixed value.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef fixed_value(\n    self,\n    transform_type: Optional[TTransform] = None,\n) -&gt; Union[None, List[str], List[float]]:\n    \"\"\"Method to return the fixed value in case of a fixed feature.\n\n    Returns:\n        Union[None,str,float]: None in case the feature is not fixed, else the fixed value.\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input.get_bounds","title":"<code>get_bounds(transform_type=None, values=None, reference_value=None)</code>  <code>abstractmethod</code>","text":"<p>Returns the bounds of an input feature depending on the requested transform type.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>Optional[TTransform]</code> <p>The requested transform type. Defaults to None.</p> <code>None</code> <code>values</code> <code>Optional[Series]</code> <p>If values are provided the bounds are returned taking the most extreme values for the feature into account. Defaults to None.</p> <code>None</code> <code>reference_value</code> <code>Optional[float]</code> <p>If a reference value is provided, then the local bounds based on a local search region are provided. Currently only supported for continuous inputs. For more details, it is referred to https://www.merl.com/publications/docs/TR2023-057.pdf.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>Tuple[List[float], List[float]]: List of lower bound values, list of upper bound values.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef get_bounds(\n    self,\n    transform_type: Optional[TTransform] = None,\n    values: Optional[pd.Series] = None,\n    reference_value: Optional[Union[float, str]] = None,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Returns the bounds of an input feature depending on the requested transform type.\n\n    Args:\n        transform_type (Optional[TTransform], optional): The requested transform type. Defaults to None.\n        values (Optional[pd.Series], optional): If values are provided the bounds are returned taking\n            the most extreme values for the feature into account. Defaults to None.\n        reference_value (Optional[float], optional): If a reference value is provided, then the local bounds based\n            on a local search region are provided. Currently only supported for continuous inputs. For more\n            details, it is referred to https://www.merl.com/publications/docs/TR2023-057.pdf.\n\n    Returns:\n        Tuple[List[float], List[float]]: List of lower bound values, list of upper bound values.\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input.is_fixed","title":"<code>is_fixed()</code>  <code>abstractmethod</code>","text":"<p>Indicates if a variable is set to a fixed value.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if fixed, els False.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef is_fixed(self) -&gt; bool:\n    \"\"\"Indicates if a variable is set to a fixed value.\n\n    Returns:\n        bool: True if fixed, els False.\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input.sample","title":"<code>sample(n, seed=None)</code>  <code>abstractmethod</code>","text":"<p>Sample a series of allowed values.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of samples</p> required <code>seed</code> <code>int</code> <p>random seed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Sampled values.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n    \"\"\"Sample a series of allowed values.\n\n    Args:\n        n (int): Number of samples\n        seed (int, optional): random seed. Defaults to None.\n\n    Returns:\n        pd.Series: Sampled values.\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input.validate_candidental","title":"<code>validate_candidental(values)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to validate the suggested candidates</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with candidates</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The passed dataFrame with candidates</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Abstract method to validate the suggested candidates\n\n    Args:\n        values (pd.Series): A dataFrame with candidates\n\n    Returns:\n        pd.Series: The passed dataFrame with candidates\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Input.validate_experimental","title":"<code>validate_experimental(values, strict=False)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The passed dataFrame with experiments</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef validate_experimental(\n    self,\n    values: pd.Series,\n    strict: bool = False,\n) -&gt; pd.Series:\n    \"\"\"Abstract method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\n\n    Returns:\n        pd.Series: The passed dataFrame with experiments\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Output","title":"<code>Output</code>","text":"<p>               Bases: <code>Feature</code></p> <p>Base class for all output features.</p> <p>Attributes:</p> Name Type Description <code>key(str)</code> <p>Key of the Feature.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>class Output(Feature):\n    \"\"\"Base class for all output features.\n\n    Attributes:\n        key(str): Key of the Feature.\n\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self, values: pd.Series) -&gt; pd.Series:\n        pass\n\n    @abstractmethod\n    def validate_experimental(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Abstract method to validate the experimental Series\n\n        Args:\n            values (pd.Series): A dataFrame with values for the outcome\n\n        Returns:\n            pd.Series: The passed dataFrame with experiments\n\n        \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.Output.validate_experimental","title":"<code>validate_experimental(values)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to validate the experimental Series</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with values for the outcome</p> required <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: The passed dataFrame with experiments</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>@abstractmethod\ndef validate_experimental(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Abstract method to validate the experimental Series\n\n    Args:\n        values (pd.Series): A dataFrame with values for the outcome\n\n    Returns:\n        pd.Series: The passed dataFrame with experiments\n\n    \"\"\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.feature.get_encoded_name","title":"<code>get_encoded_name(feature_key, option_name)</code>","text":"<p>Get the name of the encoded column. Option could be the category or the descriptor name.</p> Source code in <code>bofire/data_models/features/feature.py</code> <pre><code>def get_encoded_name(feature_key: str, option_name: str) -&gt; str:\n    \"\"\"Get the name of the encoded column. Option could be the category or the descriptor name.\"\"\"\n    return f\"{feature_key}_{option_name}\"\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.molecular","title":"<code>molecular</code>","text":""},{"location":"ref-features/#bofire.data_models.features.molecular.CategoricalMolecularInput","title":"<code>CategoricalMolecularInput</code>","text":"<p>               Bases: <code>CategoricalInput</code>, <code>MolecularInput</code></p> Source code in <code>bofire/data_models/features/molecular.py</code> <pre><code>class CategoricalMolecularInput(CategoricalInput, MolecularInput):  # type: ignore\n    type: Literal[\"CategoricalMolecularInput\"] = \"CategoricalMolecularInput\"  # type: ignore\n    # order_id: ClassVar[int] = 7\n    order_id: ClassVar[int] = 5\n\n    @field_validator(\"categories\")\n    @classmethod\n    def validate_smiles(cls, categories: Sequence[str]):\n        \"\"\"Validates that categories are valid smiles. Note that this check can only\n        be executed when rdkit is available.\n\n        Args:\n            categories (List[str]): List of smiles\n\n        Raises:\n            ValueError: when string is not a smiles\n\n        Returns:\n            List[str]: List of the smiles\n\n        \"\"\"\n        # check on rdkit availability:\n        try:\n            smiles2mol(categories[0])\n        except NameError:\n            warnings.warn(\"rdkit not installed, categories cannot be validated.\")\n            return categories\n\n        for cat in categories:\n            smiles2mol(cat)\n        return categories\n\n    @staticmethod\n    def valid_transform_types() -&gt; List[Union[AnyMolFeatures, CategoricalEncodingEnum]]:  # type: ignore\n        return CategoricalInput.valid_transform_types() + [  # type: ignore\n            Fingerprints,\n            FingerprintsFragments,\n            Fragments,\n            MordredDescriptors,\n        ]\n\n    def get_bounds(  # type: ignore\n        self,\n        transform_type: Union[CategoricalEncodingEnum, AnyMolFeatures],\n        values: Optional[pd.Series] = None,\n        reference_value: Optional[str] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        if isinstance(transform_type, CategoricalEncodingEnum):\n            # we are just using the standard categorical transformations\n            return super().get_bounds(\n                transform_type=transform_type,\n                values=values,\n                reference_value=reference_value,\n            )\n        # in case that values is None, we return the optimization bounds\n        # else we return the complete bounds\n        data = self.to_descriptor_encoding(\n            transform_type=transform_type,\n            values=(\n                pd.Series(self.get_allowed_categories())\n                if values is None\n                else pd.Series(self.categories)\n            ),\n        )\n        lower = data.min(axis=0).values.tolist()\n        upper = data.max(axis=0).values.tolist()\n        return lower, upper\n\n    def from_descriptor_encoding(\n        self,\n        transform_type: AnyMolFeatures,\n        values: pd.DataFrame,\n    ) -&gt; pd.Series:\n        \"\"\"Converts values back from descriptor encoding.\n\n        Args:\n            values (pd.DataFrame): Descriptor encoded dataframe.\n\n        Raises:\n            ValueError: If descriptor columns not found in the dataframe.\n\n        Returns:\n            pd.Series: Series with categorical values.\n\n        \"\"\"\n        # This method is modified based on the categorical descriptor feature\n        # TODO: move it to more central place\n        cat_cols = [\n            get_encoded_name(self.key, d) for d in transform_type.get_descriptor_names()\n        ]\n        # we allow here explicitly that the dataframe can have more columns than needed to have it\n        # easier in the backtransform.\n        if np.any([c not in values.columns for c in cat_cols]):\n            raise ValueError(\n                f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\",\n            )\n        s = pd.DataFrame(\n            data=np.sqrt(\n                np.sum(\n                    (\n                        values[cat_cols].to_numpy()[:, np.newaxis, :]\n                        - self.to_descriptor_encoding(\n                            transform_type=transform_type,\n                            values=pd.Series(self.get_allowed_categories()),\n                        ).to_numpy()\n                    )\n                    ** 2,\n                    axis=2,\n                ),\n            ),\n            columns=self.get_allowed_categories(),\n            index=values.index,\n        ).idxmin(1)\n        s.name = self.key\n        return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.molecular.CategoricalMolecularInput.from_descriptor_encoding","title":"<code>from_descriptor_encoding(transform_type, values)</code>","text":"<p>Converts values back from descriptor encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>DataFrame</code> <p>Descriptor encoded dataframe.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If descriptor columns not found in the dataframe.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: Series with categorical values.</p> Source code in <code>bofire/data_models/features/molecular.py</code> <pre><code>def from_descriptor_encoding(\n    self,\n    transform_type: AnyMolFeatures,\n    values: pd.DataFrame,\n) -&gt; pd.Series:\n    \"\"\"Converts values back from descriptor encoding.\n\n    Args:\n        values (pd.DataFrame): Descriptor encoded dataframe.\n\n    Raises:\n        ValueError: If descriptor columns not found in the dataframe.\n\n    Returns:\n        pd.Series: Series with categorical values.\n\n    \"\"\"\n    # This method is modified based on the categorical descriptor feature\n    # TODO: move it to more central place\n    cat_cols = [\n        get_encoded_name(self.key, d) for d in transform_type.get_descriptor_names()\n    ]\n    # we allow here explicitly that the dataframe can have more columns than needed to have it\n    # easier in the backtransform.\n    if np.any([c not in values.columns for c in cat_cols]):\n        raise ValueError(\n            f\"{self.key}: Column names don't match categorical levels: {values.columns}, {cat_cols}.\",\n        )\n    s = pd.DataFrame(\n        data=np.sqrt(\n            np.sum(\n                (\n                    values[cat_cols].to_numpy()[:, np.newaxis, :]\n                    - self.to_descriptor_encoding(\n                        transform_type=transform_type,\n                        values=pd.Series(self.get_allowed_categories()),\n                    ).to_numpy()\n                )\n                ** 2,\n                axis=2,\n            ),\n        ),\n        columns=self.get_allowed_categories(),\n        index=values.index,\n    ).idxmin(1)\n    s.name = self.key\n    return s\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.molecular.CategoricalMolecularInput.validate_smiles","title":"<code>validate_smiles(categories)</code>  <code>classmethod</code>","text":"<p>Validates that categories are valid smiles. Note that this check can only be executed when rdkit is available.</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>List[str]</code> <p>List of smiles</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when string is not a smiles</p> <p>Returns:</p> Type Description <p>List[str]: List of the smiles</p> Source code in <code>bofire/data_models/features/molecular.py</code> <pre><code>@field_validator(\"categories\")\n@classmethod\ndef validate_smiles(cls, categories: Sequence[str]):\n    \"\"\"Validates that categories are valid smiles. Note that this check can only\n    be executed when rdkit is available.\n\n    Args:\n        categories (List[str]): List of smiles\n\n    Raises:\n        ValueError: when string is not a smiles\n\n    Returns:\n        List[str]: List of the smiles\n\n    \"\"\"\n    # check on rdkit availability:\n    try:\n        smiles2mol(categories[0])\n    except NameError:\n        warnings.warn(\"rdkit not installed, categories cannot be validated.\")\n        return categories\n\n    for cat in categories:\n        smiles2mol(cat)\n    return categories\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.molecular.MolecularInput","title":"<code>MolecularInput</code>","text":"<p>               Bases: <code>Input</code></p> Source code in <code>bofire/data_models/features/molecular.py</code> <pre><code>class MolecularInput(Input):\n    type: Literal[\"MolecularInput\"] = \"MolecularInput\"  # type: ignore\n    # order_id: ClassVar[int] = 6\n    order_id: ClassVar[int] = 4\n\n    @staticmethod\n    def valid_transform_types() -&gt; List[AnyMolFeatures]:  # type: ignore\n        return [Fingerprints, FingerprintsFragments, Fragments, MordredDescriptors]  # type: ignore\n\n    def validate_experimental(\n        self,\n        values: pd.Series,\n        strict: bool = False,\n    ) -&gt; pd.Series:\n        values = values.map(str)\n        for smi in values:\n            smiles2mol(smi)\n\n        return values\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n        values = values.map(str)\n        for smi in values:\n            smiles2mol(smi)\n        return values\n\n    def is_fixed(self) -&gt; bool:\n        return False\n\n    def fixed_value(self, transform_type: Optional[AnyMolFeatures] = None) -&gt; None:  # type: ignore\n        return None\n\n    def sample(self, n: int, seed: Optional[int] = None) -&gt; pd.Series:\n        raise ValueError(\"Sampling not supported for `MolecularInput`\")\n\n    def get_bounds(  # type: ignore\n        self,\n        transform_type: AnyMolFeatures,\n        values: pd.Series,\n        reference_value: Optional[str] = None,\n    ) -&gt; Tuple[List[float], List[float]]:\n        \"\"\"Calculates the lower and upper bounds for the feature based on the given transform type and values.\n\n        Args:\n            transform_type (AnyMolFeatures): The type of transformation to apply to the data.\n            values (pd.Series): The actual data over which the lower and upper bounds are calculated.\n            reference_value (Optional[str], optional): The reference value for the transformation. Not used here.\n                Defaults to None.\n\n        Returns:\n            Tuple[List[float], List[float]]: A tuple containing the lower and upper bounds of the transformed data.\n\n        Raises:\n            NotImplementedError: Raised when `values` is None, as it is currently required for `MolecularInput`.\n\n        \"\"\"\n        if values is None:\n            raise NotImplementedError(\n                \"`values` is currently required for `MolecularInput`\",\n            )\n        data = self.to_descriptor_encoding(transform_type, values)\n\n        lower = data.min(axis=0).values.tolist()\n        upper = data.max(axis=0).values.tolist()\n\n        return lower, upper\n\n    def to_descriptor_encoding(\n        self,\n        transform_type: AnyMolFeatures,\n        values: pd.Series,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Converts values to descriptor encoding.\n\n        Args:\n            values (pd.Series): Values to transform.\n\n        Returns:\n            pd.DataFrame: Descriptor encoded dataframe.\n\n        \"\"\"\n        descriptor_values = transform_type.get_descriptor_values(values)\n\n        descriptor_values.columns = [\n            get_encoded_name(self.key, d) for d in transform_type.get_descriptor_names()\n        ]\n        descriptor_values.index = values.index\n\n        return descriptor_values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.molecular.MolecularInput.get_bounds","title":"<code>get_bounds(transform_type, values, reference_value=None)</code>","text":"<p>Calculates the lower and upper bounds for the feature based on the given transform type and values.</p> <p>Parameters:</p> Name Type Description Default <code>transform_type</code> <code>AnyMolFeatures</code> <p>The type of transformation to apply to the data.</p> required <code>values</code> <code>Series</code> <p>The actual data over which the lower and upper bounds are calculated.</p> required <code>reference_value</code> <code>Optional[str]</code> <p>The reference value for the transformation. Not used here. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[List[float], List[float]]</code> <p>Tuple[List[float], List[float]]: A tuple containing the lower and upper bounds of the transformed data.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Raised when <code>values</code> is None, as it is currently required for <code>MolecularInput</code>.</p> Source code in <code>bofire/data_models/features/molecular.py</code> <pre><code>def get_bounds(  # type: ignore\n    self,\n    transform_type: AnyMolFeatures,\n    values: pd.Series,\n    reference_value: Optional[str] = None,\n) -&gt; Tuple[List[float], List[float]]:\n    \"\"\"Calculates the lower and upper bounds for the feature based on the given transform type and values.\n\n    Args:\n        transform_type (AnyMolFeatures): The type of transformation to apply to the data.\n        values (pd.Series): The actual data over which the lower and upper bounds are calculated.\n        reference_value (Optional[str], optional): The reference value for the transformation. Not used here.\n            Defaults to None.\n\n    Returns:\n        Tuple[List[float], List[float]]: A tuple containing the lower and upper bounds of the transformed data.\n\n    Raises:\n        NotImplementedError: Raised when `values` is None, as it is currently required for `MolecularInput`.\n\n    \"\"\"\n    if values is None:\n        raise NotImplementedError(\n            \"`values` is currently required for `MolecularInput`\",\n        )\n    data = self.to_descriptor_encoding(transform_type, values)\n\n    lower = data.min(axis=0).values.tolist()\n    upper = data.max(axis=0).values.tolist()\n\n    return lower, upper\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.molecular.MolecularInput.to_descriptor_encoding","title":"<code>to_descriptor_encoding(transform_type, values)</code>","text":"<p>Converts values to descriptor encoding.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>Values to transform.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Descriptor encoded dataframe.</p> Source code in <code>bofire/data_models/features/molecular.py</code> <pre><code>def to_descriptor_encoding(\n    self,\n    transform_type: AnyMolFeatures,\n    values: pd.Series,\n) -&gt; pd.DataFrame:\n    \"\"\"Converts values to descriptor encoding.\n\n    Args:\n        values (pd.Series): Values to transform.\n\n    Returns:\n        pd.DataFrame: Descriptor encoded dataframe.\n\n    \"\"\"\n    descriptor_values = transform_type.get_descriptor_values(values)\n\n    descriptor_values.columns = [\n        get_encoded_name(self.key, d) for d in transform_type.get_descriptor_names()\n    ]\n    descriptor_values.index = values.index\n\n    return descriptor_values\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical","title":"<code>numerical</code>","text":""},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput","title":"<code>NumericalInput</code>","text":"<p>               Bases: <code>Input</code></p> <p>Abstract base class for all numerical (ordinal) input features.</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>class NumericalInput(Input):\n    \"\"\"Abstract base class for all numerical (ordinal) input features.\"\"\"\n\n    unit: Optional[str] = None\n\n    @staticmethod\n    def valid_transform_types() -&gt; List:\n        return []\n\n    @property\n    @abstractmethod\n    def lower_bound(self) -&gt; float:\n        pass\n\n    @property\n    @abstractmethod\n    def upper_bound(self) -&gt; float:\n        pass\n\n    def to_unit_range(\n        self,\n        values: Union[pd.Series, np.ndarray],\n        use_real_bounds: bool = False,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"Convert to the unit range between 0 and 1.\n\n        Args:\n            values (pd.Series): values to be transformed\n            use_real_bounds (bool, optional): if True, use the bounds from the\n                actual values else the bounds from the feature. Defaults to False.\n\n        Raises:\n            ValueError: If lower_bound == upper bound an error is raised\n\n        Returns:\n            pd.Series: transformed values.\n\n        \"\"\"\n        if use_real_bounds:\n            lower, upper = self.get_bounds(\n                transform_type=None,\n                values=values,  # type: ignore\n            )\n            lower = lower[0]\n            upper = upper[0]\n        else:\n            lower, upper = self.lower_bound, self.upper_bound\n\n        if lower == upper:\n            raise ValueError(\"Fixed feature cannot be transformed to unit range.\")\n\n        allowed_range = upper - lower\n        return (values - lower) / allowed_range\n\n    def from_unit_range(\n        self,\n        values: Union[pd.Series, np.ndarray],\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"Convert from unit range.\n\n        Args:\n            values (pd.Series): values to transform from.\n\n        Raises:\n            ValueError: if the feature is fixed raise a value error.\n\n        Returns:\n            pd.Series: _description_\n\n        \"\"\"\n        if self.is_fixed():\n            raise ValueError(\"Fixed feature cannot be transformed from unit range.\")\n\n        allowed_range = self.upper_bound - self.lower_bound\n\n        return (values * allowed_range) + self.lower_bound\n\n    def is_fixed(self):\n        \"\"\"Method to check if the feature is fixed\n\n        Returns:\n            Boolean: True when the feature is fixed, false otherwise.\n\n        \"\"\"\n        return self.lower_bound == self.upper_bound\n\n    def fixed_value(\n        self,\n        transform_type: Optional[TTransform] = None,\n    ) -&gt; Union[None, List[float]]:\n        \"\"\"Method to get the value to which the feature is fixed\n\n        Returns:\n            Float: Return the feature value or None if the feature is not fixed.\n\n        \"\"\"\n        assert transform_type is None\n        if self.is_fixed():\n            return [self.lower_bound]\n        return None\n\n    def validate_experimental(self, values: pd.Series, strict=False) -&gt; pd.Series:\n        \"\"\"Method to validate the experimental dataFrame\n\n        Args:\n            values (pd.Series): A dataFrame with experiments\n            strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not.\n                Defaults to False.\n\n        Raises:\n            ValueError: when a value is not numerical\n            ValueError: when there is no variation in a feature provided by the experimental data\n\n        Returns:\n            pd.Series: A dataFrame with experiments\n\n        \"\"\"\n        try:\n            values = pd.to_numeric(values, errors=\"raise\").astype(\"float64\")\n        except ValueError:\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are numerical\",\n            )\n\n        values = values.astype(\"float64\")\n        if strict:\n            lower, upper = self.get_bounds(transform_type=None, values=values)\n            if lower == upper:\n                raise ValueError(\n                    f\"No variation present or planned for feature {self.key}. Remove it.\",\n                )\n        return values\n\n    def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n        \"\"\"Validate the suggested candidates for the feature.\n\n        Args:\n            values (pd.Series): suggested candidates for the feature\n\n        Raises:\n            ValueError: Error is raised when one of the values is not numerical.\n\n        Returns:\n            pd.Series: the original provided candidates\n\n        \"\"\"\n        try:\n            return pd.to_numeric(values, errors=\"raise\").astype(\"float64\")\n        except ValueError:\n            raise ValueError(\n                f\"not all values of input feature `{self.key}` are numerical\",\n            )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput.fixed_value","title":"<code>fixed_value(transform_type=None)</code>","text":"<p>Method to get the value to which the feature is fixed</p> <p>Returns:</p> Name Type Description <code>Float</code> <code>Union[None, List[float]]</code> <p>Return the feature value or None if the feature is not fixed.</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>def fixed_value(\n    self,\n    transform_type: Optional[TTransform] = None,\n) -&gt; Union[None, List[float]]:\n    \"\"\"Method to get the value to which the feature is fixed\n\n    Returns:\n        Float: Return the feature value or None if the feature is not fixed.\n\n    \"\"\"\n    assert transform_type is None\n    if self.is_fixed():\n        return [self.lower_bound]\n    return None\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput.from_unit_range","title":"<code>from_unit_range(values)</code>","text":"<p>Convert from unit range.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>values to transform from.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the feature is fixed raise a value error.</p> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>pd.Series: description</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>def from_unit_range(\n    self,\n    values: Union[pd.Series, np.ndarray],\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"Convert from unit range.\n\n    Args:\n        values (pd.Series): values to transform from.\n\n    Raises:\n        ValueError: if the feature is fixed raise a value error.\n\n    Returns:\n        pd.Series: _description_\n\n    \"\"\"\n    if self.is_fixed():\n        raise ValueError(\"Fixed feature cannot be transformed from unit range.\")\n\n    allowed_range = self.upper_bound - self.lower_bound\n\n    return (values * allowed_range) + self.lower_bound\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput.is_fixed","title":"<code>is_fixed()</code>","text":"<p>Method to check if the feature is fixed</p> <p>Returns:</p> Name Type Description <code>Boolean</code> <p>True when the feature is fixed, false otherwise.</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>def is_fixed(self):\n    \"\"\"Method to check if the feature is fixed\n\n    Returns:\n        Boolean: True when the feature is fixed, false otherwise.\n\n    \"\"\"\n    return self.lower_bound == self.upper_bound\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput.to_unit_range","title":"<code>to_unit_range(values, use_real_bounds=False)</code>","text":"<p>Convert to the unit range between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>values to be transformed</p> required <code>use_real_bounds</code> <code>bool</code> <p>if True, use the bounds from the actual values else the bounds from the feature. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If lower_bound == upper bound an error is raised</p> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>pd.Series: transformed values.</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>def to_unit_range(\n    self,\n    values: Union[pd.Series, np.ndarray],\n    use_real_bounds: bool = False,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"Convert to the unit range between 0 and 1.\n\n    Args:\n        values (pd.Series): values to be transformed\n        use_real_bounds (bool, optional): if True, use the bounds from the\n            actual values else the bounds from the feature. Defaults to False.\n\n    Raises:\n        ValueError: If lower_bound == upper bound an error is raised\n\n    Returns:\n        pd.Series: transformed values.\n\n    \"\"\"\n    if use_real_bounds:\n        lower, upper = self.get_bounds(\n            transform_type=None,\n            values=values,  # type: ignore\n        )\n        lower = lower[0]\n        upper = upper[0]\n    else:\n        lower, upper = self.lower_bound, self.upper_bound\n\n    if lower == upper:\n        raise ValueError(\"Fixed feature cannot be transformed to unit range.\")\n\n    allowed_range = upper - lower\n    return (values - lower) / allowed_range\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput.validate_candidental","title":"<code>validate_candidental(values)</code>","text":"<p>Validate the suggested candidates for the feature.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>suggested candidates for the feature</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Error is raised when one of the values is not numerical.</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: the original provided candidates</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>def validate_candidental(self, values: pd.Series) -&gt; pd.Series:\n    \"\"\"Validate the suggested candidates for the feature.\n\n    Args:\n        values (pd.Series): suggested candidates for the feature\n\n    Raises:\n        ValueError: Error is raised when one of the values is not numerical.\n\n    Returns:\n        pd.Series: the original provided candidates\n\n    \"\"\"\n    try:\n        return pd.to_numeric(values, errors=\"raise\").astype(\"float64\")\n    except ValueError:\n        raise ValueError(\n            f\"not all values of input feature `{self.key}` are numerical\",\n        )\n</code></pre>"},{"location":"ref-features/#bofire.data_models.features.numerical.NumericalInput.validate_experimental","title":"<code>validate_experimental(values, strict=False)</code>","text":"<p>Method to validate the experimental dataFrame</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Series</code> <p>A dataFrame with experiments</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when a value is not numerical</p> <code>ValueError</code> <p>when there is no variation in a feature provided by the experimental data</p> <p>Returns:</p> Type Description <code>Series</code> <p>pd.Series: A dataFrame with experiments</p> Source code in <code>bofire/data_models/features/numerical.py</code> <pre><code>def validate_experimental(self, values: pd.Series, strict=False) -&gt; pd.Series:\n    \"\"\"Method to validate the experimental dataFrame\n\n    Args:\n        values (pd.Series): A dataFrame with experiments\n        strict (bool, optional): Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not.\n            Defaults to False.\n\n    Raises:\n        ValueError: when a value is not numerical\n        ValueError: when there is no variation in a feature provided by the experimental data\n\n    Returns:\n        pd.Series: A dataFrame with experiments\n\n    \"\"\"\n    try:\n        values = pd.to_numeric(values, errors=\"raise\").astype(\"float64\")\n    except ValueError:\n        raise ValueError(\n            f\"not all values of input feature `{self.key}` are numerical\",\n        )\n\n    values = values.astype(\"float64\")\n    if strict:\n        lower, upper = self.get_bounds(transform_type=None, values=values)\n        if lower == upper:\n            raise ValueError(\n                f\"No variation present or planned for feature {self.key}. Remove it.\",\n            )\n    return values\n</code></pre>"},{"location":"ref-objectives/","title":"Objectives","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.categorical","title":"<code>categorical</code>","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.categorical.ConstrainedCategoricalObjective","title":"<code>ConstrainedCategoricalObjective</code>","text":"<p>               Bases: <code>ConstrainedObjective</code>, <code>Objective</code></p> <p>Compute the categorical objective value as:</p> <pre><code>Po where P is an [n, c] matrix where each row is a probability vector\n(P[i, :].sum()=1 for all i) and o is a vector of size [c] of objective values\n</code></pre> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>desirability</code> <code>list</code> <p>list of values of size c (c is number of categories) such that the i-th entry is in {True, False}</p> Source code in <code>bofire/data_models/objectives/categorical.py</code> <pre><code>class ConstrainedCategoricalObjective(ConstrainedObjective, Objective):\n    \"\"\"Compute the categorical objective value as:\n\n        Po where P is an [n, c] matrix where each row is a probability vector\n        (P[i, :].sum()=1 for all i) and o is a vector of size [c] of objective values\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        desirability (list): list of values of size c (c is number of categories) such that the i-th entry is in {True, False}\n\n    \"\"\"\n\n    w: TWeight = 1.0\n    categories: CategoryVals\n    desirability: List[bool]\n    type: Literal[\"ConstrainedCategoricalObjective\"] = \"ConstrainedCategoricalObjective\"\n\n    @model_validator(mode=\"after\")\n    def validate_desireability(self):\n        \"\"\"Validates that categories have unique names\n\n        Args:\n            categories (List[str]): List or tuple of category names\n\n        Raises:\n            ValueError: when categories do not match objective categories\n\n        Returns:\n            Tuple[str]: Tuple of the categories\n\n        \"\"\"\n        if len(self.desirability) != len(self.categories):\n            raise ValueError(\n                \"number of categories differs from number of desirabilities\",\n            )\n        return self\n\n    def to_dict(self) -&gt; Dict:\n        \"\"\"Returns the categories and corresponding objective values as dictionary\"\"\"\n        return dict(zip(self.categories, self.desirability))\n\n    def to_dict_label(self) -&gt; Dict:\n        \"\"\"Returns the categories and label location of categories\"\"\"\n        return {c: i for i, c in enumerate(self.categories)}\n\n    def from_dict_label(self) -&gt; Dict:\n        \"\"\"Returns the label location and the categories\"\"\"\n        d = self.to_dict_label()\n        return dict(zip(d.values(), d.keys()))\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray, float]:\n        \"\"\"The call function returning a probabilistic reward for x.\n\n        Args:\n            x (np.ndarray): A matrix of x values\n            x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n                update the objective parameters on the fly. Defaults to None.\n\n        Returns:\n            np.ndarray: A reward calculated as inner product of probabilities and feasible objectives.\n\n        \"\"\"\n        return np.dot(x, np.array(self.desirability))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.categorical.ConstrainedCategoricalObjective.__call__","title":"<code>__call__(x, x_adapt=None)</code>","text":"<p>The call function returning a probabilistic reward for x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>A matrix of x values</p> required <code>x_adapt</code> <code>Optional[ndarray]</code> <p>An array of x values which are used to update the objective parameters on the fly. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray, float]</code> <p>np.ndarray: A reward calculated as inner product of probabilities and feasible objectives.</p> Source code in <code>bofire/data_models/objectives/categorical.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray, float]:\n    \"\"\"The call function returning a probabilistic reward for x.\n\n    Args:\n        x (np.ndarray): A matrix of x values\n        x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n            update the objective parameters on the fly. Defaults to None.\n\n    Returns:\n        np.ndarray: A reward calculated as inner product of probabilities and feasible objectives.\n\n    \"\"\"\n    return np.dot(x, np.array(self.desirability))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.categorical.ConstrainedCategoricalObjective.from_dict_label","title":"<code>from_dict_label()</code>","text":"<p>Returns the label location and the categories</p> Source code in <code>bofire/data_models/objectives/categorical.py</code> <pre><code>def from_dict_label(self) -&gt; Dict:\n    \"\"\"Returns the label location and the categories\"\"\"\n    d = self.to_dict_label()\n    return dict(zip(d.values(), d.keys()))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.categorical.ConstrainedCategoricalObjective.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns the categories and corresponding objective values as dictionary</p> Source code in <code>bofire/data_models/objectives/categorical.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Returns the categories and corresponding objective values as dictionary\"\"\"\n    return dict(zip(self.categories, self.desirability))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.categorical.ConstrainedCategoricalObjective.to_dict_label","title":"<code>to_dict_label()</code>","text":"<p>Returns the categories and label location of categories</p> Source code in <code>bofire/data_models/objectives/categorical.py</code> <pre><code>def to_dict_label(self) -&gt; Dict:\n    \"\"\"Returns the categories and label location of categories\"\"\"\n    return {c: i for i, c in enumerate(self.categories)}\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.categorical.ConstrainedCategoricalObjective.validate_desireability","title":"<code>validate_desireability()</code>","text":"<p>Validates that categories have unique names</p> <p>Parameters:</p> Name Type Description Default <code>categories</code> <code>List[str]</code> <p>List or tuple of category names</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when categories do not match objective categories</p> <p>Returns:</p> Type Description <p>Tuple[str]: Tuple of the categories</p> Source code in <code>bofire/data_models/objectives/categorical.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_desireability(self):\n    \"\"\"Validates that categories have unique names\n\n    Args:\n        categories (List[str]): List or tuple of category names\n\n    Raises:\n        ValueError: when categories do not match objective categories\n\n    Returns:\n        Tuple[str]: Tuple of the categories\n\n    \"\"\"\n    if len(self.desirability) != len(self.categories):\n        raise ValueError(\n            \"number of categories differs from number of desirabilities\",\n        )\n    return self\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.desirabilities","title":"<code>desirabilities</code>","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.desirabilities.DecreasingDesirabilityObjective","title":"<code>DecreasingDesirabilityObjective</code>","text":"<p>               Bases: <code>DesirabilityObjective</code></p> <p>An objective returning a reward the negative, shifted scaled identity, but trimmed at the bounds:</p> <pre><code>d = ((upper_bound - x) / (upper_bound - lower_bound))^t\n</code></pre> <p>where:</p> <pre><code>t = exp(log_shape_factor)\n</code></pre> <p>Note, that with clipping the reward is always between zero and one.</p> <p>Attributes:</p> Name Type Description <code>clip</code> <code>bool</code> <p>Whether to clip the values below/above the lower/upper bound, by default True.</p> <code>log_shape_factor</code> <code>float</code> <p>Logarithm of the shape factor: Whether the interpolation between the lower bound and the upper is linear (=0), convex (&gt;0) or concave (&lt;0) , by default 0.0.</p> <code>w</code> <code>float</code> <p>relative weight, by default = 1.</p> <code>bounds</code> <code>tuple[float]</code> <p>lower and upper bound of the desirability. Below bounds[0] the desirability is =1 (if clip=True) or &gt;1 (if clip=False). Above bounds[1] the desirability is =0  (if clip=True) or &lt;0 (if clip=False). Defaults to (0, 1).</p> Source code in <code>bofire/data_models/objectives/desirabilities.py</code> <pre><code>class DecreasingDesirabilityObjective(DesirabilityObjective):\n    \"\"\"An objective returning a reward the negative, shifted scaled identity, but trimmed at the bounds:\n\n        d = ((upper_bound - x) / (upper_bound - lower_bound))^t\n\n    where:\n\n        t = exp(log_shape_factor)\n\n    Note, that with clipping the reward is always between zero and one.\n\n    Attributes:\n        clip (bool): Whether to clip the values below/above the lower/upper bound, by\n            default True.\n        log_shape_factor (float): Logarithm of the shape factor:\n            Whether the interpolation between the lower bound and the upper is linear (=0),\n            convex (&gt;0) or concave (&lt;0) , by default 0.0.\n        w (float): relative weight, by default = 1.\n        bounds (tuple[float]): lower and upper bound of the desirability. Below\n            bounds[0] the desirability is =1 (if clip=True) or &gt;1 (if clip=False). Above\n            bounds[1] the desirability is =0  (if clip=True) or &lt;0 (if clip=False).\n            Defaults to (0, 1).\n    \"\"\"\n\n    type: Literal[\"DecreasingDesirabilityObjective\"] = \"DecreasingDesirabilityObjective\"  # type: ignore\n    log_shape_factor: float = 0.0\n\n    def call_numpy(\n        self,\n        x: np.ndarray,\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; np.ndarray:\n        y = np.zeros(x.shape)\n        if self.clip:\n            y[x &lt; self.lower_bound] = 1.0\n            y[x &gt; self.upper_bound] = 0.0\n            between = (x &gt;= self.lower_bound) &amp; (x &lt;= self.upper_bound)\n        else:\n            between = np.full(x.shape, True)\n\n        t = np.exp(self.log_shape_factor)\n\n        y[between] = np.power(\n            (self.upper_bound - x[between]) / (self.upper_bound - self.lower_bound), t\n        )\n\n        return y\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.desirabilities.DesirabilityObjective","title":"<code>DesirabilityObjective</code>","text":"<p>               Bases: <code>IdentityObjective</code></p> <p>Abstract class for desirability objectives. Works as Identity Objective</p> Source code in <code>bofire/data_models/objectives/desirabilities.py</code> <pre><code>class DesirabilityObjective(IdentityObjective):\n    \"\"\"Abstract class for desirability objectives. Works as Identity Objective\"\"\"\n\n    type: Literal[\"DesirabilityObjective\"] = \"DesirabilityObjective\"  # type: ignore\n    clip: bool = True\n\n    @pydantic.model_validator(mode=\"after\")\n    def validate_clip(self):\n        if self.clip:\n            return self\n\n        log_shapes = {\n            key: val\n            for (key, val) in self.__dict__.items()\n            if key.startswith(\"log_shape_factor\")\n        }\n        for key, log_shape_ in log_shapes.items():\n            if log_shape_ != 0:\n                raise ValueError(\n                    f\"Log shape factor {key} must be zero if clip is False.\"\n                )\n        return self\n\n    def __call__(\n        self, x: Union[pd.Series, np.ndarray], x_adapt\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"Wrapper function for to call numpy and torch functions with series\n        or numpy arrays. matches __call__ signature of objectives.\"\"\"\n        if isinstance(x, pd.Series):\n            s: pd.Series = x\n            return pd.Series(self.call_numpy(s.to_numpy()), name=s.name)\n\n        return self.call_numpy(x)\n\n    @abstractmethod\n    def call_numpy(self, x: np.ndarray) -&gt; np.ndarray:\n        raise NotImplementedError()\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.desirabilities.DesirabilityObjective.__call__","title":"<code>__call__(x, x_adapt)</code>","text":"<p>Wrapper function for to call numpy and torch functions with series or numpy arrays. matches call signature of objectives.</p> Source code in <code>bofire/data_models/objectives/desirabilities.py</code> <pre><code>def __call__(\n    self, x: Union[pd.Series, np.ndarray], x_adapt\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"Wrapper function for to call numpy and torch functions with series\n    or numpy arrays. matches __call__ signature of objectives.\"\"\"\n    if isinstance(x, pd.Series):\n        s: pd.Series = x\n        return pd.Series(self.call_numpy(s.to_numpy()), name=s.name)\n\n    return self.call_numpy(x)\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.desirabilities.IncreasingDesirabilityObjective","title":"<code>IncreasingDesirabilityObjective</code>","text":"<p>               Bases: <code>DesirabilityObjective</code></p> <p>An objective returning a reward the scaled identity, but trimmed at the bounds:</p> <pre><code>d = ((x - lower_bound) / (upper_bound - lower_bound))^t\n</code></pre> <p>if clip is True, the reward is zero for x &lt; lower_bound and one for x &gt; upper_bound.</p> <p>where:</p> <pre><code>t = exp(log_shape_factor)\n</code></pre> <p>Note, that with clipping the reward is always between zero and one.</p> <p>Attributes:</p> Name Type Description <code>clip</code> <code>bool</code> <p>Whether to clip the values below/above the lower/upper bound, by default True.</p> <code>log_shape_factor</code> <code>float</code> <p>Logarithm of the shape factor: Whether the interpolation between the lower bound and the upper is linear (=0), convex (&gt;0) or concave (&lt;0) , by default 0.0.</p> <code>w</code> <code>float</code> <p>relative weight, by default = 1.</p> <code>bounds</code> <code>tuple[float]</code> <p>lower and upper bound of the desirability. Below bounds[0] the desirability is =0 (if clip=True) or &lt;0 (if clip=False). Above bounds[1] the desirability is =1  (if clip=True) or &gt;1 (if clip=False). Defaults to (0, 1).</p> Source code in <code>bofire/data_models/objectives/desirabilities.py</code> <pre><code>class IncreasingDesirabilityObjective(DesirabilityObjective):\n    \"\"\"An objective returning a reward the scaled identity, but trimmed at the bounds:\n\n        d = ((x - lower_bound) / (upper_bound - lower_bound))^t\n\n    if clip is True, the reward is zero for x &lt; lower_bound and one for x &gt; upper_bound.\n\n    where:\n\n        t = exp(log_shape_factor)\n\n    Note, that with clipping the reward is always between zero and one.\n\n    Attributes:\n        clip (bool): Whether to clip the values below/above the lower/upper bound, by\n            default True.\n        log_shape_factor (float): Logarithm of the shape factor:\n            Whether the interpolation between the lower bound and the upper is linear (=0),\n            convex (&gt;0) or concave (&lt;0) , by default 0.0.\n        w (float): relative weight, by default = 1.\n        bounds (tuple[float]): lower and upper bound of the desirability. Below\n            bounds[0] the desirability is =0 (if clip=True) or &lt;0 (if clip=False). Above\n            bounds[1] the desirability is =1  (if clip=True) or &gt;1 (if clip=False).\n            Defaults to (0, 1).\n    \"\"\"\n\n    type: Literal[\"IncreasingDesirabilityObjective\"] = \"IncreasingDesirabilityObjective\"  # type: ignore\n    log_shape_factor: float = 0.0\n\n    def call_numpy(\n        self,\n        x: np.ndarray,\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; np.ndarray:\n        y = np.zeros(x.shape)\n        if self.clip:\n            y[x &lt; self.lower_bound] = 0.0\n            y[x &gt; self.upper_bound] = 1.0\n            between = (x &gt;= self.lower_bound) &amp; (x &lt;= self.upper_bound)\n        else:\n            between = np.full(x.shape, True)\n\n        t = np.exp(self.log_shape_factor)\n\n        y[between] = np.power(\n            (x[between] - self.lower_bound) / (self.upper_bound - self.lower_bound), t\n        )\n\n        return y\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.desirabilities.PeakDesirabilityObjective","title":"<code>PeakDesirabilityObjective</code>","text":"<p>               Bases: <code>DesirabilityObjective</code></p> <p>A piecewise (linear or convex/concave) objective that increases from the lower bound to the peak position and decreases from the peak position to the upper bound.</p> <p>Attributes:</p> Name Type Description <code>clip</code> <code>bool</code> <p>Whether to clip the values below/above the lower/upper bound, by default True.</p> <code>log_shape_factor</code> <code>float</code> <p>Logarithm of the shape factor for the increasing part: Whether the interpolation between the lower bound and the peak is linear (=0), convex (&gt;1) or concave (&lt;1) , by default 0.0.</p> <code>log_shape_factor_decreasing</code> <code>float</code> <p>Logarithm of the shape factor for the decreasing part. Whether the interpolation between the peak and the upper bound is linear (=0), convex (&gt;0) or concave (&lt;0), by default 0.0.</p> <code>peak_position</code> <code>float</code> <p>Position of the peak, by default 0.5.</p> <code>w</code> <code>float</code> <p>relative weight: desirability, when x=peak_position, by default = 1.</p> <code>bounds</code> <code>tuple[float]</code> <p>lower and upper bound of the desirability. Below bounds[0] the desirability is =0 (if clip=True) or &lt;0 (if clip=False). Above bounds[1] the desirability is =0  (if clip=True) or &lt;0 (if clip=False). Defaults to (0, 1).</p> Source code in <code>bofire/data_models/objectives/desirabilities.py</code> <pre><code>class PeakDesirabilityObjective(DesirabilityObjective):\n    \"\"\"\n    A piecewise (linear or convex/concave) objective that increases from the lower bound\n    to the peak position and decreases from the peak position to the upper bound.\n\n    Attributes:\n        clip (bool): Whether to clip the values below/above the lower/upper bound, by\n            default True.\n        log_shape_factor (float): Logarithm of the shape factor for the increasing part:\n            Whether the interpolation between the lower bound and the peak is linear (=0),\n            convex (&gt;1) or concave (&lt;1) , by default 0.0.\n        log_shape_factor_decreasing (float): Logarithm of the shape factor for the\n            decreasing part. Whether the interpolation between the peak and the upper\n            bound is linear (=0), convex (&gt;0) or concave (&lt;0), by default 0.0.\n        peak_position (float): Position of the peak, by default 0.5.\n        w (float): relative weight: desirability, when x=peak_position, by default = 1.\n        bounds (tuple[float]): lower and upper bound of the desirability. Below\n            bounds[0] the desirability is =0 (if clip=True) or &lt;0 (if clip=False). Above\n            bounds[1] the desirability is =0  (if clip=True) or &lt;0 (if clip=False).\n            Defaults to (0, 1).\n    \"\"\"\n\n    type: Literal[\"PeakDesirabilityObjective\"] = \"PeakDesirabilityObjective\"  # type: ignore\n    log_shape_factor: float = 0.0\n    log_shape_factor_decreasing: float = 0.0  # often named log_t\n    peak_position: float = 0.5  # often named T\n\n    def call_numpy(\n        self,\n        x: np.ndarray,\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; np.ndarray:\n        y = np.zeros(x.shape)\n        if self.clip:\n            Incr = (x &gt;= self.lower_bound) &amp; (x &lt;= self.peak_position)\n            Decr = (x &lt;= self.upper_bound) &amp; (x &gt; self.peak_position)\n        else:\n            Incr, Decr = x &lt;= self.peak_position, x &gt; self.peak_position\n\n        s: float = np.exp(self.log_shape_factor)\n        t: float = np.exp(self.log_shape_factor_decreasing)\n        y[Incr] = np.power(\n            np.divide(\n                (x[Incr] - self.lower_bound), (self.peak_position - self.lower_bound)\n            ),\n            s,\n        )\n        y[Decr] = np.power(\n            np.divide(\n                (x[Decr] - self.upper_bound), (self.peak_position - self.upper_bound)\n            ),\n            t,\n        )\n\n        return y * self.w\n\n    @pydantic.model_validator(mode=\"after\")\n    def validate_peak_position(self):\n        bounds = self.bounds\n        if self.peak_position &lt; bounds[0] or self.peak_position &gt; bounds[1]:\n            raise ValueError(\n                f\"Peak position must be within bounds {bounds}, got {self.peak_position}\"\n            )\n        return self\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.identity","title":"<code>identity</code>","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.identity.IdentityObjective","title":"<code>IdentityObjective</code>","text":"<p>               Bases: <code>Objective</code></p> <p>An objective returning the identity as reward. The return can be scaled, when a lower and upper bound are provided.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>bounds</code> <code>Tuple[float]</code> <p>Bound for normalizing the objective between zero and one. Defaults to (0,1).</p> Source code in <code>bofire/data_models/objectives/identity.py</code> <pre><code>class IdentityObjective(Objective):\n    \"\"\"An objective returning the identity as reward.\n    The return can be scaled, when a lower and upper bound are provided.\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        bounds (Tuple[float], optional): Bound for normalizing the objective between zero and one. Defaults to (0,1).\n\n    \"\"\"\n\n    type: Literal[\"IdentityObjective\"] = \"IdentityObjective\"  # type: ignore\n    w: TWeight = 1\n    bounds: Bounds = [0, 1]\n\n    @property\n    def lower_bound(self) -&gt; float:\n        return self.bounds[0]\n\n    @property\n    def upper_bound(self) -&gt; float:\n        return self.bounds[1]\n\n    @field_validator(\"bounds\")\n    @classmethod\n    def validate_lower_upper(cls, bounds):\n        \"\"\"Validation function to ensure that lower bound is always greater the upper bound\n\n        Args:\n            values (Dict): The attributes of the class\n\n        Raises:\n            ValueError: when a lower bound higher than the upper bound is passed\n\n        Returns:\n            Dict: The attributes of the class\n\n        \"\"\"\n        if bounds[0] &gt; bounds[1]:\n            raise ValueError(\n                f\"lower bound must be &lt;= upper bound, got {bounds[0]} &gt; {bounds[1]}\",\n            )\n        return bounds\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"The call function returning a reward for passed x values\n\n        Args:\n            x (np.ndarray): An array of x values\n            x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n                update the objective parameters on the fly. Defaults to None.\n\n        Returns:\n            np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds\n\n        \"\"\"\n        return (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.identity.IdentityObjective.__call__","title":"<code>__call__(x, x_adapt=None)</code>","text":"<p>The call function returning a reward for passed x values</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values</p> required <code>x_adapt</code> <code>Optional[ndarray]</code> <p>An array of x values which are used to update the objective parameters on the fly. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds</p> Source code in <code>bofire/data_models/objectives/identity.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"The call function returning a reward for passed x values\n\n    Args:\n        x (np.ndarray): An array of x values\n        x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n            update the objective parameters on the fly. Defaults to None.\n\n    Returns:\n        np.ndarray: The identity as reward, might be normalized to the passed lower and upper bounds\n\n    \"\"\"\n    return (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.identity.IdentityObjective.validate_lower_upper","title":"<code>validate_lower_upper(bounds)</code>  <code>classmethod</code>","text":"<p>Validation function to ensure that lower bound is always greater the upper bound</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Dict</code> <p>The attributes of the class</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>when a lower bound higher than the upper bound is passed</p> <p>Returns:</p> Name Type Description <code>Dict</code> <p>The attributes of the class</p> Source code in <code>bofire/data_models/objectives/identity.py</code> <pre><code>@field_validator(\"bounds\")\n@classmethod\ndef validate_lower_upper(cls, bounds):\n    \"\"\"Validation function to ensure that lower bound is always greater the upper bound\n\n    Args:\n        values (Dict): The attributes of the class\n\n    Raises:\n        ValueError: when a lower bound higher than the upper bound is passed\n\n    Returns:\n        Dict: The attributes of the class\n\n    \"\"\"\n    if bounds[0] &gt; bounds[1]:\n        raise ValueError(\n            f\"lower bound must be &lt;= upper bound, got {bounds[0]} &gt; {bounds[1]}\",\n        )\n    return bounds\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.identity.MaximizeObjective","title":"<code>MaximizeObjective</code>","text":"<p>               Bases: <code>IdentityObjective</code></p> <p>Child class from the identity function without modifications, since the parent class is already defined as maximization</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>bounds</code> <code>Tuple[float]</code> <p>Bound for normalizing the objective between zero and one. Defaults to (0,1).</p> Source code in <code>bofire/data_models/objectives/identity.py</code> <pre><code>class MaximizeObjective(IdentityObjective):\n    \"\"\"Child class from the identity function without modifications, since the parent class is already defined as maximization\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        bounds (Tuple[float], optional): Bound for normalizing the objective between zero and one. Defaults to (0,1).\n\n    \"\"\"\n\n    type: Literal[\"MaximizeObjective\"] = \"MaximizeObjective\"\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.identity.MinimizeObjective","title":"<code>MinimizeObjective</code>","text":"<p>               Bases: <code>IdentityObjective</code></p> <p>Class returning the negative identity as reward.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective</p> <code>bounds</code> <code>Tuple[float]</code> <p>Bound for normalizing the objective between zero and one. Defaults to (0,1).</p> Source code in <code>bofire/data_models/objectives/identity.py</code> <pre><code>class MinimizeObjective(IdentityObjective):\n    \"\"\"Class returning the negative identity as reward.\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective\n        bounds (Tuple[float], optional): Bound for normalizing the objective between zero and one. Defaults to (0,1).\n\n    \"\"\"\n\n    type: Literal[\"MinimizeObjective\"] = \"MinimizeObjective\"\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"The call function returning a reward for passed x values\n\n        Args:\n            x (np.ndarray): An array of x values\n            x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n                update the objective parameters on the fly. Defaults to None.\n\n        Returns:\n            np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds\n\n        \"\"\"\n        return -1.0 * (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.identity.MinimizeObjective.__call__","title":"<code>__call__(x, x_adapt=None)</code>","text":"<p>The call function returning a reward for passed x values</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values</p> required <code>x_adapt</code> <code>Optional[ndarray]</code> <p>An array of x values which are used to update the objective parameters on the fly. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds</p> Source code in <code>bofire/data_models/objectives/identity.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"The call function returning a reward for passed x values\n\n    Args:\n        x (np.ndarray): An array of x values\n        x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n            update the objective parameters on the fly. Defaults to None.\n\n    Returns:\n        np.ndarray: The negative identity as reward, might be normalized to the passed lower and upper bounds\n\n    \"\"\"\n    return -1.0 * (x - self.lower_bound) / (self.upper_bound - self.lower_bound)\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.objective","title":"<code>objective</code>","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.objective.ConstrainedObjective","title":"<code>ConstrainedObjective</code>","text":"<p>This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints.</p> Source code in <code>bofire/data_models/objectives/objective.py</code> <pre><code>class ConstrainedObjective:\n    \"\"\"This abstract class offers a convenience routine for transforming sigmoid based objectives to botorch output constraints.\"\"\"\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.objective.Objective","title":"<code>Objective</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>The base class for all objectives</p> Source code in <code>bofire/data_models/objectives/objective.py</code> <pre><code>class Objective(BaseModel):\n    \"\"\"The base class for all objectives\"\"\"\n\n    type: str\n\n    @abstractmethod\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"Abstract method to define the call function for the class Objective\n\n        Args:\n            x (np.ndarray): An array of x values for which the objective should be evaluated.\n            x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n                update the objective parameters on the fly. Defaults to None.\n\n        Returns:\n            np.ndarray: The desirability of the passed x values\n\n        \"\"\"\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.objective.Objective.__call__","title":"<code>__call__(x, x_adapt=None)</code>  <code>abstractmethod</code>","text":"<p>Abstract method to define the call function for the class Objective</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values for which the objective should be evaluated.</p> required <code>x_adapt</code> <code>Optional[ndarray]</code> <p>An array of x values which are used to update the objective parameters on the fly. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.ndarray: The desirability of the passed x values</p> Source code in <code>bofire/data_models/objectives/objective.py</code> <pre><code>@abstractmethod\ndef __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"Abstract method to define the call function for the class Objective\n\n    Args:\n        x (np.ndarray): An array of x values for which the objective should be evaluated.\n        x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n            update the objective parameters on the fly. Defaults to None.\n\n    Returns:\n        np.ndarray: The desirability of the passed x values\n\n    \"\"\"\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid","title":"<code>sigmoid</code>","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MaximizeSigmoidObjective","title":"<code>MaximizeSigmoidObjective</code>","text":"<p>               Bases: <code>SigmoidObjective</code></p> <p>Class for a maximizing sigmoid objective</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Turning point of the sigmoid function.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>class MaximizeSigmoidObjective(SigmoidObjective):\n    \"\"\"Class for a maximizing sigmoid objective\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Turning point of the sigmoid function.\n\n    \"\"\"\n\n    type: Literal[\"MaximizeSigmoidObjective\"] = \"MaximizeSigmoidObjective\"\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n        Args:\n            x (np.ndarray): An array of x values\n            x_adapt (np.ndarray): An array of x values which are used to update the objective parameters on the fly.\n\n        Returns:\n            np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n\n        \"\"\"\n        return 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MaximizeSigmoidObjective.__call__","title":"<code>__call__(x, x_adapt=None)</code>","text":"<p>The call function returning a sigmoid shaped reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values</p> required <code>x_adapt</code> <code>ndarray</code> <p>An array of x values which are used to update the objective parameters on the fly.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n    Args:\n        x (np.ndarray): An array of x values\n        x_adapt (np.ndarray): An array of x values which are used to update the objective parameters on the fly.\n\n    Returns:\n        np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n\n    \"\"\"\n    return 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MinimizeSigmoidObjective","title":"<code>MinimizeSigmoidObjective</code>","text":"<p>               Bases: <code>SigmoidObjective</code></p> <p>Class for a minimizing a sigmoid objective</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Turning point of the sigmoid function.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>class MinimizeSigmoidObjective(SigmoidObjective):\n    \"\"\"Class for a minimizing a sigmoid objective\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Turning point of the sigmoid function.\n\n    \"\"\"\n\n    type: Literal[\"MinimizeSigmoidObjective\"] = \"MinimizeSigmoidObjective\"\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n        Args:\n            x (np.ndarray): An array of x values\n            x_adapt (np.ndarray): An array of x values which are used to update the objective parameters on the fly.\n\n        Returns:\n            np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n\n        \"\"\"\n        return 1 - 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MinimizeSigmoidObjective.__call__","title":"<code>__call__(x, x_adapt=None)</code>","text":"<p>The call function returning a sigmoid shaped reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values</p> required <code>x_adapt</code> <code>ndarray</code> <p>An array of x values which are used to update the objective parameters on the fly.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n    Args:\n        x (np.ndarray): An array of x values\n        x_adapt (np.ndarray): An array of x values which are used to update the objective parameters on the fly.\n\n    Returns:\n        np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n\n    \"\"\"\n    return 1 - 1 / (1 + np.exp(-1 * self.steepness * (x - self.tp)))\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MovingMaximizeSigmoidObjective","title":"<code>MovingMaximizeSigmoidObjective</code>","text":"<p>               Bases: <code>SigmoidObjective</code></p> <p>Class for a maximizing sigmoid objective with a moving turning point that depends on so far observed x values.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective when used in a weighting based strategy.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Relative turning point of the sigmoid function. The actual turning point is calculated by adding the maximum of the observed x values to the relative turning point.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>class MovingMaximizeSigmoidObjective(SigmoidObjective):\n    \"\"\"Class for a maximizing sigmoid objective with a moving turning point that depends on so far observed x values.\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective when used in a weighting based strategy.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Relative turning point of the sigmoid function. The actual turning point is calculated by adding\n            the maximum of the observed x values to the relative turning point.\n\n    \"\"\"\n\n    type: Literal[\"MovingMaximizeSigmoidObjective\"] = \"MovingMaximizeSigmoidObjective\"\n\n    def get_adjusted_tp(self, x: Union[pd.Series, np.ndarray]) -&gt; float:\n        \"\"\"Get the adjusted turning point for the sigmoid function.\n\n        Args:\n            x (np.ndarray): An array of x values\n\n        Returns:\n            float: The adjusted turning point for the sigmoid function.\n\n        \"\"\"\n        return x.max() + self.tp\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Union[pd.Series, np.ndarray],\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n        Args:\n            x (np.ndarray): An array of x values\n            x_adapt (np.ndarray): An array of x values which are used to update the objective parameters on the fly.\n\n        Returns:\n            np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n\n        \"\"\"\n        return 1 / (\n            1 + np.exp(-1 * self.steepness * (x - self.get_adjusted_tp(x_adapt)))\n        )\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MovingMaximizeSigmoidObjective.__call__","title":"<code>__call__(x, x_adapt)</code>","text":"<p>The call function returning a sigmoid shaped reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values</p> required <code>x_adapt</code> <code>ndarray</code> <p>An array of x values which are used to update the objective parameters on the fly.</p> required <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Union[pd.Series, np.ndarray],\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"The call function returning a sigmoid shaped reward for passed x values.\n\n    Args:\n        x (np.ndarray): An array of x values\n        x_adapt (np.ndarray): An array of x values which are used to update the objective parameters on the fly.\n\n    Returns:\n        np.ndarray: A reward calculated with a sigmoid function. The stepness and the tipping point can be modified via passed arguments.\n\n    \"\"\"\n    return 1 / (\n        1 + np.exp(-1 * self.steepness * (x - self.get_adjusted_tp(x_adapt)))\n    )\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.MovingMaximizeSigmoidObjective.get_adjusted_tp","title":"<code>get_adjusted_tp(x)</code>","text":"<p>Get the adjusted turning point for the sigmoid function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>An array of x values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The adjusted turning point for the sigmoid function.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>def get_adjusted_tp(self, x: Union[pd.Series, np.ndarray]) -&gt; float:\n    \"\"\"Get the adjusted turning point for the sigmoid function.\n\n    Args:\n        x (np.ndarray): An array of x values\n\n    Returns:\n        float: The adjusted turning point for the sigmoid function.\n\n    \"\"\"\n    return x.max() + self.tp\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.sigmoid.SigmoidObjective","title":"<code>SigmoidObjective</code>","text":"<p>               Bases: <code>Objective</code>, <code>ConstrainedObjective</code></p> <p>Base class for all sigmoid shaped objectives</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> <code>tp</code> <code>float</code> <p>Turning point of the sigmoid function.</p> Source code in <code>bofire/data_models/objectives/sigmoid.py</code> <pre><code>class SigmoidObjective(Objective, ConstrainedObjective):\n    \"\"\"Base class for all sigmoid shaped objectives\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n        tp (float): Turning point of the sigmoid function.\n\n    \"\"\"\n\n    steepness: TGt0\n    tp: float\n    w: TWeight = 1\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.target","title":"<code>target</code>","text":""},{"location":"ref-objectives/#bofire.data_models.objectives.target.CloseToTargetObjective","title":"<code>CloseToTargetObjective</code>","text":"<p>               Bases: <code>Objective</code></p> <p>Optimize towards a target value. It can be used as objective in multiobjective scenarios.</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>target_value</code> <code>float</code> <p>target value that should be reached.</p> <code>exponent</code> <code>float</code> <p>the exponent of the expression.</p> Source code in <code>bofire/data_models/objectives/target.py</code> <pre><code>class CloseToTargetObjective(Objective):\n    \"\"\"Optimize towards a target value. It can be used as objective\n    in multiobjective scenarios.\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        target_value (float): target value that should be reached.\n        exponent (float): the exponent of the expression.\n\n    \"\"\"\n\n    type: Literal[\"CloseToTargetObjective\"] = \"CloseToTargetObjective\"\n    w: TWeight = 1\n    target_value: float\n    exponent: float\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        return -1 * (np.abs(x - self.target_value) ** self.exponent)\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.target.TargetObjective","title":"<code>TargetObjective</code>","text":"<p>               Bases: <code>Objective</code>, <code>ConstrainedObjective</code></p> <p>Class for objectives for optimizing towards a target value</p> <p>Attributes:</p> Name Type Description <code>w</code> <code>float</code> <p>float between zero and one for weighting the objective.</p> <code>target_value</code> <code>float</code> <p>target value that should be reached.</p> <code>tolerance</code> <code>float</code> <p>Tolerance for reaching the target. Has to be greater than zero.</p> <code>steepness</code> <code>float</code> <p>Steepness of the sigmoid function. Has to be greater than zero.</p> Source code in <code>bofire/data_models/objectives/target.py</code> <pre><code>class TargetObjective(Objective, ConstrainedObjective):\n    \"\"\"Class for objectives for optimizing towards a target value\n\n    Attributes:\n        w (float): float between zero and one for weighting the objective.\n        target_value (float): target value that should be reached.\n        tolerance (float): Tolerance for reaching the target. Has to be greater than zero.\n        steepness (float): Steepness of the sigmoid function. Has to be greater than zero.\n\n    \"\"\"\n\n    type: Literal[\"TargetObjective\"] = \"TargetObjective\"\n    w: TWeight = 1\n    target_value: float\n    tolerance: TGe0\n    steepness: TGt0\n\n    def __call__(\n        self,\n        x: Union[pd.Series, np.ndarray],\n        x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n    ) -&gt; Union[pd.Series, np.ndarray]:\n        \"\"\"The call function returning a reward for passed x values.\n\n        Args:\n            x (np.array): An array of x values\n            x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n                update the objective parameters on the fly. Defaults to None.\n\n        Returns:\n            np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value.\n\n        \"\"\"\n        return (\n            1\n            / (\n                1\n                + np.exp(\n                    -1 * self.steepness * (x - (self.target_value - self.tolerance)),\n                )\n            )\n            * (\n                1\n                - 1\n                / (\n                    1.0\n                    + np.exp(\n                        -1\n                        * self.steepness\n                        * (x - (self.target_value + self.tolerance)),\n                    )\n                )\n            )\n        )\n</code></pre>"},{"location":"ref-objectives/#bofire.data_models.objectives.target.TargetObjective.__call__","title":"<code>__call__(x, x_adapt=None)</code>","text":"<p>The call function returning a reward for passed x values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>array</code> <p>An array of x values</p> required <code>x_adapt</code> <code>Optional[ndarray]</code> <p>An array of x values which are used to update the objective parameters on the fly. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[Series, ndarray]</code> <p>np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value.</p> Source code in <code>bofire/data_models/objectives/target.py</code> <pre><code>def __call__(\n    self,\n    x: Union[pd.Series, np.ndarray],\n    x_adapt: Optional[Union[pd.Series, np.ndarray]] = None,\n) -&gt; Union[pd.Series, np.ndarray]:\n    \"\"\"The call function returning a reward for passed x values.\n\n    Args:\n        x (np.array): An array of x values\n        x_adapt (Optional[np.ndarray], optional): An array of x values which are used to\n            update the objective parameters on the fly. Defaults to None.\n\n    Returns:\n        np.array: An array of reward values calculated by the product of two sigmoidal shaped functions resulting in a maximum at the target value.\n\n    \"\"\"\n    return (\n        1\n        / (\n            1\n            + np.exp(\n                -1 * self.steepness * (x - (self.target_value - self.tolerance)),\n            )\n        )\n        * (\n            1\n            - 1\n            / (\n                1.0\n                + np.exp(\n                    -1\n                    * self.steepness\n                    * (x - (self.target_value + self.tolerance)),\n                )\n            )\n        )\n    )\n</code></pre>"},{"location":"ref-utils/","title":"Utils","text":""},{"location":"ref-utils/#bofire.utils.cheminformatics","title":"<code>cheminformatics</code>","text":""},{"location":"ref-utils/#bofire.utils.cheminformatics.smiles2fingerprints","title":"<code>smiles2fingerprints(smiles, bond_radius=5, n_bits=2048)</code>","text":"<p>Transforms a list of smiles to an array of morgan fingerprints.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>List[str]</code> <p>List of smiles</p> required <code>bond_radius</code> <code>int</code> <p>Bond radius to use. Defaults to 5.</p> <code>5</code> <code>n_bits</code> <code>int</code> <p>Number of bits. Defaults to 2048.</p> <code>2048</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Numpy array holding the fingerprints</p> Source code in <code>bofire/utils/cheminformatics.py</code> <pre><code>def smiles2fingerprints(\n    smiles: List[str],\n    bond_radius: int = 5,\n    n_bits: int = 2048,\n) -&gt; np.ndarray:\n    \"\"\"Transforms a list of smiles to an array of morgan fingerprints.\n\n    Args:\n        smiles (List[str]): List of smiles\n        bond_radius (int, optional): Bond radius to use. Defaults to 5.\n        n_bits (int, optional): Number of bits. Defaults to 2048.\n\n    Returns:\n        np.ndarray: Numpy array holding the fingerprints\n\n    \"\"\"\n    rdkit_mols = [smiles2mol(m) for m in smiles]\n    fps = [\n        AllChem.GetMorganFingerprintAsBitVect(mol, radius=bond_radius, nBits=n_bits)  # type: ignore\n        for mol in rdkit_mols\n    ]\n\n    return np.asarray(fps)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.cheminformatics.smiles2fragments","title":"<code>smiles2fragments(smiles, fragments_list=None)</code>","text":"<p>Transforms smiles to an array of fragments.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>list[str]</code> <p>List of smiles</p> required <code>fragments_list</code> <code>list[str]</code> <p>List of desired fragments. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array holding the fragment information.</p> Source code in <code>bofire/utils/cheminformatics.py</code> <pre><code>def smiles2fragments(\n    smiles: List[str],\n    fragments_list: Optional[List[str]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Transforms smiles to an array of fragments.\n\n    Args:\n        smiles (list[str]): List of smiles\n        fragments_list (list[str], optional): List of desired fragments. Defaults to None.\n\n    Returns:\n        np.ndarray: Array holding the fragment information.\n\n    \"\"\"\n    rdkit_fragment_list = [\n        item\n        for item in Descriptors.descList\n        if item[0].startswith(\"fr_\")  # type: ignore\n    ]\n    if fragments_list is None:\n        fragments = {d[0]: d[1] for d in rdkit_fragment_list}\n    else:\n        fragments = {d[0]: d[1] for d in rdkit_fragment_list if d[0] in fragments_list}\n\n    frags = np.zeros((len(smiles), len(fragments)))\n    for i, smi in enumerate(smiles):\n        mol = smiles2mol(smi)\n        features = [fragments[d](mol) for d in fragments]\n        frags[i, :] = features\n\n    return frags\n</code></pre>"},{"location":"ref-utils/#bofire.utils.cheminformatics.smiles2mol","title":"<code>smiles2mol(smiles)</code>","text":"<p>Transforms a smiles string to an rdkit mol object.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>str</code> <p>Smiles string.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If string is not a valid smiles.</p> <p>Returns:</p> Type Description <p>rdkit.Mol: rdkit.mol object</p> Source code in <code>bofire/utils/cheminformatics.py</code> <pre><code>def smiles2mol(smiles: str):\n    \"\"\"Transforms a smiles string to an rdkit mol object.\n\n    Args:\n        smiles (str): Smiles string.\n\n    Raises:\n        ValueError: If string is not a valid smiles.\n\n    Returns:\n        rdkit.Mol: rdkit.mol object\n\n    \"\"\"\n    mol = MolFromSmiles(smiles)  # type: ignore\n    if mol is None:\n        raise ValueError(f\"{smiles} is not a valid smiles string.\")\n    return mol\n</code></pre>"},{"location":"ref-utils/#bofire.utils.cheminformatics.smiles2mordred","title":"<code>smiles2mordred(smiles, descriptors_list)</code>","text":"<p>Transforms list of smiles to mordred moelcular descriptors.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>List[str]</code> <p>List of smiles</p> required <code>descriptors_list</code> <code>List[str]</code> <p>List of desired mordred descriptors</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array holding the mordred moelcular descriptors.</p> Source code in <code>bofire/utils/cheminformatics.py</code> <pre><code>def smiles2mordred(smiles: List[str], descriptors_list: List[str]) -&gt; np.ndarray:\n    \"\"\"Transforms list of smiles to mordred moelcular descriptors.\n\n    Args:\n        smiles (List[str]): List of smiles\n        descriptors_list (List[str]): List of desired mordred descriptors\n\n    Returns:\n        np.ndarray: Array holding the mordred moelcular descriptors.\n\n    \"\"\"\n    mols = [smiles2mol(smi) for smi in smiles]\n\n    calc = Calculator(descriptors, ignore_3D=True)  # type: ignore\n    calc.descriptors = [d for d in calc.descriptors if str(d) in descriptors_list]\n\n    descriptors_df = calc.pandas(mols)\n    nan_list = [\n        pd.to_numeric(descriptors_df[col], errors=\"coerce\").isnull().values.any()  # type: ignore\n        for col in descriptors_df.columns\n    ]\n    if any(nan_list):\n        raise ValueError(\n            f\"Found NaN values in descriptors {list(descriptors_df.columns[nan_list])}\",  # type: ignore\n        )\n\n    return descriptors_df.astype(float).values\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe","title":"<code>doe</code>","text":""},{"location":"ref-utils/#bofire.utils.doe.apply_block_generator","title":"<code>apply_block_generator(design, gen)</code>","text":"<p>Applies blocking to a design matrix.</p> <p>Parameters:</p> Name Type Description Default <code>design</code> <code>ndarray</code> <p>The design matrix.</p> required <code>gen</code> <code>str</code> <p>The generator.</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List of integers which assigns an experiment in the design matrix to a block.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def apply_block_generator(design: np.ndarray, gen: str) -&gt; List[int]:\n    \"\"\"Applies blocking to a design matrix.\n\n    Args:\n        design: The design matrix.\n        gen: The generator.\n\n    Returns:\n        List of integers which assigns an experiment in the design matrix to a block.\n\n    \"\"\"\n    generators = [i.strip().lower() for i in gen.split(\";\")]\n\n    # Fill in design with two level factorial design\n    blocking_design = np.zeros((design.shape[0], len(generators)))\n\n    # Recognize combinations and fill in the rest of matrix H2 with the proper\n    # products\n    for i, g in enumerate(generators):\n        # For lowercase letters\n        xx = np.array([ord(c) for c in g]) - 97\n        blocking_design[:, i] = np.prod(design[:, xx], axis=1)\n\n    # Create a list to store the block assignments\n    blocks = []\n\n    # Iterate over each row in the design matrix\n    for row in blocking_design:\n        # Find the index of the unique row in the blocking design\n        block = np.where((blocking_design == row).all(axis=1))[0][0]\n        blocks.append(block)\n\n    return blocks\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.compute_generator","title":"<code>compute_generator(n_factors, n_generators)</code>","text":"<p>Computes a generator for a given number of factors and generators.</p> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>The number of factors.</p> required <code>n_generators</code> <code>int</code> <p>The number of generators.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generator.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def compute_generator(n_factors: int, n_generators: int) -&gt; str:\n    \"\"\"Computes a generator for a given number of factors and generators.\n\n    Args:\n        n_factors: The number of factors.\n        n_generators: The number of generators.\n\n    Returns:\n        The generator.\n\n    \"\"\"\n    if n_generators == 0:\n        return \" \".join(list(string.ascii_lowercase[:n_factors]))\n    n_base_factors = n_factors - n_generators\n    if n_generators == 1:\n        if n_base_factors == 1:\n            raise ValueError(\n                \"Design not possible, as main factors are confounded with each other.\",\n            )\n        return \" \".join(\n            list(string.ascii_lowercase[:n_base_factors])\n            + [string.ascii_lowercase[:n_base_factors]],\n        )\n    n_base_factors = n_factors - n_generators\n    if n_base_factors - 1 &lt; 2:\n        raise ValueError(\n            \"Design not possible, as main factors are confounded with each other.\",\n        )\n    generators = [\n        \"\".join(i)\n        for i in (\n            itertools.combinations(\n                string.ascii_lowercase[:n_base_factors],\n                n_base_factors - 1,\n            )\n        )\n    ]\n    if len(generators) &gt; n_generators:\n        generators = generators[:n_generators]\n    elif (n_generators - len(generators) == 1) and (n_base_factors &gt; 1):\n        generators += [string.ascii_lowercase[:n_base_factors]]\n    elif n_generators - len(generators) &gt;= 1:\n        raise ValueError(\n            \"Design not possible, as main factors are confounded with each other.\",\n        )\n    return \" \".join(list(string.ascii_lowercase[:n_base_factors]) + generators)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.ff2n","title":"<code>ff2n(n_factors)</code>","text":"<p>Computes the full factorial design for a given number of factors.</p> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>The number of factors.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The full factorial design.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def ff2n(n_factors: int) -&gt; np.ndarray:\n    \"\"\"Computes the full factorial design for a given number of factors.\n\n    Args:\n        n_factors: The number of factors.\n\n    Returns:\n        The full factorial design.\n\n    \"\"\"\n    return np.array(list(itertools.product([-1, 1], repeat=n_factors)))\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.fracfact","title":"<code>fracfact(gen)</code>","text":"<p>Computes the fractional factorial design for a given generator.</p> <p>Parameters:</p> Name Type Description Default <code>gen</code> <code>str</code> <p>The generator.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The fractional factorial design.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def fracfact(gen: str) -&gt; np.ndarray:\n    \"\"\"Computes the fractional factorial design for a given generator.\n\n    Args:\n        gen: The generator.\n\n    Returns:\n        The fractional factorial design.\n\n    \"\"\"\n    gen = validate_generator(n_factors=gen.count(\" \") + 1, generator=gen)\n\n    generators = [item for item in re.split(r\"\\-|\\s|\\+\", gen) if item]\n    lengths = [len(i) for i in generators]\n\n    # Indices of single letters (main factors)\n    idx_main = [i for i, item in enumerate(lengths) if item == 1]\n\n    # Indices of letter combinations.\n    idx_combi = [i for i, item in enumerate(generators) if item != 1]\n\n    # Check if there are \"-\" operators in gen\n    idx_negative = [\n        i for i, item in enumerate(gen.split(\" \")) if item[0] == \"-\"\n    ]  # remove empty strings\n\n    # Fill in design with two level factorial design\n    H1 = ff2n(len(idx_main))\n    H = np.zeros((H1.shape[0], len(lengths)))\n    H[:, idx_main] = H1\n\n    # Recognize combinations and fill in the rest of matrix H2 with the proper\n    # products\n    for k in idx_combi:\n        # For lowercase letters\n        xx = np.array([ord(c) for c in generators[k]]) - 97\n\n        H[:, k] = np.prod(H1[:, xx], axis=1)\n\n    # Update design if gen includes \"-\" operator\n    if len(idx_negative) &gt; 0:\n        H[:, idx_negative] *= -1\n\n    # Return the fractional factorial design\n    return H\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.get_alias_structure","title":"<code>get_alias_structure(gen, order=4)</code>","text":"<p>Computes the alias structure of the design matrix. Works only for generators with positive signs.</p> <p>Parameters:</p> Name Type Description Default <code>gen</code> <code>str</code> <p>The generator.</p> required <code>order</code> <code>int</code> <p>The order up to which the alias structure should be calculated. Defaults to 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>The alias structure of the design matrix.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def get_alias_structure(gen: str, order: int = 4) -&gt; List[str]:\n    \"\"\"Computes the alias structure of the design matrix. Works only for generators\n    with positive signs.\n\n    Args:\n        gen: The generator.\n        order: The order up to which the alias structure should be calculated. Defaults to 4.\n\n    Returns:\n        The alias structure of the design matrix.\n\n    \"\"\"\n    design = fracfact(gen)\n\n    n_experiments, n_factors = design.shape\n\n    all_names = string.ascii_lowercase + \"I\"\n    factors = range(n_factors)\n    all_combinations = itertools.chain.from_iterable(\n        itertools.combinations(factors, n) for n in range(1, min(n_factors, order) + 1)\n    )\n    aliases = {n_experiments * \"+\": [(26,)]}  # 26 is mapped to I\n\n    for combination in all_combinations:\n        # positive sign\n        contrast = np.prod(\n            design[:, combination],\n            axis=1,\n        )  # this is the product of the combination\n        scontrast = \"\".join(np.where(contrast == 1, \"+\", \"-\").tolist())\n        aliases[scontrast] = aliases.get(scontrast, [])\n        aliases[scontrast].append(combination)  # type: ignore\n\n    aliases_list = []\n    for alias in aliases.values():\n        aliases_list.append(\n            sorted(alias, key=lambda a: (len(a), a)),\n        )  # sort by length and then by the combination\n    aliases_list = sorted(\n        aliases_list,\n        key=lambda list: ([len(a) for a in list], list),\n    )  # sort by the length of the alias\n\n    aliases_readable = []\n\n    for alias in aliases_list:\n        aliases_readable.append(\n            \" = \".join([\"\".join([all_names[f] for f in a]) for a in alias]),\n        )\n\n    return aliases_readable\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.get_block_generator","title":"<code>get_block_generator(n_factors, n_generators, n_repetitions, n_blocks)</code>","text":"<p>Gets the block generator for a given number of factors, generators, repetitions, and blocks.</p> <p>Should be only used if blocking cannot be reached by repetitions only.</p> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>number of factors</p> required <code>n_generators</code> <code>int</code> <p>number of generators/reducing factors</p> required <code>n_repetitions</code> <code>int</code> <p>number of repetitions</p> required <code>n_blocks</code> <code>int</code> <p>number of blocks that should be realized</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If blocking can be reached by repetitions only.</p> <p>Returns:</p> Type Description <code>str</code> <p>The blocking generator.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def get_block_generator(\n    n_factors: int, n_generators: int, n_repetitions: int, n_blocks: int\n) -&gt; str:\n    \"\"\"Gets the block generator for a given number of factors, generators, repetitions, and blocks.\n\n    Should be only used if blocking cannot be reached by repetitions only.\n\n    Args:\n        n_factors: number of factors\n        n_generators: number of generators/reducing factors\n        n_repetitions: number of repetitions\n        n_blocks: number of blocks that should be realized\n\n    Raises:\n        ValueError: If blocking can be reached by repetitions only.\n\n    Returns:\n        The blocking generator.\n    \"\"\"\n    if n_repetitions % n_blocks == 0:\n        raise ValueError(\"Blocking can be reached by repetitions only.\")\n\n    possible_blocks = sorted(\n        set(\n            default_blocking_generators.loc[\n                default_blocking_generators.n_factors == n_factors\n            ].n_blocks.to_list()\n        )\n    )\n\n    if n_blocks in possible_blocks:\n        return default_blocking_generators.loc[\n            (default_blocking_generators.n_factors == n_factors)\n            &amp; (default_blocking_generators.n_blocks == n_blocks)\n        ].block_generator.to_list()[0]\n\n    for b in possible_blocks:\n        if b * n_repetitions % n_blocks == 0:\n            return default_blocking_generators.loc[\n                (default_blocking_generators.n_factors == n_factors)\n                &amp; (default_blocking_generators.n_blocks == b)\n            ].block_generator.to_list()[0]\n\n    raise ValueError(\"No block generator available for the requested combination.\")\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.get_confounding_matrix","title":"<code>get_confounding_matrix(inputs, design, powers=None, interactions=None)</code>","text":"<p>Analyzes the confounding of a design and returns the confounding matrix.</p> <p>Only takes continuous features into account.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Inputs</code> <p>Input features.</p> required <code>design</code> <code>DataFrame</code> <p>Design matrix.</p> required <code>powers</code> <code>List[int]</code> <p>List of powers of the individual factors/features that should be considered. Integers has to be larger than 1. Defaults to [].</p> <code>None</code> <code>interactions</code> <code>List[int]</code> <p>List with interaction levels to be considered. Integers has to be larger than 1. Defaults to [2].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>_type_</code> <p>description</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def get_confounding_matrix(\n    inputs: Inputs,\n    design: pd.DataFrame,\n    powers: Optional[List[int]] = None,\n    interactions: Optional[List[int]] = None,\n):\n    \"\"\"Analyzes the confounding of a design and returns the confounding matrix.\n\n    Only takes continuous features into account.\n\n    Args:\n        inputs (Inputs): Input features.\n        design (pd.DataFrame): Design matrix.\n        powers (List[int], optional): List of powers of the individual factors/features that should be considered.\n            Integers has to be larger than 1. Defaults to [].\n        interactions (List[int], optional): List with interaction levels to be considered.\n            Integers has to be larger than 1. Defaults to [2].\n\n    Returns:\n        _type_: _description_\n\n    \"\"\"\n    from sklearn.preprocessing import MinMaxScaler\n\n    if len(inputs.get(CategoricalInput)) &gt; 0:\n        warnings.warn(\"Categorical input features will be ignored.\")\n\n    keys = inputs.get_keys(ContinuousInput)\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    scaled_design = pd.DataFrame(\n        data=scaler.fit_transform(design[keys]),\n        columns=keys,\n    )\n\n    # add powers\n    if powers is not None:\n        for p in powers:\n            assert p &gt; 1, \"Power has to be at least of degree two.\"\n            for key in keys:\n                scaled_design[f\"{key}**{p}\"] = scaled_design[key] ** p\n\n    # add interactions\n    if interactions is None:\n        interactions = [2]\n\n    for i in interactions:\n        assert i &gt; 1, \"Interaction has to be at least of degree two.\"\n        assert i &lt; len(keys) + 1, f\"Interaction has to be smaller than {len(keys) + 1}.\"\n        for combi in itertools.combinations(keys, i):\n            scaled_design[\":\".join(combi)] = scaled_design[list(combi)].prod(axis=1)\n\n    return scaled_design.corr()\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.get_default_generator","title":"<code>get_default_generator(n_factors, n_generators)</code>","text":"<p>Returns the default generator for a given number of factors and generators.</p> <p>In case the combination is not available, the function will raise an error.</p> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>The number of factors.</p> required <code>n_generators</code> <code>int</code> <p>The number of generators.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generator.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def get_default_generator(n_factors: int, n_generators: int) -&gt; str:\n    \"\"\"Returns the default generator for a given number of factors and generators.\n\n    In case the combination is not available, the function will raise an error.\n\n    Args:\n        n_factors: The number of factors.\n        n_generators: The number of generators.\n\n    Returns:\n        The generator.\n\n    \"\"\"\n    if n_generators == 0:\n        return \" \".join(list(string.ascii_lowercase[:n_factors]))\n    df_generators = default_fracfac_generators\n    n_base_factors = n_factors - n_generators\n    if df_generators.loc[\n        (df_generators.n_factors == n_factors)\n        &amp; (df_generators.n_generators == n_generators)\n    ].empty:\n        raise ValueError(\"No generator available for the requested combination.\")\n    generators = (\n        df_generators.loc[\n            (df_generators.n_factors == n_factors)\n            &amp; (df_generators.n_generators == n_generators),\n            \"generator\",\n        ]\n        .to_list()[0]\n        .split(\";\")\n    )\n    assert len(generators) == n_generators, \"Number of generators does not match.\"\n    generators = [generator.split(\"=\")[1].strip().lower() for generator in generators]\n    return \" \".join(list(string.ascii_lowercase[:n_base_factors]) + generators)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.get_generator","title":"<code>get_generator(n_factors, n_generators)</code>","text":"<p>Returns a generator for a given number of factors and generators.</p> <p>If the requested combination is available in the default generators, it will return this one. Otherwise, it will compute a new one using <code>get_bofire_generator</code>.</p> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>The number of factors.</p> required <code>n_generators</code> <code>int</code> <p>The number of generators.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The generator.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def get_generator(n_factors: int, n_generators: int) -&gt; str:\n    \"\"\"Returns a generator for a given number of factors and generators.\n\n    If the requested combination is available in the default generators, it will return\n    this one. Otherwise, it will compute a new one using `get_bofire_generator`.\n\n    Args:\n        n_factors: The number of factors.\n        n_generators: The number of generators.\n\n    Returns:\n        The generator.\n\n    \"\"\"\n    try:\n        return get_default_generator(n_factors, n_generators)\n    except ValueError:\n        return compute_generator(n_factors, n_generators)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.get_n_blocks","title":"<code>get_n_blocks(n_factors, n_generators, n_repetitions)</code>","text":"<p>Computes the number of possible blocks for a given number of factors, generators, and repetitions.</p> <p>Parameters:</p> Name Type Description Default <code>n_factors</code> <code>int</code> <p>number of factors</p> required <code>n_generators</code> <code>int</code> <p>number of generators/reducing factors</p> required <code>n_repetitions</code> <code>int</code> <p>number of repetitions</p> required <p>Returns:</p> Type Description <code>List[int]</code> <p>List[int]: List of possible number of blocks.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def get_n_blocks(n_factors: int, n_generators: int, n_repetitions: int) -&gt; List[int]:\n    \"\"\"Computes the number of possible blocks for a given number of factors, generators, and repetitions.\n\n    Args:\n        n_factors: number of factors\n        n_generators: number of generators/reducing factors\n        n_repetitions: number of repetitions\n\n    Returns:\n        List[int]: List of possible number of blocks.\n    \"\"\"\n    n_blocks = []\n    # check if no repetitions are planned\n    if n_repetitions == 1:\n        return sorted(\n            set(\n                default_blocking_generators.loc[\n                    default_blocking_generators.n_factors == n_factors\n                ].n_blocks.to_list()\n            )\n        )\n    else:\n        # check if blocking can be reached just by repetitions\n        for i in range(2, n_repetitions + 1):\n            if n_repetitions % i == 0:\n                n_blocks.append(i)\n\n        # check if blocking can be reached by a combination of explicit blocks and repetitions\n        n_blocks += (\n            default_blocking_generators.loc[\n                default_blocking_generators.n_factors == n_factors, \"n_blocks\"\n            ].to_numpy()\n            * (n_repetitions)\n        ).tolist() + default_blocking_generators.loc[\n            default_blocking_generators.n_factors == n_factors, \"n_blocks\"\n        ].to_list()\n\n        return sorted(set(n_blocks))\n</code></pre>"},{"location":"ref-utils/#bofire.utils.doe.validate_generator","title":"<code>validate_generator(n_factors, generator)</code>","text":"<p>Validates the generator and thows an error if it is not valid.</p> Source code in <code>bofire/utils/doe.py</code> <pre><code>def validate_generator(n_factors: int, generator: str) -&gt; str:\n    \"\"\"Validates the generator and thows an error if it is not valid.\"\"\"\n    if len(generator.split(\" \")) != n_factors:\n        raise ValueError(\"Generator does not match the number of factors.\")\n    # clean it and transform it into a list\n    generators = [item for item in re.split(r\"\\-|\\s|\\+\", generator) if item]\n    lengths = [len(i) for i in generators]\n\n    # Indices of single letters (main factors)\n    idx_main = [i for i, item in enumerate(lengths) if item == 1]\n\n    if len(idx_main) == 0:\n        raise ValueError(\"At least one unconfounded main factor is needed.\")\n\n    # Check that single letters (main factors) are unique\n    if len(idx_main) != len({generators[i] for i in idx_main}):\n        raise ValueError(\"Main factors are confounded with each other.\")\n\n    # Check that single letters (main factors) follow the alphabet\n    if (\n        \"\".join(sorted([generators[i] for i in idx_main]))\n        != string.ascii_lowercase[: len(idx_main)]\n    ):\n        raise ValueError(\n            f\"Use the letters `{' '.join(string.ascii_lowercase[: len(idx_main)])}` for the main factors.\",\n        )\n\n    # Indices of letter combinations.\n    idx_combi = [i for i, item in enumerate(generators) if item != 1]\n\n    # check that main factors come before combinations\n    if min(idx_combi) &gt; max(idx_main):\n        raise ValueError(\"Main factors have to come before combinations.\")\n\n    # Check that letter combinations are unique\n    if len(idx_combi) != len({generators[i] for i in idx_combi}):\n        raise ValueError(\"Generators are not unique.\")\n\n    # Check that only letters are used in the combinations that are also single letters (main factors)\n    if not all(\n        set(item).issubset({generators[i] for i in idx_main})\n        for item in [generators[i] for i in idx_combi]\n    ):\n        raise ValueError(\"Generators are not valid.\")\n\n    return generator\n</code></pre>"},{"location":"ref-utils/#bofire.utils.multiobjective","title":"<code>multiobjective</code>","text":""},{"location":"ref-utils/#bofire.utils.multiobjective.compute_hypervolume","title":"<code>compute_hypervolume(domain, optimal_experiments, ref_point)</code>","text":"<p>Method to compute the hypervolume for a given domain and pareto optimal experiments.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain for which the hypervolume should be computed.</p> required <code>optimal_experiments</code> <code>DataFrame</code> <p>Pareto optimal experiments for which the hypervolume should be computed.</p> required <code>ref_point</code> <code>dict</code> <p>Unmasked reference point for the hypervolume computation. Masking is happening inside the method.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Hypervolume for the given domain and pareto optimal experiments.</p> Source code in <code>bofire/utils/multiobjective.py</code> <pre><code>def compute_hypervolume(\n    domain: Domain,\n    optimal_experiments: pd.DataFrame,\n    ref_point: dict,\n) -&gt; float:\n    \"\"\"Method to compute the hypervolume for a given domain and pareto optimal experiments.\n\n    Args:\n        domain: Domain for which the hypervolume should be computed.\n        optimal_experiments: Pareto optimal experiments for which the hypervolume\n            should be computed.\n        ref_point: Unmasked reference point for the hypervolume computation.\n            Masking is happening inside the method.\n\n    Returns:\n        Hypervolume for the given domain and pareto optimal experiments.\n    \"\"\"\n    outputs = domain.outputs.get_by_objective(\n        includes=[MaximizeObjective, MinimizeObjective, CloseToTargetObjective],\n    )\n    objective = get_multiobjective_objective(\n        outputs=outputs,\n        experiments=optimal_experiments,\n    )\n    ref_point_mask = torch.from_numpy(get_ref_point_mask(domain)).to(**tkwargs)\n    hv = Hypervolume(\n        ref_point=torch.tensor(\n            [\n                ref_point[feat]\n                for feat in domain.outputs.get_keys_by_objective(\n                    includes=[\n                        MaximizeObjective,\n                        MinimizeObjective,\n                        CloseToTargetObjective,\n                    ],\n                )\n            ],\n        ).to(**tkwargs)\n        * ref_point_mask,\n    )\n\n    return hv.compute(\n        objective(\n            torch.from_numpy(\n                optimal_experiments[\n                    domain.outputs.get_keys_by_objective(\n                        includes=[\n                            MaximizeObjective,\n                            MinimizeObjective,\n                            CloseToTargetObjective,\n                        ],\n                    )\n                ].values,  # type: ignore\n            ).to(**tkwargs),\n        ),\n    )\n</code></pre>"},{"location":"ref-utils/#bofire.utils.multiobjective.get_pareto_front","title":"<code>get_pareto_front(domain, experiments, output_feature_keys=None)</code>","text":"<p>Method to compute the pareto optimal experiments for a given domain and experiments.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain for which the pareto front should be computed.</p> required <code>experiments</code> <code>DataFrame</code> <p>Experiments for which the pareto front should be computed.</p> required <code>output_feature_keys</code> <code>Optional[list]</code> <p>Key of output feature that should be considered in the pareto front. If <code>None</code> is provided, all keys belonging to output features with one of the following objectives will be considered: <code>MaximizeObjective</code>, <code>MinimizeObjective</code>, <code>CloseToTargetObjective</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with pareto optimal experiments.</p> Source code in <code>bofire/utils/multiobjective.py</code> <pre><code>def get_pareto_front(\n    domain: Domain,\n    experiments: pd.DataFrame,\n    output_feature_keys: Optional[list] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Method to compute the pareto optimal experiments for a given domain and\n    experiments.\n\n    Args:\n        domain: Domain for which the pareto front should be computed.\n        experiments: Experiments for which the pareto front should be computed.\n        output_feature_keys: Key of output feature that should be considered\n            in the pareto front. If `None` is provided, all keys\n            belonging to output features with one of the following objectives will\n            be considered: `MaximizeObjective`, `MinimizeObjective`,\n            `CloseToTargetObjective`. Defaults to None.\n\n    Returns:\n        DataFrame with pareto optimal experiments.\n\n    \"\"\"\n    if output_feature_keys is None:\n        outputs = domain.outputs.get_by_objective(\n            includes=[MaximizeObjective, MinimizeObjective, CloseToTargetObjective],\n        )\n    else:\n        outputs = domain.outputs.get_by_keys(output_feature_keys)\n    assert len(outputs) &gt;= 2, \"At least two output features have to be provided.\"\n    output_feature_keys = [f.key for f in outputs]\n    df = domain.outputs.preprocess_experiments_all_valid_outputs(\n        experiments,\n        output_feature_keys,\n    )\n    objective = get_multiobjective_objective(outputs=outputs, experiments=experiments)\n    pareto_mask = np.array(\n        is_non_dominated(\n            objective(\n                torch.from_numpy(df[output_feature_keys].values).to(**tkwargs),\n                None,\n            ),\n        ),\n    )\n    return df.loc[pareto_mask]\n</code></pre>"},{"location":"ref-utils/#bofire.utils.multiobjective.get_ref_point_mask","title":"<code>get_ref_point_mask(domain, output_feature_keys=None)</code>","text":"<p>Method to get a mask for the reference points taking into account if we want to maximize or minimize an objective. In case it is maximize the value in the mask is 1, in case we want to minimize it is -1.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain for which the mask should be generated.</p> required <code>output_feature_keys</code> <code>Optional[list]</code> <p>Name of output feature keys that should be considered in the mask. If <code>None</code> is provided, all keys belonging to output features with one of the following objectives will be considered: <code>MaximizeObjective</code>, <code>MinimizeObjective</code>, <code>CloseToTargetObjective</code>. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of ones for maximization and array of negative ones for minimization.</p> Source code in <code>bofire/utils/multiobjective.py</code> <pre><code>def get_ref_point_mask(\n    domain: Domain,\n    output_feature_keys: Optional[list] = None,\n) -&gt; np.ndarray:\n    \"\"\"Method to get a mask for the reference points taking into account if we\n    want to maximize or minimize an objective. In case it is maximize the value\n    in the mask is 1, in case we want to minimize it is -1.\n\n    Args:\n        domain: Domain for which the mask should be generated.\n        output_feature_keys: Name of output feature keys\n            that should be considered in the mask. If `None` is provided, all keys\n            belonging to output features with one of the following objectives will\n            be considered: `MaximizeObjective`, `MinimizeObjective`,\n            `CloseToTargetObjective`. Defaults to None.\n\n    Returns:\n        Array of ones for maximization and array of negative ones for\n            minimization.\n\n    \"\"\"\n    if output_feature_keys is None:\n        output_feature_keys = domain.outputs.get_keys_by_objective(\n            includes=[MaximizeObjective, MinimizeObjective, CloseToTargetObjective],\n        )\n    if len(output_feature_keys) &lt; 2:\n        raise ValueError(\"At least two output features have to be provided.\")\n    mask = []\n    for key in output_feature_keys:\n        feat = domain.outputs.get_by_key(key)\n        if isinstance(feat.objective, MaximizeObjective):\n            mask.append(1.0)\n        elif isinstance(feat.objective, MinimizeObjective) or isinstance(\n            feat.objective,\n            CloseToTargetObjective,\n        ):\n            mask.append(-1.0)\n        else:\n            raise ValueError(\n                \"Only `MaximizeObjective` and `MinimizeObjective` supported\",\n            )\n    return np.array(mask)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.multiobjective.infer_ref_point","title":"<code>infer_ref_point(domain, experiments, return_masked=False, reference_point=None)</code>","text":"<p>Method to infer the reference point for a given domain and experiments.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain for which the reference point should be inferred.</p> required <code>experiments</code> <code>DataFrame</code> <p>Experiments for which the reference point should be inferred.</p> required <code>return_masked</code> <code>bool</code> <p>If True, the masked reference point is returned. If False, the unmasked reference point is returned. Defaults to False.</p> <code>False</code> <code>reference_point</code> <code>Optional[ExplicitReferencePoint]</code> <p>Reference point to be used. If None is provided, the reference value is inferred by the worst values seen so far for every objective. Defaults to None.</p> <code>None</code> Source code in <code>bofire/utils/multiobjective.py</code> <pre><code>def infer_ref_point(\n    domain: Domain,\n    experiments: pd.DataFrame,\n    return_masked: bool = False,\n    reference_point: Optional[ExplicitReferencePoint] = None,\n) -&gt; Dict[str, float]:\n    \"\"\"Method to infer the reference point for a given domain and experiments.\n\n    Args:\n        domain: Domain for which the reference point should be inferred.\n        experiments: Experiments for which the reference point should be inferred.\n        return_masked: If True, the masked reference point is returned. If False,\n            the unmasked reference point is returned. Defaults to False.\n        reference_point: Reference point to be used. If None is provided, the\n            reference value is inferred by the worst values seen so far for\n            every objective. Defaults to None.\n    \"\"\"\n    outputs = domain.outputs.get_by_objective(\n        includes=[MaximizeObjective, MinimizeObjective, CloseToTargetObjective],\n    )\n    keys = outputs.get_keys()\n\n    if reference_point is None:\n        reference_point = ExplicitReferencePoint(\n            values={\n                key: AbsoluteMovingReferenceValue(orient_at_best=False, offset=0.0)\n                for key in keys\n            }\n        )\n\n    objective = get_multiobjective_objective(outputs=outputs, experiments=experiments)\n\n    df = domain.outputs.preprocess_experiments_all_valid_outputs(\n        experiments,\n        output_feature_keys=keys,\n    )\n\n    worst_values_array = (\n        objective(torch.from_numpy(df[keys].values).to(**tkwargs), None)\n        .numpy()\n        .min(axis=0)\n    )\n\n    best_values_array = (\n        objective(torch.from_numpy(df[keys].values).to(**tkwargs), None)\n        .numpy()\n        .max(axis=0)\n    )\n    # In the ref_point_array want masked values, which means that\n    # maximization is assumed for everything, this is because we use\n    # botorch objective for getting the best and worst values\n    # that are passed to the reference values. Botorch always assumes\n    # maximization.\n    # In case of FixedReferenceValue, the unmasked values are stored in the data model,\n    # this means the need to mask them to account for the botorch convention.\n    # In case of FixedReferenceValue, we multiply with -1 in for `MinimizeObjective`\n    # and `CloseToTargetObjective`.\n    ref_point_array = np.array(\n        [\n            -reference_point.values[key].get_reference_value(\n                best=best_values_array[i], worst=worst_values_array[i]\n            )\n            if isinstance(reference_point.values[key], FixedReferenceValue)\n            and isinstance(\n                outputs[i].objective, (MinimizeObjective, CloseToTargetObjective)\n            )\n            else reference_point.values[key].get_reference_value(\n                best=best_values_array[i], worst=worst_values_array[i]\n            )\n            for i, key in enumerate(keys)\n        ]\n    )\n\n    mask = get_ref_point_mask(domain)\n\n    # here we unmask again by dividing by the mask\n    if return_masked is False:\n        ref_point_array /= mask\n    return {feat: ref_point_array[i] for i, feat in enumerate(keys)}\n</code></pre>"},{"location":"ref-utils/#bofire.utils.naming_conventions","title":"<code>naming_conventions</code>","text":""},{"location":"ref-utils/#bofire.utils.naming_conventions.get_column_names","title":"<code>get_column_names(outputs)</code>","text":"<p>Specifies column names for given Outputs type.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Outputs</code> <p>The Outputs object containing the individual outputs.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[str]]</code> <p>Tuple[List[str], List[str]]: A tuple containing the prediction column names and the standard deviation column names</p> Source code in <code>bofire/utils/naming_conventions.py</code> <pre><code>def get_column_names(outputs: Outputs) -&gt; Tuple[List[str], List[str]]:\n    \"\"\"Specifies column names for given Outputs type.\n\n    Args:\n        outputs (Outputs): The Outputs object containing the individual outputs.\n\n    Returns:\n        Tuple[List[str], List[str]]: A tuple containing the prediction column names and the standard deviation column names\n\n    \"\"\"\n    pred_cols, sd_cols = [], []\n    for featkey in outputs.get_keys(CategoricalOutput):\n        pred_cols = pred_cols + [\n            f\"{featkey}_{cat}_prob\"\n            for cat in outputs.get_by_key(featkey).categories  # type: ignore\n        ]\n        sd_cols = sd_cols + [\n            f\"{featkey}_{cat}_sd\"\n            for cat in outputs.get_by_key(featkey).categories  # type: ignore\n        ]\n    for featkey in outputs.get_keys(ContinuousOutput):\n        pred_cols = pred_cols + [f\"{featkey}_pred\"]\n        sd_cols = sd_cols + [f\"{featkey}_sd\"]\n\n    return pred_cols, sd_cols\n</code></pre>"},{"location":"ref-utils/#bofire.utils.naming_conventions.postprocess_categorical_predictions","title":"<code>postprocess_categorical_predictions(predictions, outputs)</code>","text":"<p>Postprocess categorical predictions by finding the maximum probability location</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>DataFrame</code> <p>The dataframe containing the predictions.</p> required <code>outputs</code> <code>Outputs</code> <p>The Outputs object containing the individual outputs.</p> required <p>Returns:</p> Name Type Description <code>predictions</code> <code>DataFrame</code> <p>The (potentially modified) original dataframe with categorical predictions added</p> Source code in <code>bofire/utils/naming_conventions.py</code> <pre><code>def postprocess_categorical_predictions(\n    predictions: pd.DataFrame,\n    outputs: Outputs,\n) -&gt; pd.DataFrame:\n    \"\"\"Postprocess categorical predictions by finding the maximum probability location\n\n    Args:\n        predictions (pd.DataFrame): The dataframe containing the predictions.\n        outputs (Outputs): The Outputs object containing the individual outputs.\n\n    Returns:\n        predictions (pd.DataFrame): The (potentially modified) original dataframe with categorical predictions added\n\n    \"\"\"\n    for feat in outputs.get():\n        if isinstance(feat, CategoricalOutput):\n            predictions.insert(\n                loc=0,\n                column=f\"{feat.key}_pred\",\n                value=predictions.filter(regex=f\"{feat.key}(.*)_prob\")\n                .idxmax(1)\n                .str.replace(f\"{feat.key}_\", \"\")\n                .str.replace(\"_prob\", \"\")\n                .values,  # type: ignore\n            )\n            predictions.insert(\n                loc=1,\n                column=f\"{feat.key}_sd\",\n                value=0.0,\n            )\n    return predictions\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce","title":"<code>reduce</code>","text":""},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform","title":"<code>AffineTransform</code>","text":"<p>Class to switch back and forth from the reduced to the original domain.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>class AffineTransform:\n    \"\"\"Class to switch back and forth from the reduced to the original domain.\"\"\"\n\n    def __init__(self, equalities: List[Tuple[str, List[str], List[float]]]):\n        \"\"\"Initializes a `AffineTransformation` object.\n\n        Args:\n            equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality\n                is defined as a tuple, in which the first entry is the key of the reduced feature, the second\n                one is a list of feature keys that can be used to compute the feature and the third list of floats\n                are the corresponding coefficients.\n\n        \"\"\"\n        self.equalities = equalities\n\n    def augment_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Restore the eliminated features in a dataframe\n\n        Args:\n            data (pd.DataFrame): Dataframe that should be restored.\n\n        Returns:\n            pd.DataFrame: Restored dataframe\n\n        \"\"\"\n        if len(self.equalities) == 0:\n            return data\n        data = data.copy()\n        for name_lhs, names_rhs, coeffs in self.equalities:\n            data[name_lhs] = coeffs[-1]\n            for i, name in enumerate(names_rhs):\n                data[name_lhs] += coeffs[i] * data[name]\n        return data\n\n    def drop_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Drop eliminated features from a dataframe.\n\n        Args:\n            data (pd.DataFrame): Dataframe with features to be dropped.\n\n        Returns:\n            pd.DataFrame: Reduced dataframe.\n\n        \"\"\"\n        if len(self.equalities) == 0:\n            return data\n        drop = []\n        for name_lhs, _, _ in self.equalities:\n            if name_lhs in data.columns:\n                drop.append(name_lhs)\n        return data.drop(columns=drop)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.__init__","title":"<code>__init__(equalities)</code>","text":"<p>Initializes a <code>AffineTransformation</code> object.</p> <p>Parameters:</p> Name Type Description Default <code>equalities</code> <code>List[Tuple[str, List[str], List[float]]]</code> <p>List of equalities. Every equality is defined as a tuple, in which the first entry is the key of the reduced feature, the second one is a list of feature keys that can be used to compute the feature and the third list of floats are the corresponding coefficients.</p> required Source code in <code>bofire/utils/reduce.py</code> <pre><code>def __init__(self, equalities: List[Tuple[str, List[str], List[float]]]):\n    \"\"\"Initializes a `AffineTransformation` object.\n\n    Args:\n        equalities (List[Tuple[str,List[str],List[float]]]): List of equalities. Every equality\n            is defined as a tuple, in which the first entry is the key of the reduced feature, the second\n            one is a list of feature keys that can be used to compute the feature and the third list of floats\n            are the corresponding coefficients.\n\n    \"\"\"\n    self.equalities = equalities\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.augment_data","title":"<code>augment_data(data)</code>","text":"<p>Restore the eliminated features in a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe that should be restored.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Restored dataframe</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def augment_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Restore the eliminated features in a dataframe\n\n    Args:\n        data (pd.DataFrame): Dataframe that should be restored.\n\n    Returns:\n        pd.DataFrame: Restored dataframe\n\n    \"\"\"\n    if len(self.equalities) == 0:\n        return data\n    data = data.copy()\n    for name_lhs, names_rhs, coeffs in self.equalities:\n        data[name_lhs] = coeffs[-1]\n        for i, name in enumerate(names_rhs):\n            data[name_lhs] += coeffs[i] * data[name]\n    return data\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.AffineTransform.drop_data","title":"<code>drop_data(data)</code>","text":"<p>Drop eliminated features from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Dataframe with features to be dropped.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Reduced dataframe.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def drop_data(self, data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Drop eliminated features from a dataframe.\n\n    Args:\n        data (pd.DataFrame): Dataframe with features to be dropped.\n\n    Returns:\n        pd.DataFrame: Reduced dataframe.\n\n    \"\"\"\n    if len(self.equalities) == 0:\n        return data\n    drop = []\n    for name_lhs, _, _ in self.equalities:\n        if name_lhs in data.columns:\n            drop.append(name_lhs)\n    return data.drop(columns=drop)\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.adjust_boundary","title":"<code>adjust_boundary(feature, coef, rhs)</code>","text":"<p>Adjusts the boundaries of a feature.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>ContinuousInput</code> <p>Feature to be adjusted.</p> required <code>coef</code> <code>float</code> <p>Coefficient.</p> required <code>rhs</code> <code>float</code> <p>Right-hand-side of the constraint.</p> required Source code in <code>bofire/utils/reduce.py</code> <pre><code>def adjust_boundary(feature: ContinuousInput, coef: float, rhs: float):\n    \"\"\"Adjusts the boundaries of a feature.\n\n    Args:\n        feature (ContinuousInput): Feature to be adjusted.\n        coef (float): Coefficient.\n        rhs (float): Right-hand-side of the constraint.\n\n    \"\"\"\n    boundary = rhs / coef\n    if coef &gt; 0:\n        if boundary &gt; feature.lower_bound:\n            feature.bounds = [boundary, feature.upper_bound]\n    elif boundary &lt; feature.upper_bound:\n        feature.bounds = [feature.lower_bound, boundary]\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.check_domain_for_reduction","title":"<code>check_domain_for_reduction(domain)</code>","text":"<p>Check if the reduction can be applied or if a trivial case is present.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain to be checked.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if reducable, else False.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def check_domain_for_reduction(domain: Domain) -&gt; bool:\n    \"\"\"Check if the reduction can be applied or if a trivial case is present.\n\n    Args:\n        domain (Domain): Domain to be checked.\n\n    Returns:\n        bool: True if reducable, else False.\n\n    \"\"\"\n    # are there any constraints?\n    if len(domain.constraints) == 0:\n        return False\n\n    # are there any linear equality constraints?\n    linear_equalities = domain.constraints.get(LinearEqualityConstraint)\n    if len(linear_equalities) == 0:\n        return False\n\n    # are there no NChooseKConstraint constraints?\n    if len(domain.constraints.get([NChooseKConstraint])) &gt; 0:\n        return False\n\n    # are there continuous inputs\n    continuous_inputs = domain.inputs.get(ContinuousInput)\n    if len(continuous_inputs) == 0:\n        return False\n\n    # check that equality constraints only contain continuous inputs\n    for c in linear_equalities:\n        assert isinstance(c, LinearConstraint)\n        for feat in c.features:\n            if feat not in domain.inputs.get_keys(ContinuousInput):\n                return False\n    return True\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.check_existence_of_solution","title":"<code>check_existence_of_solution(A_aug)</code>","text":"<p>Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def check_existence_of_solution(A_aug):\n    \"\"\"Given an augmented coefficient matrix this function determines the existence (and uniqueness) of solution using the rank theorem.\"\"\"\n    A = A_aug[:, :-1]\n    b = A_aug[:, -1]\n    len_inputs = np.shape(A)[1]\n\n    # catch special cases\n    rk_A_aug = np.linalg.matrix_rank(A_aug)\n    rk_A = np.linalg.matrix_rank(A)\n\n    if rk_A == rk_A_aug:\n        if rk_A &lt; len_inputs:\n            return  # all good\n        x = np.linalg.solve(A, b)\n        raise Exception(\n            f\"There is a unique solution x for the linear equality constraints: x={x}\",\n        )\n    if rk_A &lt; rk_A_aug:\n        raise Exception(\n            \"There is no solution fulfilling the linear equality constraints.\",\n        )\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.reduce_domain","title":"<code>reduce_domain(domain)</code>","text":"<p>Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain to be reduced.</p> required <p>Returns:</p> Type Description <code>Tuple[Domain, AffineTransform]</code> <p>Tuple[Domain, AffineTransform]: reduced domain and the according transformation to switch between the reduced and original domain.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def reduce_domain(domain: Domain) -&gt; Tuple[Domain, AffineTransform]:\n    \"\"\"Reduce a domain with linear equality constraints to a subdomain where linear equality constraints are eliminated.\n\n    Args:\n        domain (Domain): Domain to be reduced.\n\n    Returns:\n        Tuple[Domain, AffineTransform]: reduced domain and the according transformation to switch between the\n            reduced and original domain.\n\n    \"\"\"\n    # check if the domain can be reduced\n    if not check_domain_for_reduction(domain):\n        return domain, AffineTransform([])\n\n    # find linear equality constraints\n    linear_equalities = domain.constraints.get(LinearEqualityConstraint)\n    other_constraints = domain.constraints.get(\n        Constraint,\n        excludes=[LinearEqualityConstraint],\n    )\n\n    # only consider continuous inputs\n    continuous_inputs = [\n        cast(ContinuousInput, f) for f in domain.inputs.get(ContinuousInput)\n    ]\n    other_inputs = domain.inputs.get(Input, excludes=[ContinuousInput])\n\n    # assemble Matrix A from equality constraints\n    N = len(linear_equalities)\n    M = len(continuous_inputs) + 1\n    names = np.concatenate(([feat.key for feat in continuous_inputs], [\"rhs\"]))\n\n    A_aug = pd.DataFrame(data=np.zeros(shape=(N, M)), columns=names)\n\n    for i in range(len(linear_equalities)):\n        c = linear_equalities[i]\n        assert isinstance(c, LinearEqualityConstraint)\n        A_aug.loc[i, c.features] = c.coefficients\n        A_aug.loc[i, \"rhs\"] = c.rhs\n    A_aug = A_aug.values\n\n    # catch special cases\n    check_existence_of_solution(A_aug)\n\n    # bring A_aug to reduced row-echelon form\n    A_aug_rref, pivots = rref(A_aug)\n    pivots = np.array(pivots)\n    A_aug_rref = np.array(A_aug_rref).astype(np.float64)\n\n    # formulate box bounds as linear inequality constraints in matrix form\n    B = np.zeros(shape=(2 * (M - 1), M))\n    B[: M - 1, : M - 1] = np.eye(M - 1)\n    B[M - 1 :, : M - 1] = -np.eye(M - 1)\n\n    B[: M - 1, -1] = np.array([feat.upper_bound for feat in continuous_inputs])\n    B[M - 1 :, -1] = -1.0 * np.array([feat.lower_bound for feat in continuous_inputs])\n\n    # eliminate columns with pivot element\n    for i in range(len(pivots)):\n        p = pivots[i]\n        B[p, :] -= A_aug_rref[i, :]\n        B[p + M - 1, :] += A_aug_rref[i, :]\n\n    # build up reduced domain\n    _domain = Domain.model_construct(\n        # _fields_set = {\"inputs\", \"outputs\", \"constraints\"}\n        inputs=deepcopy(other_inputs),\n        outputs=deepcopy(domain.outputs),\n        constraints=deepcopy(other_constraints),\n    )\n    new_inputs = [\n        deepcopy(feat) for i, feat in enumerate(continuous_inputs) if i not in pivots\n    ]\n    all_inputs = _domain.inputs + new_inputs\n    assert isinstance(all_inputs, Inputs)\n    _domain.inputs.features = all_inputs.features\n\n    constraints: List[AnyConstraint] = []\n    for i in pivots:\n        # reduce equation system of upper bounds\n        ind = np.where(B[i, :-1] != 0)[0]\n        if len(ind) &gt; 0 and B[i, -1] &lt; np.inf:\n            if len(list(names[ind])) &gt; 1:\n                c = LinearInequalityConstraint.from_greater_equal(\n                    features=list(names[ind]),\n                    coefficients=(-1.0 * B[i, ind]).tolist(),\n                    rhs=B[i, -1] * -1.0,\n                )\n                constraints.append(c)\n            else:\n                key = names[ind][0]\n                feat = cast(ContinuousInput, _domain.inputs.get_by_key(key))\n                adjust_boundary(feat, (-1.0 * B[i, ind])[0], B[i, -1] * -1.0)\n        elif B[i, -1] &lt; -1e-16:\n            raise Exception(\"There is no solution that fulfills the constraints.\")\n\n        # reduce equation system of lower bounds\n        ind = np.where(B[i + M - 1, :-1] != 0)[0]\n        if len(ind) &gt; 0 and B[i + M - 1, -1] &lt; np.inf:\n            if len(list(names[ind])) &gt; 1:\n                c = LinearInequalityConstraint.from_greater_equal(\n                    features=list(names[ind]),\n                    coefficients=(-1.0 * B[i + M - 1, ind]).tolist(),\n                    rhs=B[i + M - 1, -1] * -1.0,\n                )\n                constraints.append(c)\n            else:\n                key = names[ind][0]\n                feat = cast(ContinuousInput, _domain.inputs.get_by_key(key))\n                adjust_boundary(\n                    feat,\n                    (-1.0 * B[i + M - 1, ind])[0],\n                    B[i + M - 1, -1] * -1.0,\n                )\n        elif B[i + M - 1, -1] &lt; -1e-16:\n            raise Exception(\"There is no solution that fulfills the constraints.\")\n\n    if len(constraints) &gt; 0:\n        _domain.constraints.constraints = _domain.constraints.constraints + constraints  # type: ignore\n\n    # assemble equalities\n    _equalities = []\n    for i in range(len(pivots)):\n        name_lhs = names[pivots[i]]\n        names_rhs = []\n        coeffs = []\n\n        for j in range(len(names) - 1):\n            if A_aug_rref[i, j] != 0 and j != pivots[i]:\n                coeffs.append(-A_aug_rref[i, j])\n                names_rhs.append(names[j])\n\n        coeffs.append(A_aug_rref[i, -1])\n\n        _equalities.append((name_lhs, names_rhs, coeffs))\n\n    trafo = AffineTransform(_equalities)\n    # remove remaining dependencies of eliminated inputs from the problem\n    _domain = remove_eliminated_inputs(_domain, trafo)\n    return _domain, trafo\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.remove_eliminated_inputs","title":"<code>remove_eliminated_inputs(domain, transform)</code>","text":"<p>Eliminates remaining occurrences of eliminated inputs in linear constraints.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Domain in which the linear constraints should be purged.</p> required <code>transform</code> <code>AffineTransform</code> <p>Affine transformation object that defines the obsolete features.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature occurs in a constraint different from a linear one.</p> <p>Returns:</p> Name Type Description <code>Domain</code> <code>Domain</code> <p>Purged domain.</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def remove_eliminated_inputs(domain: Domain, transform: AffineTransform) -&gt; Domain:\n    \"\"\"Eliminates remaining occurrences of eliminated inputs in linear constraints.\n\n    Args:\n        domain (Domain): Domain in which the linear constraints should be purged.\n        transform (AffineTransform): Affine transformation object that defines the obsolete features.\n\n    Raises:\n        ValueError: If feature occurs in a constraint different from a linear one.\n\n    Returns:\n        Domain: Purged domain.\n\n    \"\"\"\n    inputs_names = domain.inputs.get_keys()\n    M = len(inputs_names)\n\n    # write the equalities for the backtransformation into one matrix\n    inputs_dict = {inputs_names[i]: i for i in range(M)}\n\n    # build up dict from domain.equalities e.g. {\"xi1\": [coeff(xj1), ..., coeff(xjn)], ... \"xik\":...}\n    coeffs_dict = {}\n    for e in transform.equalities:\n        coeffs = np.zeros(M + 1)\n        for j, name in enumerate(e[1]):\n            coeffs[inputs_dict[name]] = e[2][j]\n        coeffs[-1] = e[2][-1]\n        coeffs_dict[e[0]] = coeffs\n\n    constraints = []\n    for c in domain.constraints.get():\n        # Nonlinear constraints not supported\n        if not isinstance(c, LinearConstraint):\n            raise ValueError(\n                \"Elimination of variables is only supported for LinearEquality and LinearInequality constraints.\",\n            )\n\n        # no changes, if the constraint does not contain eliminated inputs\n        if all(name in inputs_names for name in c.features):\n            constraints.append(c)\n\n        # remove inputs from the constraint that were eliminated from the inputs before\n        else:\n            totally_removed = False\n            _features = np.array(inputs_names)\n            _rhs = c.rhs\n\n            # create new lhs and rhs from the old one and knowledge from problem._equalities\n            _coefficients = np.zeros(M)\n            for j, name in enumerate(c.features):\n                if name in inputs_names:\n                    _coefficients[inputs_dict[name]] += c.coefficients[j]\n                else:\n                    _coefficients += c.coefficients[j] * coeffs_dict[name][:-1]\n                    _rhs -= c.coefficients[j] * coeffs_dict[name][-1]\n\n            _features = _features[np.abs(_coefficients) &gt; 1e-16]\n            _coefficients = _coefficients[np.abs(_coefficients) &gt; 1e-16]\n            _c = None\n            if isinstance(c, LinearEqualityConstraint):\n                if len(_features) &gt; 1:\n                    _c = LinearEqualityConstraint(\n                        features=_features.tolist(),\n                        coefficients=_coefficients.tolist(),\n                        rhs=_rhs,\n                    )\n                elif len(_features) == 0:\n                    totally_removed = True\n                else:\n                    feat: ContinuousInput = ContinuousInput(\n                        **domain.inputs.get_by_key(_features[0]).model_dump(),\n                    )\n                    feat.bounds = [_coefficients[0], _coefficients[0]]\n                    totally_removed = True\n            elif len(_features) &gt; 1:\n                _c = LinearInequalityConstraint(\n                    features=_features.tolist(),\n                    coefficients=_coefficients.tolist(),\n                    rhs=_rhs,\n                )\n            elif len(_features) == 0:\n                totally_removed = True\n            else:\n                feat = cast(ContinuousInput, domain.inputs.get_by_key(_features[0]))\n                adjust_boundary(feat, _coefficients[0], _rhs)\n                totally_removed = True\n\n            # check if constraint is always fulfilled/not fulfilled\n            if not totally_removed:\n                assert _c is not None\n                if len(_c.features) == 0 and _c.rhs &gt;= 0:\n                    pass\n                elif len(_c.features) == 0 and _c.rhs &lt; 0:\n                    raise Exception(\"Linear constraints cannot be fulfilled.\")\n                elif np.isinf(_c.rhs):\n                    pass\n                else:\n                    constraints.append(_c)\n    domain.constraints = Constraints(constraints=constraints)\n    return domain\n</code></pre>"},{"location":"ref-utils/#bofire.utils.reduce.rref","title":"<code>rref(A, tol=1e-08)</code>","text":"<p>Computes the reduced row echelon form of a Matrix</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>ndarray</code> <p>2d array representing a matrix.</p> required <code>tol</code> <code>float</code> <p>tolerance for rounding to 0. Defaults to 1e-8.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>(A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots</p> <code>List[int]</code> <p>is a numpy array containing the pivot columns of A_rref</p> Source code in <code>bofire/utils/reduce.py</code> <pre><code>def rref(A: np.ndarray, tol: float = 1e-8) -&gt; Tuple[np.ndarray, List[int]]:\n    \"\"\"Computes the reduced row echelon form of a Matrix\n\n    Args:\n        A (ndarray): 2d array representing a matrix.\n        tol (float, optional): tolerance for rounding to 0. Defaults to 1e-8.\n\n    Returns:\n        (A_rref, pivots), where A_rref is the reduced row echelon form of A and pivots\n        is a numpy array containing the pivot columns of A_rref\n\n    \"\"\"\n    A = np.array(A, dtype=np.float64)\n    n, m = np.shape(A)\n\n    col = 0\n    row = 0\n    pivots = []\n\n    for col in range(m):\n        # does a pivot element exist?\n        if all(np.abs(A[row:, col]) &lt; tol):\n            pass\n        # if yes: start elimination\n        else:\n            pivots.append(col)\n            max_row = np.argmax(np.abs(A[row:, col])) + row\n            # switch to most stable row\n            A[[row, max_row], :] = A[[max_row, row], :]\n            # normalize row\n            A[row, :] /= A[row, col]\n            # eliminate other elements from column\n            for r in range(n):\n                if r != row:\n                    A[r, :] -= A[r, col] / A[row, col] * A[row, :]\n            row += 1\n\n    prec = int(-np.log10(tol))\n    return np.round(A, prec), pivots\n</code></pre>"},{"location":"ref-utils/#bofire.utils.subdomain","title":"<code>subdomain</code>","text":""},{"location":"ref-utils/#bofire.utils.subdomain.get_subdomain","title":"<code>get_subdomain(domain, feature_keys)</code>","text":"<p>Removes all features not defined as argument creating a subdomain of the provided domain</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>the original domain wherefrom a subdomain should be created</p> required <code>feature_keys</code> <code>List</code> <p>List of features that shall be included in the subdomain</p> required <p>Raises:</p> Type Description <code>Assert</code> <p>when in total less than 2 features are provided</p> <code>ValueError</code> <p>when a provided feature key is not present in the provided domain</p> <code>Assert</code> <p>when no output feature is provided</p> <code>Assert</code> <p>when no input feature is provided</p> <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>Domain</code> <code>Domain</code> <p>A new domain containing only parts of the original domain</p> Source code in <code>bofire/utils/subdomain.py</code> <pre><code>def get_subdomain(\n    domain: Domain,\n    feature_keys: List,\n) -&gt; Domain:\n    \"\"\"Removes all features not defined as argument creating a subdomain of the provided domain\n\n    Args:\n        domain (Domain): the original domain wherefrom a subdomain should be created\n        feature_keys (List): List of features that shall be included in the subdomain\n\n    Raises:\n        Assert: when in total less than 2 features are provided\n        ValueError: when a provided feature key is not present in the provided domain\n        Assert: when no output feature is provided\n        Assert: when no input feature is provided\n        ValueError: _description_\n\n    Returns:\n        Domain: A new domain containing only parts of the original domain\n\n    \"\"\"\n    assert len(feature_keys) &gt;= 2, \"At least two features have to be provided.\"\n    outputs = []\n    inputs = []\n    for key in feature_keys:\n        try:\n            feat = (domain.inputs + domain.outputs).get_by_key(key)\n        except KeyError:\n            raise ValueError(f\"Feature {key} not present in domain.\")\n        if isinstance(feat, Input):\n            inputs.append(feat)\n        else:\n            outputs.append(feat)\n    assert len(outputs) &gt; 0, \"At least one output feature has to be provided.\"\n    assert len(inputs) &gt; 0, \"At least one input feature has to be provided.\"\n    inputs = Inputs(features=inputs)\n    outputs = Outputs(features=outputs)\n    # loop over constraints and make sure that all features used in constraints are in the input_feature_keys\n    for c in domain.constraints:\n        for key in c.features:\n            if key not in inputs.get_keys():\n                raise ValueError(\n                    f\"Removed input feature {key} is used in a constraint.\",\n                )\n    subdomain = deepcopy(domain)\n    subdomain.inputs = inputs\n    subdomain.outputs = outputs\n    return subdomain\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools","title":"<code>torch_tools</code>","text":""},{"location":"ref-utils/#bofire.utils.torch_tools.InterpolateTransform","title":"<code>InterpolateTransform</code>","text":"<p>               Bases: <code>InputTransform</code>, <code>Module</code></p> <p>Botorch input transform that interpolates values between given x and y values.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>class InterpolateTransform(InputTransform, Module):\n    \"\"\"Botorch input transform that interpolates values between given x and y values.\"\"\"\n\n    def __init__(\n        self,\n        new_x: Tensor,\n        idx_x: List[int],\n        idx_y: List[int],\n        prepend_x: Tensor,\n        prepend_y: Tensor,\n        append_x: Tensor,\n        append_y: Tensor,\n        keep_original: bool = False,\n        transform_on_train: bool = True,\n        transform_on_eval: bool = True,\n        transform_on_fantasize: bool = True,\n    ):\n        super().__init__()\n        if len(set(idx_x + idx_y)) != len(idx_x) + len(idx_y):\n            raise ValueError(\"Indices are not unique.\")\n\n        self.idx_x = torch.as_tensor(idx_x, dtype=torch.long)\n        self.idx_y = torch.as_tensor(idx_y, dtype=torch.long)\n\n        self.transform_on_train = transform_on_train\n        self.transform_on_eval = transform_on_eval\n        self.transform_on_fantasize = transform_on_fantasize\n        self.new_x = new_x\n\n        self.prepend_x = prepend_x\n        self.prepend_y = prepend_y\n        self.append_x = append_x\n        self.append_y = append_y\n\n        self.keep_original = keep_original\n\n        if len(self.idx_x) + len(self.prepend_x) + len(self.append_x) != len(\n            self.idx_y,\n        ) + len(self.prepend_y) + len(self.append_y):\n            raise ValueError(\"The number of x and y indices must be equal.\")\n\n    def _to(self, X: Tensor) -&gt; None:\n        self.new_x = self.coefficient.to(X)\n\n    def append(self, X: Tensor, values: Tensor) -&gt; Tensor:\n        shape = X.shape\n        values_reshaped = values.view(*([1] * (len(shape) - 1)), -1)\n        values_expanded = values_reshaped.expand(*shape[:-1], -1).to(X)\n        return torch.cat([X, values_expanded], dim=-1)\n\n    def prepend(self, X: Tensor, values: Tensor) -&gt; Tensor:\n        shape = X.shape\n        values_reshaped = values.view(*([1] * (len(shape) - 1)), -1)\n        values_expanded = values_reshaped.expand(*shape[:-1], -1).to(X)\n        return torch.cat([values_expanded, X], dim=-1)\n\n    def transform(self, X: Tensor):\n        shapeX = X.shape\n\n        x = X[..., self.idx_x]\n        x = self.prepend(x, self.prepend_x)\n        x = self.append(x, self.append_x)\n\n        y = X[..., self.idx_y]\n        y = self.prepend(y, self.prepend_y)\n        y = self.append(y, self.append_y)\n\n        if X.dim() == 3:\n            x = x.reshape((shapeX[0] * shapeX[1], x.shape[-1]))\n            y = y.reshape((shapeX[0] * shapeX[1], y.shape[-1]))\n\n        new_x = self.new_x.expand(x.shape[0], -1)\n        new_y = torch.vmap(interp1d)(x, y, new_x)\n\n        if X.dim() == 3:\n            new_y = new_y.reshape((shapeX[0], shapeX[1], new_y.shape[-1]))\n\n        if self.keep_original:\n            return torch.cat([new_y, X], dim=-1)\n\n        return new_y\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.constrained_objective2botorch","title":"<code>constrained_objective2botorch(idx, objective, x_adapt, eps=1e-08)</code>","text":"<p>Create a callable that can be used by <code>botorch.utils.objective.apply_constraints</code> to setup output constrained optimizations.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the constraint objective in the list of outputs.</p> required <code>objective</code> <code>BotorchConstrainedObjective</code> <p>The objective that should be transformed.</p> required <code>x_adapt</code> <code>Optional[Tensor]</code> <p>The tensor that should be used to adapt the objective, for example in case of a moving turning point in the <code>MovingMaximizeSigmoidObjective</code>.</p> required <code>eps</code> <code>float</code> <p>Small value to avoid numerical instabilities in case of the <code>ConstrainedCategoricalObjective</code>. Defaults to 1e-8.</p> <code>1e-08</code> <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float], int]</code> <p>Tuple[List[Callable[[Tensor], Tensor]], List[float], int]: List of callables that can be used by botorch for setting up the constrained objective, list of the corresponding botorch eta values, final index used by the method (to track for categorical variables)</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def constrained_objective2botorch(\n    idx: int,\n    objective: ConstrainedObjective,\n    x_adapt: Optional[Tensor],\n    eps: float = 1e-8,\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float], int]:\n    \"\"\"Create a callable that can be used by `botorch.utils.objective.apply_constraints`\n    to setup output constrained optimizations.\n\n    Args:\n        idx (int): Index of the constraint objective in the list of outputs.\n        objective (BotorchConstrainedObjective): The objective that should be transformed.\n        x_adapt (Optional[Tensor]): The tensor that should be used to adapt the objective,\n            for example in case of a moving turning point in the `MovingMaximizeSigmoidObjective`.\n        eps (float, optional): Small value to avoid numerical instabilities in case of the `ConstrainedCategoricalObjective`.\n            Defaults to 1e-8.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float], int]: List of callables that can be used by botorch for setting up the constrained objective,\n            list of the corresponding botorch eta values, final index used by the method (to track for categorical variables)\n\n    \"\"\"\n    assert isinstance(\n        objective,\n        ConstrainedObjective,\n    ), \"Objective is not a `ConstrainedObjective`.\"\n    if isinstance(objective, MaximizeSigmoidObjective):\n        return (\n            [lambda Z: (Z[..., idx] - objective.tp) * -1.0],\n            [1.0 / objective.steepness],\n            idx + 1,\n        )\n    if isinstance(objective, MovingMaximizeSigmoidObjective):\n        assert x_adapt is not None\n        tp = x_adapt.max().item() + objective.tp\n        return (\n            [lambda Z: (Z[..., idx] - tp) * -1.0],\n            [1.0 / objective.steepness],\n            idx + 1,\n        )\n    if isinstance(objective, MinimizeSigmoidObjective):\n        return (\n            [lambda Z: (Z[..., idx] - objective.tp)],\n            [1.0 / objective.steepness],\n            idx + 1,\n        )\n    if isinstance(objective, TargetObjective):\n        return (\n            [\n                lambda Z: (Z[..., idx] - (objective.target_value - objective.tolerance))\n                * -1.0,\n                lambda Z: (\n                    Z[..., idx] - (objective.target_value + objective.tolerance)\n                ),\n            ],\n            [1.0 / objective.steepness, 1.0 / objective.steepness],\n            idx + 1,\n        )\n    if isinstance(objective, ConstrainedCategoricalObjective):\n        # The output of a categorical objective has final dim `c` where `c` is number of classes\n        # Pass in the expected acceptance probability and perform an inverse sigmoid to attain the original probabilities\n        # as Botorch does sigmoid transformation for the constraint by default, therefore we need to unsigmoid the probability\n        # (0-1) to (-inf,inf) also we need to invert the probability, where -inf means the constraint is satisfied.\n        # Finally, we add some slack (`eps`) 1e-8 to avoid log(0). Have also a look at this botorch community notebook\n        # https://github.com/pytorch/botorch/blob/main/notebooks_community/clf_constrained_bo.ipynb and this botorch issue\n        # https://github.com/pytorch/botorch/issues/725 which is inspired by this implementation.\n        return (\n            [\n                lambda Z: torch.log(\n                    1\n                    / torch.clamp(\n                        (\n                            Z[..., idx : idx + len(objective.desirability)]\n                            * torch.tensor(objective.desirability).to(**tkwargs)\n                        ).sum(-1),\n                        min=eps,\n                        max=1 - eps,\n                    )\n                    - 1,\n                ),\n            ],\n            [1.0],\n            idx + len(objective.desirability),\n        )\n    raise ValueError(f\"Objective {objective.__class__.__name__} not known.\")\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_initial_conditions_generator","title":"<code>get_initial_conditions_generator(strategy, transform_specs, ask_options=None, sequential=True)</code>","text":"<p>Takes a strategy object and returns a callable which uses this strategy to return a generator callable which can be used in botorch<code>s</code>gen_batch_initial_conditions` to generate samples.</p> <p>Parameters:</p> Name Type Description Default <code>strategy</code> <code>Strategy</code> <p>Strategy that should be used to generate samples.</p> required <code>transform_specs</code> <code>Dict</code> <p>Dictionary indicating how the samples should be transformed.</p> required <code>ask_options</code> <code>Dict</code> <p>Dictionary of keyword arguments that are passed to the <code>ask</code> method of the strategy. Defaults to {}.</p> <code>None</code> <code>sequential</code> <code>bool</code> <p>If True, samples for every q-batch are generate independent from each other. If False, the <code>n x q</code> samples are generated at once.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable[[int, int, int], Tensor]</code> <p>Callable[[int, int, int], Tensor]: Callable that can be passed to <code>batch_initial_conditions</code>.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_initial_conditions_generator(\n    strategy: Strategy,\n    transform_specs: Dict,\n    ask_options: Optional[Dict] = None,\n    sequential: bool = True,\n) -&gt; Callable[[int, int, int], Tensor]:\n    \"\"\"Takes a strategy object and returns a callable which uses this\n    strategy to return a generator callable which can be used in botorch`s\n    `gen_batch_initial_conditions` to generate samples.\n\n    Args:\n        strategy (Strategy): Strategy that should be used to generate samples.\n        transform_specs (Dict): Dictionary indicating how the samples should be\n            transformed.\n        ask_options (Dict, optional): Dictionary of keyword arguments that are\n            passed to the `ask` method of the strategy. Defaults to {}.\n        sequential (bool, optional): If True, samples for every q-batch are\n            generate independent from each other. If False, the `n x q` samples\n            are generated at once.\n\n    Returns:\n        Callable[[int, int, int], Tensor]: Callable that can be passed to\n            `batch_initial_conditions`.\n\n    \"\"\"\n    if ask_options is None:\n        ask_options = {}\n\n    def generator(n: int, q: int, seed: int) -&gt; Tensor:\n        if sequential:\n            initial_conditions = []\n            for _ in range(n):\n                candidates = strategy.ask(q, **ask_options)\n                # transform it\n                transformed_candidates = strategy.domain.inputs.transform(\n                    candidates,\n                    transform_specs,\n                )\n                # transform to tensor\n                initial_conditions.append(\n                    torch.from_numpy(transformed_candidates.values).to(**tkwargs),\n                )\n            return torch.stack(initial_conditions, dim=0)\n        candidates = strategy.ask(n * q, **ask_options)\n        # transform it\n        transformed_candidates = strategy.domain.inputs.transform(\n            candidates,\n            transform_specs,\n        )\n        return (\n            torch.from_numpy(transformed_candidates.values)\n            .to(**tkwargs)\n            .reshape(n, q, transformed_candidates.shape[1])\n        )\n\n    return generator\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_interpoint_constraints","title":"<code>get_interpoint_constraints(domain, n_candidates)</code>","text":"<p>Converts interpoint equality constraints to linear equality constraints,     that can be processed by botorch. For more information, see the docstring     of <code>optimize_acqf</code> in botorch     (https://github.com/pytorch/botorch/blob/main/botorch/optim/optimize.py).</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Optimization problem definition.</p> required <code>n_candidates</code> <code>int</code> <p>Number of candidates that should be requested.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Tensor, Tensor, float]]</code> <p>List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_interpoint_constraints(\n    domain: Domain,\n    n_candidates: int,\n) -&gt; List[Tuple[Tensor, Tensor, float]]:\n    \"\"\"Converts interpoint equality constraints to linear equality constraints,\n        that can be processed by botorch. For more information, see the docstring\n        of `optimize_acqf` in botorch\n        (https://github.com/pytorch/botorch/blob/main/botorch/optim/optimize.py).\n\n    Args:\n        domain (Domain): Optimization problem definition.\n        n_candidates (int): Number of candidates that should be requested.\n\n    Returns:\n        List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists\n            of a tensor with the feature indices, coefficients and a float for the rhs.\n\n    \"\"\"\n    constraints = []\n    if n_candidates == 1:\n        return constraints\n    for constraint in domain.constraints.get(InterpointEqualityConstraint):\n        assert isinstance(constraint, InterpointEqualityConstraint)\n        coefficients = torch.tensor([1.0, -1.0]).to(**tkwargs)\n        feat_idx = domain.inputs.get_keys(Input).index(constraint.feature)\n        feat = domain.inputs.get_by_key(constraint.feature)\n        assert isinstance(feat, ContinuousInput)\n        if feat.is_fixed():\n            continue\n        multiplicity = constraint.multiplicity or n_candidates\n        for i in range(math.ceil(n_candidates / multiplicity)):\n            all_indices = torch.arange(\n                i * multiplicity,\n                min((i + 1) * multiplicity, n_candidates),\n            )\n            for k in range(len(all_indices) - 1):\n                indices = torch.tensor(\n                    [[all_indices[0], feat_idx], [all_indices[k + 1], feat_idx]],\n                    dtype=torch.int64,\n                )\n                constraints.append((indices, coefficients, 0.0))\n    return constraints\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_linear_constraints","title":"<code>get_linear_constraints(domain, constraint, unit_scaled=False)</code>","text":"<p>Converts linear constraints to the form required by BoTorch. For inequality constraints, this is A * x &gt;= b (!).</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Optimization problem definition.</p> required <code>constraint</code> <code>Union[Type[LinearEqualityConstraint], Type[LinearInequalityConstraint]]</code> <p>Type of constraint that should be converted.</p> required <code>unit_scaled</code> <code>bool</code> <p>If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tuple[Tensor, Tensor, float]]</code> <p>List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_linear_constraints(\n    domain: Domain,\n    constraint: Union[Type[LinearEqualityConstraint], Type[LinearInequalityConstraint]],\n    unit_scaled: bool = False,\n) -&gt; List[Tuple[Tensor, Tensor, float]]:\n    \"\"\"Converts linear constraints to the form required by BoTorch. For inequality constraints, this is A * x &gt;= b (!).\n\n    Args:\n        domain: Optimization problem definition.\n        constraint: Type of constraint that should be converted.\n        unit_scaled: If True, transforms constraints by assuming that the bound for the continuous features are [0,1]. Defaults to False.\n\n    Returns:\n        List[Tuple[Tensor, Tensor, float]]: List of tuples, each tuple consists of a tensor with the feature indices, coefficients and a float for the rhs.\n\n    \"\"\"\n    constraints = []\n    for c in domain.constraints.get(constraint):\n        indices = []\n        coefficients = []\n        lower = []\n        upper = []\n        rhs = 0.0\n        for i, featkey in enumerate(c.features):\n            idx = domain.inputs.get_keys(Input).index(featkey)\n            feat = domain.inputs.get_by_key(featkey)\n            if feat.is_fixed():\n                rhs -= feat.fixed_value()[0] * c.coefficients[i]  # type: ignore\n            else:\n                lower.append(feat.lower_bound)  # type: ignore\n                upper.append(feat.upper_bound)  # type: ignore\n                indices.append(idx)\n                coefficients.append(\n                    c.coefficients[i],\n                )  # if unit_scaled == False else c_scaled.coefficients[i])\n        if unit_scaled:\n            lower = np.array(lower)\n            upper = np.array(upper)\n            s = upper - lower\n            scaled_coefficients = s * np.array(coefficients)\n            constraints.append(\n                (\n                    torch.tensor(indices),\n                    -torch.tensor(scaled_coefficients).to(**tkwargs),\n                    -(rhs + c.rhs - np.sum(np.array(coefficients) * lower)),\n                ),\n            )\n        else:\n            constraints.append(\n                (\n                    torch.tensor(indices),\n                    -torch.tensor(coefficients).to(**tkwargs),\n                    -(rhs + c.rhs),\n                ),\n            )\n    return constraints\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_multiobjective_objective","title":"<code>get_multiobjective_objective(outputs, experiments)</code>","text":"<p>Returns a callable that can be used by botorch for multiobjective optimization.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Outputs</code> <p>Outputs object for which the callable should be generated.</p> required <code>experiments</code> <code>DataFrame</code> <p>DataFrame containing the experiments that are used for adapting the objectives on the fly, for example in the case of the <code>MovingMaximizeSigmoidObjective</code>.</p> required <p>Returns:</p> Type Description <code>Callable[[Tensor, Optional[Tensor]], Tensor]</code> <p>Callable[[Tensor], Tensor]: description</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_multiobjective_objective(\n    outputs: Outputs,\n    experiments: pd.DataFrame,\n) -&gt; Callable[[Tensor, Optional[Tensor]], Tensor]:\n    \"\"\"Returns a callable that can be used by botorch for multiobjective optimization.\n\n    Args:\n        outputs (Outputs): Outputs object for which the callable should be generated.\n        experiments (pd.DataFrame): DataFrame containing the experiments that are used for\n            adapting the objectives on the fly, for example in the case of the\n            `MovingMaximizeSigmoidObjective`.\n\n    Returns:\n        Callable[[Tensor], Tensor]: _description_\n\n    \"\"\"\n    allowed_objectives = [MaximizeObjective, MinimizeObjective, CloseToTargetObjective]\n\n    callables_outputs, _, _ = _callables_and_weights(\n        outputs,\n        experiments,\n        allowed_objectives=allowed_objectives,\n        adapt_weights_to_1_inf=False,\n    )\n\n    def objective(samples: Tensor, X: Optional[Tensor] = None) -&gt; Tensor:\n        return torch.stack([c(samples, None) for c in callables_outputs], dim=-1)\n\n    return objective\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_multiplicative_additive_objective","title":"<code>get_multiplicative_additive_objective(outputs, experiments, exclude_constraints=True, additive_features=None, adapt_weights_to_1_inf=True)</code>","text":"<p>Computes the objective as a mix of multiplicative and additive objectives. By default, all objectives are multiplicative. Additive features (inputs or outputs) can be specified in the <code>additive_features</code> list.</p> <p>The formular for a mixed objective with two multiplicative features (f1, and f2 with weights w1 and w2) and two additive features (f3 and f4 with weights w3 and w4) is:</p> <pre><code>additive_objective = 1 + f3*w3 + f4*w4\n\nobjective = f1^w1 * f2^w2 * additive_objective\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>additive_features</code> <code>List[str]</code> <p>list of features that should be treated as additive</p> <code>None</code> <code>adapt_weights_to_1_inf</code> <code>bool</code> <p>will transform weights from [0,1] to [1,inf) space</p> <code>True</code> <p>Returns:</p> Name Type Description <code>objective</code> <code>callable</code> <p>callable that can be used by botorch for optimization</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_multiplicative_additive_objective(\n    outputs: Outputs,\n    experiments: pd.DataFrame,\n    exclude_constraints: bool = True,\n    additive_features: Optional[List[str]] = None,\n    adapt_weights_to_1_inf: bool = True,\n) -&gt; Callable[[Tensor, Tensor], Tensor]:\n    \"\"\"Computes the objective as a mix of multiplicative and additive objectives. By default, all objectives are multiplicative.\n    Additive features (inputs or outputs) can be specified in the `additive_features` list.\n\n    The formular for a mixed objective with two multiplicative features (f1, and f2 with weights w1 and w2) and two\n    additive features (f3 and f4 with weights w3 and w4) is:\n\n        additive_objective = 1 + f3*w3 + f4*w4\n\n        objective = f1^w1 * f2^w2 * additive_objective\n\n\n    Args:\n        outputs\n        experiments\n        exclude_constraints\n        additive_features (List[str]): list of features that should be treated as additive\n        adapt_weights_to_1_inf (bool): will transform weights from [0,1] to [1,inf) space\n\n    Returns:\n        objective (callable): callable that can be used by botorch for optimization\n\n    \"\"\"\n\n    callables, weights, keys = _callables_and_weights(\n        outputs,\n        experiments,\n        exclude_constraints=exclude_constraints,\n        adapt_weights_to_1_inf=adapt_weights_to_1_inf,\n    )\n\n    if additive_features is None:\n        additive_features = []\n\n    def _differ_additive_and_multiplicative_features(callables, weights, feature_names):\n        callables_additive, weights_additive = [], []\n        callables_multiplicative, weights_multiplicative = [], []\n        for c, w, key in zip(callables, weights, feature_names):\n            if key in additive_features:\n                callables_additive.append(c)\n                weights_additive.append(w)\n            else:\n                callables_multiplicative.append(c)\n                weights_multiplicative.append(w)\n        return (\n            callables_additive,\n            weights_additive,\n            callables_multiplicative,\n            weights_multiplicative,\n        )\n\n    (\n        callables_additive,\n        weights_additive,\n        callables_multiplicative,\n        weights_multiplicative,\n    ) = _differ_additive_and_multiplicative_features(callables, weights, keys)\n\n    def objective(samples: Tensor, X: Optional[Tensor] = None) -&gt; Tensor:\n        additive_objective = torch.tensor(1.0).to(**tkwargs)\n        for c, w in zip(callables_additive, weights_additive):\n            additive_objective = additive_objective + c(samples, None) * w\n\n        multiplicative_objective = torch.tensor(1.0).to(**tkwargs)\n        for c, w in zip(callables_multiplicative, weights_multiplicative):\n            multiplicative_objective = multiplicative_objective * c(samples, None) ** w\n\n        y: Tensor = multiplicative_objective * additive_objective\n        return y\n\n    return objective\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_nchoosek_constraints","title":"<code>get_nchoosek_constraints(domain)</code>","text":"<p>Transforms NChooseK constraints into a list of non-linear inequality constraint callables that can be parsed by pydantic. For this purpose the NChooseK constraint is continuously relaxed by countig the number of zeros in a candidate by a sum of narrow gaussians centered at zero.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>Optimization problem definition.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Callable[[Tensor], float], bool]]</code> <p>List[Callable[[Tensor], float]]: List of callables that can be used as nonlinear equality constraints in botorch.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_nchoosek_constraints(\n    domain: Domain,\n) -&gt; List[Tuple[Callable[[Tensor], float], bool]]:\n    \"\"\"Transforms NChooseK constraints into a list of non-linear inequality constraint callables\n    that can be parsed by pydantic. For this purpose the NChooseK constraint is continuously\n    relaxed by countig the number of zeros in a candidate by a sum of narrow gaussians centered\n    at zero.\n\n    Args:\n        domain (Domain): Optimization problem definition.\n\n    Returns:\n        List[Callable[[Tensor], float]]: List of callables that can be used\n            as nonlinear equality constraints in botorch.\n\n    \"\"\"\n\n    def narrow_gaussian(x, ell=1e-3):\n        return torch.exp(-0.5 * (x / ell) ** 2)\n\n    def max_constraint(indices: Tensor, num_features: int, max_count: int):\n        return lambda x: narrow_gaussian(x=x[..., indices]).sum(dim=-1) - (\n            num_features - max_count\n        )\n\n    def min_constraint(indices: Tensor, num_features: int, min_count: int):\n        return lambda x: -narrow_gaussian(x=x[..., indices]).sum(dim=-1) + (\n            num_features - min_count\n        )\n\n    constraints = []\n    # ignore none also valid for the start\n    for c in domain.constraints.get(NChooseKConstraint):\n        assert isinstance(c, NChooseKConstraint)\n        indices = torch.tensor(\n            [domain.inputs.get_keys(ContinuousInput).index(key) for key in c.features],\n            dtype=torch.int64,\n        )\n        if c.max_count != len(c.features):\n            constraints.append(\n                (\n                    max_constraint(\n                        indices=indices,\n                        num_features=len(c.features),\n                        max_count=c.max_count,\n                    ),\n                    True,\n                )\n            )\n        if c.min_count &gt; 0:\n            constraints.append(\n                (\n                    min_constraint(\n                        indices=indices,\n                        num_features=len(c.features),\n                        min_count=c.min_count,\n                    ),\n                    True,\n                )\n            )\n    return constraints\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_nonlinear_constraints","title":"<code>get_nonlinear_constraints(domain, includes=None)</code>","text":"<p>Returns a list of callable functions that represent the nonlinear constraints for the given domain that can be processed by botorch.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>The domain for which to generate the nonlinear constraints.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Callable[[Tensor], float], bool]]</code> <p>List[Callable[[Tensor], float]]: A list of callable functions that take a tensor</p> <code>List[Tuple[Callable[[Tensor], float], bool]]</code> <p>as input and return a float value representing the constraint evaluation.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_nonlinear_constraints(\n    domain: Domain,\n    includes: Optional[List[Type[Constraint]]] = None,\n) -&gt; List[Tuple[Callable[[Tensor], float], bool]]:\n    \"\"\"Returns a list of callable functions that represent the nonlinear constraints\n    for the given domain that can be processed by botorch.\n\n    Args:\n        domain (Domain): The domain for which to generate the nonlinear constraints.\n\n    Returns:\n        List[Callable[[Tensor], float]]: A list of callable functions that take a tensor\n        as input and return a float value representing the constraint evaluation.\n\n    \"\"\"\n    includes = includes or [NChooseKConstraint, ProductInequalityConstraint]\n    assert all(\n        (c in (NChooseKConstraint, ProductInequalityConstraint) for c in includes)\n    ), \"Only NChooseK and ProductInequality constraints are supported.\"\n\n    callables = []\n    if NChooseKConstraint in includes:\n        callables += get_nchoosek_constraints(domain)\n    if ProductInequalityConstraint in includes:\n        callables += get_product_constraints(domain)\n\n    return callables\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_output_constraints","title":"<code>get_output_constraints(outputs, experiments)</code>","text":"<p>Method to translate output constraint objectives into a list of callables and list of etas for use in botorch.</p> <p>Parameters:</p> Name Type Description Default <code>outputs</code> <code>Outputs</code> <p>Output feature object that should be processed.</p> required <code>experiments</code> <code>DataFrame</code> <p>DataFrame containing the experiments that are used for adapting the objectives on the fly, for example in the case of the <code>MovingMaximizeSigmoidObjective</code>.</p> required <p>Returns:</p> Type Description <code>Tuple[List[Callable[[Tensor], Tensor]], List[float]]</code> <p>Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of constraint callables, list of associated etas.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_output_constraints(\n    outputs: Outputs,\n    experiments: pd.DataFrame,\n) -&gt; Tuple[List[Callable[[Tensor], Tensor]], List[float]]:\n    \"\"\"Method to translate output constraint objectives into a list of\n    callables and list of etas for use in botorch.\n\n    Args:\n        outputs (Outputs): Output feature object that should\n            be processed.\n        experiments (pd.DataFrame): DataFrame containing the experiments that are used for\n            adapting the objectives on the fly, for example in the case of the\n            `MovingMaximizeSigmoidObjective`.\n\n    Returns:\n        Tuple[List[Callable[[Tensor], Tensor]], List[float]]: List of constraint callables,\n            list of associated etas.\n\n    \"\"\"\n    constraints = []\n    etas = []\n    idx = 0\n    for feat in outputs.get():\n        if isinstance(feat.objective, ConstrainedObjective):\n            cleaned_experiments = outputs.preprocess_experiments_one_valid_output(\n                feat.key,\n                experiments,\n            )\n            iconstraints, ietas, idx = constrained_objective2botorch(\n                idx,\n                objective=feat.objective,\n                x_adapt=torch.from_numpy(cleaned_experiments[feat.key].values).to(\n                    **tkwargs,\n                )\n                if not isinstance(feat.objective, ConstrainedCategoricalObjective)\n                else None,\n            )\n            constraints += iconstraints\n            etas += ietas\n        else:\n            idx += 1\n    return constraints, etas\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.get_product_constraints","title":"<code>get_product_constraints(domain)</code>","text":"<p>Returns a list of nonlinear constraint functions that can be processed by botorch based on the given domain.</p> <p>Parameters:</p> Name Type Description Default <code>domain</code> <code>Domain</code> <p>The domain object containing the constraints.</p> required <p>Returns:</p> Type Description <code>List[Tuple[Callable[[Tensor], float], bool]]</code> <p>List[Callable[[Tensor], float]]: A list of product constraint functions.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>def get_product_constraints(\n    domain: Domain,\n) -&gt; List[Tuple[Callable[[Tensor], float], bool]]:\n    \"\"\"Returns a list of nonlinear constraint functions that can be processed by botorch\n    based on the given domain.\n\n    Args:\n        domain (Domain): The domain object containing the constraints.\n\n    Returns:\n        List[Callable[[Tensor], float]]: A list of product constraint functions.\n\n    \"\"\"\n\n    def product_constraint(indices: Tensor, exponents: Tensor, rhs: float, sign: int):\n        return lambda x: -1.0 * sign * (x[..., indices] ** exponents).prod(dim=-1) + rhs\n\n    constraints = []\n    for c in domain.constraints.get(ProductInequalityConstraint):\n        assert isinstance(c, ProductInequalityConstraint)\n        indices = torch.tensor(\n            [domain.inputs.get_keys(ContinuousInput).index(key) for key in c.features],\n            dtype=torch.int64,\n        )\n        constraints.append(\n            (\n                product_constraint(indices, torch.tensor(c.exponents), c.rhs, c.sign),\n                True,\n            )\n        )\n    return constraints\n</code></pre>"},{"location":"ref-utils/#bofire.utils.torch_tools.interp1d","title":"<code>interp1d(x, y, x_new)</code>","text":"<p>Interpolates values in the y tensor based on the x tensor using linear interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The x-coordinates of the data points.</p> required <code>y</code> <code>Tensor</code> <p>The y-coordinates of the data points.</p> required <code>x_new</code> <code>Tensor</code> <p>The x-coordinates at which to interpolate the values.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The interpolated values at the x_new x-coordinates.</p> Source code in <code>bofire/utils/torch_tools.py</code> <pre><code>@torch.jit.script  # type: ignore\ndef interp1d(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    x_new: torch.Tensor,\n) -&gt; torch.Tensor:\n    \"\"\"Interpolates values in the y tensor based on the x tensor using linear interpolation.\n\n    Args:\n        x (torch.Tensor): The x-coordinates of the data points.\n        y (torch.Tensor): The y-coordinates of the data points.\n        x_new (torch.Tensor): The x-coordinates at which to interpolate the values.\n\n    Returns:\n        torch.Tensor: The interpolated values at the x_new x-coordinates.\n\n    \"\"\"\n    m = (y[1:] - y[:-1]) / (x[1:] - x[:-1])\n    b = y[:-1] - (m * x[:-1])\n\n    idx = torch.sum(torch.ge(x_new[:, None], x[None, :]), 1) - 1\n    idx = torch.clamp(idx, 0, len(m) - 1)\n\n    itp = m[idx] * x_new + b[idx]\n\n    return itp\n</code></pre>"},{"location":"userguide_surrogates/","title":"Surrogate models","text":"<p>In Bayesian Optimization, information from previous experiments is taken into account to generate proposals for future experiments. This information is leveraged by creating a surrogate model for the black-box function that is to be optimized based on the available data. Naturally, experimental candidates for which the surrogate model makes a promising prediction (e.g., high predicted values of a quantity we want to maximize) should be chosen over ones for which this is not the case. However, since the available data might cover only a small part of the input space, the model is likely to only be able to make very uncertain predictions far away from the data. Therefore, the surrogate model should be able to express the degree to which the predictions are uncertain so that we can use this information - combining the prediction and the associated uncertainty - to select the settings for the next experimental iteration.</p> <p>The acquisition function is the object that turns the predicted distribution (you can think of this as the prediction and the prediction uncertainty) into a single quantity representing how promising a candidate experimental point seems. This function determines if one rather wants to focus on exploitation, i.e., quickly approaching a close local optimum of the black-box function, or on exploration, i.e., exploring different regions of the input space first.</p> <p>Therefore, three criteria typically determine whether any candidate is selected as experimental proposal: the value of the surrogate model, the uncertainty of the model, and the acquisition function.</p>"},{"location":"userguide_surrogates/#surrogate-model-options","title":"Surrogate model options","text":"<p>BoFire offers the following classes of surrogate models.</p> Surrogate Optimization of When to use Type SingleTaskGPSurrogate a single objective with real valued inputs Limited data and black-box function is smooth Gaussian process RandomForestSurrogate a single objective Rich data; black-box function does not have to be smooth sklearn random forest implementation MLP a single objective with real-valued inputs Rich data and black-box function is smooth Multi layer perceptron MixedSingleTaskGPSurrogate a single objective with categorical and real valued inputs Limited data and black-box function is smooth Gaussian process XGBoostSurrogate a single objective Rich data; black-box function does not have to be smooth xgboost implementation of gradient boosting trees TanimotoGP a single objective At least one input feature is a molecule represented as fingerprint Gaussian process on a molecule space for which Tanimoto similarity determines the similarity between points <p>All of these are single-objective surrogate models. For optimization of multiple objectives at the same time, a suitable Strategy has to be chosen. Then for each objective a different surrogate model can be specified. By default the SingleTaskGPSurrogate is used.</p> <p>Example:</p> <pre><code>surrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n)\nsurrogate_data_1 = XGBoostSurrogate(\n    inputs=domain.inputs,\n    outputs=Outputs(features=[domain.outputs[1]]),\n)\nqparego_data_model = QparegoStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data_0, surrogate_data_1]\n    ),\n)\n</code></pre> <p>Note:</p> <ul> <li>The standard Kernel for all Gaussian Process (GP) surrogates is a 5/2 matern kernel with automated relevance detection and normalization of the input features.</li> <li>The tree-based models (RandomForestSurrogate and XGBoostSurrogate) do not have kernels but quantify uncertainty using the standard deviation of the predictions of their individual trees.</li> <li>MLP quantifies uncertainty using the standard deviation of multiple predictions that come from different dropout rates (randomly setting neural network weights to zero).</li> </ul>"},{"location":"userguide_surrogates/#customization","title":"Customization","text":"<p>BoFire also offers the option to customize surrogate models. In particular, it is possible to customize the SingleTaskGPSurrogate in the following ways.</p>"},{"location":"userguide_surrogates/#kernel-customization","title":"Kernel customization","text":"<p>Specify the Kernel:</p> Kernel Description Translation invariant Input variable type RBFKernel Based on Gaussian distribution Yes Continuous MaternKernel Based on Gamma function; allows setting a smoothness parameter Yes Continuous PolynomialKernel Based on dot-product of two vectors of input points No Continuous LinearKernel Equal to dot-product of two vectors of input points No Continuous TanimotoKernel Measures similarities between binary vectors using Tanimoto Similarity Not applicable MolecularInput HammingDistanceKernel Similarity is defined by the Hamming distance which considers the number of equal entries between two vectors (e.g., in One-Hot-encoding) Not applicable Categorical <p>Translational invariance means that the similarity between two input points is not affected by shifting both points by the same amount but only determined by their distance. Example: with a translationally invariant kernel, the values 10 and 20 are equally similar to each other as the values 20 and 30, while with a polynomial kernel the latter pair has potentially higher similarity. Polynomial kernels are often suitable for high-dimensional inputs while for low-dimensional inputs an RBF or Mat\u00e9rn kernel is recommended.</p> <p>Note:</p> <ul> <li>SingleTaskGPSurrogate with PolynomialKernel is equivalent to PolynomialSurrogate.</li> <li>SingleTaskGPSurrogate with LinearKernel is equivalent to LinearSurrogate.</li> <li>SingleTaskGPSurrogate with TanimotoKernel is equivalent to TanimotoGP.</li> <li>One can combine two Kernels by using AdditiveKernel or MultiplicativeKernel.</li> </ul> <p>Example:</p> <pre><code>surrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2)\n)\n</code></pre>"},{"location":"userguide_surrogates/#noise-model-customization","title":"Noise model customization","text":"<p>For experimental data subject to noise, one can specify the distribution of this noise. The options are:</p> Noise Model When to use NormalPrior Noise is Gaussian GammaPrior Noise has a Gamma distribution <p>Example:</p> <pre><code>surrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2),\n        noise_prior=NormalPrior(loc=0, scale=1)\n)\n</code></pre>"}]}