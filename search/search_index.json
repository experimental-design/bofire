{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#introduction","title":"Introduction","text":"<p>BoFire is a framework to define and solve black-box optimization problems. These problems can arise in a number of closely related fields including experimental design, multi-objective optimization and active learning.</p> <p>BoFire problem specifications are json serializable for use in RESTful APIs and are to a large extent agnostic to the specific methods and frameworks in which the problems are solved.</p> <p>You can find code-examples in the Getting Started section of this document, as well as full worked-out examples of code-usage in the /tutorials section of this repository!</p>"},{"location":"#experimental-design","title":"Experimental design","text":"<p>In the context of experimental design BoFire allows to define a design space</p> \\[ \\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D \\] <p>where the design parameters may take values depending on their type and domain, e.g.</p> <ul> <li>continuous: \\(x_1 \\in [0, 1]\\)</li> <li>discrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\)</li> <li>categorical: \\(x_3 \\in \\{A, B, C\\}\\)</li> </ul> <p>and a set of equations define additional experimental constraints, e.g.</p> <ul> <li>linear equality: \\(\\sum x_i = 1\\)</li> <li>linear inequality: \\(2 x_1 \\leq x_2\\)</li> <li>non-linear inequality: \\(\\sum x_i^2 \\leq 1\\)</li> <li>n-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.</li> </ul>"},{"location":"#multi-objective-optimization","title":"Multi-objective optimization","text":"<p>In the context of multi-objective optimization BoFire allows to define a vector-valued optimization problem</p> \\[ \\argmax_{x \\in \\mathbb{X}} s(y(x)) \\] <p>where</p> <ul> <li>\\(\\mathbb{X}\\) is again the experimental design space</li> <li>\\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and</li> <li>\\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be maximized. For instance, \\(s_1\\) is the identity function if \\(y_1\\) is to be maximized.</li> </ul> <p>Since the objectives are usually conflicting, there is no point \\(x\\) that simultaneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises.</p> <p>A decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.</p>"},{"location":"#bayesian-optimization","title":"Bayesian optimization","text":"<p>In the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, \\mathrm{data})\\) that estimates the distribution of possible outcomes for \\(y\\). An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation</p> \\[ \\argmax_{x \\in \\mathbb{X}} a(s(p_y(x))) \\] <p>and the maximizer \\(x_\\mathrm{opt}\\) of this acquisition function determines the next experiment \\(y(x)\\) to run.</p> <p>When there are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.</p>"},{"location":"#design-of-experiments","title":"Design of Experiments","text":"<p>BoFire can be used to generate optimal experimental designs with respect to various optimality criteria like D-optimality, A-optimality or uniform space filling.</p> <p>For this, the user specifies a design space and a model formula, then chooses an optimality criterion and the desired number of experiments in the design. The resulting optimization problem is then solved by IPOPT.</p> <p>The doe subpackage also supports a wide range of constraints on the design space including linear and nonlinear equalities and inequalities as well a (limited) use of NChooseK constraints. The user can provide fixed experiments that will be treated as part of the design but remain fixed during the optimization process. While some of the optimization algorithms support non-continuous design variables, the doe subpackage only supports those that are continuous.</p> <p>By default IPOPT uses the freely available linear solver MUMPS. For large models choosing a different linear solver (e.g. ma57 from Coin-HSL) can vastly reduce optimization time. A free academic license for Coin-HSL can be obtained here. Instructions on how to install additional linear solvers for IPOPT are given in the IPOPT documentation. For choosing a specific (HSL) linear solver in BoFire you can just pass the name of the solver to <code>find_local_max_ipopt()</code> with the <code>linear_solver</code> option together with the library's name in the option <code>hsllib</code>, e.g. <pre><code>find_local_max_ipopt(domain, \"fully-quadratic\", ipopt_options={\"linear_solver\":\"ma57\", \"hsllib\":\"libcoinhsl.so\"})\n</code></pre></p>"},{"location":"#reference","title":"Reference","text":"<p>We would love for you to use BoFire in your work! If you do, please cite our paper:</p> <pre><code>@misc{durholt2024bofire,\n  title={BoFire: Bayesian Optimization Framework Intended for Real Experiments},\n  author={Johannes P. D{\\\"{u}}rholt and Thomas S. Asche and Johanna Kleinekorte and Gabriel Mancino-Ball and Benjamin Schiller and Simon Sung and Julian Keupp and Aaron Osburg and Toby Boyne and Ruth Misener and Rosona Eldred and Wagner Steuer Costa and Chrysoula Kappatou and Robert M. Lee and Dominik Linzner and David Walz and Niklas Wulkow and Behrang Shafei},\n  year={2024},\n  eprint={2408.05040},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2408.05040},\n}\n</code></pre>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Contributions to BoFire are highly welcome!</p>"},{"location":"CONTRIBUTING/#pull-requests","title":"Pull Requests","text":"<p>Pull requests are highly welcome:</p> <ol> <li>Create a fork from main.</li> <li>Add or adapt unit tests according to your change.</li> <li>Add doc-strings and update the documentation. You might consider contributing to the tutorials section.</li> <li>Make sure that the GitHub pipelines passes.</li> </ol>"},{"location":"CONTRIBUTING/#development-environment","title":"Development Environment","text":"<p>We recommend an editable installation. After cloning the repository via <pre><code>git clone https://github.com/experimental-design/bofire.git\n</code></pre> and cd <code>bofire</code>, you can proceed with <pre><code>pip install -e \".[all]\"\n</code></pre> Afterwards, you can check that the tests are successful via <pre><code>pytest tests/\n</code></pre></p>"},{"location":"CONTRIBUTING/#coding-style","title":"Coding Style","text":"<p>We use Ruff for linting, sorting and formatting of our code. Our doc-strings are in Google-style.</p> <p>In our CI/CD pipeline we check if contributions are compliant to Ruff. To make contributors' lives easier, we have pre-commit hooks for Ruff configured in the versions corresponding to the pipeline. They can be installed via</p> <p><pre><code>pip install pre-commit\npre-commit install\n</code></pre> in you local project root folder, if you want to use <code>pre-commit</code>.</p>"},{"location":"CONTRIBUTING/#type-checks","title":"Type checks","text":"<p>We make heavy use of Pydantic to enforce type checks during runtime. Further, we use Pyright for static type checking. We enforce Pyright type checks in our CI/CD pipeline.</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>If you add new functionality, make sure that it is tested properly and that it does not break existing code. Our tests run in our CI/CD pipeline. The test coverage is hidden from our Readme because it is not a very robust metric. However, you can find it in the outputs of our test-CI/CD-pipeline. See example.</p>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>We use MkDocs with material theme and deploy our documentation to https://experimental-design.github.io/bofire/. Thereby, an API description is extracted from the doc-strings. Additionally, we have tutorials and getting-started-sections.</p>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing you agree that your contributions will be licensed under the same BSD 3-Clause License as BoFire.</p>"},{"location":"basic_examples/","title":"Basic Examples for the DoE Subpackage","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    InterpointEqualityConstraint,\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NonlinearEqualityConstraint,\n    NonlinearInequalityConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion, IOptimalityCriterion\n</pre> import matplotlib.pyplot as plt import numpy as np from matplotlib.ticker import FormatStrFormatter  import bofire.strategies.api as strategies from bofire.data_models.constraints.api import (     InterpointEqualityConstraint,     LinearEqualityConstraint,     LinearInequalityConstraint,     NonlinearEqualityConstraint,     NonlinearInequalityConstraint, ) from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import ContinuousInput, ContinuousOutput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion, IOptimalityCriterion In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0.1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 0.6)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n        LinearInequalityConstraint(features=[\"x1\", \"x2\"], coefficients=[5, 4], rhs=3.9),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\"],\n            coefficients=[-20, 5],\n            rhs=-3,\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(0, 1)),         ContinuousInput(key=\"x2\", bounds=(0.1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 0.6)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         LinearEqualityConstraint(             features=[\"x1\", \"x2\", \"x3\"],             coefficients=[1, 1, 1],             rhs=1,         ),         LinearInequalityConstraint(features=[\"x1\", \"x2\"], coefficients=[5, 4], rhs=3.9),         LinearInequalityConstraint(             features=[\"x1\", \"x2\"],             coefficients=[-20, 5],             rhs=-3,         ),     ], )  data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"disp\": 0}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(candidate_count=12) <pre>\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n</pre> <p>Let's visualize the experiments that were chosen. We will see that such a design puts the experiments at the extremes of the experimental space - these are the points that best allow us to estimate the parameters of the linear model we chose.</p> In\u00a0[\u00a0]: Copied! <pre>fig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"Linear model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n</pre> fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.view_init(45, 45) ax.set_title(\"Linear model\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(     xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],     ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],     zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],     linewidth=2, )  # plot D-optimal solutions ax.scatter(     xs=candidates[\"x1\"],     ys=candidates[\"x2\"],     zs=candidates[\"x3\"],     marker=\"o\",     s=40,     color=\"orange\",     label=\"optimal_design solution, 12 points\", )  plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x312f78d50&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>data_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\"\n    ),\n    ipopt_options={\"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12)\n</pre> data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(         formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\"     ),     ipopt_options={\"disp\": 0}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(12) <p>In this case we can compare with the result reported in the paper of Coetzer and Haines.</p> In\u00a0[\u00a0]: Copied! <pre>d_opt = np.array(\n    [\n        [\n            0.7,\n            0.3,\n            0.2,\n            0.3,\n            0.5902,\n            0.4098,\n            0.2702,\n            0.2279,\n            0.4118,\n            0.5738,\n            0.4211,\n            0.3360,\n        ],\n        [0.1, 0.6, 0.2, 0.1, 0.2373, 0.4628, 0.4808, 0.3117, 0.1, 0.1, 0.2911, 0.2264],\n        [\n            0.2,\n            0.1,\n            0.6,\n            0.6,\n            0.1725,\n            0.1274,\n            0.249,\n            0.4604,\n            0.4882,\n            0.3262,\n            0.2878,\n            0.4376,\n        ],\n    ],\n)  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solution\nax.scatter(\n    xs=d_opt[0],\n    ys=d_opt[1],\n    zs=d_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"D-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n</pre> d_opt = np.array(     [         [             0.7,             0.3,             0.2,             0.3,             0.5902,             0.4098,             0.2702,             0.2279,             0.4118,             0.5738,             0.4211,             0.3360,         ],         [0.1, 0.6, 0.2, 0.1, 0.2373, 0.4628, 0.4808, 0.3117, 0.1, 0.1, 0.2911, 0.2264],         [             0.2,             0.1,             0.6,             0.6,             0.1725,             0.1274,             0.249,             0.4604,             0.4882,             0.3262,             0.2878,             0.4376,         ],     ], )  # values taken from paper   fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.set_title(\"cubic model\") ax.view_init(45, 45) ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(     xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],     ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],     zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],     linewidth=2, )  # plot D-optimal solution ax.scatter(     xs=d_opt[0],     ys=d_opt[1],     zs=d_opt[2],     marker=\"o\",     s=40,     color=\"darkgreen\",     label=\"D-optimal design, 12 points\", )  ax.scatter(     xs=candidates[\"x1\"],     ys=candidates[\"x2\"],     zs=candidates[\"x3\"],     marker=\"o\",     s=40,     color=\"orange\",     label=\"optimal_design solution, 12 points\", )  plt.legend() Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x32fb49710&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>data_model = DoEStrategy(\n    domain=domain,\n    criterion=IOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\",\n        n_space_filling_points=60,\n        ipopt_options={\"maxiter\": 500},\n    ),\n    ipopt_options={\"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12).to_numpy().T\n\n\ni_opt = np.array(\n    [\n        [0.7000, 0.1000, 0.2000],\n        [0.3000, 0.6000, 0.1000],\n        [0.2031, 0.1969, 0.6000],\n        [0.5899, 0.2376, 0.1725],\n        [0.4105, 0.4619, 0.1276],\n        [0.2720, 0.4882, 0.2398],\n        [0.2281, 0.3124, 0.4595],\n        [0.3492, 0.1000, 0.5508],\n        [0.5614, 0.1000, 0.3386],\n        [0.4621, 0.2342, 0.3037],\n        [0.3353, 0.2228, 0.4419],\n        [0.3782, 0.3618, 0.2600],\n    ]\n).T  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot I-optimal solution\nax.scatter(\n    xs=i_opt[0],\n    ys=i_opt[1],\n    zs=i_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"I-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[0],\n    ys=candidates[1],\n    zs=candidates[2],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n</pre> data_model = DoEStrategy(     domain=domain,     criterion=IOptimalityCriterion(         formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\",         n_space_filling_points=60,         ipopt_options={\"maxiter\": 500},     ),     ipopt_options={\"disp\": 0}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(12).to_numpy().T   i_opt = np.array(     [         [0.7000, 0.1000, 0.2000],         [0.3000, 0.6000, 0.1000],         [0.2031, 0.1969, 0.6000],         [0.5899, 0.2376, 0.1725],         [0.4105, 0.4619, 0.1276],         [0.2720, 0.4882, 0.2398],         [0.2281, 0.3124, 0.4595],         [0.3492, 0.1000, 0.5508],         [0.5614, 0.1000, 0.3386],         [0.4621, 0.2342, 0.3037],         [0.3353, 0.2228, 0.4419],         [0.3782, 0.3618, 0.2600],     ] ).T  # values taken from paper   fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.set_title(\"cubic model\") ax.view_init(45, 45) ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(     xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],     ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],     zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],     linewidth=2, )  # plot I-optimal solution ax.scatter(     xs=i_opt[0],     ys=i_opt[1],     zs=i_opt[2],     marker=\"o\",     s=40,     color=\"darkgreen\",     label=\"I-optimal design, 12 points\", )  ax.scatter(     xs=candidates[0],     ys=candidates[1],     zs=candidates[2],     marker=\"o\",     s=40,     color=\"orange\",     label=\"optimal_design solution, 12 points\", )  plt.legend() <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/objective.py:222: UserWarning: Equality constraints were detected. No equidistant grid of points can be generated. The design space will be filled via SpaceFilling.\n  warnings.warn(\n</pre> Out[\u00a0]: <pre>&lt;matplotlib.legend.Legend at 0x32fb7ded0&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>def plot_results_3d(result, surface_func):\n    u, v = np.mgrid[0 : 2 * np.pi : 100j, 0 : np.pi : 80j]\n    X = np.cos(u) * np.sin(v)\n    Y = np.sin(u) * np.sin(v)\n    Z = surface_func(X, Y)\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(X, Y, Z, alpha=0.3)\n    ax.scatter(\n        xs=result[\"x1\"],\n        ys=result[\"x2\"],\n        zs=result[\"x3\"],\n        marker=\"o\",\n        s=40,\n        color=\"red\",\n    )\n    ax.set(xlabel=\"x1\", ylabel=\"x2\", zlabel=\"x3\")\n    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n</pre> def plot_results_3d(result, surface_func):     u, v = np.mgrid[0 : 2 * np.pi : 100j, 0 : np.pi : 80j]     X = np.cos(u) * np.sin(v)     Y = np.sin(u) * np.sin(v)     Z = surface_func(X, Y)      fig = plt.figure(figsize=(8, 8))     ax = fig.add_subplot(111, projection=\"3d\")     ax.plot_surface(X, Y, Z, alpha=0.3)     ax.scatter(         xs=result[\"x1\"],         ys=result[\"x2\"],         zs=result[\"x3\"],         marker=\"o\",         s=40,         color=\"red\",     )     ax.set(xlabel=\"x1\", ylabel=\"x2\", zlabel=\"x3\")     ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))     ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\")) In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"maxiter\": 100, \"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(-1, 1)),         ContinuousInput(key=\"x2\", bounds=(-1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         NonlinearInequalityConstraint(             expression=\"(x1**2 + x2**2)**0.5 - x3\",             features=[\"x1\", \"x2\", \"x3\"],         ),     ], )  data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"maxiter\": 100, \"disp\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(     strategy.get_required_number_of_experiments(), raise_validation_error=False ) result.round(3) plot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2)) <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:106: UserWarning: Nonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:130: UserWarning: Sampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:204: UserWarning: Some points do not lie inside the domain or violate constraints. Please check if the                 results lie within your tolerance.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/data_models/domain/domain.py:454: UserWarning: Not all constraints are fulfilled.\n  warnings.warn(\"Not all constraints are fulfilled.\")\n</pre> <p>We can do the same for a design space limited by an elliptical cone $x_1^2 + x_2^2 - x_3 \\leq 0$.</p> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"x1**2 + x2**2 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"maxiter\": 100, \"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: x1**2 + x2**2)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(-1, 1)),         ContinuousInput(key=\"x2\", bounds=(-1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         NonlinearInequalityConstraint(             expression=\"x1**2 + x2**2 - x3\",             features=[\"x1\", \"x2\", \"x3\"],         ),     ], ) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"maxiter\": 100, \"disp\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(     strategy.get_required_number_of_experiments(), raise_validation_error=False ) result.round(3) plot_results_3d(result, surface_func=lambda x1, x2: x1**2 + x2**2) <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:106: UserWarning: Nonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:130: UserWarning: Sampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearEqualityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"maxiter\": 100, \"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12, raise_validation_error=False)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(-1, 1)),         ContinuousInput(key=\"x2\", bounds=(-1, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         NonlinearEqualityConstraint(             expression=\"(x1**2 + x2**2)**0.5 - x3\",             features=[\"x1\", \"x2\", \"x3\"],         ),     ], ) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"maxiter\": 100, \"disp\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(12, raise_validation_error=False) result.round(3) plot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2)) <pre>/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:106: UserWarning: Nonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n  warnings.warn(\n/Users/aaron/Desktop/bofire/bofire/strategies/doe/design.py:130: UserWarning: Sampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[InterpointEqualityConstraint(feature=\"x1\", multiplicity=3)],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"maxiter\": 500, \"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12)\nresult.round(3)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"x1\", bounds=(0, 1)),         ContinuousInput(key=\"x2\", bounds=(0, 1)),         ContinuousInput(key=\"x3\", bounds=(0, 1)),     ],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[InterpointEqualityConstraint(feature=\"x1\", multiplicity=3)], ) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"linear\"),     ipopt_options={\"maxiter\": 500, \"disp\": 0}, ) strategy = strategies.map(data_model=data_model) result = strategy.ask(12) result.round(3) Out[\u00a0]: x1 x2 x3 0 1.0 -0.0 -0.0 1 1.0 1.0 1.0 2 1.0 1.0 1.0 3 1.0 1.0 -0.0 4 1.0 -0.0 -0.0 5 1.0 -0.0 1.0 6 -0.0 -0.0 1.0 7 -0.0 1.0 -0.0 8 -0.0 -0.0 -0.0 9 -0.0 -0.0 -0.0 10 -0.0 1.0 1.0 11 -0.0 -0.0 1.0"},{"location":"basic_examples/#basic-examples-for-the-doe-subpackage","title":"Basic Examples for the DoE Subpackage\u00b6","text":"<p>The following example has been taken from the paper \"The construction of D- and I-optimal designs for mixture experiments with linear constraints on the components\" by R. Coetzer and L. M. Haines (https://www.sciencedirect.com/science/article/pii/S0169743917303106).</p>"},{"location":"basic_examples/#linear-model","title":"Linear model\u00b6","text":"<p>Creating an experimental design that is D-optimal with respect to a linear model is done the same way as making proposals using other methods in BoFire; you</p> <ol> <li>create a domain</li> <li>construct a stategy data model (here we want DoEStrategy)</li> <li>map the strategy to its functional version, and finally</li> <li>ask the strategy for proposals.</li> </ol> <p>We will start with the simplest case: make a design based on a linear model containing main-effects (i.e., simply the inputs themselves and an intercept, without any second-order terms).</p>"},{"location":"basic_examples/#cubic-model","title":"cubic model\u00b6","text":"<p>While the previous design is optimal for the main-effects model, we might prefer to see something that does not allocate all the experimental effort to values at the boundary of the space. This implies that we think there might be some higher-order effects present in the system - if we were sure that the target variable would follow straight-line behavior across the domain, we would not need to investigate any points away from the extremes.</p> <p>We can address this by specifying our own linear model that includes higher-order terms.</p>"},{"location":"basic_examples/#nonlinear-constraints","title":"Nonlinear Constraints\u00b6","text":"<p>Design generation also supports nonlinear constraints. The following 3 examples show what is possible.</p> <p>First, a convenience function for plotting.</p>"},{"location":"basic_examples/#example-1-design-inside-a-cone-nonlinear-inequality","title":"Example 1: Design inside a cone / nonlinear inequality\u00b6","text":"<p>In the following example we have three design variables. We impose the constraint that all experiments have to be contained in the interior of a cone, which corresponds to the nonlinear inequality constraint $\\sqrt{x_1^2 + x_2^2} - x_3 \\leq 0$. The optimization is done for a linear model and we will see that it places the points on the surface of the cone so as to maximize the distance between them (although this is not explicitly the objective of the optimization).</p>"},{"location":"basic_examples/#example-2-design-on-the-surface-of-a-cone-nonlinear-equality","title":"Example 2: Design on the surface of a cone / nonlinear equality\u00b6","text":"<p>We can also limit the design space to the surface of a cone, defined by the equality constraint $\\sqrt{x_1^2 + x_2^2} - x_3 = 0$. Before, we observed that the experimental proposals happened to be on the surface of the cone, but now they are constrained so that this must be the case.</p> <p>Remark: Due to missing sampling methods, the initial points provided to IPOPT don't satisfy the constraints. But this does not matter for the solution.</p>"},{"location":"basic_examples/#example-3-batch-constraints","title":"Example 3: Batch constraints\u00b6","text":"<p>Batch constraints can be used to create designs where each set of <code>multiplicity</code> subsequent experiments have the same value for a certain feature. This can be useful for setups where experiments are done in parallel and some parameters must be shared by experiments in the same parallel batch.</p> <p>In the following example we fix the value of the decision variable <code>x1</code> for each batch of 3 experiments.</p>"},{"location":"data_models_functionals/","title":"Data Models vs. Functional Components","text":"<p>Data models in BoFire hold static data of an optimization problem. These are input and output features as well as constraints making up the domain. They further include possible optimization objectives, acquisition functions, and kernels.</p> <p>All data models in <code>bofire.data_models</code>, are specified as pydantic models and inherit from <code>bofire.data_models.base.BaseModel</code>. These data models can be (de)serialized via <code>.dict()</code> and <code>.model_dump_json()</code> (provided by pydantic). A json schema of each data model can be obtained using <code>.schema()</code>.</p> <p>For surrogates and strategies, all functional parts are located in <code>bofire.surrogates</code> and <code>bofire.strategies</code>. These functionalities include the <code>ask</code> and <code>tell</code> as well as <code>fit</code> and <code>predict</code> methods. All class attributes (used by these method) are also removed from the data models. Each functional entity is initialized using the corresponding data model. As an example, consider the following data model of a <code>RandomStrategy</code>:</p> <pre><code>import bofire.data_models.domain.api as dm_domain\nimport bofire.data_models.features.api as dm_features\nimport bofire.data_models.strategies.api as dm_strategies\n\nin1 = dm_features.ContinuousInput(key=\"in1\", bounds=[0.0,1.0])\nin2 = dm_features.ContinuousInput(key=\"in2\", bounds=[0.0,2.0])\nin3 = dm_features.ContinuousInput(key=\"in3\", bounds=[0.0,3.0])\n\nout1 = dm_features.ContinuousOutput(key=\"out1\")\n\ninputs = dm_domain.Inputs(features=[in1, in2, in3])\noutputs = dm_domain.Outputs(features=[out1])\nconstraints = dm_domain.Constraints()\n\ndomain = dm_domain.Domain(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints,\n)\n\ndata_model = dm_strategies.RandomStrategy(domain=domain)\n</code></pre> <p>Such a data model can be (de)serialized as follows:</p> <p><pre><code>from pydantic import TypeAdapter\nfrom bofire.data_models.strategies.api import AnyStrategy\n\nserialized = data_model.model_dump_json()\n\ndata_model_ = TypeAdapter(AnyStrategy).validate_json(serialized)\n\nassert data_model_ == data_model\n</code></pre> The data model of a strategy contains its hyperparameters. Using this data model of a strategy, we can create an instance of a (functional) strategy:</p> <pre><code>import bofire.strategies.api as strategies\nstrategy = strategies.RandomStrategy(data_model=data_model)\n</code></pre> <p>As each strategy data model should be mapped to a specific (functional) strategy, we provide such a mapping:</p> <pre><code>strategy = strategies.map(data_model)\n</code></pre>"},{"location":"design_with_explicit_formula/","title":"Design with explicit Formula","text":"In\u00a0[\u00a0]: Copied! <pre>import bofire.strategies.api as strategies\nfrom bofire.data_models.api import Domain, Inputs\nfrom bofire.data_models.features.api import ContinuousInput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\nfrom bofire.utils.doe import get_confounding_matrix\n</pre> import bofire.strategies.api as strategies from bofire.data_models.api import Domain, Inputs from bofire.data_models.features.api import ContinuousInput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion from bofire.utils.doe import get_confounding_matrix In\u00a0[\u00a0]: Copied! <pre>input_features = Inputs(\n    features=[\n        ContinuousInput(key=\"a\", bounds=(0, 5)),\n        ContinuousInput(key=\"b\", bounds=(40, 800)),\n        ContinuousInput(key=\"c\", bounds=(80, 180)),\n        ContinuousInput(key=\"d\", bounds=(200, 800)),\n    ],\n)\ndomain = Domain(inputs=input_features)\n</pre> input_features = Inputs(     features=[         ContinuousInput(key=\"a\", bounds=(0, 5)),         ContinuousInput(key=\"b\", bounds=(40, 800)),         ContinuousInput(key=\"c\", bounds=(80, 180)),         ContinuousInput(key=\"d\", bounds=(200, 800)),     ], ) domain = Domain(inputs=input_features) In\u00a0[\u00a0]: Copied! <pre>model_type = \"a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d\"\nmodel_type\n</pre> model_type = \"a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d\" model_type Out[\u00a0]: <pre>'a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d'</pre> In\u00a0[\u00a0]: Copied! <pre>data_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=model_type),\n    ipopt_options={\"maxiter\": 100, \"disp\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ndesign = strategy.ask(17)\ndesign\n</pre> data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=model_type),     ipopt_options={\"maxiter\": 100, \"disp\": 0}, ) strategy = strategies.map(data_model=data_model) design = strategy.ask(17) design Out[\u00a0]: a b c d 0 5.000000e+00 40.000000 180.000002 199.999998 1 2.047832e+00 40.000000 180.000002 800.000008 2 5.000000e+00 40.000000 180.000002 800.000008 3 5.000000e+00 800.000008 79.999999 199.999998 4 5.000000e+00 800.000008 79.999999 800.000008 5 -9.973772e-09 40.000000 180.000002 199.999998 6 5.000000e+00 800.000008 180.000002 199.999998 7 -9.973772e-09 800.000008 180.000002 800.000008 8 -8.091201e-09 40.000000 79.999999 199.999998 9 5.000000e+00 40.000000 79.999999 199.999998 10 -9.981833e-09 40.000000 79.999999 800.000008 11 2.907832e+00 40.000000 79.999999 800.000008 12 -9.976118e-09 800.000008 180.000002 199.999998 13 -9.906825e-09 800.000008 79.999999 199.999998 14 -9.981833e-09 40.000000 79.999999 800.000008 15 -8.091050e-09 800.000008 79.999999 800.000008 16 5.000000e+00 800.000008 180.000002 800.000008 In\u00a0[\u00a0]: Copied! <pre>import matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nmatplotlib.rcParams[\"figure.dpi\"] = 120\n\nm = get_confounding_matrix(\n    domain.inputs,\n    design=design,\n    interactions=[2, 3],\n    powers=[2],\n)\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> import matplotlib import matplotlib.pyplot as plt import seaborn as sns   matplotlib.rcParams[\"figure.dpi\"] = 120  m = get_confounding_matrix(     domain.inputs,     design=design,     interactions=[2, 3],     powers=[2], )  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show()"},{"location":"design_with_explicit_formula/#design-with-explicit-formula","title":"Design with explicit Formula\u00b6","text":"<p>This tutorial notebook shows how to setup a D-optimal design with BoFire while providing an explicit formula and not just one of the four available keywords <code>linear</code>, <code>linear-and-interaction</code>, <code>linear-and-quadratic</code>, <code>fully-quadratic</code>.</p> <p>Make sure that <code>cyipopt</code>is installed. The recommend way is the installation via conda <code>conda install -c conda-forge cyipopt</code>.</p>"},{"location":"design_with_explicit_formula/#imports","title":"Imports\u00b6","text":""},{"location":"design_with_explicit_formula/#setup-of-the-problem","title":"Setup of the problem\u00b6","text":""},{"location":"design_with_explicit_formula/#definition-of-the-formula-for-which-the-optimal-points-should-be-found","title":"Definition of the formula for which the optimal points should be found\u00b6","text":""},{"location":"design_with_explicit_formula/#find-d-optimal-design","title":"Find D-optimal Design\u00b6","text":""},{"location":"design_with_explicit_formula/#analyze-confounding","title":"Analyze Confounding\u00b6","text":""},{"location":"examples/","title":"Examples","text":"<p>This is a collection of code examples to allow for an easy exploration of the functionalities that BoFire offers.</p>"},{"location":"examples/#doe","title":"DoE","text":"<ul> <li>creating designs for constrained design spaces</li> <li>optimizing designs with respect to various optimality criteria</li> <li>creating designs for a custom model</li> <li>creating designs with NChooseK constraints</li> <li>creating full and fractional factorial designs</li> </ul>"},{"location":"examples/#api-with-bofire","title":"API with BoFire","text":"<p>You can find an examples of how BoFire can be used in APIs in separate repositories:</p> <ul> <li>The Candidates API demonstrates an API that provides get new experimental candidates based on DoE or Bayesian optimization.</li> <li>The Types API is an API to check serialized data models. For instance, a JavaScript frontend that allows the user to define an optimization domain can check its validity explicitly.</li> </ul>"},{"location":"fractional_factorial/","title":"Full and Fractional Factorial Designs","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import CategoricalInput, ContinuousInput\nfrom bofire.data_models.strategies.api import FractionalFactorialStrategy\nfrom bofire.utils.doe import get_alias_structure, get_confounding_matrix, get_generator\n\n\ndef plot_design(design: pd.DataFrame):\n    # we do a plot with three subplots in one row in which the three degrees of freedom (temperature, time and ph) are plotted\n    _, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].scatter(design[\"temperature\"], design[\"time\"])\n    axs[0].set_xlabel(\"Temperature\")\n    axs[0].set_ylabel(\"Time\")\n    axs[1].scatter(design[\"temperature\"], design[\"ph\"])\n    axs[1].set_xlabel(\"Temperature\")\n    axs[1].set_ylabel(\"pH\")\n    axs[2].scatter(design[\"time\"], design[\"ph\"])\n    axs[2].set_xlabel(\"Time\")\n    axs[2].set_ylabel(\"pH\")\n    plt.show()\n</pre> import matplotlib.pyplot as plt import pandas as pd import seaborn as sns  import bofire.strategies.api as strategies from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import CategoricalInput, ContinuousInput from bofire.data_models.strategies.api import FractionalFactorialStrategy from bofire.utils.doe import get_alias_structure, get_confounding_matrix, get_generator   def plot_design(design: pd.DataFrame):     # we do a plot with three subplots in one row in which the three degrees of freedom (temperature, time and ph) are plotted     _, axs = plt.subplots(1, 3, figsize=(15, 5))     axs[0].scatter(design[\"temperature\"], design[\"time\"])     axs[0].set_xlabel(\"Temperature\")     axs[0].set_ylabel(\"Time\")     axs[1].scatter(design[\"temperature\"], design[\"ph\"])     axs[1].set_xlabel(\"Temperature\")     axs[1].set_ylabel(\"pH\")     axs[2].scatter(design[\"time\"], design[\"ph\"])     axs[2].set_xlabel(\"Time\")     axs[2].set_ylabel(\"pH\")     plt.show() <pre>/opt/homebrew/Caskroom/miniforge/base/envs/bofire-2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[\u00a0]: Copied! <pre>domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n    ],\n)\n</pre> domain = Domain(     inputs=[         ContinuousInput(key=\"temperature\", bounds=(20, 80)),         ContinuousInput(key=\"time\", bounds=(60, 120)),         ContinuousInput(key=\"ph\", bounds=(7, 13)),     ], ) In\u00a0[\u00a0]: Copied! <pre>strategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n</pre> strategy_data = FractionalFactorialStrategy(     domain=domain,     n_center=1,  # number of center points     n_repetitions=1,  # number of repetitions, we do only one round here ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design)  plot_design(design=design) ph temperature time 0 7.0 20.0 60.0 1 7.0 20.0 120.0 2 7.0 80.0 60.0 3 7.0 80.0 120.0 4 13.0 20.0 60.0 5 13.0 20.0 120.0 6 13.0 80.0 60.0 7 13.0 80.0 120.0 8 10.0 50.0 90.0 <p>The confounding structure is shown below, as expected for a full factorial design, no confound is present.</p> In\u00a0[\u00a0]: Copied! <pre>m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show() In\u00a0[\u00a0]: Copied! <pre>blocked_domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n        CategoricalInput(key=\"operator\", categories=[\"A\", \"B\", \"C\", \"D\"]),\n    ],\n)\n\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=blocked_domain,\n    n_center=1,  # number of center points per block\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    block_feature_key=\"operator\",\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n</pre> blocked_domain = Domain(     inputs=[         ContinuousInput(key=\"temperature\", bounds=(20, 80)),         ContinuousInput(key=\"time\", bounds=(60, 120)),         ContinuousInput(key=\"ph\", bounds=(7, 13)),         CategoricalInput(key=\"operator\", categories=[\"A\", \"B\", \"C\", \"D\"]),     ], )   strategy_data = FractionalFactorialStrategy(     domain=blocked_domain,     n_center=1,  # number of center points per block     n_repetitions=1,  # number of repetitions, we do only one round here     block_feature_key=\"operator\", ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design)  plot_design(design=design) ph temperature time operator 0 7.0 20.0 60.0 A 1 13.0 80.0 120.0 A 2 10.0 50.0 90.0 A 3 7.0 20.0 120.0 B 4 13.0 80.0 60.0 B 5 10.0 50.0 90.0 B 6 7.0 80.0 60.0 C 7 13.0 20.0 120.0 C 8 10.0 50.0 90.0 C 9 7.0 80.0 120.0 D 10 13.0 20.0 60.0 D 11 10.0 50.0 90.0 D <p>Here a fractional factorial design of the form $2^{3-1}$ is setup by specifying the number of generators (here 1). In comparison to the full factorial design with 9 candidates, it features only 5 experiments.</p> In\u00a0[\u00a0]: Copied! <pre>strategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    n_generators=1,  # number of generators, ie number of reducing factors\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n</pre> strategy_data = FractionalFactorialStrategy(     domain=domain,     n_center=1,  # number of center points     n_repetitions=1,  # number of repetitions, we do only one round here     n_generators=1,  # number of generators, ie number of reducing factors ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design) <p>The generator string is automatically generated by making use of the method <code>get_generator</code> and specifying the total number of factors (here 3) and the number of generators (here 1).</p> In\u00a0[\u00a0]: Copied! <pre>get_generator(n_factors=3, n_generators=1)\n</pre> get_generator(n_factors=3, n_generators=1) <p>As expected for a type III design the main effects are confounded with the two factor interactions:</p> In\u00a0[\u00a0]: Copied! <pre>m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show() <p>This can also be expressed by the so called alias structure that can be calculated as following:</p> In\u00a0[\u00a0]: Copied! <pre>get_alias_structure(\"a b ab\")\n</pre> get_alias_structure(\"a b ab\") <p>Here again a fractional factorial design of the form $2^{3-1}$ is setup by providing the complete generator string of the form <code>a b -ab</code> explicitly to the strategy.</p> In\u00a0[\u00a0]: Copied! <pre>strategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    generator=\"a b -ab\",  # the exact generator\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n</pre> strategy_data = FractionalFactorialStrategy(     domain=domain,     n_center=1,  # number of center points     n_repetitions=1,  # number of repetitions, we do only one round here     generator=\"a b -ab\",  # the exact generator ) strategy = strategies.map(strategy_data) design = strategy.ask() display(design) <p>The last two designs differ only in the last feature <code>time</code>, since the generator strings are different. In the first one it holds <code>time=ph x temperature</code> whereas in the second it holds <code>time=-ph x temperature</code>, which is also reflected in the confounding structure.</p> In\u00a0[\u00a0]: Copied! <pre>m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n</pre> m = get_confounding_matrix(domain.inputs, design=design, interactions=[2])  sns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\") plt.show()"},{"location":"fractional_factorial/#full-and-fractional-factorial-designs","title":"Full and Fractional Factorial Designs\u00b6","text":"<p>BoFire can be used to setup full (two level) and fractional factorial designs (https://en.wikipedia.org/wiki/Fractional_factorial_design). This tutorial notebook shows how.</p>"},{"location":"fractional_factorial/#imports-and-helper-functions","title":"Imports and helper functions\u00b6","text":""},{"location":"fractional_factorial/#setup-the-problem-domain","title":"Setup the problem domain\u00b6","text":"<p>The designs are generated for a simple three dimensional problem comprised of three continuous factors/features.</p>"},{"location":"fractional_factorial/#setup-a-full-factorial-design","title":"Setup a full factorial design\u00b6","text":"<p>Here we setup a full two-level factorial design including a center point and plot it.</p>"},{"location":"fractional_factorial/#setup-a-full-factorial-design-with-blocking","title":"Setup a full factorial design with blocking\u00b6","text":"<p>Here we setup a blocked full two-level factorial design including a center point and plot it.</p>"},{"location":"fractional_factorial/#setup-a-fractional-factorial-design","title":"Setup a fractional factorial design\u00b6","text":""},{"location":"getting_started/","title":"Getting started","text":"In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.features.api import (\n    CategoricalDescriptorInput,\n    CategoricalInput,\n    ContinuousInput,\n    DiscreteInput,\n)\n\n\nx1 = ContinuousInput(key=\"x1\", bounds=(0, 1))\nx2 = ContinuousInput(key=\"x2\", bounds=(0, 1))\nx3 = ContinuousInput(key=\"x3\", bounds=(0, 1))\nx4 = DiscreteInput(key=\"x4\", values=[1, 2, 5, 7.5])\nx5 = CategoricalInput(key=\"x5\", categories=[\"A\", \"B\", \"C\"], allowed=[True, True, False])\nx6 = CategoricalDescriptorInput(\n    key=\"x6\",\n    categories=[\"c1\", \"c2\", \"c3\"],\n    descriptors=[\"d1\", \"d2\"],\n    values=[[1, 2], [2, 5], [1, 7]],\n)\n</pre> from bofire.data_models.features.api import (     CategoricalDescriptorInput,     CategoricalInput,     ContinuousInput,     DiscreteInput, )   x1 = ContinuousInput(key=\"x1\", bounds=(0, 1)) x2 = ContinuousInput(key=\"x2\", bounds=(0, 1)) x3 = ContinuousInput(key=\"x3\", bounds=(0, 1)) x4 = DiscreteInput(key=\"x4\", values=[1, 2, 5, 7.5]) x5 = CategoricalInput(key=\"x5\", categories=[\"A\", \"B\", \"C\"], allowed=[True, True, False]) x6 = CategoricalDescriptorInput(     key=\"x6\",     categories=[\"c1\", \"c2\", \"c3\"],     descriptors=[\"d1\", \"d2\"],     values=[[1, 2], [2, 5], [1, 7]], ) <p>As output features, currently only continuous output features are supported. Each output feature should have an objective, which can be a minimize or maximize objective. Furthermore, we can define weights between 0 and 1 in case the objectives should not be weighted equally.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective\n\n\nobjective1 = MaximizeObjective(\n    w=1.0,\n    bounds=[0.0, 1.0],\n)\ny1 = ContinuousOutput(key=\"y1\", objective=objective1)\n\nobjective2 = MinimizeObjective(w=1.0)\ny2 = ContinuousOutput(key=\"y2\", objective=objective2)\n</pre> from bofire.data_models.features.api import ContinuousOutput from bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective   objective1 = MaximizeObjective(     w=1.0,     bounds=[0.0, 1.0], ) y1 = ContinuousOutput(key=\"y1\", objective=objective1)  objective2 = MinimizeObjective(w=1.0) y2 = ContinuousOutput(key=\"y2\", objective=objective2) <p>In- and output features are collected in respective feature lists.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Inputs, Outputs\n\n\ninput_features = Inputs(features=[x1, x2, x3, x4, x5, x6])\noutput_features = Outputs(features=[y1, y2])\n</pre> from bofire.data_models.domain.api import Inputs, Outputs   input_features = Inputs(features=[x1, x2, x3, x4, x5, x6]) output_features = Outputs(features=[y1, y2]) <p>A summary of the constraints can be obtained by the method <code>get_reps_df</code>:</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get_reps_df()\n</pre> input_features.get_reps_df() Out[\u00a0]: Type Description x1 ContinuousInput [0.0,1.0] x2 ContinuousInput [0.0,1.0] x3 ContinuousInput [0.0,1.0] x4 DiscreteInput type='DiscreteInput' key='x4' unit=None values... x6 CategoricalDescriptorInput 3 categories x5 CategoricalInput 3 categories In\u00a0[\u00a0]: Copied! <pre>output_features.get_reps_df()\n</pre> output_features.get_reps_df() Out[\u00a0]: Type Description y1 ContinuousOutput ContinuousOutputFeature y2 ContinuousOutput ContinuousOutputFeature <p>Individual features can be retrieved by name.</p> In\u00a0[\u00a0]: Copied! <pre>x5 = input_features.get_by_key(\"x5\")\nx5\n</pre> x5 = input_features.get_by_key(\"x5\") x5 Out[\u00a0]: <pre>CategoricalInput(type='CategoricalInput', key='x5', categories=['A', 'B', 'C'], allowed=[True, True, False])</pre> <p>This is also possible with list of feature names.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get_by_keys([\"x5\", \"x2\"])\n</pre> input_features.get_by_keys([\"x5\", \"x2\"]) Out[\u00a0]: <pre>Inputs(type='Inputs', features=[ContinuousInput(type='ContinuousInput', key='x2', unit=None, bounds=[0.0, 1.0], local_relative_bounds=None, stepsize=None), CategoricalInput(type='CategoricalInput', key='x5', categories=['A', 'B', 'C'], allowed=[True, True, False])])</pre> <p>Features of a specific type can be returned by the <code>get</code> method, by default it returns all features that are an instance of the provided class.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get(CategoricalInput)\n</pre> input_features.get(CategoricalInput) Out[\u00a0]: <pre>Inputs(type='Inputs', features=[CategoricalDescriptorInput(type='CategoricalDescriptorInput', key='x6', categories=['c1', 'c2', 'c3'], allowed=[True, True, True], descriptors=['d1', 'd2'], values=[[1.0, 2.0], [2.0, 5.0], [1.0, 7.0]]), CategoricalInput(type='CategoricalInput', key='x5', categories=['A', 'B', 'C'], allowed=[True, True, False])])</pre> <p>By using the <code>exact</code> argument one can force it to only return feature of the exact same class.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get(CategoricalInput, exact=True)\n</pre> input_features.get(CategoricalInput, exact=True) Out[\u00a0]: <pre>Inputs(type='Inputs', features=[CategoricalInput(type='CategoricalInput', key='x5', categories=['A', 'B', 'C'], allowed=[True, True, False])])</pre> <p>The <code>get_keys</code> method follows the same logic as the <code>get</code> method but returns just the keys of the features instead of the features itself.</p> In\u00a0[\u00a0]: Copied! <pre>input_features.get_keys(CategoricalInput)\n</pre> input_features.get_keys(CategoricalInput) Out[\u00a0]: <pre>['x6', 'x5']</pre> <p>The input feature container further provides methods to return a feature container with only all fixed or all free features.</p> In\u00a0[\u00a0]: Copied! <pre>free_inputs = input_features.get_free()\nfixed_inputs = input_features.get_fixed()\n</pre> free_inputs = input_features.get_free() fixed_inputs = input_features.get_fixed() <p>One can uniformly sample from individual input features.</p> In\u00a0[\u00a0]: Copied! <pre>x5.sample(2)\n</pre> x5.sample(2) Out[\u00a0]: <pre>0    B\n1    B\nName: x5, dtype: object</pre> <p>Or directly from input feature containers, uniform, sobol and LHS sampling is possible. A default, uniform sampling is used.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.enum import SamplingMethodEnum\n\n\nX = input_features.sample(n=10, method=SamplingMethodEnum.LHS)\n\nX\n</pre> from bofire.data_models.enum import SamplingMethodEnum   X = input_features.sample(n=10, method=SamplingMethodEnum.LHS)  X Out[\u00a0]: x1 x2 x3 x4 x6 x5 0 0.625975 0.752207 0.499393 1.0 c2 A 1 0.150882 0.647464 0.399124 5.0 c3 B 2 0.359207 0.278614 0.229591 2.0 c1 B 3 0.030239 0.901921 0.120284 7.5 c3 A 4 0.808204 0.444743 0.545444 2.0 c1 A 5 0.793128 0.322496 0.840092 5.0 c2 A 6 0.505801 0.898125 0.071639 7.5 c1 B 7 0.418812 0.183636 0.928908 2.0 c3 B 8 0.940291 0.528987 0.742469 1.0 c3 A 9 0.291382 0.073107 0.646837 7.5 c1 B In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n)\n\n\n# A mixture: x1 + x2 + x3 = 1\nconstr1 = LinearEqualityConstraint(\n    features=[\"x1\", \"x2\", \"x3\"],\n    coefficients=[1, 1, 1],\n    rhs=1,\n)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = LinearInequalityConstraint(\n    features=[\"x1\", \"x3\"],\n    coefficients=[1, 2],\n    rhs=0.8,\n)\n</pre> from bofire.data_models.constraints.api import (     LinearEqualityConstraint,     LinearInequalityConstraint, )   # A mixture: x1 + x2 + x3 = 1 constr1 = LinearEqualityConstraint(     features=[\"x1\", \"x2\", \"x3\"],     coefficients=[1, 1, 1],     rhs=1, )  # x1 + 2 * x3 &lt; 0.8 constr2 = LinearInequalityConstraint(     features=[\"x1\", \"x3\"],     coefficients=[1, 2],     rhs=0.8, ) <p>Linear constraints can only operate on <code>ContinuousInput</code> features.</p> <p><code>NonlinearEqualityConstraint</code> and <code>NonlinearInequalityConstraint</code> take any expression that can be evaluated by pandas.eval, including mathematical operators such as <code>sin</code>, <code>exp</code>, <code>log10</code> or exponentiation. So far, they cannot be used in any optimizations.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.constraints.api import NonlinearEqualityConstraint\n\n\n# The unit circle: x1**2 + x2**2 = 1\nconst3 = NonlinearEqualityConstraint(expression=\"x1**2 + x2**2 - 1\")\nconst3\n</pre> from bofire.data_models.constraints.api import NonlinearEqualityConstraint   # The unit circle: x1**2 + x2**2 = 1 const3 = NonlinearEqualityConstraint(expression=\"x1**2 + x2**2 - 1\") const3 Out[\u00a0]: <pre>NonlinearEqualityConstraint(type='NonlinearEqualityConstraint', expression='x1**2 + x2**2 - 1', features=None, jacobian_expression=None)</pre> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.constraints.api import NChooseKConstraint\n\n\n# Only 2 or 3 out of 3 parameters can be greater than zero\nconstr5 = NChooseKConstraint(\n    features=[\"x1\", \"x2\", \"x3\"],\n    min_count=2,\n    max_count=3,\n    none_also_valid=True,\n)\nconstr5\n</pre> from bofire.data_models.constraints.api import NChooseKConstraint   # Only 2 or 3 out of 3 parameters can be greater than zero constr5 = NChooseKConstraint(     features=[\"x1\", \"x2\", \"x3\"],     min_count=2,     max_count=3,     none_also_valid=True, ) constr5 Out[\u00a0]: <pre>NChooseKConstraint(type='NChooseKConstraint', features=['x1', 'x2', 'x3'], min_count=2, max_count=3, none_also_valid=True)</pre> <p>Note that we have to set a boolean, if None is also a valid selection, e.g. if we want to have 2 or 3 or none of the ingredients in our recipe.</p> <p>Similar to the features, constraints can be grouped in a container which acts as the union constraints.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Constraints\n\n\nconstraints = Constraints(constraints=[constr1, constr2])\n</pre> from bofire.data_models.domain.api import Constraints   constraints = Constraints(constraints=[constr1, constr2]) <p>A summary of the constraints can be obtained by the method <code>get_reps_df</code>:</p> In\u00a0[\u00a0]: Copied! <pre>constraints.get_reps_df()\n</pre> constraints.get_reps_df() Out[\u00a0]: Type Description 0 LinearEqualityConstraint type='LinearEqualityConstraint' features=['x1'... 1 LinearInequalityConstraint type='LinearInequalityConstraint' features=['x... <p>We can check whether a point satisfies individual constraints or the list of constraints.</p> In\u00a0[\u00a0]: Copied! <pre>constr2.is_fulfilled(X).values\n</pre> constr2.is_fulfilled(X).values Out[\u00a0]: <pre>array([False, False, False,  True, False, False,  True, False, False,\n       False])</pre> <p>Output constraints can be setup via sigmoid-shaped objectives passed as argument to the respective feature, which can then also be plotted.</p> In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.objectives.api import MinimizeSigmoidObjective\nfrom bofire.plot.api import plot_objective_plotly\n\n\noutput_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5)\ny3 = ContinuousOutput(key=\"y3\", objective=output_constraint)\n\noutput_features = Outputs(features=[y1, y2, y3])\n\nfig = plot_objective_plotly(feature=y3, lower=0, upper=1)\n\nfig.show()\n</pre> from bofire.data_models.objectives.api import MinimizeSigmoidObjective from bofire.plot.api import plot_objective_plotly   output_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5) y3 = ContinuousOutput(key=\"y3\", objective=output_constraint)  output_features = Outputs(features=[y1, y2, y3])  fig = plot_objective_plotly(feature=y3, lower=0, upper=1)  fig.show() In\u00a0[\u00a0]: Copied! <pre>from bofire.data_models.domain.api import Domain\n\n\ndomain = Domain(inputs=input_features, outputs=output_features, constraints=constraints)\n</pre> from bofire.data_models.domain.api import Domain   domain = Domain(inputs=input_features, outputs=output_features, constraints=constraints) <p>In addition one can instantiate the domain also just from lists.</p> In\u00a0[\u00a0]: Copied! <pre>domain_single_objective = Domain.from_lists(\n    inputs=[x1, x2, x3, x4, x5, x6],\n    outputs=[y1],\n    constraints=[],\n)\n</pre> domain_single_objective = Domain.from_lists(     inputs=[x1, x2, x3, x4, x5, x6],     outputs=[y1],     constraints=[], ) In\u00a0[\u00a0]: Copied! <pre>import bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy\n\n\nstrategy_data_model = RandomStrategy(domain=domain)\n\nrandom_strategy = strategies.map(strategy_data_model)\nrandom_candidates = random_strategy.ask(2)\n\nrandom_candidates\n</pre> import bofire.strategies.api as strategies from bofire.data_models.strategies.api import RandomStrategy   strategy_data_model = RandomStrategy(domain=domain)  random_strategy = strategies.map(strategy_data_model) random_candidates = random_strategy.ask(2)  random_candidates Out[\u00a0]: x1 x2 x3 x4 x6 x5 0 0.175363 0.783373 0.041264 7.5 c3 B 1 0.458070 0.402594 0.139336 5.0 c1 A In\u00a0[\u00a0]: Copied! <pre>from bofire.benchmarks.single import Himmelblau\n\n\nbenchmark = Himmelblau()\n\n(benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df()\n</pre> from bofire.benchmarks.single import Himmelblau   benchmark = Himmelblau()  (benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df() Out[\u00a0]: Type Description x_1 ContinuousInput [-6.0,6.0] x_2 ContinuousInput [-6.0,6.0] y ContinuousOutput ContinuousOutputFeature <p>Generating some initial data works as follows:</p> In\u00a0[\u00a0]: Copied! <pre>samples = benchmark.domain.inputs.sample(10)\n\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments\n</pre> samples = benchmark.domain.inputs.sample(10)  experiments = benchmark.f(samples, return_complete=True)  experiments Out[\u00a0]: x_1 x_2 y valid_y 0 3.908092 2.941075 82.937019 1 1 1.047758 0.752722 112.718250 1 2 -1.096835 3.150400 47.518938 1 3 3.518885 -5.692627 855.225763 1 4 5.902686 -3.394139 526.738914 1 5 2.636934 3.096377 28.198141 1 6 0.950476 2.825733 56.610616 1 7 1.149791 4.091170 149.749360 1 8 -3.567315 -4.367514 79.362666 1 9 0.417997 5.683543 687.989967 1 <p>Let's setup the SOBO strategy and ask for a candidate. First we need a serializable data model that contains the hyperparameters.</p> In\u00a0[\u00a0]: Copied! <pre>from pprint import pprint\n\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM\n\n\nsobo_strategy_data_model = SoboStrategyDM(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# print information about hyperparameters\nprint(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function)\nprint()\nprint(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type)\nprint()\nprint(\"Surrogate's kernel:\")\npprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump())\n</pre> from pprint import pprint  from bofire.data_models.acquisition_functions.api import qLogNEI from bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM   sobo_strategy_data_model = SoboStrategyDM(     domain=benchmark.domain,     acquisition_function=qLogNEI(), )  # print information about hyperparameters print(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function) print() print(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type) print() print(\"Surrogate's kernel:\") pprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump()) <pre>Acquisition function: type='qLogNEI' prune_baseline=True n_mc_samples=512\n\nSurrogate type: SingleTaskGPSurrogate\n\nSurrogate's kernel:\n{'ard': True,\n 'features': None,\n 'lengthscale_prior': {'loc': 1.4142135623730951,\n                       'loc_scaling': 0.5,\n                       'scale': 1.7320508075688772,\n                       'scale_scaling': 0.0,\n                       'type': 'DimensionalityScaledLogNormalPrior'},\n 'type': 'RBFKernel'}\n</pre> <p>The actual strategy can then be created via the mapper function.</p> In\u00a0[\u00a0]: Copied! <pre>sobo_strategy = strategies.map(sobo_strategy_data_model)\nsobo_strategy.tell(experiments=experiments)\nsobo_strategy.ask(candidate_count=1)\n</pre> sobo_strategy = strategies.map(sobo_strategy_data_model) sobo_strategy.tell(experiments=experiments) sobo_strategy.ask(candidate_count=1) Out[\u00a0]: x_1 x_2 y_pred y_sd y_des 0 -6.0 -3.575202 39.885892 184.035499 -39.885892 <p>An alternative way is calling the strategy's constructor directly.</p> In\u00a0[\u00a0]: Copied! <pre>sobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model)\n</pre> sobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model) <p>The latter way is helpful to keep type information.</p> In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1])\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n</pre> import numpy as np  from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion   domain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1]) data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"fully-quadratic\"), ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(candidate_count=12) np.round(candidates, 3) <pre>\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit http://projects.coin-or.org/Ipopt\n******************************************************************************\n\n</pre> Out[\u00a0]: x1 x2 x3 0 1.0 0.0 0.0 1 0.5 0.5 0.0 2 0.0 0.0 1.0 3 0.5 0.0 0.5 4 0.0 1.0 0.0 5 0.0 0.5 0.5 6 0.5 0.0 0.5 7 0.5 0.5 0.0 8 1.0 0.0 0.0 9 0.5 0.0 0.5 10 0.0 0.5 0.5 11 0.5 0.5 0.0 <p>The resulting design looks like this:</p> In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"fully-quadratic model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n)\n</pre> import matplotlib.pyplot as plt   fig = plt.figure(figsize=((10, 10))) ax = fig.add_subplot(111, projection=\"3d\") ax.view_init(45, 45) ax.set_title(\"fully-quadratic model\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\") plt.rcParams[\"figure.figsize\"] = (10, 8)  # plot feasible polytope ax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)  # plot D-optimal solutions ax.scatter(     xs=candidates[\"x1\"],     ys=candidates[\"x2\"],     zs=candidates[\"x3\"],     marker=\"o\",     s=40,     color=\"orange\", ) Out[\u00a0]: <pre>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7f8c8d637910&gt;</pre>"},{"location":"getting_started/#getting-started","title":"Getting started\u00b6","text":"<p>In the following it is showed how to setup optimization problems in BoFire and how to use strategies to solve them.</p>"},{"location":"getting_started/#setting-up-the-optimization-problem","title":"Setting up the optimization problem\u00b6","text":"<p>In BoFire, an optimization problem is defined by defining a domain containing input and output features as well as constraints (optional).</p>"},{"location":"getting_started/#features","title":"Features\u00b6","text":"<p>Input features can be continuous, discrete, categorical, or categorical with descriptors:</p>"},{"location":"getting_started/#constraints","title":"Constraints\u00b6","text":"<p>The search space can be further defined by constraints on the input features. BoFire supports linear equality and inequality constraints, as well as non-linear equality and inequality constraints.</p>"},{"location":"getting_started/#linear-constraints","title":"Linear constraints\u00b6","text":"<p><code>LinearEqualityConstraint</code> and <code>LinearInequalityConstraint</code> are expressions of the form $\\sum_i a_i x_i = b$ or $\\leq b$ for equality and inequality constraints respectively. They take a list of names of the input features they are operating on, a list of left-hand-side coefficients $a_i$ and a right-hand-side constant $b$.</p>"},{"location":"getting_started/#nonlinear-constraints","title":"Nonlinear constraints\u00b6","text":""},{"location":"getting_started/#combinatorial-constraint","title":"Combinatorial constraint\u00b6","text":"<p>Use <code>NChooseKConstraint</code> to express that we only want to have $k$ out of the $n$ parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe.</p>"},{"location":"getting_started/#the-domain","title":"The domain\u00b6","text":"<p>The domain holds then all information about an optimization problem and can be understood as a search space definition.</p>"},{"location":"getting_started/#optimization","title":"Optimization\u00b6","text":"<p>To solve the optimization problem, we further need a solving strategy. BoFire supports strategies without a prediction model such as a random strategy and predictive strategies which are based on a prediction model.</p> <p>All strategies contain an <code>ask</code> method returning a defined number of candidate experiments.</p>"},{"location":"getting_started/#random-strategy","title":"Random Strategy\u00b6","text":""},{"location":"getting_started/#single-objective-bayesian-optimization-strategy","title":"Single objective Bayesian Optimization strategy\u00b6","text":"<p>Since a predictive strategy includes a prediction model, we need to generate some historical data, which we can afterwards pass as training data to the strategy via the tell method.</p> <p>For didactic purposes we just choose here from one of our benchmark methods.</p>"},{"location":"getting_started/#design-of-experiments","title":"Design of Experiments\u00b6","text":"<p>As a simple example for the DoE functionalities we consider the task of finding a D-optimal design for a fully-quadratic model with three design variables with bounds (0,1) and a mixture constraint.</p> <p>We define the design space including the constraint as a domain. Then we pass it to the optimization routine and specify the model. If the user does not indicate a number of experiments it will be chosen automatically based on the number of model terms.</p>"},{"location":"install/","title":"Installation Guide","text":""},{"location":"install/#installation-from-python-package-index-pypi","title":"Installation from Python Package Index (PyPI)","text":"<p>BoFire can be installed to your Python environment by using <code>pip</code>. It can be done by executing</p> <pre><code>pip install bofire\n</code></pre> <p>Tip</p> <p>The command from above will install a minimal BoFire version, consisting only of the data models. To install BoFire's including its core optimization features, execute: <pre><code>pip install 'bofire[optimization]'\n</code></pre></p>"},{"location":"install/#additional-optional-dependencies","title":"Additional optional dependencies","text":"<p>In BoFire, there are several optional dependencies that can be selected during installation via pip, like</p> <pre><code>pip install 'bofire[optimization, cheminfo] # will install bofire with additional dependencies `optimization` and `cheminfo`\n</code></pre> <p>To get the most our of BoFire, it is recommended to install at least <pre><code>pip install 'bofire[optimization]'\n</code></pre></p> <p>The available dependencies are:</p> <ul> <li><code>optimization</code>: Core Bayesian optimization features.</li> <li><code>cheminfo</code>: Cheminformatics utilities.</li> <li><code>entmoot</code>: Entmoot functionality.</li> <li><code>xgb</code>: XGboost surrogates.</li> <li><code>tests</code>: Required for running the test suite.</li> <li><code>docs</code>: Required for building the documentation.</li> <li><code>tutorials</code>: Required for running the tutorials.</li> <li><code>all</code>: Install all possible options (except DoE)</li> </ul> <p>Warning</p> <p>BoFire has the functionalities for creating D, E, A, G, K and I-optimal experimental designs via the <code>DoEStrategy</code>. This feature depends on cyipopt which is a python interface to <code>ipopt</code>. Unfortunately, it is not possible to install <code>cyipopt</code> including <code>ipopt</code> via pip. A solution is to install <code>cyipopt</code> and its dependencies via conda:</p> <pre><code>conda install -c conda-forge cyipopt\n</code></pre> <p>We are working on a solution that makes BoFire's model based DoE functionalities also accessible to users which do not have <code>cyipopt</code> available.</p>"},{"location":"install/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to BoFire, it is recommended to install the repository in editable mode (<code>-e</code>).</p> <p>After cloning the repository via <pre><code>git clone https://github.com/experimental-design/bofire.git\n</code></pre> and navigating to the repositories root folder (<code>cd bofire</code>), you can proceed with <pre><code>pip install -e \".[optimization, tests]\" # include optional dependencies as you wish\n</code></pre></p>"},{"location":"nchoosek_constraint/","title":"Nchoosek constraint","text":"In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NChooseKConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(8)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[f\"x{i+1}\" for i in range(8)],\n            coefficients=[1, 1, 1, 1, 1, 1, 1, 1],\n            rhs=1,\n        ),\n        NChooseKConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            min_count=0,\n            max_count=1,\n            none_also_valid=True,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=0.7,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x7\", \"x8\"],\n            coefficients=[-1, -1],\n            rhs=-0.1,\n        ),\n        LinearInequalityConstraint(features=[\"x7\", \"x8\"], coefficients=[1, 1], rhs=0.9),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n    ipopt_options={\"maxiter\": 500},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n</pre> import numpy as np  import bofire.strategies.api as strategies from bofire.data_models.constraints.api import (     LinearEqualityConstraint,     LinearInequalityConstraint,     NChooseKConstraint, ) from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import ContinuousInput, ContinuousOutput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import DOptimalityCriterion   domain = Domain(     inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(8)],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         LinearEqualityConstraint(             features=[f\"x{i+1}\" for i in range(8)],             coefficients=[1, 1, 1, 1, 1, 1, 1, 1],             rhs=1,         ),         NChooseKConstraint(             features=[\"x1\", \"x2\", \"x3\"],             min_count=0,             max_count=1,             none_also_valid=True,         ),         LinearInequalityConstraint(             features=[\"x1\", \"x2\", \"x3\"],             coefficients=[1, 1, 1],             rhs=0.7,         ),         LinearInequalityConstraint(             features=[\"x7\", \"x8\"],             coefficients=[-1, -1],             rhs=-0.1,         ),         LinearInequalityConstraint(features=[\"x7\", \"x8\"], coefficients=[1, 1], rhs=0.9),     ], )  data_model = DoEStrategy(     domain=domain,     criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),     ipopt_options={\"maxiter\": 500}, ) strategy = strategies.map(data_model=data_model) candidates = strategy.ask(candidate_count=12) np.round(candidates, 3) <pre>/home/linznedd/miniforge3/envs/bofire/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <pre>\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n</pre> Out[\u00a0]: x1 x2 x3 x4 x5 x6 x7 x8 0 -0.0 -0.0 -0.0 0.9 -0.0 -0.0 -0.0 0.1 1 -0.0 -0.0 -0.0 0.1 -0.0 -0.0 -0.0 0.9 2 -0.0 0.7 -0.0 -0.0 -0.0 -0.0 0.3 -0.0 3 -0.0 -0.0 -0.0 -0.0 -0.0 0.1 -0.0 0.9 4 -0.0 -0.0 -0.0 -0.0 -0.0 0.9 0.1 -0.0 5 -0.0 -0.0 0.7 -0.0 -0.0 -0.0 -0.0 0.3 6 -0.0 -0.0 -0.0 0.9 -0.0 -0.0 0.1 -0.0 7 -0.0 -0.0 -0.0 -0.0 -0.0 0.9 -0.0 0.1 8 0.7 -0.0 -0.0 -0.0 -0.0 -0.0 0.3 -0.0 9 -0.0 -0.0 -0.0 -0.0 0.1 -0.0 0.9 -0.0 10 -0.0 -0.0 -0.0 -0.0 0.9 -0.0 -0.0 0.1 11 -0.0 -0.0 -0.0 -0.0 -0.0 0.9 -0.0 0.1"},{"location":"nchoosek_constraint/#design-with-nchoosek-constraint","title":"Design with NChooseK constraint\u00b6","text":"<p>The doe subpackage also supports problems with NChooseK constraints. Since IPOPT has problems finding feasible solutions using the gradient of the NChooseK constraint violation, a closely related (but stricter) constraint that suffices to fulfill the NChooseK constraint is imposed onto the problem: For each experiment $j$ N-K decision variables $x_{i_1,j},...,x_{i_{N-K,j}}$ from the NChooseK constraints' names attribute are picked that are forced to be zero. This is done by setting the upper and lower bounds of the picked variables are set to 0 in the corresponding experiments. This causes IPOPT to treat them as \"fixed variables\" (i.e. it will not optimize for them) and will always stick to the only feasible value (which is 0 here). However, this constraint is stricter than the original NChooseK constraint. In combination with other constraints on the same decision variables this can result in a situation where the constraints cannot be fulfilled even though the original constraints would allow for a solution. For example consider a problem with four decision variables $x_1, x_2, x_3, x_4$, an NChooseK constraint on the first four variable that restricts the number of nonzero variables to two. Additionally, we have a linear constraint $$ x_3 + x_4 \\geq 0.1 $$ We can easily find points that fulfill both constraints (e.g. $(0,0,0,0.1)$). Now consider the stricter, linear constraint from above. Eventually, it will happen that $x_3$ and $x_4$ are chosen to be zero for one experiment. For this experiment it is impossible to fulfill the linear constraint $x_3 + x_4 \\geq 0.1$ since $x_3 = x_4 = 0$.</p> <p>Therefore one has to be very careful when imposing linear constraints upon decision variables that already show up in an NChooseK constraint.</p> <p>For practical reasons it necessary that two NChooseK constraints of the same problem must not share any variables.</p> <p>You can find an example for a problem with NChooseK constraints and additional linear constraints imposed on the same variables.</p>"},{"location":"optimality_criteria/","title":"Optimality criteria","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import (\n    AOptimalityCriterion,\n    DOptimalityCriterion,\n    EOptimalityCriterion,\n    IOptimalityCriterion,\n    KOptimalityCriterion,\n    SpaceFillingCriterion,\n)\nfrom bofire.strategies.doe.objective import get_objective_function\n</pre> import matplotlib.pyplot as plt  import bofire.strategies.api as strategies from bofire.data_models.constraints.api import LinearEqualityConstraint from bofire.data_models.domain.api import Domain from bofire.data_models.features.api import ContinuousInput, ContinuousOutput from bofire.data_models.strategies.api import DoEStrategy from bofire.data_models.strategies.doe import (     AOptimalityCriterion,     DOptimalityCriterion,     EOptimalityCriterion,     IOptimalityCriterion,     KOptimalityCriterion,     SpaceFillingCriterion, ) from bofire.strategies.doe.objective import get_objective_function <pre>/home/linznedd/miniforge3/envs/bofire/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> In\u00a0[\u00a0]: Copied! <pre># Optimal designs for a quadratic model on the unit square\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(2)],\n    outputs=[ContinuousOutput(key=\"y\")],\n)\nmodel_type = \"fully-quadratic\"\nn_experiments = 13\n\ndesigns = {}\nfor crit in [\n    DOptimalityCriterion,\n    AOptimalityCriterion,\n    KOptimalityCriterion,\n    EOptimalityCriterion,\n    IOptimalityCriterion,\n]:\n    criterion = crit(formula=model_type)\n    data_model = DoEStrategy(\n        domain=domain,\n        criterion=criterion,\n        ipopt_options={\"maxiter\": 300},\n    )\n    strategy = strategies.map(data_model=data_model)\n    design = strategy.ask(candidate_count=n_experiments)\n    obj_value = get_objective_function(\n        criterion=criterion, domain=domain, n_experiments=n_experiments\n    ).evaluate(design.to_numpy().flatten())\n    designs[obj_value] = design.to_numpy()\n\nfig = plt.figure(figsize=((8, 8)))\nax = fig.add_subplot(111)\nax.set_title(\"Designs with different optimality criteria\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nfor obj, X in designs.items():\n    ax.scatter(X[:, 0], X[:, 1], s=40, label=obj)\nax.grid(alpha=0.3)\nax.legend();\n</pre> # Optimal designs for a quadratic model on the unit square domain = Domain(     inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(2)],     outputs=[ContinuousOutput(key=\"y\")], ) model_type = \"fully-quadratic\" n_experiments = 13  designs = {} for crit in [     DOptimalityCriterion,     AOptimalityCriterion,     KOptimalityCriterion,     EOptimalityCriterion,     IOptimalityCriterion, ]:     criterion = crit(formula=model_type)     data_model = DoEStrategy(         domain=domain,         criterion=criterion,         ipopt_options={\"maxiter\": 300},     )     strategy = strategies.map(data_model=data_model)     design = strategy.ask(candidate_count=n_experiments)     obj_value = get_objective_function(         criterion=criterion, domain=domain, n_experiments=n_experiments     ).evaluate(design.to_numpy().flatten())     designs[obj_value] = design.to_numpy()  fig = plt.figure(figsize=((8, 8))) ax = fig.add_subplot(111) ax.set_title(\"Designs with different optimality criteria\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") for obj, X in designs.items():     ax.scatter(X[:, 0], X[:, 1], s=40, label=obj) ax.grid(alpha=0.3) ax.legend(); <pre>\n******************************************************************************\nThis program contains Ipopt, a library for large-scale nonlinear optimization.\n Ipopt is released as open source code under the Eclipse Public License (EPL).\n         For more information visit https://github.com/coin-or/Ipopt\n******************************************************************************\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Space filling design on the unit 2-simplex\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(3)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain, criterion=SpaceFillingCriterion(), ipopt_options={\"maxiter\": 500}\n)\nstrategy = strategies.map(data_model=data_model)\nX = strategy.ask(candidate_count=40).to_numpy()\n\nfig = plt.figure(figsize=((10, 8)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 20)\nax.set_title(\"Space filling design\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\n\n# plot feasible polytope\nax.plot(xs=[0, 0, 1, 0], ys=[0, 1, 0, 0], zs=[1, 0, 0, 1], linewidth=2)\n\n# plot design points\nax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], s=40)\n</pre> # Space filling design on the unit 2-simplex domain = Domain(     inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(3)],     outputs=[ContinuousOutput(key=\"y\")],     constraints=[         LinearEqualityConstraint(             features=[\"x1\", \"x2\", \"x3\"],             coefficients=[1, 1, 1],             rhs=1,         ),     ], ) data_model = DoEStrategy(     domain=domain, criterion=SpaceFillingCriterion(), ipopt_options={\"maxiter\": 500} ) strategy = strategies.map(data_model=data_model) X = strategy.ask(candidate_count=40).to_numpy()  fig = plt.figure(figsize=((10, 8))) ax = fig.add_subplot(111, projection=\"3d\") ax.view_init(45, 20) ax.set_title(\"Space filling design\") ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.set_zlabel(\"$x_3$\")  # plot feasible polytope ax.plot(xs=[0, 0, 1, 0], ys=[0, 1, 0, 0], zs=[1, 0, 0, 1], linewidth=2)  # plot design points ax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], s=40) Out[\u00a0]: <pre>&lt;mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7cf598509510&gt;</pre>"},{"location":"optimality_criteria/#designs-for-different-optimality-criteria","title":"Designs for different optimality criteria\u00b6","text":""},{"location":"optimality_criteria/#space-filling-design","title":"Space filling design\u00b6","text":""},{"location":"ref-constraints/","title":"Domain","text":""},{"location":"ref-constraints/#bofire.data_models.domain.constraints.Constraints","title":"<code>Constraints</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>Generic[C]</code></p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>class Constraints(BaseModel, Generic[C]):\n    type: Literal[\"Constraints\"] = \"Constraints\"\n    constraints: Sequence[C] = Field(default_factory=list)\n\n    def __iter__(self) -&gt; Iterator[C]:\n        return iter(self.constraints)\n\n    def __len__(self):\n        return len(self.constraints)\n\n    def __getitem__(self, i) -&gt; C:\n        return self.constraints[i]\n\n    def __add__(\n        self,\n        other: Union[Sequence[CIncludes], \"Constraints[CIncludes]\"],\n    ) -&gt; \"Constraints[Union[C, CIncludes]]\":\n        if isinstance(other, collections.abc.Sequence):\n            other_constraints = other\n        else:\n            other_constraints = other.constraints\n        constraints = list(chain(self.constraints, other_constraints))\n        return Constraints(constraints=constraints)\n\n    def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Numerically evaluate all constraints\n\n        Args:\n            experiments (pd.DataFrame): data to evaluate the constraint on\n\n        Returns:\n            pd.DataFrame: Constraint evaluation for each of the constraints\n\n        \"\"\"\n        return pd.concat([c(experiments) for c in self.constraints], axis=1)\n\n    def jacobian(self, experiments: pd.DataFrame) -&gt; list:\n        \"\"\"Numerically evaluate the jacobians of all constraints\n\n        Args:\n            experiments (pd.DataFrame): data to evaluate the constraint jacobians on\n\n        Returns:\n            list: A list containing the jacobians as pd.DataFrames\n\n        \"\"\"\n        return [c.jacobian(experiments) for c in self.constraints]\n\n    def is_fulfilled(self, experiments: pd.DataFrame, tol: float = 1e-6) -&gt; pd.Series:\n        \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with data, the constraint validity should be tested on\n            tol (float, optional): tolerance parameter. A constraint is considered as not fulfilled if\n                the violation is larger than tol. Defaults to 0.\n\n        Returns:\n            Boolean: True if all constraints are fulfilled for all rows, false if not\n\n        \"\"\"\n        if len(self.constraints) == 0:\n            return pd.Series([True] * len(experiments), index=experiments.index)\n        return (\n            pd.concat(\n                [c.is_fulfilled(experiments, tol) for c in self.constraints],\n                axis=1,\n            )\n            .fillna(True)\n            .all(axis=1)\n        )\n\n    def get(\n        self,\n        includes: Union[Type[CIncludes], Sequence[Type[CIncludes]]] = Constraint,\n        excludes: Optional[Union[Type[CExcludes], List[Type[CExcludes]]]] = None,\n        exact: bool = False,\n    ) -&gt; \"Constraints[CIncludes]\":\n        \"\"\"Get constraints of the domain\n\n        Args:\n            includes: Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.\n            excludes: Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\n            exact: Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n        Returns:\n            Constraints: constraints in the domain fitting to the passed requirements.\n\n        \"\"\"\n        return Constraints(\n            constraints=filter_by_class(\n                self.constraints,\n                includes=includes,\n                excludes=excludes,\n                exact=exact,\n            ),\n        )\n\n    def get_reps_df(self):\n        \"\"\"Provides a tabular overwiev of all constraints within the domain\n\n        Returns:\n            pd.DataFrame: DataFrame listing all constraints of the domain with a description\n\n        \"\"\"\n        df = pd.DataFrame(\n            index=range(len(self.constraints)),\n            columns=[\"Type\", \"Description\"],\n            data={\n                \"Type\": [feat.__class__.__name__ for feat in self.get(Constraint)],\n                \"Description\": [\n                    constraint.__str__() for constraint in self.get(Constraint)\n                ],\n            },\n        )\n        return df\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.domain.constraints.Constraints.__call__","title":"<code>__call__(experiments)</code>","text":"<p>Numerically evaluate all constraints</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>data to evaluate the constraint on</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Constraint evaluation for each of the constraints</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def __call__(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Numerically evaluate all constraints\n\n    Args:\n        experiments (pd.DataFrame): data to evaluate the constraint on\n\n    Returns:\n        pd.DataFrame: Constraint evaluation for each of the constraints\n\n    \"\"\"\n    return pd.concat([c(experiments) for c in self.constraints], axis=1)\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.domain.constraints.Constraints.get","title":"<code>get(includes=Constraint, excludes=None, exact=False)</code>","text":"<p>Get constraints of the domain</p> <p>Parameters:</p> Name Type Description Default <code>includes</code> <code>Union[Type[CIncludes], Sequence[Type[CIncludes]]]</code> <p>Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.</p> <code>Constraint</code> <code>excludes</code> <code>Optional[Union[Type[CExcludes], List[Type[CExcludes]]]]</code> <p>Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.</p> <code>None</code> <code>exact</code> <code>bool</code> <p>Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Constraints</code> <code>Constraints[CIncludes]</code> <p>constraints in the domain fitting to the passed requirements.</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def get(\n    self,\n    includes: Union[Type[CIncludes], Sequence[Type[CIncludes]]] = Constraint,\n    excludes: Optional[Union[Type[CExcludes], List[Type[CExcludes]]]] = None,\n    exact: bool = False,\n) -&gt; \"Constraints[CIncludes]\":\n    \"\"\"Get constraints of the domain\n\n    Args:\n        includes: Constraint class or list of specific constraint classes to be returned. Defaults to Constraint.\n        excludes: Constraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\n        exact: Boolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\n\n    Returns:\n        Constraints: constraints in the domain fitting to the passed requirements.\n\n    \"\"\"\n    return Constraints(\n        constraints=filter_by_class(\n            self.constraints,\n            includes=includes,\n            excludes=excludes,\n            exact=exact,\n        ),\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.domain.constraints.Constraints.get_reps_df","title":"<code>get_reps_df()</code>","text":"<p>Provides a tabular overwiev of all constraints within the domain</p> <p>Returns:</p> Type Description <p>pd.DataFrame: DataFrame listing all constraints of the domain with a description</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def get_reps_df(self):\n    \"\"\"Provides a tabular overwiev of all constraints within the domain\n\n    Returns:\n        pd.DataFrame: DataFrame listing all constraints of the domain with a description\n\n    \"\"\"\n    df = pd.DataFrame(\n        index=range(len(self.constraints)),\n        columns=[\"Type\", \"Description\"],\n        data={\n            \"Type\": [feat.__class__.__name__ for feat in self.get(Constraint)],\n            \"Description\": [\n                constraint.__str__() for constraint in self.get(Constraint)\n            ],\n        },\n    )\n    return df\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.domain.constraints.Constraints.is_fulfilled","title":"<code>is_fulfilled(experiments, tol=1e-06)</code>","text":"<p>Check if all constraints are fulfilled on all rows of the provided dataframe</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with data, the constraint validity should be tested on</p> required <code>tol</code> <code>float</code> <p>tolerance parameter. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 0.</p> <code>1e-06</code> <p>Returns:</p> Name Type Description <code>Boolean</code> <code>Series</code> <p>True if all constraints are fulfilled for all rows, false if not</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def is_fulfilled(self, experiments: pd.DataFrame, tol: float = 1e-6) -&gt; pd.Series:\n    \"\"\"Check if all constraints are fulfilled on all rows of the provided dataframe\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with data, the constraint validity should be tested on\n        tol (float, optional): tolerance parameter. A constraint is considered as not fulfilled if\n            the violation is larger than tol. Defaults to 0.\n\n    Returns:\n        Boolean: True if all constraints are fulfilled for all rows, false if not\n\n    \"\"\"\n    if len(self.constraints) == 0:\n        return pd.Series([True] * len(experiments), index=experiments.index)\n    return (\n        pd.concat(\n            [c.is_fulfilled(experiments, tol) for c in self.constraints],\n            axis=1,\n        )\n        .fillna(True)\n        .all(axis=1)\n    )\n</code></pre>"},{"location":"ref-constraints/#bofire.data_models.domain.constraints.Constraints.jacobian","title":"<code>jacobian(experiments)</code>","text":"<p>Numerically evaluate the jacobians of all constraints</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>data to evaluate the constraint jacobians on</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>A list containing the jacobians as pd.DataFrames</p> Source code in <code>bofire/data_models/domain/constraints.py</code> <pre><code>def jacobian(self, experiments: pd.DataFrame) -&gt; list:\n    \"\"\"Numerically evaluate the jacobians of all constraints\n\n    Args:\n        experiments (pd.DataFrame): data to evaluate the constraint jacobians on\n\n    Returns:\n        list: A list containing the jacobians as pd.DataFrames\n\n    \"\"\"\n    return [c.jacobian(experiments) for c in self.constraints]\n</code></pre>"},{"location":"ref-domain-util/","title":"Domain","text":""},{"location":"ref-domain/","title":"Domain","text":""},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain","title":"<code>Domain</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>class Domain(BaseModel):\n    type: Literal[\"Domain\"] = \"Domain\"\n\n    inputs: Inputs = Field(default_factory=lambda: Inputs())\n    outputs: Outputs = Field(default_factory=lambda: Outputs())\n    constraints: Constraints = Field(default_factory=lambda: Constraints())\n\n    \"\"\"Representation of the optimization problem/domain\n\n    Attributes:\n        inputs (List[Input], optional): List of input features. Defaults to [].\n        outputs (List[Output], optional): List of output features. Defaults to [].\n        constraints (List[Constraint], optional): List of constraints. Defaults to [].\n    \"\"\"\n\n    @classmethod\n    def from_lists(\n        cls,\n        inputs: Optional[Sequence[AnyInput]] = None,\n        outputs: Optional[Sequence[AnyOutput]] = None,\n        constraints: Optional[Sequence[AnyConstraint]] = None,\n    ):\n        inputs = [] if inputs is None else inputs\n        outputs = [] if outputs is None else outputs\n        constraints = [] if constraints is None else constraints\n        return cls(\n            inputs=Inputs(features=inputs),\n            outputs=Outputs(features=outputs),\n            constraints=Constraints(constraints=constraints),\n        )\n\n    @field_validator(\"inputs\", mode=\"before\")\n    @classmethod\n    def validate_inputs_list(cls, v):\n        if isinstance(v, collections.abc.Sequence):\n            v = Inputs(features=v)\n            return v\n        if isinstance_or_union(v, AnyInput):\n            return Inputs(features=[v])\n        return v\n\n    @field_validator(\"outputs\", mode=\"before\")\n    @classmethod\n    def validate_outputs_list(cls, v):\n        if isinstance(v, collections.abc.Sequence):\n            return Outputs(features=v)\n        if isinstance_or_union(v, AnyOutput):\n            return Outputs(features=[v])\n        return v\n\n    @field_validator(\"constraints\", mode=\"before\")\n    @classmethod\n    def validate_constraints_list(cls, v):\n        if isinstance(v, list):\n            return Constraints(constraints=v)\n        if isinstance_or_union(v, AnyConstraint):\n            return Constraints(constraints=[v])\n        return v\n\n    @model_validator(mode=\"after\")\n    def validate_unique_feature_keys(self):\n        \"\"\"Validates if provided input and output feature keys are unique\n\n        Args:\n            v (Outputs): List of all output features of the domain.\n            value (Dict[str, Inputs]): Dict containing a list of input features as single entry.\n\n        Raises:\n            ValueError: Feature keys are not unique.\n\n        Returns:\n            Outputs: Keeps output features as given.\n\n        \"\"\"\n        keys = self.outputs.get_keys() + self.inputs.get_keys()\n        if len(set(keys)) != len(keys):\n            raise ValueError(\"Feature keys are not unique\")\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_constraints(self):\n        \"\"\"Validate that the constraints defined in the domain fit to the input features.\n\n        Args:\n            v (List[Constraint]): List of constraints or empty if no constraints are defined\n            values (List[Input]): List of input features of the domain\n\n        Raises:\n            ValueError: Feature key in constraint is unknown.\n\n        Returns:\n            List[Constraint]: List of constraints defined for the domain\n\n        \"\"\"\n        for c in self.constraints.get():\n            c.validate_inputs(self.inputs)\n        return self\n\n    # TODO: tidy this up\n    def get_nchoosek_combinations(self, exhaustive: bool = False):\n        \"\"\"Get all possible NChooseK combinations\n\n        Args:\n            exhaustive (bool, optional): if True all combinations are returned. Defaults to False.\n\n        Returns:\n            Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination.\n                unused_features_list is a list of lists containing features unused in each NChooseK combination.\n\n        \"\"\"\n        if len(self.constraints.get(NChooseKConstraint)) == 0:\n            used_continuous_features = self.inputs.get_keys(ContinuousInput)\n            return used_continuous_features, []\n\n        used_features_list_all = []\n\n        # loops through each NChooseK constraint\n        for con in self.constraints.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            used_features_list = []\n\n            if exhaustive:\n                for n in range(con.min_count, con.max_count + 1):\n                    used_features_list.extend(itertools.combinations(con.features, n))\n\n                if con.none_also_valid:\n                    used_features_list.append(())\n            else:\n                used_features_list.extend(\n                    itertools.combinations(con.features, con.max_count),\n                )\n\n            used_features_list_all.append(used_features_list)\n\n        used_features_list_all = list(\n            itertools.product(*used_features_list_all),\n        )  # product between NChooseK constraints\n\n        # format into a list of used features\n        used_features_list_formatted = []\n        for used_features_list in used_features_list_all:\n            used_features_list_flattened = [\n                item for sublist in used_features_list for item in sublist\n            ]\n            used_features_list_formatted.append(list(set(used_features_list_flattened)))\n\n        # sort lists\n        used_features_list_sorted = []\n        for used_features in used_features_list_formatted:\n            used_features_list_sorted.append(sorted(used_features))\n\n        # drop duplicates\n        used_features_list_no_dup = []\n        for used_features in used_features_list_sorted:\n            if used_features not in used_features_list_no_dup:\n                used_features_list_no_dup.append(used_features)\n\n        # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\")\n\n        # remove combinations not fulfilling constraints\n        used_features_list_final = []\n        for combo in used_features_list_no_dup:\n            fulfil_constraints = []  # list of bools tracking if constraints are fulfilled\n            for con in self.constraints.get(NChooseKConstraint):\n                assert isinstance(con, NChooseKConstraint)\n                count = 0  # count of features in combo that are in con.features\n                for f in combo:\n                    if f in con.features:\n                        count += 1\n                if (\n                    count &gt;= con.min_count\n                    and count &lt;= con.max_count\n                    or count == 0\n                    and con.none_also_valid\n                ):\n                    fulfil_constraints.append(True)\n                else:\n                    fulfil_constraints.append(False)\n            if np.all(fulfil_constraints):\n                used_features_list_final.append(combo)\n\n        # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\")\n\n        # features unused\n        features_in_cc = []\n        for con in self.constraints.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            features_in_cc.extend(con.features)\n        features_in_cc = list(set(features_in_cc))\n        features_in_cc.sort()\n        unused_features_list = []\n        for used_features in used_features_list_final:\n            unused_features_list.append(\n                [f_key for f_key in features_in_cc if f_key not in used_features],\n            )\n\n        # postprocess\n        # used_features_list_final2 = []\n        # unused_features_list2 = []\n        # for used, unused in zip(used_features_list_final,unused_features_list):\n        #     if len(used) == 3:\n        #         used_features_list_final2.append(used), unused_features_list2.append(unused)\n\n        return used_features_list_final, unused_features_list\n\n    def coerce_invalids(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Coerces all invalid output measurements to np.nan\n\n        Args:\n            experiments (pd.DataFrame): Dataframe containing experimental data\n\n        Returns:\n            pd.DataFrame: coerced dataframe\n\n        \"\"\"\n        # coerce invalid to nan\n        for feat in self.outputs.get_keys(Output):\n            experiments.loc[experiments[f\"valid_{feat}\"] == 0, feat] = np.nan\n        return experiments\n\n    def aggregate_by_duplicates(\n        self,\n        experiments: pd.DataFrame,\n        prec: int,\n        delimiter: str = \"-\",\n        method: Literal[\"mean\", \"median\"] = \"mean\",\n    ) -&gt; Tuple[pd.DataFrame, list]:\n        \"\"\"Aggregate the dataframe by duplicate experiments\n\n        Duplicates are identified based on the experiments with the same input\n        features. Continuous input features are rounded before identifying the\n        duplicates. Aggregation is performed by taking the average of the\n        involved output features.\n\n        Args:\n            experiments (pd.DataFrame): Dataframe containing experimental data\n            prec (int): Precision of the rounding of the continuous input features\n            delimiter (str, optional): Delimiter used when combining the orig.\n                labcodes to a new one. Defaults to \"-\".\n            method (Literal[\"mean\", \"median\"], optional): Which aggregation\n                method to use. Defaults to \"mean\".\n\n        Returns:\n            Tuple[pd.DataFrame, list]: Dataframe holding the aggregated\n                experiments, list of lists holding the labcodes of the duplicates\n\n        \"\"\"\n        # prepare the parent frame\n        if method not in [\"mean\", \"median\"]:\n            raise ValueError(f\"Unknown aggregation type provided: {method}\")\n\n        preprocessed = self.outputs.preprocess_experiments_any_valid_output(experiments)\n        assert preprocessed is not None\n        experiments = preprocessed.copy()\n        if \"labcode\" not in experiments.columns:\n            experiments[\"labcode\"] = [\n                str(i + 1).zfill(int(np.ceil(np.log10(experiments.shape[0]))))\n                for i in range(experiments.shape[0])\n            ]\n\n        # round it if continuous inputs are present\n        if len(self.inputs.get(ContinuousInput)) &gt; 0:\n            experiments[self.inputs.get_keys(ContinuousInput)] = experiments[\n                self.inputs.get_keys(ContinuousInput)\n            ].round(prec)\n\n        # coerce invalid to nan\n        experiments = self.coerce_invalids(experiments)\n\n        # group and aggregate\n        agg: Dict[str, Any] = {\n            feat: method for feat in self.outputs.get_keys(ContinuousOutput)\n        }\n        agg[\"labcode\"] = lambda x: delimiter.join(sorted(x.tolist()))\n        for feat in self.outputs.get_keys(Output):\n            agg[f\"valid_{feat}\"] = lambda x: 1\n\n        grouped = experiments.groupby(self.inputs.get_keys(Input))\n        duplicated_labcodes = [\n            sorted(group.labcode.to_numpy().tolist())\n            for _, group in grouped\n            if group.shape[0] &gt; 1\n        ]\n\n        experiments = grouped.aggregate(agg).reset_index(drop=False)\n        for feat in self.outputs.get_keys(Output):\n            experiments.loc[experiments[feat].isna(), f\"valid_{feat}\"] = 0\n\n        experiments = experiments.sort_values(by=\"labcode\")\n        experiments = experiments.reset_index(drop=True)\n        return experiments, sorted(duplicated_labcodes)\n\n    def validate_experiments(\n        self,\n        experiments: pd.DataFrame,\n        strict: bool = False,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Checks the experimental data on validity\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n            strict (bool, optional): Boolean to distinguish if the occurrence of\n                fixed features in the dataset should be considered or not.\n                Defaults to False.\n\n        Raises:\n            ValueError: empty dataframe\n            ValueError: the column for a specific feature is missing the provided data\n            ValueError: there are labcodes with null value\n            ValueError: there are labcodes with nan value\n            ValueError: labcodes are not unique\n            ValueError: the provided columns do no match to the defined domain\n            ValueError: the provided columns do no match to the defined domain\n            ValueError: Input with null values\n            ValueError: Input with nan values\n\n        Returns:\n            pd.DataFrame: The provided dataframe with experimental data\n\n        \"\"\"\n        if len(experiments) == 0:\n            raise ValueError(\"no experiments provided (empty dataframe)\")\n\n        # we allow here for a column named labcode used to identify experiments\n        if \"labcode\" in experiments.columns:\n            # test that labcodes are not na\n            if experiments.labcode.isnull().to_numpy().any():\n                raise ValueError(\"there are labcodes with null value\")\n            if experiments.labcode.isna().to_numpy().any():\n                raise ValueError(\"there are labcodes with nan value\")\n            # test that labcodes are distinct\n            if (\n                len(set(experiments.labcode.to_numpy().tolist()))\n                != experiments.shape[0]\n            ):\n                raise ValueError(\"labcodes are not unique\")\n\n        # run the individual validators\n        experiments = self.inputs.validate_experiments(\n            experiments=experiments,\n            strict=strict,\n        )\n        experiments = self.outputs.validate_experiments(experiments=experiments)\n        return experiments\n\n    def describe_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n        Args:\n            experiments (pd.DataFrame): Dataframe with experimental data\n\n        Returns:\n            pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n\n        \"\"\"\n        data = {}\n        for feat in self.outputs.get_keys(Output):\n            data[feat] = [\n                experiments.loc[experiments[feat].notna()].shape[0],\n                experiments.loc[experiments[feat].notna(), \"valid_%s\" % feat].sum(),\n            ]\n        preprocessed = self.outputs.preprocess_experiments_all_valid_outputs(\n            experiments,\n        )\n        assert preprocessed is not None\n        data[\"all\"] = [\n            experiments.shape[0],\n            preprocessed.shape[0],\n        ]\n        return pd.DataFrame.from_dict(\n            data,\n            orient=\"index\",\n            columns=[\"measured\", \"valid\"],\n        )\n\n    def validate_candidates(\n        self,\n        candidates: pd.DataFrame,\n        only_inputs: bool = False,\n        tol: float = 1e-5,\n        raise_validation_error: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Method to check the validty of proposed candidates\n\n        Args:\n            candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates)\n            only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False.\n            tol (float,optional): tolerance parameter for constraints. A constraint is considered as not fulfilled if the violation\n                is larger than tol. Defaults to 1e-6.\n            raise_validation_error (bool, optional): If true an error will be raised if candidates violate constraints,\n                otherwise only a warning will be displayed. Defaults to True.\n\n        Raises:\n            ValueError: when a column is missing for a defined input feature\n            ValueError: when a column is missing for a defined output feature\n            ValueError: when a non-numerical value is proposed\n            ValueError: when an additional column is found\n            ConstraintNotFulfilledError: when the constraints are not fulfilled and `raise_validation_error = True`\n\n        Returns:\n            pd.DataFrame: dataframe with suggested experiments (candidates)\n\n        \"\"\"\n        # check that each input feature has a col and is valid in itself\n        assert isinstance(self.inputs, Inputs)\n        candidates = self.inputs.validate_candidates(candidates)\n        # check if all constraints are fulfilled\n        if not self.constraints.is_fulfilled(candidates, tol=tol).all():\n            if raise_validation_error:\n                raise ConstraintNotFulfilledError(\n                    f\"Constraints not fulfilled: {candidates}\",\n                )\n            warnings.warn(\"Not all constraints are fulfilled.\")\n        # for each continuous output feature with an attached objective object\n        if not only_inputs:\n            assert isinstance(self.outputs, Outputs)\n            candidates = self.outputs.validate_candidates(candidates=candidates)\n        return candidates\n\n    @property\n    def experiment_column_names(self):\n        \"\"\"The columns in the experimental dataframe\n\n        Returns:\n            List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys)\n\n        \"\"\"\n        return (self.inputs + self.outputs).get_keys() + [\n            f\"valid_{output_feature_key}\"\n            for output_feature_key in self.outputs.get_keys(Output)\n        ]\n\n    @property\n    def candidate_column_names(self):\n        \"\"\"The columns in the candidate dataframe\n\n        Returns:\n            List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)\n\n        \"\"\"\n        assert isinstance(self.outputs, Outputs)\n        return (\n            self.inputs.get_keys(Input)\n            + [\n                f\"{output_feature_key}_pred\"\n                for output_feature_key in self.outputs.get_keys_by_objective(Objective)\n            ]\n            + [\n                f\"{output_feature_key}_sd\"\n                for output_feature_key in self.outputs.get_keys_by_objective(Objective)\n            ]\n            + [\n                f\"{output_feature_key}_des\"\n                for output_feature_key in self.outputs.get_keys_by_objective(Objective)\n            ]\n        )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.candidate_column_names","title":"<code>candidate_column_names</code>  <code>property</code>","text":"<p>The columns in the candidate dataframe</p> <p>Returns:</p> Type Description <p>List[str]: List of columns in the candidate dataframe (input feature keys + input feature keys_pred, input feature keys_sd, input feature keys_des)</p>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.constraints","title":"<code>constraints = Field(default_factory=lambda: Constraints())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Representation of the optimization problem/domain</p> <p>Attributes:</p> Name Type Description <code>inputs</code> <code>List[Input]</code> <p>List of input features. Defaults to [].</p> <code>outputs</code> <code>List[Output]</code> <p>List of output features. Defaults to [].</p> <code>constraints</code> <code>List[Constraint]</code> <p>List of constraints. Defaults to [].</p>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.experiment_column_names","title":"<code>experiment_column_names</code>  <code>property</code>","text":"<p>The columns in the experimental dataframe</p> <p>Returns:</p> Type Description <p>List[str]: List of columns in the experiment dataframe (output feature keys + valid_output feature keys)</p>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.aggregate_by_duplicates","title":"<code>aggregate_by_duplicates(experiments, prec, delimiter='-', method='mean')</code>","text":"<p>Aggregate the dataframe by duplicate experiments</p> <p>Duplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features.</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe containing experimental data</p> required <code>prec</code> <code>int</code> <p>Precision of the rounding of the continuous input features</p> required <code>delimiter</code> <code>str</code> <p>Delimiter used when combining the orig. labcodes to a new one. Defaults to \"-\".</p> <code>'-'</code> <code>method</code> <code>Literal['mean', 'median']</code> <p>Which aggregation method to use. Defaults to \"mean\".</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, list]</code> <p>Tuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def aggregate_by_duplicates(\n    self,\n    experiments: pd.DataFrame,\n    prec: int,\n    delimiter: str = \"-\",\n    method: Literal[\"mean\", \"median\"] = \"mean\",\n) -&gt; Tuple[pd.DataFrame, list]:\n    \"\"\"Aggregate the dataframe by duplicate experiments\n\n    Duplicates are identified based on the experiments with the same input\n    features. Continuous input features are rounded before identifying the\n    duplicates. Aggregation is performed by taking the average of the\n    involved output features.\n\n    Args:\n        experiments (pd.DataFrame): Dataframe containing experimental data\n        prec (int): Precision of the rounding of the continuous input features\n        delimiter (str, optional): Delimiter used when combining the orig.\n            labcodes to a new one. Defaults to \"-\".\n        method (Literal[\"mean\", \"median\"], optional): Which aggregation\n            method to use. Defaults to \"mean\".\n\n    Returns:\n        Tuple[pd.DataFrame, list]: Dataframe holding the aggregated\n            experiments, list of lists holding the labcodes of the duplicates\n\n    \"\"\"\n    # prepare the parent frame\n    if method not in [\"mean\", \"median\"]:\n        raise ValueError(f\"Unknown aggregation type provided: {method}\")\n\n    preprocessed = self.outputs.preprocess_experiments_any_valid_output(experiments)\n    assert preprocessed is not None\n    experiments = preprocessed.copy()\n    if \"labcode\" not in experiments.columns:\n        experiments[\"labcode\"] = [\n            str(i + 1).zfill(int(np.ceil(np.log10(experiments.shape[0]))))\n            for i in range(experiments.shape[0])\n        ]\n\n    # round it if continuous inputs are present\n    if len(self.inputs.get(ContinuousInput)) &gt; 0:\n        experiments[self.inputs.get_keys(ContinuousInput)] = experiments[\n            self.inputs.get_keys(ContinuousInput)\n        ].round(prec)\n\n    # coerce invalid to nan\n    experiments = self.coerce_invalids(experiments)\n\n    # group and aggregate\n    agg: Dict[str, Any] = {\n        feat: method for feat in self.outputs.get_keys(ContinuousOutput)\n    }\n    agg[\"labcode\"] = lambda x: delimiter.join(sorted(x.tolist()))\n    for feat in self.outputs.get_keys(Output):\n        agg[f\"valid_{feat}\"] = lambda x: 1\n\n    grouped = experiments.groupby(self.inputs.get_keys(Input))\n    duplicated_labcodes = [\n        sorted(group.labcode.to_numpy().tolist())\n        for _, group in grouped\n        if group.shape[0] &gt; 1\n    ]\n\n    experiments = grouped.aggregate(agg).reset_index(drop=False)\n    for feat in self.outputs.get_keys(Output):\n        experiments.loc[experiments[feat].isna(), f\"valid_{feat}\"] = 0\n\n    experiments = experiments.sort_values(by=\"labcode\")\n    experiments = experiments.reset_index(drop=True)\n    return experiments, sorted(duplicated_labcodes)\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.coerce_invalids","title":"<code>coerce_invalids(experiments)</code>","text":"<p>Coerces all invalid output measurements to np.nan</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe containing experimental data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: coerced dataframe</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def coerce_invalids(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Coerces all invalid output measurements to np.nan\n\n    Args:\n        experiments (pd.DataFrame): Dataframe containing experimental data\n\n    Returns:\n        pd.DataFrame: coerced dataframe\n\n    \"\"\"\n    # coerce invalid to nan\n    for feat in self.outputs.get_keys(Output):\n        experiments.loc[experiments[f\"valid_{feat}\"] == 0, feat] = np.nan\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.describe_experiments","title":"<code>describe_experiments(experiments)</code>","text":"<p>Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def describe_experiments(self, experiments: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Method to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n\n    Returns:\n        pd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n\n    \"\"\"\n    data = {}\n    for feat in self.outputs.get_keys(Output):\n        data[feat] = [\n            experiments.loc[experiments[feat].notna()].shape[0],\n            experiments.loc[experiments[feat].notna(), \"valid_%s\" % feat].sum(),\n        ]\n    preprocessed = self.outputs.preprocess_experiments_all_valid_outputs(\n        experiments,\n    )\n    assert preprocessed is not None\n    data[\"all\"] = [\n        experiments.shape[0],\n        preprocessed.shape[0],\n    ]\n    return pd.DataFrame.from_dict(\n        data,\n        orient=\"index\",\n        columns=[\"measured\", \"valid\"],\n    )\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.get_nchoosek_combinations","title":"<code>get_nchoosek_combinations(exhaustive=False)</code>","text":"<p>Get all possible NChooseK combinations</p> <p>Parameters:</p> Name Type Description Default <code>exhaustive</code> <code>bool</code> <p>if True all combinations are returned. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>(used_features_list, unused_features_list)</code> <p>used_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination.</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def get_nchoosek_combinations(self, exhaustive: bool = False):\n    \"\"\"Get all possible NChooseK combinations\n\n    Args:\n        exhaustive (bool, optional): if True all combinations are returned. Defaults to False.\n\n    Returns:\n        Tuple(used_features_list, unused_features_list): used_features_list is a list of lists containing features used in each NChooseK combination.\n            unused_features_list is a list of lists containing features unused in each NChooseK combination.\n\n    \"\"\"\n    if len(self.constraints.get(NChooseKConstraint)) == 0:\n        used_continuous_features = self.inputs.get_keys(ContinuousInput)\n        return used_continuous_features, []\n\n    used_features_list_all = []\n\n    # loops through each NChooseK constraint\n    for con in self.constraints.get(NChooseKConstraint):\n        assert isinstance(con, NChooseKConstraint)\n        used_features_list = []\n\n        if exhaustive:\n            for n in range(con.min_count, con.max_count + 1):\n                used_features_list.extend(itertools.combinations(con.features, n))\n\n            if con.none_also_valid:\n                used_features_list.append(())\n        else:\n            used_features_list.extend(\n                itertools.combinations(con.features, con.max_count),\n            )\n\n        used_features_list_all.append(used_features_list)\n\n    used_features_list_all = list(\n        itertools.product(*used_features_list_all),\n    )  # product between NChooseK constraints\n\n    # format into a list of used features\n    used_features_list_formatted = []\n    for used_features_list in used_features_list_all:\n        used_features_list_flattened = [\n            item for sublist in used_features_list for item in sublist\n        ]\n        used_features_list_formatted.append(list(set(used_features_list_flattened)))\n\n    # sort lists\n    used_features_list_sorted = []\n    for used_features in used_features_list_formatted:\n        used_features_list_sorted.append(sorted(used_features))\n\n    # drop duplicates\n    used_features_list_no_dup = []\n    for used_features in used_features_list_sorted:\n        if used_features not in used_features_list_no_dup:\n            used_features_list_no_dup.append(used_features)\n\n    # print(f\"duplicates dropped: {len(used_features_list_sorted)-len(used_features_list_no_dup)}\")\n\n    # remove combinations not fulfilling constraints\n    used_features_list_final = []\n    for combo in used_features_list_no_dup:\n        fulfil_constraints = []  # list of bools tracking if constraints are fulfilled\n        for con in self.constraints.get(NChooseKConstraint):\n            assert isinstance(con, NChooseKConstraint)\n            count = 0  # count of features in combo that are in con.features\n            for f in combo:\n                if f in con.features:\n                    count += 1\n            if (\n                count &gt;= con.min_count\n                and count &lt;= con.max_count\n                or count == 0\n                and con.none_also_valid\n            ):\n                fulfil_constraints.append(True)\n            else:\n                fulfil_constraints.append(False)\n        if np.all(fulfil_constraints):\n            used_features_list_final.append(combo)\n\n    # print(f\"violators dropped: {len(used_features_list_no_dup)-len(used_features_list_final)}\")\n\n    # features unused\n    features_in_cc = []\n    for con in self.constraints.get(NChooseKConstraint):\n        assert isinstance(con, NChooseKConstraint)\n        features_in_cc.extend(con.features)\n    features_in_cc = list(set(features_in_cc))\n    features_in_cc.sort()\n    unused_features_list = []\n    for used_features in used_features_list_final:\n        unused_features_list.append(\n            [f_key for f_key in features_in_cc if f_key not in used_features],\n        )\n\n    # postprocess\n    # used_features_list_final2 = []\n    # unused_features_list2 = []\n    # for used, unused in zip(used_features_list_final,unused_features_list):\n    #     if len(used) == 3:\n    #         used_features_list_final2.append(used), unused_features_list2.append(unused)\n\n    return used_features_list_final, unused_features_list\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_candidates","title":"<code>validate_candidates(candidates, only_inputs=False, tol=1e-05, raise_validation_error=True)</code>","text":"<p>Method to check the validty of proposed candidates</p> <p>Parameters:</p> Name Type Description Default <code>candidates</code> <code>DataFrame</code> <p>Dataframe with suggested new experiments (candidates)</p> required <code>only_inputs</code> <code>(bool, optional)</code> <p>If True, only the input columns are validated. Defaults to False.</p> <code>False</code> <code>tol</code> <code>(float, optional)</code> <p>tolerance parameter for constraints. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 1e-6.</p> <code>1e-05</code> <code>raise_validation_error</code> <code>bool</code> <p>If true an error will be raised if candidates violate constraints, otherwise only a warning will be displayed. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>when a column is missing for a defined input feature</p> <code>ValueError</code> <p>when a column is missing for a defined output feature</p> <code>ValueError</code> <p>when a non-numerical value is proposed</p> <code>ValueError</code> <p>when an additional column is found</p> <code>ConstraintNotFulfilledError</code> <p>when the constraints are not fulfilled and <code>raise_validation_error = True</code></p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: dataframe with suggested experiments (candidates)</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def validate_candidates(\n    self,\n    candidates: pd.DataFrame,\n    only_inputs: bool = False,\n    tol: float = 1e-5,\n    raise_validation_error: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Method to check the validty of proposed candidates\n\n    Args:\n        candidates (pd.DataFrame): Dataframe with suggested new experiments (candidates)\n        only_inputs (bool,optional): If True, only the input columns are validated. Defaults to False.\n        tol (float,optional): tolerance parameter for constraints. A constraint is considered as not fulfilled if the violation\n            is larger than tol. Defaults to 1e-6.\n        raise_validation_error (bool, optional): If true an error will be raised if candidates violate constraints,\n            otherwise only a warning will be displayed. Defaults to True.\n\n    Raises:\n        ValueError: when a column is missing for a defined input feature\n        ValueError: when a column is missing for a defined output feature\n        ValueError: when a non-numerical value is proposed\n        ValueError: when an additional column is found\n        ConstraintNotFulfilledError: when the constraints are not fulfilled and `raise_validation_error = True`\n\n    Returns:\n        pd.DataFrame: dataframe with suggested experiments (candidates)\n\n    \"\"\"\n    # check that each input feature has a col and is valid in itself\n    assert isinstance(self.inputs, Inputs)\n    candidates = self.inputs.validate_candidates(candidates)\n    # check if all constraints are fulfilled\n    if not self.constraints.is_fulfilled(candidates, tol=tol).all():\n        if raise_validation_error:\n            raise ConstraintNotFulfilledError(\n                f\"Constraints not fulfilled: {candidates}\",\n            )\n        warnings.warn(\"Not all constraints are fulfilled.\")\n    # for each continuous output feature with an attached objective object\n    if not only_inputs:\n        assert isinstance(self.outputs, Outputs)\n        candidates = self.outputs.validate_candidates(candidates=candidates)\n    return candidates\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_constraints","title":"<code>validate_constraints()</code>","text":"<p>Validate that the constraints defined in the domain fit to the input features.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>List[Constraint]</code> <p>List of constraints or empty if no constraints are defined</p> required <code>values</code> <code>List[Input]</code> <p>List of input features of the domain</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Feature key in constraint is unknown.</p> <p>Returns:</p> Type Description <p>List[Constraint]: List of constraints defined for the domain</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_constraints(self):\n    \"\"\"Validate that the constraints defined in the domain fit to the input features.\n\n    Args:\n        v (List[Constraint]): List of constraints or empty if no constraints are defined\n        values (List[Input]): List of input features of the domain\n\n    Raises:\n        ValueError: Feature key in constraint is unknown.\n\n    Returns:\n        List[Constraint]: List of constraints defined for the domain\n\n    \"\"\"\n    for c in self.constraints.get():\n        c.validate_inputs(self.inputs)\n    return self\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_experiments","title":"<code>validate_experiments(experiments, strict=False)</code>","text":"<p>Checks the experimental data on validity</p> <p>Parameters:</p> Name Type Description Default <code>experiments</code> <code>DataFrame</code> <p>Dataframe with experimental data</p> required <code>strict</code> <code>bool</code> <p>Boolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>empty dataframe</p> <code>ValueError</code> <p>the column for a specific feature is missing the provided data</p> <code>ValueError</code> <p>there are labcodes with null value</p> <code>ValueError</code> <p>there are labcodes with nan value</p> <code>ValueError</code> <p>labcodes are not unique</p> <code>ValueError</code> <p>the provided columns do no match to the defined domain</p> <code>ValueError</code> <p>the provided columns do no match to the defined domain</p> <code>ValueError</code> <p>Input with null values</p> <code>ValueError</code> <p>Input with nan values</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The provided dataframe with experimental data</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>def validate_experiments(\n    self,\n    experiments: pd.DataFrame,\n    strict: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Checks the experimental data on validity\n\n    Args:\n        experiments (pd.DataFrame): Dataframe with experimental data\n        strict (bool, optional): Boolean to distinguish if the occurrence of\n            fixed features in the dataset should be considered or not.\n            Defaults to False.\n\n    Raises:\n        ValueError: empty dataframe\n        ValueError: the column for a specific feature is missing the provided data\n        ValueError: there are labcodes with null value\n        ValueError: there are labcodes with nan value\n        ValueError: labcodes are not unique\n        ValueError: the provided columns do no match to the defined domain\n        ValueError: the provided columns do no match to the defined domain\n        ValueError: Input with null values\n        ValueError: Input with nan values\n\n    Returns:\n        pd.DataFrame: The provided dataframe with experimental data\n\n    \"\"\"\n    if len(experiments) == 0:\n        raise ValueError(\"no experiments provided (empty dataframe)\")\n\n    # we allow here for a column named labcode used to identify experiments\n    if \"labcode\" in experiments.columns:\n        # test that labcodes are not na\n        if experiments.labcode.isnull().to_numpy().any():\n            raise ValueError(\"there are labcodes with null value\")\n        if experiments.labcode.isna().to_numpy().any():\n            raise ValueError(\"there are labcodes with nan value\")\n        # test that labcodes are distinct\n        if (\n            len(set(experiments.labcode.to_numpy().tolist()))\n            != experiments.shape[0]\n        ):\n            raise ValueError(\"labcodes are not unique\")\n\n    # run the individual validators\n    experiments = self.inputs.validate_experiments(\n        experiments=experiments,\n        strict=strict,\n    )\n    experiments = self.outputs.validate_experiments(experiments=experiments)\n    return experiments\n</code></pre>"},{"location":"ref-domain/#bofire.data_models.domain.domain.Domain.validate_unique_feature_keys","title":"<code>validate_unique_feature_keys()</code>","text":"<p>Validates if provided input and output feature keys are unique</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Outputs</code> <p>List of all output features of the domain.</p> required <code>value</code> <code>Dict[str, Inputs]</code> <p>Dict containing a list of input features as single entry.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Feature keys are not unique.</p> <p>Returns:</p> Name Type Description <code>Outputs</code> <p>Keeps output features as given.</p> Source code in <code>bofire/data_models/domain/domain.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_unique_feature_keys(self):\n    \"\"\"Validates if provided input and output feature keys are unique\n\n    Args:\n        v (Outputs): List of all output features of the domain.\n        value (Dict[str, Inputs]): Dict containing a list of input features as single entry.\n\n    Raises:\n        ValueError: Feature keys are not unique.\n\n    Returns:\n        Outputs: Keeps output features as given.\n\n    \"\"\"\n    keys = self.outputs.get_keys() + self.inputs.get_keys()\n    if len(set(keys)) != len(keys):\n        raise ValueError(\"Feature keys are not unique\")\n    return self\n</code></pre>"},{"location":"ref-features/","title":"Domain","text":""},{"location":"ref-objectives/","title":"Domain","text":""},{"location":"ref-utils/","title":"Utils","text":""},{"location":"userguide_surrogates/","title":"Surrogate models","text":"<p>In Bayesian Optimization, information from previous experiments is taken into account to generate proposals for future experiments. This information is leveraged by creating a surrogate model for the black-box function that is to be optimized based on the available data. Naturally, experimental candidates for which the surrogate model makes a promising prediction (e.g., high predicted values of a quantity we want to maximize) should be chosen over ones for which this is not the case. However, since the available data might cover only a small part of the input space, the model is likely to only be able to make very uncertain predictions far away from the data. Therefore, the surrogate model should be able to express the degree to which the predictions are uncertain so that we can use this information - combining the prediction and the associated uncertainty - to select the settings for the next experimental iteration.</p> <p>The acquisition function is the object that turns the predicted distribution (you can think of this as the prediction and the prediction uncertainty) into a single quantity representing how promising a candidate experimental point seems. This function determines if one rather wants to focus on exploitation, i.e., quickly approaching a close local optimum of the black-box function, or on exploration, i.e., exploring different regions of the input space first.</p> <p>Therefore, three criteria typically determine whether any candidate is selected as experimental proposal: the value of the surrogate model, the uncertainty of the model, and the acquisition function.</p>"},{"location":"userguide_surrogates/#surrogate-model-options","title":"Surrogate model options","text":"<p>BoFire offers the following classes of surrogate models.</p> Surrogate Optimization of When to use Type SingleTaskGPSurrogate a single objective with real valued inputs Limited data and black-box function is smooth Gaussian process RandomForestSurrogate a single objective Rich data; black-box function does not have to be smooth sklearn random forest implementation MLP a single objective with real-valued inputs Rich data and black-box function is smooth Multi layer perceptron MixedSingleTaskGPSurrogate a single objective with categorical and real valued inputs Limited data and black-box function is smooth Gaussian process XGBoostSurrogate a single objective Rich data; black-box function does not have to be smooth xgboost implementation of gradient boosting trees TanimotoGP a single objective At least one input feature is a molecule represented as fingerprint Gaussian process on a molecule space for which Tanimoto similarity determines the similarity between points <p>All of these are single-objective surrogate models. For optimization of multiple objectives at the same time, a suitable Strategy has to be chosen. Then for each objective a different surrogate model can be specified. By default the SingleTaskGPSurrogate is used.</p> <p>Example:</p> <pre><code>surrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n)\nsurrogate_data_1 = XGBoostSurrogate(\n    inputs=domain.inputs,\n    outputs=Outputs(features=[domain.outputs[1]]),\n)\nqparego_data_model = QparegoStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data_0, surrogate_data_1]\n    ),\n)\n</code></pre> <p>Note:</p> <ul> <li>The standard Kernel for all Gaussian Process (GP) surrogates is a 5/2 matern kernel with automated relevance detection and normalization of the input features.</li> <li>The tree-based models (RandomForestSurrogate and XGBoostSurrogate) do not have kernels but quantify uncertainty using the standard deviation of the predictions of their individual trees.</li> <li>MLP quantifies uncertainty using the standard deviation of multiple predictions that come from different dropout rates (randomly setting neural network weights to zero).</li> </ul>"},{"location":"userguide_surrogates/#customization","title":"Customization","text":"<p>BoFire also offers the option to customize surrogate models. In particular, it is possible to customize the SingleTaskGPSurrogate in the following ways.</p>"},{"location":"userguide_surrogates/#kernel-customization","title":"Kernel customization","text":"<p>Specify the Kernel:</p> Kernel Description Translation invariant Input variable type RBFKernel Based on Gaussian distribution Yes Continuous MaternKernel Based on Gamma function; allows setting a smoothness parameter Yes Continuous PolynomialKernel Based on dot-product of two vectors of input points No Continuous LinearKernel Equal to dot-product of two vectors of input points No Continuous TanimotoKernel Measures similarities between binary vectors using Tanimoto Similarity Not applicable MolecularInput HammingDistanceKernel Similarity is defined by the Hamming distance which considers the number of equal entries between two vectors (e.g., in One-Hot-encoding) Not applicable Categorical <p>Translational invariance means that the similarity between two input points is not affected by shifting both points by the same amount but only determined by their distance. Example: with a translationally invariant kernel, the values 10 and 20 are equally similar to each other as the values 20 and 30, while with a polynomial kernel the latter pair has potentially higher similarity. Polynomial kernels are often suitable for high-dimensional inputs while for low-dimensional inputs an RBF or Mat\u00e9rn kernel is recommended.</p> <p>Note: - SingleTaskGPSurrogate with PolynomialKernel is equivalent to PolynomialSurrogate. - SingleTaskGPSurrogate with LinearKernel is equivalent to LinearSurrogate. - SingleTaskGPSurrogate with TanimotoKernel is equivalent to TanimotoGP. - One can combine two Kernels by using AdditiveKernel or MultiplicativeKernel.</p> <p>Example:</p> <pre><code>surrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2)\n)\n</code></pre>"},{"location":"userguide_surrogates/#noise-model-customization","title":"Noise model customization","text":"<p>For experimental data subject to noise, one can specify the distribution of this noise. The options are:</p> Noise Model When to use NormalPrior Noise is Gaussian GammaPrior Noise has a Gamma distribution <p>Example:</p> <pre><code>surrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2),\n        noise_prior=NormalPrior(loc=0, scale=1)\n)\n</code></pre>"}]}