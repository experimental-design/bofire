{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building with BoFire\n",
    "\n",
    "This notebooks shows how to setup and analyze models trained with BoFire. It is stil WIP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.domain.api import Inputs, Outputs\n",
    "from bofire.data_models.surrogates.api import SingleTaskGPSurrogate, RandomForestSurrogate, MixedSingleTaskGPSurrogate, AnySurrogate, RandomForestSurrogate, EmpiricalSurrogate, MLPEnsemble\n",
    "from bofire.benchmarks.single import Himmelblau\n",
    "from bofire.benchmarks.multi import CrossCoupling\n",
    "import bofire.surrogates.api as surrogates\n",
    "import json\n",
    "from bofire.data_models.enum import CategoricalEncodingEnum\n",
    "\n",
    "from pydantic import parse_obj_as"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "For didactic purposes, we sample data from a Himmelblau benchmark function and use them to train a SingleTaskGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = Himmelblau()\n",
    "samples = benchmark.domain.inputs.sample(n=50)\n",
    "experiments = benchmark.f(samples, return_complete=True)\n",
    "\n",
    "experiments.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = benchmark.domain.inputs\n",
    "output_features = benchmark.domain.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_features.json()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Task GP\n",
    "\n",
    "Generate the json spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we setup the data model, here a Single Task GP\n",
    "surrogate_data = SingleTaskGPSurrogate(\n",
    "    inputs=input_features,\n",
    "    outputs=output_features\n",
    ")\n",
    "\n",
    "# we generate the json spec\n",
    "jspec = surrogate_data.json()\n",
    "\n",
    "jspec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load it from the spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate = surrogates.map(surrogate_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit it. This is not 100% finished. In the future we will call here hyperfit which will return the CV results etc. This has to be finished. So ignore this for now and just call fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate.fit(experiments=experiments)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dump it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump it\n",
    "dump = surrogate.dumps()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict with it\n",
    "df_predictions = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions = surrogate.to_predictions(predictions=df_predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load again from spec and dump and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "surrogate.loads(dump)\n",
    "\n",
    "# predict with it\n",
    "df_predictions2 = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions2 = surrogate.to_predictions(predictions=df_predictions2)\n",
    "\n",
    "# check for equality\n",
    "predictions==predictions2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Generate the json spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we setup the data model, here a Single Task GP\n",
    "surrogate_data = RandomForestSurrogate(\n",
    "    inputs=input_features,\n",
    "    outputs=output_features,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# we generate the json spec\n",
    "jspec = surrogate_data.json()\n",
    "\n",
    "jspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it from the spec\n",
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "# Map it \n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "# Fit it\n",
    "surrogate.fit(experiments=experiments)\n",
    "# dump it\n",
    "dump = surrogate.dumps()\n",
    "# predict with it\n",
    "df_predictions = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions = surrogate.to_predictions(predictions=df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "surrogate.loads(dump)\n",
    "\n",
    "# predict with it\n",
    "df_predictions2 = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions2 = surrogate.to_predictions(predictions=df_predictions2)\n",
    "\n",
    "# check for equality\n",
    "predictions==predictions2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Ensemble\n",
    "\n",
    "Generate the json spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we setup the data model, here a Single Task GP\n",
    "surrogate_data = MLPEnsemble(\n",
    "    inputs=input_features,\n",
    "    outputs=output_features,\n",
    "    n_estimators=2\n",
    ")\n",
    "\n",
    "# we generate the json spec\n",
    "jspec = surrogate_data.json()\n",
    "\n",
    "jspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it from the spec\n",
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "# Map it \n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "# Fit it\n",
    "surrogate.fit(experiments=experiments)\n",
    "# dump it\n",
    "dump = surrogate.dumps()\n",
    "# predict with it\n",
    "df_predictions = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions = surrogate.to_predictions(predictions=df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "surrogate.loads(dump)\n",
    "\n",
    "# predict with it\n",
    "df_predictions2 = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions2 = surrogate.to_predictions(predictions=df_predictions2)\n",
    "\n",
    "# check for equality\n",
    "predictions==predictions2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empirical Surrogate\n",
    "\n",
    "The empirical model is special as it has per default no fit and you need cloudpickle. There can be empirical models which implement a fit, but for this they also have to inherit from `Trainable`. The current example is the default without any fit functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.deterministic import DeterministicModel\n",
    "from torch import Tensor\n",
    "\n",
    "class HimmelblauModel(DeterministicModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._num_outputs = 1\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        return (\n",
    "            (X[..., 0] ** 2 + X[..., 1] - 11.0) ** 2\n",
    "            + (X[..., 0] + X[..., 1] ** 2 - 7.0) ** 2\n",
    "        ).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we setup the data model, here a Single Task GP\n",
    "surrogate_data = EmpiricalSurrogate(\n",
    "    inputs=input_features,\n",
    "    outputs=output_features,\n",
    ")\n",
    "\n",
    "# we generate the json spec\n",
    "jspec = surrogate_data.json()\n",
    "\n",
    "jspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it from the spec\n",
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "# Map it \n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "# attach the actual model to it\n",
    "surrogate.model = HimmelblauModel()\n",
    "# dump it\n",
    "dump = surrogate.dumps()\n",
    "# predict with it\n",
    "df_predictions = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions = surrogate.to_predictions(predictions=df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "surrogate.loads(dump)\n",
    "\n",
    "# predict with it\n",
    "df_predictions2 = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions2 = surrogate.to_predictions(predictions=df_predictions2)\n",
    "\n",
    "# check for equality\n",
    "predictions==predictions2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixed GP\n",
    "\n",
    "Generate data for a mixed problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = CrossCoupling()\n",
    "samples = benchmark.domain.inputs.sample(n=50)\n",
    "experiments = benchmark.f(samples, return_complete=True)\n",
    "\n",
    "experiments.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we setup the data model, here a Single Task GP\n",
    "surrogate_data = MixedSingleTaskGPSurrogate(\n",
    "    inputs=benchmark.domain.inputs,\n",
    "    outputs=Outputs(features=[benchmark.domain.outputs.features[0]]),\n",
    "    input_preprocessing_specs={\"catalyst\": CategoricalEncodingEnum.ONE_HOT}\n",
    ")\n",
    "\n",
    "# we generate the json spec\n",
    "jspec = surrogate_data.json()\n",
    "\n",
    "jspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it from the spec\n",
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "# Map it \n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "# Fit it\n",
    "surrogate.fit(experiments=experiments)\n",
    "# dump it\n",
    "dump = surrogate.dumps()\n",
    "# predict with it\n",
    "df_predictions = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions = surrogate.to_predictions(predictions=df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_data = parse_obj_as(AnySurrogate, json.loads(jspec))\n",
    "surrogate = surrogates.map(surrogate_data)\n",
    "surrogate.loads(dump)\n",
    "\n",
    "# predict with it\n",
    "df_predictions2 = surrogate.predict(experiments)\n",
    "# transform to spec\n",
    "predictions2 = surrogate.to_predictions(predictions=df_predictions2)\n",
    "\n",
    "# check for equality\n",
    "predictions==predictions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bofire_devel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "b9bdc9e617e457afdaedd2563eddde9c04c87768cb2f63795a1786b83528ca68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
