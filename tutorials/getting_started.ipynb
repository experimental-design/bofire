{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=https://experimental-design.github.io/bofire/>\n",
    "  <img width=\"350\" src=\"https://raw.githubusercontent.com/experimental-design/bofire/main/graphics/logos/bofire-long.png\" alt=\"BoFire Logo\" />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Getting started with BoFire\n",
    "\n",
    "In our [docs](https://experimental-design.github.io/bofire/), you can find all different options for the [BoFire installation](https://experimental-design.github.io/bofire/install/). For basic BoFire Bayesian optimization features using [BoTorch](https://botorch.org/) which depends on\n",
    "[PyTorch](https://pytorch.org/), you need to run\n",
    "\n",
    "```\n",
    "pip install bofire[optimization]\n",
    "```\n",
    "\n",
    "For a more complete introduction to BoFire, please look in our [docs](https://experimental-design.github.io/bofire/).\n",
    "\n",
    "### Optimization problem\n",
    "\n",
    "Let us consider a test function for single-objective optimization - the [Himmelblau's function](https://en.wikipedia.org/wiki/Himmelblau%27s_function). The Himmelblau's function has four identical local minima used to test the performance of optimization algorithms. The optimization domain of the Himmelblau's function is illustrated below together with the four minima marked red.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/experimental-design/bofire/main/graphics/tutorials/himmelblau.png\"\n",
    "    alt=\"Himmelblau's function\"\n",
    "    width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Defining the optimization output\n",
    "\n",
    "Let's consider the single continuous output variable *y* of the Himmelblau's function with the objective to minimize it. In BoFire's terminology, we create a `MinimizeObjective` object to define the optimization objective of a `Continuous Output` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.features.api import ContinuousOutput\n",
    "from bofire.data_models.objectives.api import MinimizeObjective\n",
    "\n",
    "\n",
    "objective = MinimizeObjective()\n",
    "output_feature = ContinuousOutput(key=\"y\", objective=objective)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on `Output` features and `Objective` objects, see the respective sections in our [docs](https://experimental-design.github.io/bofire/).\n",
    "\n",
    "\n",
    "### Defining the optimization inputs\n",
    "\n",
    "For the two continuous input variables of the Himmelblau's function *x1* and *x2*, we create two `ContinuousInput` features including boundaries following BoFire's terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.features.api import ContinuousInput\n",
    "\n",
    "\n",
    "input_feature_1 = ContinuousInput(key=\"x1\", bounds=[-5, 5])\n",
    "input_feature_2 = ContinuousInput(key=\"x2\", bounds=[-5, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on `Input` features, see the respective sections in our [docs](https://experimental-design.github.io/bofire/).\n",
    "\n",
    "\n",
    "### Defining the optimization domain\n",
    "\n",
    "In BoFire's terminology, `Domain` objects fully describe the search space of the optimization problem. `Input` and `Output` features are optionally bound with `Constraint` objects to specify allowed relationships between the parameters. Here, we will run an unconstrained optimization. For more details, see the respective sections in our [docs](https://experimental-design.github.io/bofire/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.domain.api import Domain, Inputs, Outputs\n",
    "\n",
    "\n",
    "domain = Domain(\n",
    "    inputs=Inputs(features=[input_feature_1, input_feature_2]),\n",
    "    outputs=Outputs(features=[output_feature]),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw candidates and execute experiments\n",
    "\n",
    "Let's define the Himmelblau's function to evaluate points in the domain space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau(x1, x2):\n",
    "    return (x1**2 + x2 - 11) ** 2 + (x1 + x2**2 - 7) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To initialize an iterative Bayesian optimization loop, let's first randomly draw 10 samples from the domain. In BoFire's terminology, those suggested samples are called `Candidates`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = domain.inputs.sample(10, seed=13)\n",
    "\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the function output for the randomly drawn candidates using the `himmelblau` function to obtain `Experiments` in BoFire's terminology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_output = candidates.apply(\n",
    "    lambda row: himmelblau(row[\"x1\"], row[\"x2\"]), axis=1\n",
    ")\n",
    "\n",
    "experiments = candidates.copy()\n",
    "experiments[\"y\"] = experimental_output\n",
    "\n",
    "print(experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more details on candidates and experiments, see the respective sections in our [docs](https://experimental-design.github.io/bofire/).\n",
    "\n",
    "\n",
    "### Defining an optimization strategy\n",
    "\n",
    "Let's specify the strategy how the Bayesian optimization campaign should be conducted. Here, we define a single-objective Bayesian optimization strategy and pass the optimization domain together with a acquisition function. Here, we use logarithmic expected improvement `qLogEI` as the acquisition function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.acquisition_functions.api import qLogEI\n",
    "from bofire.strategies.api import SoboStrategy\n",
    "\n",
    "\n",
    "sobo_strategy = SoboStrategy.make(domain=domain, acquisition_function=qLogEI(), seed=19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to separate BoFire into serializable parameters and a functional part. We call the serializable parameters usually data models.\n",
    "This is especially helpful when working with REST APIs.\n",
    "See the respective section in our [docs](https://experimental-design.github.io/bofire/data_models_functionals/).\n",
    "\n",
    "\n",
    "### Run the optimization loop\n",
    "\n",
    "To run the optimization loop using BoFire's terminology, we first `tell` the strategy object about the experiments we have already executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sobo_strategy.tell(experiments=experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the optimization loop for 30 iterations. In each iteration, we `ask` the strategy object to suggest one new candidate, which is returned as a list containing a single item. We then perform a new experiment by evaluating the Himmelblau function output of this candidate. After completing the experiment, we add the new data to our existing experiments and `tell` the strategy object about the updated dataset. This process is repeated for each of the 30 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(30):\n",
    "    new_candidates = sobo_strategy.ask(candidate_count=1)\n",
    "\n",
    "    new_experiments = new_candidates.copy()\n",
    "    new_experiments[\"y\"] = new_candidates.apply(\n",
    "        lambda row: himmelblau(row[\"x1\"], row[\"x2\"]), axis=1\n",
    "    )\n",
    "\n",
    "    sobo_strategy.tell(experiments=new_experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization behavior of the strategy is shown in the animated figure below. The four minima are marked red, the experiments carried out are marked blue with blue lines connecting them. The contours are indicating the predicted mean of the current model of each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/experimental-design/bofire/main/graphics/tutorials/himmelblau_optimization.gif\"\n",
    "    alt=\"Optimization of Himmelblau's function\"\n",
    "    width=\"300\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
