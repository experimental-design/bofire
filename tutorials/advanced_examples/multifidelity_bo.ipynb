{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {
    "papermill": {
     "duration": 2.980124,
     "end_time": "2024-10-10T20:34:18.797038",
     "exception": false,
     "start_time": "2024-10-10T20:34:15.816914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import bofire.strategies.api as strategies\n",
    "from bofire.benchmarks.api import Ackley, Benchmark, Branin\n",
    "from bofire.data_models.acquisition_functions.api import qLogEI\n",
    "from bofire.data_models.domain.api import Domain\n",
    "from bofire.data_models.features.api import TaskInput\n",
    "from bofire.data_models.surrogates.api import BotorchSurrogates, MultiTaskGPSurrogate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "papermill": {
     "duration": 0.262804,
     "end_time": "2024-10-10T20:34:19.062415",
     "exception": false,
     "start_time": "2024-10-10T20:34:18.799611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes import Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n",
    "NUM_INIT_HF = 4\n",
    "NUM_INIT_LF = 10\n",
    "if SMOKE_TEST:\n",
    "    num_runs = 5\n",
    "    num_iters = 2\n",
    "    verbose = False\n",
    "else:\n",
    "    num_runs = 10\n",
    "    num_iters = 10\n",
    "    verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "*This notebook is a sequel to \"Transfer Learning in BO\".*\n",
    "\n",
    "# Multi-fidelity Bayesian Optimization\n",
    "\n",
    "In the previous notebook, we saw how using low-fidelity approximations to our \n",
    "target function can improve the predictions from our surrogate model, leading to \n",
    "a faster optimization procedure. In this notebook, we show how we can gain even\n",
    "further performance gains by querying the cheap low-fidelity approximations during \n",
    "the BO loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Problem definition\n",
    "We use the same problem as the transfer learning notebook; optimizing the Branin\n",
    "benchmark, with a low-fidelity function biased by the Ackley function (with fewer initial\n",
    "points, to demonstrate the strength of being able to query low fidelities). Below, we\n",
    "define the problem domain, and the strategies we will use to optimize.\n",
    "\n",
    "As a baseline, we use the `SoboStrategy` with the `MultiTaskSurrogate`, as in the \n",
    "previous notebook. We also introduce the `MultiFidelityStrategy` here, which uses \n",
    "the same surrogate, but is able to query the lower fidelity functions using a \n",
    "variance-based acquisition function [Kandasamy et al. 2016, Folch et al. 2023].\n",
    "\n",
    "Both strategies first select a design point $x$ by optimizing the target fidelity. \n",
    "The `MultiFidelityStrategy` then selects the fidelity, $m$, by selecting the \n",
    "lowest fidelity that has a variance over a fixed threshold. This means that the \n",
    "strategy will explore the cheapest fidelities first, and only query the expensive \n",
    "fidelities when there is no information to be gained by the cheap approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraninMultiTask(Benchmark):\n",
    "    def __init__(self, low_fidelity_allowed=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._branin = Branin()\n",
    "        self._ackley = Ackley()\n",
    "        task_input = TaskInput(\n",
    "            key=\"task\",\n",
    "            categories=[\"task_hf\", \"task_lf\"],\n",
    "            allowed=[True, low_fidelity_allowed],\n",
    "            fidelities=[0, 1],\n",
    "        )\n",
    "        self._domain = Domain(\n",
    "            inputs=self._branin.domain.inputs + (task_input,),\n",
    "            outputs=self._branin.domain.outputs,\n",
    "        )\n",
    "\n",
    "    def _f(self, candidates: pd.DataFrame) -> pd.DataFrame:\n",
    "        candidates_no_task = candidates.drop(columns=[\"task\"])\n",
    "        f_branin = self._branin.f(candidates_no_task)\n",
    "        f_ackley = self._ackley.f(candidates_no_task)\n",
    "        bias_scale = np.where(candidates[\"task\"] == \"task_hf\", 0.0, 0.15).reshape(-1, 1)\n",
    "        bias_scale = pd.DataFrame(bias_scale, columns=self._domain.outputs.get_keys())\n",
    "        bias_scale[\"valid_y\"] = 0.0\n",
    "        return f_branin + bias_scale * f_ackley\n",
    "\n",
    "    def get_optima(self) -> pd.DataFrame:\n",
    "        optima = self._branin.get_optima()\n",
    "        optima[\"task\"] = \"task_hf\"\n",
    "        return optima\n",
    "\n",
    "\n",
    "mf_benchmark = BraninMultiTask(low_fidelity_allowed=True)\n",
    "tl_benchmark = BraninMultiTask(low_fidelity_allowed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set(seed: int):\n",
    "    # use the tl_benchmark to sample without the low fidelity\n",
    "    experiments = tl_benchmark.domain.inputs.sample(\n",
    "        NUM_INIT_HF + NUM_INIT_LF, seed=seed\n",
    "    )\n",
    "    experiments[\"task\"] = np.where(\n",
    "        experiments.index < NUM_INIT_LF, \"task_lf\", \"task_hf\"\n",
    "    )\n",
    "\n",
    "    # then use the ml_benchmark to evaluate the low fidelity\n",
    "    return mf_benchmark.f(experiments, return_complete=True)\n",
    "\n",
    "\n",
    "create_data_set(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.strategies.api import MultiFidelityStrategy\n",
    "\n",
    "\n",
    "# It isn't necessary to define the surrogate specs here, as the MFStrategy\n",
    "# will use a MultiTaskGP by default.\n",
    "\n",
    "mf_data_model = MultiFidelityStrategy(\n",
    "    domain=mf_benchmark.domain,\n",
    "    acquisition_function=qLogEI(),\n",
    "    fidelity_thresholds=0.1,\n",
    ")\n",
    "mf_data_model.surrogate_specs.surrogates[0].inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bofire.data_models.strategies.api import SoboStrategy\n",
    "\n",
    "\n",
    "surrogate_specs = BotorchSurrogates(\n",
    "    surrogates=[\n",
    "        MultiTaskGPSurrogate(\n",
    "            inputs=tl_benchmark.domain.inputs,\n",
    "            outputs=tl_benchmark.domain.outputs,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "tl_data_model = SoboStrategy(\n",
    "    domain=tl_benchmark.domain,\n",
    "    acquisition_function=qLogEI(),\n",
    "    surrogate_specs=surrogate_specs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Multi-fidelity Bayesian Optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We first optimize only on the target fidelity (the \"Transfer Learning\" baseline). \n",
    "This uses the `SoboStrategy` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tl_results = pd.DataFrame(columns=pd.MultiIndex.from_tuples([], names=(\"col\", \"run\")))\n",
    "for run in range(num_runs):\n",
    "    seed = 2048 * run + 123\n",
    "    experiments = create_data_set(seed)\n",
    "\n",
    "    tl_strategy = strategies.map(tl_data_model)\n",
    "    tl_strategy.tell(experiments)\n",
    "\n",
    "    assert tl_strategy.experiments is not None\n",
    "\n",
    "    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n",
    "    for _ in pbar:\n",
    "        candidate = tl_strategy.ask(1)\n",
    "        y = tl_benchmark.f(candidate, return_complete=True)\n",
    "        tl_strategy.tell(y)\n",
    "\n",
    "        hf_experiments = tl_strategy.experiments[\n",
    "            tl_strategy.experiments[\"task\"] == \"task_hf\"\n",
    "        ]\n",
    "        regret = hf_experiments[\"y\"].min() - tl_benchmark.get_optima()[\"y\"][0].item()\n",
    "\n",
    "        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n",
    "\n",
    "    tl_results[\"fidelity\", f\"{run}\"] = tl_strategy.experiments[\"task\"]\n",
    "    tl_results[\"y\", f\"{run}\"] = tl_strategy.experiments[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We now repeat the experiment using multi-fidelity BO, allowing the strategy to query \n",
    "the low fidelity function as well as the high fidelity function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mf_results = pd.DataFrame(columns=pd.MultiIndex.from_tuples([], names=(\"col\", \"run\")))\n",
    "for run in range(num_runs):\n",
    "    seed = 2048 * run + 123\n",
    "    experiments = create_data_set(seed)\n",
    "\n",
    "    mf_strategy = strategies.map(mf_data_model)\n",
    "    mf_strategy.tell(experiments)\n",
    "\n",
    "    assert mf_strategy.experiments is not None\n",
    "\n",
    "    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n",
    "    for _ in pbar:\n",
    "        candidate = mf_strategy.ask(1)\n",
    "        y = mf_benchmark.f(candidate, return_complete=True)\n",
    "        mf_strategy.tell(y)\n",
    "\n",
    "        hf_experiments = mf_strategy.experiments[\n",
    "            mf_strategy.experiments[\"task\"] == \"task_hf\"\n",
    "        ]\n",
    "        regret = hf_experiments[\"y\"].min() - mf_benchmark.get_optima()[\"y\"][0].item()\n",
    "\n",
    "        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n",
    "\n",
    "    mf_results[\"fidelity\", f\"{run}\"] = mf_strategy.experiments[\"task\"]\n",
    "    mf_results[\"y\", f\"{run}\"] = mf_strategy.experiments[\"y\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "When evaluating the performance, we need to consider how cheap the low-fidelity (LF) is \n",
    "to query. When the LF has the same cost as the target fidelity, then we\n",
    "gain very little from the multi-fidelity approach. However, if the LF is cheaper \n",
    "than the target (in the example below, 3x cheaper) then we observe an improvement in \n",
    "BO performance.\n",
    "\n",
    "Specifically, although both strategies have a budget of 10 function queries, the MF \n",
    "approach uses some of them on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_regret(\n",
    "    ax: Axes, bo_results: pd.DataFrame, fidelity_cost_ratio: float, **plot_kwargs\n",
    "):\n",
    "    cummin = (\n",
    "        bo_results[\"y\"]\n",
    "        .where(bo_results[\"fidelity\"] == \"task_hf\", other=np.inf)\n",
    "        .cummin(axis=0)\n",
    "    )\n",
    "    # only select iterations, and the final training point\n",
    "    cummin = cummin.iloc[-num_iters - 1 :]\n",
    "    regret: np.ndarray = (cummin - mf_benchmark.get_optima()[\"y\"][0].item()).to_numpy()\n",
    "\n",
    "    # keep track of \"real time\", where low fidelities are cheaper to evaluate.\n",
    "    time_taken = np.where(bo_results[\"fidelity\"] == \"task_hf\", fidelity_cost_ratio, 1)[\n",
    "        -num_iters - 1 :\n",
    "    ].cumsum(axis=0)\n",
    "    time_taken -= time_taken[0, 0]  # start from T=0 after training data\n",
    "    iterations = np.arange(num_iters * fidelity_cost_ratio)\n",
    "    before_time = time_taken[:, :, np.newaxis] <= iterations[np.newaxis, np.newaxis, :]\n",
    "    regret_before_time = regret[:, :, np.newaxis] * np.where(before_time, 1.0, np.inf)\n",
    "    # regret_before_time.shape == (num_iters+1, num_runs, len(iterations))\n",
    "    # project into time dimension\n",
    "    regret = regret_before_time.min(axis=0)\n",
    "\n",
    "    ax.plot(\n",
    "        iterations,\n",
    "        np.median(regret, axis=0),\n",
    "        label=plot_kwargs.get(\"label\"),\n",
    "        color=plot_kwargs.get(\"color\"),\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        iterations,\n",
    "        np.quantile(regret, 0.75, axis=0),\n",
    "        np.quantile(regret, 0.25, axis=0),\n",
    "        color=plot_kwargs.get(\"color\"),\n",
    "        alpha=0.2,\n",
    "    )\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(8, 4), sharey=True)\n",
    "cost_ratios = (1, 3)\n",
    "\n",
    "for ax, cost_ratio in zip(axs, cost_ratios):\n",
    "    plot_regret(\n",
    "        ax,\n",
    "        tl_results,\n",
    "        fidelity_cost_ratio=cost_ratio,\n",
    "        label=\"Transfer Learning\",\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    plot_regret(\n",
    "        ax,\n",
    "        mf_results,\n",
    "        fidelity_cost_ratio=cost_ratio,\n",
    "        label=\"Multi-fidelity\",\n",
    "        color=\"green\",\n",
    "    )\n",
    "    ax.set_xlabel(\"Time step\")\n",
    "    ax.set_title(f\"Fidelity cost ratio = {cost_ratio}\")\n",
    "\n",
    "\n",
    "axs[1].legend()\n",
    "axs[0].set_ylabel(\"Regret\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We can see that allowing the lower fidelities to be queries leads to stronger optimization performance. \n",
    "We can also see below that the MF approach only samples the target fidelity in later \n",
    "iterations, once the variance of the LF has been sufficiently reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mf_results[\"fidelity\"] == \"task_hf\")[-num_iters:].mean(axis=1)  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.535794,
   "end_time": "2024-10-10T20:34:20.632563",
   "environment_variables": {},
   "exception": true,
   "parameters": {},
   "start_time": "2024-10-10T20:34:15.096769",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
