[
  {
    "objectID": "docs/reference/surrogates.api.html",
    "href": "docs/reference/surrogates.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/surrogates.api.html#classes",
    "href": "docs/reference/surrogates.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nSurrogate\n\n\n\nTrainableSurrogate\nMixin for surrogates that can be trained. Concrete subclasses must also\n\n\nSingleTaskGPSurrogate\n\n\n\nMultiTaskGPSurrogate\n\n\n\nBotorchSurrogates\n\n\n\nLinearDeterministicSurrogate\n\n\n\nEmpiricalSurrogate\nAll necessary functions has to be implemented in the model which can then be loaded\n\n\nRandomForestSurrogate\nBoFire Random Forest model.\n\n\nMLPEnsemble\n\n\n\nPiecewiseLinearGPSurrogate\n\n\n\n\n\nSurrogate\nsurrogates.api.Surrogate(data_model)\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\nis_fitted\nReturn True if model is fitted, else False.\n\n\nre_init_kwargs\nthis method will return properties of the class, which are constructed during initialization, and can be\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ndumps\nDumps the actual model to a string as this is not directly json serializable.\n\n\nloads\nLoads the actual model from a string and writes it to the model attribute.\n\n\n\n\ndumps\nsurrogates.api.Surrogate.dumps()\nDumps the actual model to a string as this is not directly json serializable.\n\n\nloads\nsurrogates.api.Surrogate.loads(data)\nLoads the actual model from a string and writes it to the model attribute.\n\n\n\n\nTrainableSurrogate\nsurrogates.api.TrainableSurrogate()\nMixin for surrogates that can be trained. Concrete subclasses must also inherit from :class:Surrogate, which provides inputs, outputs, and predict.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ncross_validate\nPerform a cross validation for the provided training data.\n\n\nfit\nFit the surrogate model to the provided experiments.\n\n\n\n\ncross_validate\nsurrogates.api.TrainableSurrogate.cross_validate(\n    experiments,\n    folds=-1,\n    include_X=False,\n    include_labcodes=False,\n    random_state=None,\n    stratified_feature=None,\n    group_split_column=None,\n    hooks=None,\n    hook_kwargs=None,\n)\nPerform a cross validation for the provided training data.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nData on which the cross validation should be performed.\nrequired\n\n\nfolds\nint\nNumber of folds. -1 is equal to LOO CV. Defaults to -1.\n-1\n\n\ninclude_X\nbool\nIf true the X values of the fold are written to respective CvResult objects for later analysis. Defaults to False.\nFalse\n\n\nrandom_state\nint\nControls the randomness of the indices in the train and test sets of each fold. Defaults to None.\nNone\n\n\nstratified_feature\nstr\nThe feature name to preserve the percentage of samples for each class in the stratified folds. Defaults to None.\nNone\n\n\ngroup_split_column\nstr\nThe column name of the group id. This parameter is used to ensure that the splits are made such that the same group is not present in both training and testing sets. This is useful in scenarios where data points are related or dependent on each other, and splitting them into different sets would violate the assumption of independence. The number of unique groups must be greater than or equal to the number of folds. Defaults to None.\nNone\n\n\nhooks\nDict[str, Callable[[Model, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame], Any]]\nDictionary of callable hooks that are called within the CV loop. The callable retrieves the current trained modeld and the current CV folds in the following order: X_train, y_train, X_test, y_test. Defaults to {}.\nNone\n\n\nhook_kwargs\nDict[str, Dict[str, Any]]\nDictionary holding hook specific keyword arguments. Defaults to {}.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[CvResults, CvResults, Dict[str, List[Any]]]\nTuple[CvResults, CvResults, Dict[str, List[Any]]]: First CvResults object reflects the training data, second CvResults object the test data, dictionary object holds the return values of the applied hooks.\n\n\n\n\n\n\nfit\nsurrogates.api.TrainableSurrogate.fit(experiments, options=None)\nFit the surrogate model to the provided experiments.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nThe experimental data to fit the model.\nrequired\n\n\noptions\nOptional[Dict]\nAdditional options for fitting the model. Defaults to None.\nNone\n\n\n\n\n\n\n\n\nSingleTaskGPSurrogate\nsurrogates.api.SingleTaskGPSurrogate(data_model, **kwargs)\n\n\nMultiTaskGPSurrogate\nsurrogates.api.MultiTaskGPSurrogate(data_model, **kwargs)\n\n\nBotorchSurrogates\nsurrogates.api.BotorchSurrogates(data_model, re_init_kwargs=None, **kwargs)\n\n\nLinearDeterministicSurrogate\nsurrogates.api.LinearDeterministicSurrogate(data_model, **kwargs)\n\n\nEmpiricalSurrogate\nsurrogates.api.EmpiricalSurrogate(data_model, **kwargs)\nAll necessary functions has to be implemented in the model which can then be loaded from cloud pickle.\n\nAttributes\n\n\n\nName\nType\nDescription\n\n\n\n\nmodel\nDeterministicModel\nBotorch model instance.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nloads\nLoads the actual model from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nloads\nsurrogates.api.EmpiricalSurrogate.loads(data)\nLoads the actual model from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nRandomForestSurrogate\nsurrogates.api.RandomForestSurrogate(data_model, **kwargs)\nBoFire Random Forest model.\nThe same hyperparameters are available as for the wrapped sklearn RandomForestRegreesor.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nloads\nLoads the actual random forest from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nloads\nsurrogates.api.RandomForestSurrogate.loads(data)\nLoads the actual random forest from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nMLPEnsemble\nsurrogates.api.MLPEnsemble(data_model, **kwargs)\n\n\nPiecewiseLinearGPSurrogate\nsurrogates.api.PiecewiseLinearGPSurrogate(data_model, **kwargs)"
  },
  {
    "objectID": "docs/reference/index.html",
    "href": "docs/reference/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/index.html#data-models---domain",
    "href": "docs/reference/index.html#data-models---domain",
    "title": "",
    "section": "Data Models - Domain",
    "text": "Data Models - Domain\nDomain configuration data models\n\n\n\ndata_models.domain.domain.Domain\n\n\n\ndata_models.domain.features.Inputs\nContainer of input features, only input features are allowed.\n\n\ndata_models.domain.features.Outputs\nContainer of output features, only output features are allowed.\n\n\ndata_models.domain.constraints.Constraints"
  },
  {
    "objectID": "docs/reference/index.html#data-models---features",
    "href": "docs/reference/index.html#data-models---features",
    "title": "",
    "section": "Data Models - Features",
    "text": "Data Models - Features\nFeature data models\n\n\n\ndata_models.features.api"
  },
  {
    "objectID": "docs/reference/index.html#data-models---strategies",
    "href": "docs/reference/index.html#data-models---strategies",
    "title": "",
    "section": "Data Models - Strategies",
    "text": "Data Models - Strategies\nStrategy configuration data models\n\n\n\ndata_models.strategies.api"
  },
  {
    "objectID": "docs/reference/index.html#data-models---kernels",
    "href": "docs/reference/index.html#data-models---kernels",
    "title": "",
    "section": "Data Models - Kernels",
    "text": "Data Models - Kernels\nKernel configuration data models\n\n\n\ndata_models.kernels.api"
  },
  {
    "objectID": "docs/reference/index.html#data-models---surrogates",
    "href": "docs/reference/index.html#data-models---surrogates",
    "title": "",
    "section": "Data Models - Surrogates",
    "text": "Data Models - Surrogates\nSurrogate model configuration data models\n\n\n\ndata_models.surrogates.api"
  },
  {
    "objectID": "docs/reference/index.html#strategies",
    "href": "docs/reference/index.html#strategies",
    "title": "",
    "section": "Strategies",
    "text": "Strategies\nStrategy implementations\n\n\n\nstrategies.api"
  },
  {
    "objectID": "docs/reference/index.html#surrogates",
    "href": "docs/reference/index.html#surrogates",
    "title": "",
    "section": "Surrogates",
    "text": "Surrogates\nSurrogate model implementations\n\n\n\nsurrogates.api"
  },
  {
    "objectID": "docs/reference/data_models.strategies.api.html",
    "href": "docs/reference/data_models.strategies.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.strategies.api.html#classes",
    "href": "docs/reference/data_models.strategies.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nDoEStrategy\n\n\n\nFactorialStrategy\nFactorial design strategy.\n\n\nFractionalFactorialStrategy\nFractional factorial design strategy.\n\n\nActiveLearningStrategy\nDatamodel for an ActiveLearningStrategy that focusses on pure exploration of the input space.\n\n\nBotorchStrategy\n\n\n\nEntingStrategy\n\n\n\nMoboStrategy\n\n\n\nMultiFidelityStrategy\n\n\n\nMultiobjectiveStrategy\n\n\n\nAdditiveSoboStrategy\n\n\n\nCustomSoboStrategy\n\n\n\n\n\nDoEStrategy\ndata_models.strategies.api.DoEStrategy()\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\nreturn_fixed_candidates\nDatamodel for strategy for design of experiments. This strategy is used to generate a set of\n\n\n\n\n\n\nFactorialStrategy\ndata_models.strategies.api.FactorialStrategy()\nFactorial design strategy.\nThis strategy is deprecated, please use FractionalFactorialStrategy instead.\n\n\nFractionalFactorialStrategy\ndata_models.strategies.api.FractionalFactorialStrategy()\nFractional factorial design strategy.\nThis strategy generates a fractional factorial two level design for the continuous part of the domain, which is then combined with the categorical part of the domain. For every categorical combination, the continuous part of the design is repeated.\n\n\nActiveLearningStrategy\ndata_models.strategies.api.ActiveLearningStrategy()\nDatamodel for an ActiveLearningStrategy that focusses on pure exploration of the input space. This type of strategy chooses new candidate points in order to minimize the uncertainty.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_feature_implemented\nMethod to check if a specific feature type is implemented for the strategy\n\n\nis_objective_implemented\nMethod to check if a objective type is implemented for the strategy\n\n\n\n\nis_feature_implemented\ndata_models.strategies.api.ActiveLearningStrategy.is_feature_implemented(\n    my_type,\n)\nMethod to check if a specific feature type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Feature]\nFeature class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the feature type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nis_objective_implemented\ndata_models.strategies.api.ActiveLearningStrategy.is_objective_implemented(\n    my_type,\n)\nMethod to check if a objective type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Objective]\nObjective class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the objective type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\n\n\nBotorchStrategy\ndata_models.strategies.api.BotorchStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_constraint_implemented\nMethod to check if a specific constraint type is implemented for the strategy. For optimizer-specific\n\n\nvalidate_multitask_allowed\nEnsures that if a multitask model is used there is only a single allowed task category\n\n\nvalidate_outlier_detection_specs_for_domain\nEnsures that a outlier_detection model is specified for each output feature\n\n\nvalidate_surrogate_specs\nEnsures that a prediction model is specified for each output feature\n\n\n\n\nis_constraint_implemented\ndata_models.strategies.api.BotorchStrategy.is_constraint_implemented(my_type)\nMethod to check if a specific constraint type is implemented for the strategy. For optimizer-specific strategies, this is passed to the optimizer check.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Constraint]\nConstraint class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the constraint type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nvalidate_multitask_allowed\ndata_models.strategies.api.BotorchStrategy.validate_multitask_allowed()\nEnsures that if a multitask model is used there is only a single allowed task category\n\n\nvalidate_outlier_detection_specs_for_domain\ndata_models.strategies.api.BotorchStrategy.validate_outlier_detection_specs_for_domain(\n)\nEnsures that a outlier_detection model is specified for each output feature\n\n\nvalidate_surrogate_specs\ndata_models.strategies.api.BotorchStrategy.validate_surrogate_specs()\nEnsures that a prediction model is specified for each output feature\n\n\n\n\nEntingStrategy\ndata_models.strategies.api.EntingStrategy()\n\n\nMoboStrategy\ndata_models.strategies.api.MoboStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_feature_implemented\nMethod to check if a specific feature type is implemented for the strategy\n\n\nis_objective_implemented\nMethod to check if a objective type is implemented for the strategy\n\n\nvalidate_ref_point\nValidate that the provided refpoint matches the provided domain.\n\n\n\n\nis_feature_implemented\ndata_models.strategies.api.MoboStrategy.is_feature_implemented(my_type)\nMethod to check if a specific feature type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Feature]\nFeature class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the feature type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nis_objective_implemented\ndata_models.strategies.api.MoboStrategy.is_objective_implemented(my_type)\nMethod to check if a objective type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Objective]\nObjective class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the objective type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nvalidate_ref_point\ndata_models.strategies.api.MoboStrategy.validate_ref_point()\nValidate that the provided refpoint matches the provided domain.\n\n\n\n\nMultiFidelityStrategy\ndata_models.strategies.api.MultiFidelityStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_multitask_allowed\nOverwrites BotorchSurrogate.validate_multitask_allowed, as multiple tasks are allowed.\n\n\nvalidate_only_one_target_fidelity\nEnsures that there is only one target fidelity (task where fidelity==0).\n\n\nvalidate_surrogate_specs\nEnsures that a multi-task model is specified for each output feature\n\n\nvalidate_tasks_and_fidelity_thresholds\nEnsures that there is one threshold per fidelity\n\n\n\n\nvalidate_multitask_allowed\ndata_models.strategies.api.MultiFidelityStrategy.validate_multitask_allowed()\nOverwrites BotorchSurrogate.validate_multitask_allowed, as multiple tasks are allowed.\n\n\nvalidate_only_one_target_fidelity\ndata_models.strategies.api.MultiFidelityStrategy.validate_only_one_target_fidelity(\n)\nEnsures that there is only one target fidelity (task where fidelity==0).\n\n\nvalidate_surrogate_specs\ndata_models.strategies.api.MultiFidelityStrategy.validate_surrogate_specs()\nEnsures that a multi-task model is specified for each output feature\n\n\nvalidate_tasks_and_fidelity_thresholds\ndata_models.strategies.api.MultiFidelityStrategy.validate_tasks_and_fidelity_thresholds(\n)\nEnsures that there is one threshold per fidelity\n\n\n\n\nMultiobjectiveStrategy\ndata_models.strategies.api.MultiobjectiveStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_domain_is_multiobjective\nValidate that the domain is multiobjective.\n\n\n\n\nvalidate_domain_is_multiobjective\ndata_models.strategies.api.MultiobjectiveStrategy.validate_domain_is_multiobjective(\n    v,\n)\nValidate that the domain is multiobjective.\n\n\n\n\nAdditiveSoboStrategy\ndata_models.strategies.api.AdditiveSoboStrategy()\n\n\nCustomSoboStrategy\ndata_models.strategies.api.CustomSoboStrategy()"
  },
  {
    "objectID": "docs/reference/data_models.features.api.html",
    "href": "docs/reference/data_models.features.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.features.api.html#classes",
    "href": "docs/reference/data_models.features.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nCategoricalInput\nBase class for all categorical input features.\n\n\nCategoricalOutput\n\n\n\nContinuousInput\nBase class for all continuous input features.\n\n\nContinuousOutput\nThe base class for a continuous output feature\n\n\nDiscreteInput\nFeature with discretized ordinal values allowed in the optimization.\n\n\nContinuousDescriptorInput\nClass for continuous input features with descriptors\n\n\nCategoricalDescriptorInput\nClass for categorical input features with descriptors\n\n\nContinuousMolecularInput\n\n\n\nCategoricalMolecularInput\n\n\n\nTaskInput\n\n\n\n\n\nCategoricalInput\ndata_models.features.api.CategoricalInput()\nBase class for all categorical input features.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncategories\nList[str]\nNames of the categories.\n\n\nallowed\nList[bool]\nList of bools indicating if a category is allowed within the optimization.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfixed_value\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\n\nfrom_dummy_encoding\nConvert points back from dummy encoding.\n\n\nfrom_onehot_encoding\nConverts values back from one-hot encoding.\n\n\nfrom_ordinal_encoding\nConvertes values back from ordinal encoding.\n\n\ngenerate_allowed\nGenerates the list of allowed categories if not provided.\n\n\nget_allowed_categories\nReturns the allowed categories.\n\n\nget_forbidden_categories\nReturns the non-allowed categories\n\n\nget_possible_categories\nReturn the superset of categories that have been used in the experimental dataset and\n\n\nis_fixed\nReturns True if there is only one allowed category.\n\n\nis_fulfilled\nMethod to check if the values are all allowed categories.\n\n\nsample\nDraw random samples from the feature.\n\n\nto_dummy_encoding\nConverts values to a dummy-hot encoding, dropping the first categorical level.\n\n\nto_onehot_encoding\nConverts values to a one-hot encoding.\n\n\nto_ordinal_encoding\nConverts values to an ordinal integer based encoding.\n\n\nvalidate_candidental\nMethod to validate the suggested candidates\n\n\nvalidate_experimental\nMethod to validate the experimental dataFrame\n\n\n\n\nfixed_value\ndata_models.features.api.CategoricalInput.fixed_value(transform_type=None)\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[List[str], List[float], None]\nList[str]: List of categories or None\n\n\n\n\n\n\nfrom_dummy_encoding\ndata_models.features.api.CategoricalInput.from_dummy_encoding(values)\nConvert points back from dummy encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDummy-hot encoded values.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf one-hot columns not present in values.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nfrom_onehot_encoding\ndata_models.features.api.CategoricalInput.from_onehot_encoding(values)\nConverts values back from one-hot encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nOne-hot encoded values.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf one-hot columns not present in values.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nfrom_ordinal_encoding\ndata_models.features.api.CategoricalInput.from_ordinal_encoding(values)\nConvertes values back from ordinal encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nOrdinal encoded series.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\ngenerate_allowed\ndata_models.features.api.CategoricalInput.generate_allowed(allowed, info)\nGenerates the list of allowed categories if not provided.\n\n\nget_allowed_categories\ndata_models.features.api.CategoricalInput.get_allowed_categories()\nReturns the allowed categories.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nlist of str: The allowed categories\n\n\n\n\n\n\nget_forbidden_categories\ndata_models.features.api.CategoricalInput.get_forbidden_categories()\nReturns the non-allowed categories\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[str]: List of the non-allowed categories\n\n\n\n\n\n\nget_possible_categories\ndata_models.features.api.CategoricalInput.get_possible_categories(values)\nReturn the superset of categories that have been used in the experimental dataset and that can be used in the optimization\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries with the values for this feature\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nlist\nlist\nlist of possible categories\n\n\n\n\n\n\nis_fixed\ndata_models.features.api.CategoricalInput.is_fixed()\nReturns True if there is only one allowed category.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\n[bool]: True if there is only one allowed category\n\n\n\n\n\n\nis_fulfilled\ndata_models.features.api.CategoricalInput.is_fulfilled(values)\nMethod to check if the values are all allowed categories.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA series with values for the input feature.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA series with boolean values indicating if the input feature is fulfilled.\n\n\n\n\n\n\nsample\ndata_models.features.api.CategoricalInput.sample(n, seed=None)\nDraw random samples from the feature.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nnumber of samples.\nrequired\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: drawn samples.\n\n\n\n\n\n\nto_dummy_encoding\ndata_models.features.api.CategoricalInput.to_dummy_encoding(values)\nConverts values to a dummy-hot encoding, dropping the first categorical level.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries to be transformed.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dummy-hot transformed data frame.\n\n\n\n\n\n\nto_onehot_encoding\ndata_models.features.api.CategoricalInput.to_onehot_encoding(values)\nConverts values to a one-hot encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries to be transformed.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: One-hot transformed data frame.\n\n\n\n\n\n\nto_ordinal_encoding\ndata_models.features.api.CategoricalInput.to_ordinal_encoding(values)\nConverts values to an ordinal integer based encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries to be transformed.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Ordinal encoded values.\n\n\n\n\n\n\nvalidate_candidental\ndata_models.features.api.CategoricalInput.validate_candidental(values)\nMethod to validate the suggested candidates\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with candidates\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen not all values for a feature are one of the allowed categories\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The passed dataFrame with candidates\n\n\n\n\n\n\nvalidate_experimental\ndata_models.features.api.CategoricalInput.validate_experimental(\n    values,\n    strict=False,\n)\nMethod to validate the experimental dataFrame\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with experiments\nrequired\n\n\nstrict\nbool\nBoolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen an entry is not in the list of allowed categories\n\n\n\nValueError\nwhen there is no variation in a feature provided by the experimental data\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: A dataFrame with experiments\n\n\n\n\n\n\n\n\nCategoricalOutput\ndata_models.features.api.CategoricalOutput()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_objective_categories\nValidates that objective categories match the output categories\n\n\n\n\nvalidate_objective_categories\ndata_models.features.api.CategoricalOutput.validate_objective_categories()\nValidates that objective categories match the output categories\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen categories do not match objective categories\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nself\n\n\n\n\n\n\n\n\nContinuousInput\ndata_models.features.api.ContinuousInput()\nBase class for all continuous input features.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbounds\nTuple[float, float]\nA tuple that stores the lower and upper bound of the feature.\n\n\nstepsize\nPositiveFloat\nFloat indicating the allowed stepsize between lower and upper. Defaults to None.\n\n\nlocal_relative_bounds\nTuple[float, float]\nA tuple that stores the lower and upper bounds relative to a reference value. Defaults to None.\n\n\nallow_zero\nbool\nA boolean indicating if the input feature can take inactive values. Useful for features that take values between bounds, but can also take a value of 0. One may choose to use a conditional kernel for this, if taking a value of 0 represents a distinct behaviour from non-zero values.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_fulfilled\nMethod to check if the values are within the bounds of the feature.\n\n\nround\nRound values to the stepsize of the feature. If no stepsize is provided return the\n\n\nsample\nDraw random samples from the feature.\n\n\nvalidate_candidental\nMethod to validate the suggested candidates\n\n\n\n\nis_fulfilled\ndata_models.features.api.ContinuousInput.is_fulfilled(values, noise=1e-05)\nMethod to check if the values are within the bounds of the feature.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA series with values for the input feature.\nrequired\n\n\nnoise\nfloat\nA small value to allow for numerical errors. Defaults to 10e-6.\n1e-05\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA series with boolean values indicating if the input feature is fulfilled.\n\n\n\n\n\n\nround\ndata_models.features.api.ContinuousInput.round(values)\nRound values to the stepsize of the feature. If no stepsize is provided return the provided values.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nThe values that should be rounded.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The rounded values\n\n\n\n\n\n\nsample\ndata_models.features.api.ContinuousInput.sample(n, seed=None)\nDraw random samples from the feature.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nnumber of samples.\nrequired\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: drawn samples.\n\n\n\n\n\n\nvalidate_candidental\ndata_models.features.api.ContinuousInput.validate_candidental(values)\nMethod to validate the suggested candidates\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with candidates\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen non numerical values are passed\n\n\n\nValueError\nwhen values are larger than the upper bound of the feature\n\n\n\nValueError\nwhen values are lower than the lower bound of the feature\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The passed dataFrame with candidates\n\n\n\n\n\n\n\n\nContinuousOutput\ndata_models.features.api.ContinuousOutput()\nThe base class for a continuous output feature\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nobjective\nobjective\nobjective of the feature indicating in which direction it should be optimized. Defaults to MaximizeObjective.\n\n\n\n\n\n\nDiscreteInput\ndata_models.features.api.DiscreteInput()\nFeature with discretized ordinal values allowed in the optimization.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nkey(str)\n\nkey of the feature.\n\n\nvalues(List[float])\n\nthe discretized allowed values during the optimization.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfrom_continuous\nRounds continuous values to the closest discrete ones.\n\n\nis_fulfilled\nMethod to check if the values are close to the discrete values.\n\n\nsample\nDraw random samples from the feature.\n\n\nvalidate_candidental\nMethod to validate the provided candidates.\n\n\nvalidate_values_unique\nValidates that provided values are unique.\n\n\n\n\nfrom_continuous\ndata_models.features.api.DiscreteInput.from_continuous(values)\nRounds continuous values to the closest discrete ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDataframe with continuous entries.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with discrete values.\n\n\n\n\n\n\nis_fulfilled\ndata_models.features.api.DiscreteInput.is_fulfilled(values)\nMethod to check if the values are close to the discrete values.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA series with values for the input feature.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA series with boolean values indicating if the input feature is fulfilled.\n\n\n\n\n\n\nsample\ndata_models.features.api.DiscreteInput.sample(n, seed=None)\nDraw random samples from the feature.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nnumber of samples.\nrequired\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: drawn samples.\n\n\n\n\n\n\nvalidate_candidental\ndata_models.features.api.DiscreteInput.validate_candidental(values)\nMethod to validate the provided candidates.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nsuggested candidates for the feature\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nRaises error when one of the provided values is not contained in the list of allowed values.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: suggested candidates for the feature\n\n\n\n\n\n\nvalidate_values_unique\ndata_models.features.api.DiscreteInput.validate_values_unique(values)\nValidates that provided values are unique.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nList[float]\nList of values\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen values are non-unique.\n\n\n\nValueError\nwhen values contains only one entry.\n\n\n\nValueError\nwhen values is empty.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[values]: Sorted list of values\n\n\n\n\n\n\n\n\nContinuousDescriptorInput\ndata_models.features.api.ContinuousDescriptorInput()\nClass for continuous input features with descriptors\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlower_bound\nfloat\nLower bound of the feature in the optimization.\n\n\nupper_bound\nfloat\nUpper bound of the feature in the optimization.\n\n\ndescriptors\nList[str]\nNames of the descriptors.\n\n\nvalues\nList[float]\nValues of the descriptors.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nto_df\nTabular overview of the feature as DataFrame\n\n\nvalidate_list_lengths\nCompares the length of the defined descriptors list with the provided values\n\n\n\n\nto_df\ndata_models.features.api.ContinuousDescriptorInput.to_df()\nTabular overview of the feature as DataFrame\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: tabular overview of the feature as DataFrame\n\n\n\n\n\n\nvalidate_list_lengths\ndata_models.features.api.ContinuousDescriptorInput.validate_list_lengths()\nCompares the length of the defined descriptors list with the provided values\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nDict\nDictionary with all attributes\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen the number of descriptors does not math the number of provided values\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nDict\n\nDict with the attributes\n\n\n\n\n\n\n\n\nCategoricalDescriptorInput\ndata_models.features.api.CategoricalDescriptorInput()\nClass for categorical input features with descriptors\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncategories\nList[str]\nNames of the categories.\n\n\nallowed\nList[bool]\nList of bools indicating if a category is allowed within the optimization.\n\n\ndescriptors\nList[str]\nList of strings representing the names of the descriptors.\n\n\nvalues\nList[List[float]]\nList of lists representing the descriptor values.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfixed_value\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\n\nfrom_descriptor_encoding\nConverts values back from descriptor encoding.\n\n\nfrom_df\nCreates a feature from a dataframe\n\n\nto_descriptor_encoding\nConverts values to descriptor encoding.\n\n\nto_df\nTabular overview of the feature as DataFrame\n\n\nvalidate_experimental\nMethod to validate the experimental dataFrame\n\n\nvalidate_values\nValidates the compatibility of passed values for the descriptors and the defined categories\n\n\n\n\nfixed_value\ndata_models.features.api.CategoricalDescriptorInput.fixed_value(\n    transform_type=None,\n)\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[List[str], List[float], None]\nList[str]: List of categories or None\n\n\n\n\n\n\nfrom_descriptor_encoding\ndata_models.features.api.CategoricalDescriptorInput.from_descriptor_encoding(\n    values,\n)\nConverts values back from descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDescriptor encoded dataframe.\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf descriptor columns not found in the dataframe.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nfrom_df\ndata_models.features.api.CategoricalDescriptorInput.from_df(key, df)\nCreates a feature from a dataframe\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nstr\nThe name of the feature\nrequired\n\n\ndf\npd.DataFrame\nCategories as rows and descriptors as columns\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\ntype\n\ndescription\n\n\n\n\n\n\nto_descriptor_encoding\ndata_models.features.api.CategoricalDescriptorInput.to_descriptor_encoding(\n    values,\n)\nConverts values to descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nValues to transform.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Descriptor encoded dataframe.\n\n\n\n\n\n\nto_df\ndata_models.features.api.CategoricalDescriptorInput.to_df()\nTabular overview of the feature as DataFrame\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\npd.DataFrame: tabular overview of the feature as DataFrame\n\n\n\n\n\n\nvalidate_experimental\ndata_models.features.api.CategoricalDescriptorInput.validate_experimental(\n    values,\n    strict=False,\n)\nMethod to validate the experimental dataFrame\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with experiments\nrequired\n\n\nstrict\nbool\nBoolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen an entry is not in the list of allowed categories\n\n\n\nValueError\nwhen there is no variation in a feature provided by the experimental data\n\n\n\nValueError\nwhen no variation is present or planned for a given descriptor\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: A dataFrame with experiments\n\n\n\n\n\n\nvalidate_values\ndata_models.features.api.CategoricalDescriptorInput.validate_values(v, info)\nValidates the compatibility of passed values for the descriptors and the defined categories\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nList[List[float]]\nNested list with descriptor values\nrequired\n\n\nvalues\nDict\nDictionary with attributes\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen values have different length than categories\n\n\n\nValueError\nwhen rows in values have different length than descriptors\n\n\n\nValueError\nwhen a descriptor shows no variance in the data\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[List[float]]: Nested list with descriptor values\n\n\n\n\n\n\n\n\nContinuousMolecularInput\ndata_models.features.api.ContinuousMolecularInput()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_smiles\nValidates that molecule is a valid smiles. Note that this check can only\n\n\n\n\nvalidate_smiles\ndata_models.features.api.ContinuousMolecularInput.validate_smiles(v)\nValidates that molecule is a valid smiles. Note that this check can only be executed when rdkit is available.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nstr\nsmiles\nrequired\n\n\n\n\n\n\n\n\nCategoricalMolecularInput\ndata_models.features.api.CategoricalMolecularInput()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfrom_descriptor_encoding\nConverts values back from descriptor encoding.\n\n\nselect_mordred_descriptors\nFilter Mordred descriptors by removing highly correlated ones.\n\n\nto_descriptor_encoding\nConverts values to descriptor encoding.\n\n\nvalidate_smiles\nValidates that categories are valid smiles. Note that this check can only\n\n\n\n\nfrom_descriptor_encoding\ndata_models.features.api.CategoricalMolecularInput.from_descriptor_encoding(\n    transform_type,\n    values,\n)\nConverts values back from descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDescriptor encoded dataframe.\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf descriptor columns not found in the dataframe.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nselect_mordred_descriptors\ndata_models.features.api.CategoricalMolecularInput.select_mordred_descriptors(\n    transform_type,\n    cutoff=0.95,\n)\nFilter Mordred descriptors by removing highly correlated ones.\nThis function removes descriptors with zero variance and then iteratively filters out descriptors that are highly correlated with already selected ones. Uses a greedy algorithm that iteratively selects descriptors and removes those that are highly correlated with the selected ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntransform_type\nMordredDescriptors\nMordredDescriptors object containing the initial list of descriptors\nrequired\n\n\ncutoff\nAnnotated[float, Field(ge=0.0, le=1.0)]\nAbsolute correlation threshold above which descriptors are considered redundant. Range: [0.0, 1.0]. Default: 0.95\n0.95\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf no descriptors with non-zero variance are found\n\n\n\n\n\n\nto_descriptor_encoding\ndata_models.features.api.CategoricalMolecularInput.to_descriptor_encoding(\n    transform_type,\n    values,\n)\nConverts values to descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nValues to transform.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Descriptor encoded dataframe.\n\n\n\n\n\n\nvalidate_smiles\ndata_models.features.api.CategoricalMolecularInput.validate_smiles(categories)\nValidates that categories are valid smiles. Note that this check can only be executed when rdkit is available.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncategories\nList[str]\nList of smiles\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen string is not a smiles\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[str]: List of the smiles\n\n\n\n\n\n\n\n\nTaskInput\ndata_models.features.api.TaskInput()"
  },
  {
    "objectID": "docs/reference/data_models.domain.features.Inputs.html",
    "href": "docs/reference/data_models.domain.features.Inputs.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.domain.features.Inputs.html#attributes",
    "href": "docs/reference/data_models.domain.features.Inputs.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nfeatures\nList(Inputs\nlist of the features."
  },
  {
    "objectID": "docs/reference/data_models.domain.features.Inputs.html#methods",
    "href": "docs/reference/data_models.domain.features.Inputs.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nget_bounds\nReturns the boundaries of the optimization problem based on the transformations\n\n\nget_categorical_combinations\nGet a list of tuples pairing the feature keys with a list of valid categories\n\n\nget_feature_indices\nReturns a list of indices of the given feature key list.\n\n\nget_fixed\nGets all features in self that are fixed and returns them as new\n\n\nget_free\nGets all features in self that are not fixed and returns them as\n\n\nget_number_of_categorical_combinations\nGet the total number of unique categorical combinations.\n\n\ninverse_transform\nTransform a dataframe back to the original representations.\n\n\nis_fulfilled\nCheck if the provided experiments fulfill all constraints defined on the\n\n\nsample\nDraw sobol samples\n\n\ntransform\nTransform a dataframe to the representation specified in specs.\n\n\nvalidate_candidates\nValidate a pandas dataframe with input feature values.\n\n\n\n\nget_bounds\ndata_models.domain.features.Inputs.get_bounds(\n    specs,\n    experiments=None,\n    reference_experiment=None,\n)\nReturns the boundaries of the optimization problem based on the transformations defined in the specs dictionary.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\nexperiments\nOptional[pd.DataFrame]\nDataframe with input features. If provided the real feature bounds are returned based on both the opt. feature bounds and the extreme points in the dataframe. Defaults to None,\nNone\n\n\nreference_experiment\nOptional[pd.Series]\nIf a reference experiment provided,\nNone\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf a feature type is not known.\n\n\n\nValueError\nIf no transformation is provided for a categorical feature.\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[List[float], List[float]]\nTuple[List[float], List[float]]: list with lower bounds, list with upper bounds.\n\n\n\n\n\n\nget_categorical_combinations\ndata_models.domain.features.Inputs.get_categorical_combinations(\n    include=Input,\n    exclude=None,\n)\nGet a list of tuples pairing the feature keys with a list of valid categories\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude\nFeature\nFeatures to be included. Defaults to Input.\nInput\n\n\nexclude\nFeature\nFeatures to be excluded, e.g. subclasses of the included features. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[tuple[tuple[str, float] | tuple[str, str], …]]\nList[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str)\n\n\n\n\n\n\nget_feature_indices\ndata_models.domain.features.Inputs.get_feature_indices(specs, feature_keys)\nReturns a list of indices of the given feature key list.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\nfeature_keys\nList[str]\nList of feature keys.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[int]\nList[int]: The list of indices.\n\n\n\n\n\n\nget_fixed\ndata_models.domain.features.Inputs.get_fixed()\nGets all features in self that are fixed and returns them as new Inputs object.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nInputs\nInputs\nInput features object containing only fixed features.\n\n\n\n\n\n\nget_free\ndata_models.domain.features.Inputs.get_free()\nGets all features in self that are not fixed and returns them as new Inputs object.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nInputs\nInputs\nInput features object containing only non-fixed features.\n\n\n\n\n\n\nget_number_of_categorical_combinations\ndata_models.domain.features.Inputs.get_number_of_categorical_combinations(\n    include=Input,\n    exclude=None,\n)\nGet the total number of unique categorical combinations.\nThis is used before generating all of the categorical combinations, which may cause memory issues if there are too many.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude\nFeature\nFeatures to be included. Defaults to Input.\nInput\n\n\nexclude\nFeature\nFeatures to be excluded, e.g. subclasses of the included features. Defaults to None.\nNone\n\n\n\nReturns: int: Returns the number of unique combinations of discrete and categorical features.\n\n\n\ninverse_transform\ndata_models.domain.features.Inputs.inverse_transform(experiments, specs)\nTransform a dataframe back to the original representations.\nThe original applied transformation has to be provided via the specs dictionary. Currently only input categoricals are supported.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nTransformed data dataframe.\nrequired\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Back transformed dataframe. Only input features are included.\n\n\n\n\n\n\nis_fulfilled\ndata_models.domain.features.Inputs.is_fulfilled(experiments)\nCheck if the provided experiments fulfill all constraints defined on the input features itself like the bounds or the allowed categories.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with input features.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nSeries with boolean values indicating if the experiments fulfill the constraints on the input features.\n\n\n\n\n\n\nsample\ndata_models.domain.features.Inputs.sample(\n    n=1,\n    method=SamplingMethodEnum.UNIFORM,\n    seed=None,\n    sampler_kwargs=None,\n)\nDraw sobol samples\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of samples, has to be larger than 0. Defaults to 1.\n1\n\n\nmethod\nSamplingMethodEnum\nMethod to use, implemented methods are UNIFORM, SOBOL and LHS. Defaults to UNIFORM.\nSamplingMethodEnum.UNIFORM\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\nsampler_kwargs\nDict\nAdditional arguments for the sampler. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe containing the samples.\n\n\n\n\n\n\ntransform\ndata_models.domain.features.Inputs.transform(experiments, specs)\nTransform a dataframe to the representation specified in specs.\nCurrently only input categoricals are supported.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nData dataframe to be transformed.\nrequired\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Transformed dataframe. Only input features are included.\n\n\n\n\n\n\nvalidate_candidates\ndata_models.domain.features.Inputs.validate_candidates(candidates)\nValidate a pandas dataframe with input feature values.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.Dataframe\nInputs to validate.\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nRaises a Valueerror if a feature based validation raises an exception.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.Dataframe: Validated dataframe"
  },
  {
    "objectID": "docs/reference/data_models.domain.constraints.Constraints.html",
    "href": "docs/reference/data_models.domain.constraints.Constraints.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.domain.constraints.Constraints.html#methods",
    "href": "docs/reference/data_models.domain.constraints.Constraints.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nget\nGet constraints of the domain\n\n\nget_reps_df\nProvides a tabular overwiev of all constraints within the domain\n\n\nis_fulfilled\nCheck if all constraints are fulfilled on all rows of the provided dataframe\n\n\njacobian\nNumerically evaluate the jacobians of all constraints\n\n\n\n\nget\ndata_models.domain.constraints.Constraints.get(\n    includes=Constraint,\n    excludes=None,\n    exact=False,\n)\nGet constraints of the domain\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nincludes\nUnion[Type[CIncludes], Sequence[Type[CIncludes]]]\nConstraint class or list of specific constraint classes to be returned. Defaults to Constraint.\nConstraint\n\n\nexcludes\nOptional[Union[Type[CExcludes], List[Type[CExcludes]]]]\nConstraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\nNone\n\n\nexact\nbool\nBoolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nConstraints\nConstraints[CIncludes]\nconstraints in the domain fitting to the passed requirements.\n\n\n\n\n\n\nget_reps_df\ndata_models.domain.constraints.Constraints.get_reps_df()\nProvides a tabular overwiev of all constraints within the domain\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\npd.DataFrame: DataFrame listing all constraints of the domain with a description\n\n\n\n\n\n\nis_fulfilled\ndata_models.domain.constraints.Constraints.is_fulfilled(experiments, tol=1e-06)\nCheck if all constraints are fulfilled on all rows of the provided dataframe\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with data, the constraint validity should be tested on\nrequired\n\n\ntol\nfloat\ntolerance parameter. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 0.\n1e-06\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nBoolean\npd.Series\nTrue if all constraints are fulfilled for all rows, false if not\n\n\n\n\n\n\njacobian\ndata_models.domain.constraints.Constraints.jacobian(experiments)\nNumerically evaluate the jacobians of all constraints\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\ndata to evaluate the constraint jacobians on\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nlist\nlist\nA list containing the jacobians as pd.DataFrames"
  },
  {
    "objectID": "build/lib/docs/reference/strategies.api.html",
    "href": "build/lib/docs/reference/strategies.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/strategies.api.html#classes",
    "href": "build/lib/docs/reference/strategies.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nStrategy\nBase class for all strategies\n\n\nRandomStrategy\nStrategy for randomly selecting new candidates.\n\n\nDoEStrategy\nStrategy for design of experiments. This strategy is used to generate a set of\n\n\nFractionalFactorialStrategy\n\n\n\nSoboStrategy\n\n\n\nAdditiveSoboStrategy\n\n\n\nMultiplicativeSoboStrategy\n\n\n\nCustomSoboStrategy\n\n\n\nQparegoStrategy\n\n\n\nMoboStrategy\n\n\n\nBotorchStrategy\n\n\n\nEntingStrategy\nStrategy for selecting new candidates using ENTMOOT\n\n\nMultiFidelityStrategy\n\n\n\nActiveLearningStrategy\nActiveLearningStrategy that uses an acquisition function which focuses on\n\n\nShortestPathStrategy\n\n\n\nStepwiseStrategy\n\n\n\n\n\nStrategy\nstrategies.api.Strategy(data_model)\nBase class for all strategies\nAttributes:\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\ncandidates\nReturns the (pending) candidates of the strategy.\n\n\ndomain\nReturns the domain of the strategy.\n\n\nexperiments\nReturns the experiments of the strategy.\n\n\ninputs\nShortcut to access the inputs of the strategy’s domain.\n\n\nnum_candidates\nReturns number of (pending) candidates\n\n\nnum_experiments\nReturns number of experiments\n\n\noutputs\nShortcut to access the outputs of the strategy’s domain.\n\n\nseed\nReturns the seed of the strategy.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nadd_candidates\nAdd pending candidates to the strategy. Appends to existing ones.\n\n\nadd_experiments\nAdd experiments to the strategy. Appends to existing ones.\n\n\nask\nFunction to generate new candidates.\n\n\nfrom_spec\nUsed by the mapper to map from data model to functional strategy.\n\n\nhas_sufficient_experiments\nAbstract method to check if sufficient experiments are available.\n\n\npostprocess_candidates\nMethod to allow for postprocessing of candidates.\n\n\nreset_candidates\nResets the pending candidates of the strategy.\n\n\nset_candidates\nSet pending candidates of the strategy. Overwrites existing ones.\n\n\nset_experiments\nSet experiments of the strategy. Overwrites existing ones.\n\n\ntell\nThis function passes new experimental data to the optimizer\n\n\nto_candidates\nTransform candiadtes dataframe to a list of Candidate objects.\n\n\n\n\nadd_candidates\nstrategies.api.Strategy.add_candidates(candidates)\nAdd pending candidates to the strategy. Appends to existing ones.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with candidates.\nrequired\n\n\n\n\n\n\nadd_experiments\nstrategies.api.Strategy.add_experiments(experiments)\nAdd experiments to the strategy. Appends to existing ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experiments.\nrequired\n\n\n\n\n\n\nask\nstrategies.api.Strategy.ask(\n    candidate_count=None,\n    add_pending=False,\n    raise_validation_error=True,\n)\nFunction to generate new candidates.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidate_count\nPositiveInt\nNumber of candidates to be generated. If not provided, the number of candidates is determined automatically. Defaults to None.\nNone\n\n\nadd_pending\nbool\nIf true the proposed candidates are added to the set of pending experiments. Defaults to False.\nFalse\n\n\nraise_validation_error\nbool\nIf true an error will be raised if candidates violate constraints, otherwise only a warning will be displayed. Defaults to True.\nTrue\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nif candidate count is smaller than 1\n\n\n\nValueError\nif not enough experiments are available to execute the strategy\n\n\n\nValueError\nif the number of generated candidates does not match the requested number\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with candidates (proposed experiments)\n\n\n\n\n\n\nfrom_spec\nstrategies.api.Strategy.from_spec(data_model)\nUsed by the mapper to map from data model to functional strategy.\n\n\nhas_sufficient_experiments\nstrategies.api.Strategy.has_sufficient_experiments()\nAbstract method to check if sufficient experiments are available.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if number of passed experiments is sufficient, False otherwise\n\n\n\n\n\n\npostprocess_candidates\nstrategies.api.Strategy.postprocess_candidates(candidates)\nMethod to allow for postprocessing of candidates.\nBy default this methods applies the stepsize of continuous features if applicable.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\nDataFrame with candidates.\nrequired\n\n\n\nReturns: DataFrame with postprocessed candidates.\n\n\n\nreset_candidates\nstrategies.api.Strategy.reset_candidates()\nResets the pending candidates of the strategy.\n\n\nset_candidates\nstrategies.api.Strategy.set_candidates(candidates)\nSet pending candidates of the strategy. Overwrites existing ones.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with candidates.\nrequired\n\n\n\n\n\n\nset_experiments\nstrategies.api.Strategy.set_experiments(experiments)\nSet experiments of the strategy. Overwrites existing ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experiments.\nrequired\n\n\n\n\n\n\ntell\nstrategies.api.Strategy.tell(experiments, replace=False)\nThis function passes new experimental data to the optimizer\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataFrame with experimental data\nrequired\n\n\nreplace\nbool\nBoolean to decide if the experimental data should replace the former DataFrame or if the new experiments should be attached. Defaults to False.\nFalse\n\n\n\n\n\n\nto_candidates\nstrategies.api.Strategy.to_candidates(candidates)\nTransform candiadtes dataframe to a list of Candidate objects.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\ncandidates formatted as dataframe\nrequired\n\n\n\nReturns: List[Candidate]: candidates formatted as list of Candidate objects.\n\n\n\n\n\nRandomStrategy\nstrategies.api.RandomStrategy(data_model, **kwargs)\nStrategy for randomly selecting new candidates.\nThis strategy generates candidate samples using the random strategy. It first checks if the domain is compatible with polytope sampling. If so, it uses polytope sampling to generate candidate samples. If not, it performs rejection sampling by repeatedly generating candidates with polytope sampling until the desired number of valid samples is obtained.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_model\ndata_models.RandomStrategy\nThe data model for the random strategy.\nrequired\n\n\n**kwargs\n\nAdditional keyword arguments.\n{}\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nhas_sufficient_experiments\nCheck if there are sufficient experiments for the strategy.\n\n\nmake\nCreate a new instance of the RandomStrategy class.\n\n\n\n\nhas_sufficient_experiments\nstrategies.api.RandomStrategy.has_sufficient_experiments()\nCheck if there are sufficient experiments for the strategy.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if there are sufficient experiments, False otherwise.\n\n\n\n\n\n\nmake\nstrategies.api.RandomStrategy.make(\n    domain,\n    fallback_sampling_method=None,\n    n_burnin=None,\n    n_thinning=None,\n    num_base_samples=None,\n    max_iters=None,\n    seed=None,\n)\nCreate a new instance of the RandomStrategy class. Args: domain: The domain we randomly sample from. fallback_sampling_method: The fallback sampling method to use when the domain has no constraints. n_burnin: The number of burn-in samples for the polytope sampler. n_thinning: The thinning factor for the polytope sampler. num_base_samples: The number of base samples for rejection sampling. max_iters: The maximum number of iterations for rejection sampling. seed: The seed value for random number generation. Returns: RandomStrategy: A new instance of the RandomStrategy class.\n\n\n\n\nDoEStrategy\nstrategies.api.DoEStrategy(data_model, **kwargs)\nStrategy for design of experiments. This strategy is used to generate a set of experiments for a given domain. The experiments are generated via minimization of a user defined optimality criterion.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nhas_sufficient_experiments\nAbstract method to check if sufficient experiments are available.\n\n\nmake\nCreate a new design of experimence strategy instance.\n\n\n\n\nhas_sufficient_experiments\nstrategies.api.DoEStrategy.has_sufficient_experiments()\nAbstract method to check if sufficient experiments are available.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if number of passed experiments is sufficient, False otherwise\n\n\n\n\n\n\nmake\nstrategies.api.DoEStrategy.make(\n    domain,\n    seed=None,\n    criterion=None,\n    verbose=None,\n    ipopt_options=None,\n    scip_params=None,\n    use_hessian=None,\n    use_cyipopt=None,\n    sampling=None,\n    return_fixed_candidates=None,\n)\nCreate a new design of experimence strategy instance. Args: domain: The domain for the strategy. seed: Random seed for reproducibility. criterion: Optimality criterion for the strategy. Default is d-optimality. verbose: Verbosity level. ipopt_options: Options for IPOPT solver. IPOPT is used to minize the optimality criterion. scip_params: Parameters for SCIP solver. SCIP is used to for backprojection of discrete and categorical variables. use_hessian: Whether to use Hessian information. use_cyipopt: Whether to use cyipopt. sampling: Initial points for the strategy. return_fixed_candidates: Whether to return fixed candidates. Returns: DoEStrategy: A new instance of the DoEStrategy class.\n\n\n\n\nFractionalFactorialStrategy\nstrategies.api.FractionalFactorialStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreate a new instance of the strategy with the given parameters. This method will create the datamodel\n\n\nrandomize_design\nRandomize the run order of the design if self.randomize_runorder is True.\n\n\n\n\nmake\nstrategies.api.FractionalFactorialStrategy.make(\n    domain,\n    n_repetitions=None,\n    n_center=None,\n    block_feature_key=None,\n    generator=None,\n    n_generators=None,\n    randomize_runorder=None,\n    seed=None,\n)\nCreate a new instance of the strategy with the given parameters. This method will create the datamodel under the hood and pass it to the constructor of the strategy. Args: domain: The domain of the strategy. n_repetitions: The number of repetitions of the continuous part of the design. n_center: The number of center points in the continuous part of the design per block. block_feature_key: The feature key to use for blocking the design. generator: The generator for the continuous part of the design. n_generators: The number of reducing factors. randomize_runorder: If true, the run order is randomized, else it is deterministic. seed: The seed for the random number generator.\n\n\nrandomize_design\nstrategies.api.FractionalFactorialStrategy.randomize_design(design)\nRandomize the run order of the design if self.randomize_runorder is True.\n\n\n\n\nSoboStrategy\nstrategies.api.SoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates a single objective Bayesian optimization strategy.\n\n\n\n\nmake\nstrategies.api.SoboStrategy.make(\n    domain,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates a single objective Bayesian optimization strategy. Args: domain: The optimization domain of the strategy. acquisition_function: The acquisition function to use. acquisition_optimizer: The optimizer to use for the acquisition function. surrogate_specs: The specifications for the surrogate model. outlier_detection_specs: The specifications for the outlier detection. min_experiments_before_outlier_check: The minimum number of experiments before checking for outliers. frequency_check: The frequency of checking for outliers. frequency_hyperopt: The frequency of hyperparameter optimization. folds: The number of folds for cross-validation. seed: The random seed to use. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\n\n\n\n\nAdditiveSoboStrategy\nstrategies.api.AdditiveSoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates a Bayesian optimization strategy that adds multiple objectives.\n\n\n\n\nmake\nstrategies.api.AdditiveSoboStrategy.make(\n    domain,\n    use_output_constraints=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates a Bayesian optimization strategy that adds multiple objectives. The weights of the objectives are defines in the outputs of the domain. Args: domain: The optimization domain of the strategy. use_output_constraints: Whether to use output constraints. acquisition_function: The acquisition function to use. acquisition_optimizer: The optimizer to use for the acquisition function. surrogate_specs: The specifications for the surrogate model. outlier_detection_specs: The specifications for the outlier detection. min_experiments_before_outlier_check: The minimum number of experiments before checking for outliers. frequency_check: The frequency of checking for outliers. frequency_hyperopt: The frequency of hyperparameter optimization. folds: The number of folds for cross-validation for hyperparameter optimization. seed: The random seed to use. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\n\n\n\n\nMultiplicativeSoboStrategy\nstrategies.api.MultiplicativeSoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates Bayesian optimization strategy that multiplies multiple objectives. The weights of\n\n\n\n\nmake\nstrategies.api.MultiplicativeSoboStrategy.make(\n    domain,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates Bayesian optimization strategy that multiplies multiple objectives. The weights of the objectives are defines in the outputs of the domain. Args: domain: The optimization domain of the strategy. acquisition_function: The acquisition function to use. acquisition_optimizer: The optimizer to use for the acquisition function. surrogate_specs: The specifications for the surrogate model. outlier_detection_specs: The specifications for the outlier detection. min_experiments_before_outlier_check: The minimum number of experiments before checking for outliers. frequency_check: The frequency of checking for outliers. frequency_hyperopt: The frequency of hyperparameter optimization. folds: The number of folds for cross-validation for hyperparameter optimization. seed: The random seed to use. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\n\n\n\n\nCustomSoboStrategy\nstrategies.api.CustomSoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ndumps\nDumps the function to a string via pickle as this is not directly json serializable.\n\n\nloads\nLoads the function from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\nmake\nThe CustomSoboStrategy can be used to design custom objectives or objective combinations for optimizations.\n\n\n\n\ndumps\nstrategies.api.CustomSoboStrategy.dumps()\nDumps the function to a string via pickle as this is not directly json serializable.\n\n\nloads\nstrategies.api.CustomSoboStrategy.loads(data)\nLoads the function from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\nmake\nstrategies.api.CustomSoboStrategy.make(\n    domain,\n    use_output_constraints=None,\n    dump=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nThe CustomSoboStrategy can be used to design custom objectives or objective combinations for optimizations. In this tutorial notebook, it is shown how to use it to optimize a quantity that depends on a combination of an inferred quantity and one of the inputs. See tutorials/advanced_examples/custom_sobo.ipynb.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe optimization domain of the strategy.\nrequired\n\n\nuse_output_constraints\nbool | None\nWhether to use output constraints.\nNone\n\n\ndump\nstr | None\nThe function to use for the optimization.\nNone\n\n\nacquisition_function\nAnySingleObjectiveAcquisitionFunction | None\nThe acquisition function to use.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nThe optimizer to use for the acquisition function.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nThe specifications for the surrogate model.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nThe specifications for the outlier detection.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nThe minimum number of experiments before checking for outliers.\nNone\n\n\nfrequency_check\nPositiveInt | None\nThe frequency of checking for outliers.\nNone\n\n\nfrequency_hyperopt\nint | None\nThe frequency of hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nThe number of folds for cross-validation.\nNone\n\n\nseed\nint | None\nThe random seed to use.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\n\n\n\n\n\nQparegoStrategy\nstrategies.api.QparegoStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates an instance of the multi-objective strategy ParEGO using the provided configuration parameters.\n\n\n\n\nmake\nstrategies.api.QparegoStrategy.make(\n    domain,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates an instance of the multi-objective strategy ParEGO using the provided configuration parameters.\nJ. Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50-66, 2006\nS. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe optimization domain of the strategy.\nrequired\n\n\nacquisition_function\nqEI | qLogEI | qLogNEI | qNEI | None\nThe acquisition function to use.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nThe optimizer for the acquisition function.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nSpecifications for the surrogate model.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nSpecifications for outlier detection.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nMinimum number of experiments before checking for outliers.\nNone\n\n\nfrequency_check\nPositiveInt | None\nFrequency of outlier checks.\nNone\n\n\nfrequency_hyperopt\nint | None\nFrequency of hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nNumber of folds for cross-validation for hyperparameter optimization.\nNone\n\n\nseed\nint | None\nRandom seed for reproducibility.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\nReturns: An instance of the strategy configured with the provided parameters.\n\n\n\n\n\nMoboStrategy\nstrategies.api.MoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates an instance of a multi-objective strategy based on expected hypervolume improvement.\n\n\n\n\nmake\nstrategies.api.MoboStrategy.make(\n    domain,\n    ref_point=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates an instance of a multi-objective strategy based on expected hypervolume improvement.\nS. Daulton, M. Balandat, and E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Advances in Neural Information Processing Systems 34, 2021.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe domain specifying the search space.\nrequired\n\n\nref_point\nExplicitReferencePoint | Dict[str, float] | None\nReference point for hypervolume computation.\nNone\n\n\nacquisition_function\nAnyMultiObjectiveAcquisitionFunction | None\nAcquisition function.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nOptimizer for the acquisition function.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nSurrogate model specifications.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nOutlier detection configuration.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nMinimum number of experiments before performing outlier detection.\nNone\n\n\nfrequency_check\nPositiveInt | None\nFrequency at which to perform outlier checks.\nNone\n\n\nfrequency_hyperopt\nint | None\nFrequency at which to perform hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nNumber of folds for cross-validation for hyperparameter optimization.\nNone\n\n\nseed\nint | None\nRandom seed for reproducibility.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\nReturns: An instance of the strategy configured with the specified parameters.\n\n\n\n\n\nBotorchStrategy\nstrategies.api.BotorchStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ncalc_acquisition\nCalculate the acquisition value for a set of experiments.\n\n\nget_acqf_input_tensors\n\n\n\n\n\ncalc_acquisition\nstrategies.api.BotorchStrategy.calc_acquisition(candidates, combined=False)\nCalculate the acquisition value for a set of experiments.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\nDataframe with experimentes for which the acqf value should be calculated.\nrequired\n\n\ncombined\nbool\nIf combined an acquisition value for the whole batch is calculated, else individual ones. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Dataframe with the acquisition values.\n\n\n\n\n\n\nget_acqf_input_tensors\nstrategies.api.BotorchStrategy.get_acqf_input_tensors()\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nX_train\nTensor\nTensor of shape (n, d) with n training points and d input dimensions.\n\n\nX_pending\nTensor | None\nTensor of shape (m, d) with m pending points\n\n\n\n\n\n\n\n\nEntingStrategy\nstrategies.api.EntingStrategy(data_model, **kwargs)\nStrategy for selecting new candidates using ENTMOOT\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreate an enting strategy instance with the specified parameters.\n\n\n\n\nmake\nstrategies.api.EntingStrategy.make(\n    domain,\n    beta=None,\n    bound_coeff=None,\n    acq_sense=None,\n    dist_trafo=None,\n    dist_metric=None,\n    cat_metric=None,\n    kappa_fantasy=None,\n    num_boost_round=None,\n    max_depth=None,\n    min_data_in_leaf=None,\n    min_data_per_group=None,\n    verbose=None,\n    solver_name=None,\n    solver_verbose=None,\n    solver_params=None,\n    seed=None,\n)\nCreate an enting strategy instance with the specified parameters.\nhttps://github.com/cog-imperial/entmoot\nENTMOOT: A Framework for Optimization over Ensemble Tree Models A. Thebelt, J. Kronqvist, M. Mistry, R. Lee, N. Sudermann-Merx, R. Misener Computers & Chemical Engineering, 2021\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe domain object defining the problem space.\nrequired\n\n\nbeta\nPositiveFloat | None\nParameter controlling the trade-off in acquisition.\nNone\n\n\nbound_coeff\nPositiveFloat | None\nCoefficient for bounding constraints.\nNone\n\n\nacq_sense\nLiteral['exploration', 'penalty'] | None\nAcquisition sense, either “exploration” or “penalty”.\nNone\n\n\ndist_trafo\nLiteral['normal', 'standard'] | None\nTransformation applied to distances, either “normal” or “standard”.\nNone\n\n\ndist_metric\nLiteral['euclidean_squared', 'l1', 'l2'] | None\nMetric used for distance calculations, e.g., “euclidean_squared”, “l1”, or “l2”.\nNone\n\n\ncat_metric\nLiteral['overlap', 'of', 'goodall4'] | None\nMetric for categorical variables, e.g., “overlap”, “of”, or “goodall4”.\nNone\n\n\nkappa_fantasy\nfloat | None\nKappa parameter for fantasy strategy for batch proposals.\nNone\n\n\nnum_boost_round\nPositiveInt | None\nNumber of boosting rounds for the model.\nNone\n\n\nmax_depth\nPositiveInt | None\nMaximum depth of the model.\nNone\n\n\nmin_data_in_leaf\nPositiveInt | None\nMinimum data points required in a leaf.\nNone\n\n\nmin_data_per_group\nPositiveInt | None\nMinimum data points required per group.\nNone\n\n\nverbose\nLiteral[-1, 0, 1, 2] | None\nVerbosity level of the process.\nNone\n\n\nsolver_name\nstr | None\nName of the solver to be used.\nNone\n\n\nsolver_verbose\nbool | None\nWhether to enable verbose output for the solver.\nNone\n\n\nsolver_params\nDict[str, Any] | None\nAdditional parameters for the solver.\nNone\n\n\nseed\nint | None\nRandom seed for reproducibility.\nNone\n\n\n\nReturns: A strategy instance configured with the provided parameters.\n\n\n\n\n\nMultiFidelityStrategy\nstrategies.api.MultiFidelityStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreate a new instance of the multi-fidelity optimization strategy with the given parameters. This strategy\n\n\n\n\nmake\nstrategies.api.MultiFidelityStrategy.make(\n    domain,\n    fidelity_thresholds=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreate a new instance of the multi-fidelity optimization strategy with the given parameters. This strategy is useful if you have different measurement fidelities that measure the same thing with different cost and accuracy. As an example, you can have a simulation that is fast but inaccurate and the real experiment that is slow and expensive, but more accurate.\nK. Kandasamy, G. Dasarathy, J. B. Oliva, J. Schneider, B. Póczos. Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations. Advances in Neural Information Processing Systems, 29, 2016.\nJose Pablo Folch, Robert M Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark van der Wilk, Ruth Misener. Combining Multi-Fidelity Modelling and Asynchronous Batch Bayesian Optimization. Computers & Chemical Engineering Volume 172, 2023.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe optimization domain of the strategy.\nrequired\n\n\nfidelity_thresholds\nList[float] | float | None\nThe thresholds for the fidelity. If a single value is provided, it will be used for all fidelities.\nNone\n\n\nacquisition_function\nAnySingleObjectiveAcquisitionFunction | None\nThe acquisition function to use.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nThe acquisition optimizer to use.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nThe specifications for the surrogate model.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nThe specifications for the outlier detection.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nThe minimum number of experiments before checking for outliers.\nNone\n\n\nfrequency_check\nPositiveInt | None\nThe frequency of outlier checks.\nNone\n\n\nfrequency_hyperopt\nint | None\nThe frequency of hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nThe number of folds for cross-validation.\nNone\n\n\nseed\nint | None\nThe random seed to use.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\n\n\n\n\n\nActiveLearningStrategy\nstrategies.api.ActiveLearningStrategy(data_model, **kwargs)\nActiveLearningStrategy that uses an acquisition function which focuses on pure exploration of the objective function only. Can be used for single and multi-objective functions.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates an ActiveLearningStrategy instance. ActiveLearningStrategy that uses an acquisition function which focuses on\n\n\n\n\nmake\nstrategies.api.ActiveLearningStrategy.make(\n    domain,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    acquisition_function=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates an ActiveLearningStrategy instance. ActiveLearningStrategy that uses an acquisition function which focuses on pure exploration of the objective function only. Can be used for single and multi-objective functions. Args: domain: Domain of the strategy. acquisition_optimizer: Acquisition optimizer to use. surrogate_specs: Surrogate specifications. outlier_detection_specs: Outlier detection specifications. min_experiments_before_outlier_check: Minimum number of experiments before checking for outliers. frequency_check: Frequency of outlier checks. frequency_hyperopt: Frequency of hyperparameter optimization. folds: Number of folds for cross-validation in hyperparameter optimization. acquisition_function: Acquisition function to use. seed: Seed for the random number generator. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function. Returns: ActiveLearningStrategy: An instance of the ActiveLearningStrategy class.\n\n\n\n\nShortestPathStrategy\nstrategies.api.ShortestPathStrategy(data_model, **kwargs)\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\ncontinuous_inputs\nReturns the continuous inputs from the domain.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nget_linear_constraints\nReturns the linear constraints in the form of matrices A and b, where Ax = b for\n\n\nhas_sufficient_experiments\nChecks if there are sufficient experiments available.\n\n\nmake\nRepresents a strategy for finding the shortest path between two points\n\n\nstep\nTakes a starting point and returns the next step in the shortest path.\n\n\n\n\nget_linear_constraints\nstrategies.api.ShortestPathStrategy.get_linear_constraints(constraints)\nReturns the linear constraints in the form of matrices A and b, where Ax = b for equality constraints and Ax &lt;= b for inequality constraints.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nconstraints\nConstraints\nThe Constraints object containing the linear constraints.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, np.ndarray]\nTuple[np.ndarray, np.ndarray]: A tuple containing the matrices A and b.\n\n\n\n\n\n\nhas_sufficient_experiments\nstrategies.api.ShortestPathStrategy.has_sufficient_experiments()\nChecks if there are sufficient experiments available.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if there are sufficient experiments, False otherwise.\n\n\n\n\n\n\nmake\nstrategies.api.ShortestPathStrategy.make(\n    domain,\n    start=None,\n    end=None,\n    atol=None,\n    seed=None,\n)\nRepresents a strategy for finding the shortest path between two points Args: start: The starting point of the path. end: The ending point of the path. atol: The absolute tolerance used for numerical comparisons.\n\n\nstep\nstrategies.api.ShortestPathStrategy.step(start)\nTakes a starting point and returns the next step in the shortest path.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\npd.Series\nThe starting point for the shortest path.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The next step in the shortest path.\n\n\n\n\n\n\n\n\nStepwiseStrategy\nstrategies.api.StepwiseStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nget_step\nReturns the strategy at the current step and the corresponding transform if given.\n\n\nmake\nCreate a StepwiseStrategy from a list of steps. Each step is a strategy the runs until a\n\n\n\n\nget_step\nstrategies.api.StepwiseStrategy.get_step()\nReturns the strategy at the current step and the corresponding transform if given.\n\n\nmake\nstrategies.api.StepwiseStrategy.make(domain, steps=None, seed=None)\nCreate a StepwiseStrategy from a list of steps. Each step is a strategy the runs until a condition is satisfied. One example of a stepwise strategy is is to start with a few random samples to gather initial data for subsequent Bayesian optimization. An example steps-list for two random experiments followed by a SoboStrategy is:\nfrom bofire.data_models.strategies.api import (\n    Step,\n    RandomStrategy,\n    SoboStrategy,\n    NumberOfExperimentsCondition,\n    AlwaysTrueCondition\n)\nfrom bofire.stratgies.api import StepwiseStrategy\nsteps = [\n    Step(\n        strategy_data=RandomStrategy(domain=domain),\n        condition=NumberOfExperimentsCondition(n_experiments=2),\n    ),\n    Step(\n        strategy_data=SoboStrategy(domain=domain), condition=AlwaysTrueCondition()\n    )\n]\nstepwise_strategy = StepwiseStrategy.make(domain, steps)\nAll passed domains need to compatible, i.e.,\n\nthey have the same number of features,\nthe same feature keys and\nthe features with the same key have the same type and categories.\nThe bounds and allowed categories of the features can vary.\n\nFurther, the data and domain are passed to the next step. They can also be transformed before being passed to the next step. Args: steps: List of steps to be used in the strategy. seed: Seed for random number generation."
  },
  {
    "objectID": "build/lib/docs/reference/data_models.surrogates.api.html",
    "href": "build/lib/docs/reference/data_models.surrogates.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.surrogates.api.html#classes",
    "href": "build/lib/docs/reference/data_models.surrogates.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nSurrogate\n\n\n\nSingleTaskGPSurrogate\n\n\n\nMixedSingleTaskGPSurrogate\n\n\n\nMultiTaskGPSurrogate\n\n\n\nRobustSingleTaskGPSurrogate\nRobust Relevance Pursuit Single Task Gaussian Process Surrogate.\n\n\nTanimotoGPSurrogate\n\n\n\nLinearSurrogate\n\n\n\nPolynomialSurrogate\n\n\n\nRandomForestSurrogate\n\n\n\nMLPEnsemble\n\n\n\nBotorchSurrogates\n“List of botorch surrogates.\n\n\n\n\nSurrogate\ndata_models.surrogates.api.Surrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.Surrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: outputs: objective functions for the surrogate my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nSingleTaskGPSurrogate\ndata_models.surrogates.api.SingleTaskGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.SingleTaskGPSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nMixedSingleTaskGPSurrogate\ndata_models.surrogates.api.MixedSingleTaskGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.MixedSingleTaskGPSurrogate.is_output_implemented(\n    my_type,\n)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nMultiTaskGPSurrogate\ndata_models.surrogates.api.MultiTaskGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.MultiTaskGPSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nRobustSingleTaskGPSurrogate\ndata_models.surrogates.api.RobustSingleTaskGPSurrogate()\nRobust Relevance Pursuit Single Task Gaussian Process Surrogate.\nA robust single-task GP that learns a data-point specific noise level and is therefore more robust to outliers. See: https://botorch.org/docs/tutorials/relevance_pursuit_robust_regression/ Paper: https://arxiv.org/pdf/2410.24222\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nprior_mean_of_support\nOptional[int]\nThe prior mean of the support.\n\n\nconvex_parametrization\nbool\nWhether to use convex parametrization of the sparse noise model.\n\n\ncache_model_trace\nbool\nWhether to cache the model trace. This needs no be set to True if you want to view the model trace after optimization.\n\n\n\n\n\nNote\nThe definition of “outliers” depends on the model capacity, so what is an outlier with respect to a simple model might not be an outlier with respect to a complex model. For this reason, it is necessary to bound the lengthscale of the GP kernel from below.\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.RobustSingleTaskGPSurrogate.is_output_implemented(\n    my_type,\n)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nTanimotoGPSurrogate\ndata_models.surrogates.api.TanimotoGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\nvalidate_moleculars\nChecks that at least one of fingerprints, fragments, or fingerprintsfragments features are present.\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.TanimotoGPSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\nvalidate_moleculars\ndata_models.surrogates.api.TanimotoGPSurrogate.validate_moleculars()\nChecks that at least one of fingerprints, fragments, or fingerprintsfragments features are present.\n\n\n\n\nLinearSurrogate\ndata_models.surrogates.api.LinearSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.LinearSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nPolynomialSurrogate\ndata_models.surrogates.api.PolynomialSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.PolynomialSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nRandomForestSurrogate\ndata_models.surrogates.api.RandomForestSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.RandomForestSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nMLPEnsemble\ndata_models.surrogates.api.MLPEnsemble()\n\n\nBotorchSurrogates\ndata_models.surrogates.api.BotorchSurrogates()\n“List of botorch surrogates.\nBehaves similar to a Surrogate."
  },
  {
    "objectID": "build/lib/docs/reference/data_models.kernels.api.html",
    "href": "build/lib/docs/reference/data_models.kernels.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.kernels.api.html#classes",
    "href": "build/lib/docs/reference/data_models.kernels.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nKernel\n\n\n\nAdditiveKernel\n\n\n\nMultiplicativeKernel\n\n\n\nScaleKernel\n\n\n\nMaternKernel\n\n\n\nRBFKernel\n\n\n\nLinearKernel\n\n\n\nPolynomialKernel\n\n\n\nHammingDistanceKernel\n\n\n\nTanimotoKernel\n\n\n\nWassersteinKernel\nKernel based on the Wasserstein distance.\n\n\n\n\nKernel\ndata_models.kernels.api.Kernel()\n\n\nAdditiveKernel\ndata_models.kernels.api.AdditiveKernel()\n\n\nMultiplicativeKernel\ndata_models.kernels.api.MultiplicativeKernel()\n\n\nScaleKernel\ndata_models.kernels.api.ScaleKernel()\n\n\nMaternKernel\ndata_models.kernels.api.MaternKernel()\n\n\nRBFKernel\ndata_models.kernels.api.RBFKernel()\n\n\nLinearKernel\ndata_models.kernels.api.LinearKernel()\n\n\nPolynomialKernel\ndata_models.kernels.api.PolynomialKernel()\n\n\nHammingDistanceKernel\ndata_models.kernels.api.HammingDistanceKernel()\n\n\nTanimotoKernel\ndata_models.kernels.api.TanimotoKernel()\n\n\nWassersteinKernel\ndata_models.kernels.api.WassersteinKernel()\nKernel based on the Wasserstein distance.\nIt only works for 1D data that is monotonically increasing, as it is just calculating the integral of the absolute difference between two shapes. Only when both shapes are monotonically increasing, this integral is also a Wasserstein distance (https://arxiv.org/abs/2002.01878).\nThe shape are assumed to be discretized as a set of points. Make sure that the discretization is fine enough to capture the shape of the data.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nsquared\nbool\nIf True, the squared exponential Wasserstein distance is used. Note that the squared exponential Wasserstein distance kernel is not positive definite for all lengthscales. For this reason, as default the absolute exponential Wasserstein distance is used.\n\n\nlengthscale_prior\nOptional[AnyPrior]\nPrior for the lengthscale of the kernel."
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.features.Outputs.html",
    "href": "build/lib/docs/reference/data_models.domain.features.Outputs.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.features.Outputs.html#attributes",
    "href": "build/lib/docs/reference/data_models.domain.features.Outputs.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nfeatures\nList(Outputs\nlist of the features."
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.features.Outputs.html#methods",
    "href": "build/lib/docs/reference/data_models.domain.features.Outputs.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nadd_valid_columns\nAdd the valid_{feature.key} columns to the experiments dataframe,\n\n\nget_by_objective\nGet output features filtered by the type of the attached objective.\n\n\nget_keys_by_objective\nGet keys of output features filtered by the type of the attached objective.\n\n\npreprocess_experiments_all_valid_outputs\nMethod to get a dataframe where non-valid entries of all output feature are removed\n\n\npreprocess_experiments_any_valid_output\nMethod to get a dataframe where at least one output feature has a valid entry\n\n\npreprocess_experiments_one_valid_output\nMethod to get a dataframe where non-valid entries of the provided output feature are removed\n\n\n\n\nadd_valid_columns\ndata_models.domain.features.Outputs.add_valid_columns(experiments)\nAdd the valid_{feature.key} columns to the experiments dataframe, in case that they are not present.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe holding the experiments.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe holding the experiments.\n\n\n\n\n\n\nget_by_objective\ndata_models.domain.features.Outputs.get_by_objective(\n    includes=Objective,\n    excludes=None,\n    exact=False,\n)\nGet output features filtered by the type of the attached objective.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nincludes\nUnion[List[TObjective], TObjective]\nObjective class or list of objective classes to be returned. Defaults to Objective.\nObjective\n\n\nexcludes\nUnion[List[TObjective], TObjective, None]\nObjective class or list of specific objective classes to be excluded from the return. Defaults to None.\nNone\n\n\nexact\nbool\nBoolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutputs\nList[AnyOutput]: List of output features fitting to the passed requirements.\n\n\n\n\n\n\nget_keys_by_objective\ndata_models.domain.features.Outputs.get_keys_by_objective(\n    includes=Objective,\n    excludes=None,\n    exact=False,\n)\nGet keys of output features filtered by the type of the attached objective.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nincludes\nUnion[List[TObjective], TObjective]\nObjective class or list of objective classes to be returned. Defaults to Objective.\nObjective\n\n\nexcludes\nUnion[List[TObjective], TObjective, None]\nObjective class or list of specific objective classes to be excluded from the return. Defaults to None.\nNone\n\n\nexact\nbool\nBoolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[str]\nList[str]: List of output feature keys fitting to the passed requirements.\n\n\n\n\n\n\npreprocess_experiments_all_valid_outputs\ndata_models.domain.features.Outputs.preprocess_experiments_all_valid_outputs(\n    experiments,\n    output_feature_keys=None,\n)\nMethod to get a dataframe where non-valid entries of all output feature are removed\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\noutput_feature_keys\nOptional[List]\nList of output feature keys which should be considered for removal of invalid values. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included\n\n\n\n\n\n\npreprocess_experiments_any_valid_output\ndata_models.domain.features.Outputs.preprocess_experiments_any_valid_output(\n    experiments,\n)\nMethod to get a dataframe where at least one output feature has a valid entry\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry\n\n\n\n\n\n\npreprocess_experiments_one_valid_output\ndata_models.domain.features.Outputs.preprocess_experiments_one_valid_output(\n    output_feature_key,\n    experiments,\n)\nMethod to get a dataframe where non-valid entries of the provided output feature are removed\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\noutput_feature_key\nstr\nThe feature based on which non-valid entries rows are removed\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.domain.Domain.html",
    "href": "build/lib/docs/reference/data_models.domain.domain.Domain.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.domain.Domain.html#attributes",
    "href": "build/lib/docs/reference/data_models.domain.domain.Domain.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ncandidate_column_names\nThe columns in the candidate dataframe\n\n\nconstraints\nRepresentation of the optimization problem/domain\n\n\nexperiment_column_names\nThe columns in the experimental dataframe"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.domain.Domain.html#methods",
    "href": "build/lib/docs/reference/data_models.domain.domain.Domain.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate_by_duplicates\nAggregate the dataframe by duplicate experiments\n\n\ncoerce_invalids\nCoerces all invalid output measurements to np.nan\n\n\ndescribe_experiments\nMethod to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n\nget_nchoosek_combinations\nGet all possible NChooseK combinations\n\n\nis_fulfilled\nCheck if all constraints are fulfilled on all rows of the provided dataframe\n\n\nvalidate_candidates\nMethod to check the validty of proposed candidates\n\n\nvalidate_constraints\nValidate that the constraints defined in the domain fit to the input features.\n\n\nvalidate_experiments\nChecks the experimental data on validity\n\n\nvalidate_unique_feature_keys\nValidates if provided input and output feature keys are unique\n\n\n\n\naggregate_by_duplicates\ndata_models.domain.domain.Domain.aggregate_by_duplicates(\n    experiments,\n    prec,\n    delimiter='-',\n    method='mean',\n)\nAggregate the dataframe by duplicate experiments\nDuplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe containing experimental data\nrequired\n\n\nprec\nint\nPrecision of the rounding of the continuous input features\nrequired\n\n\ndelimiter\nstr\nDelimiter used when combining the orig. labcodes to a new one. Defaults to “-”.\n'-'\n\n\nmethod\nLiteral['mean', 'median']\nWhich aggregation method to use. Defaults to “mean”.\n'mean'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, list]\nTuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates\n\n\n\n\n\n\ncoerce_invalids\ndata_models.domain.domain.Domain.coerce_invalids(experiments)\nCoerces all invalid output measurements to np.nan\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe containing experimental data\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: coerced dataframe\n\n\n\n\n\n\ndescribe_experiments\ndata_models.domain.domain.Domain.describe_experiments(experiments)\nMethod to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n\n\n\n\n\n\nget_nchoosek_combinations\ndata_models.domain.domain.Domain.get_nchoosek_combinations(exhaustive=False)\nGet all possible NChooseK combinations\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexhaustive\nbool\nif True all combinations are returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nTuple\n(used_features_list, unused_features_list)\nused_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination.\n\n\n\n\n\n\nis_fulfilled\ndata_models.domain.domain.Domain.is_fulfilled(\n    experiments,\n    tol=1e-06,\n    exlude_interpoint=True,\n)\nCheck if all constraints are fulfilled on all rows of the provided dataframe both constraints and inputs are checked.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with data, the constraint validity should be tested on\nrequired\n\n\ntol\nfloat\nTolerance for checking the constraints. Defaults to 1e-6.\n1e-06\n\n\nexlude_interpoint\nbool\nIf True, InterpointConstraints are excluded from the check. Defaults to True.\nTrue\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nBoolean series indicating if all constraints are fulfilled for all rows.\n\n\n\n\n\n\nvalidate_candidates\ndata_models.domain.domain.Domain.validate_candidates(\n    candidates,\n    only_inputs=False,\n    tol=1e-05,\n    raise_validation_error=True,\n)\nMethod to check the validty of proposed candidates\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\nDataframe with suggested new experiments (candidates)\nrequired\n\n\nonly_inputs\n(bool, optional)\nIf True, only the input columns are validated. Defaults to False.\nFalse\n\n\ntol\n(float, optional)\ntolerance parameter for constraints. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 1e-6.\n1e-05\n\n\nraise_validation_error\nbool\nIf true an error will be raised if candidates violate constraints, otherwise only a warning will be displayed. Defaults to True.\nTrue\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen a column is missing for a defined input feature\n\n\n\nValueError\nwhen a column is missing for a defined output feature\n\n\n\nValueError\nwhen a non-numerical value is proposed\n\n\n\nValueError\nwhen an additional column is found\n\n\n\nConstraintNotFulfilledError\nwhen the constraints are not fulfilled and raise_validation_error = True\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: dataframe with suggested experiments (candidates)\n\n\n\n\n\n\nvalidate_constraints\ndata_models.domain.domain.Domain.validate_constraints()\nValidate that the constraints defined in the domain fit to the input features.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nList[Constraint]\nList of constraints or empty if no constraints are defined\nrequired\n\n\nvalues\nList[Input]\nList of input features of the domain\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nFeature key in constraint is unknown.\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[Constraint]: List of constraints defined for the domain\n\n\n\n\n\n\nvalidate_experiments\ndata_models.domain.domain.Domain.validate_experiments(experiments, strict=False)\nChecks the experimental data on validity\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\nstrict\nbool\nBoolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nempty dataframe\n\n\n\nValueError\nthe column for a specific feature is missing the provided data\n\n\n\nValueError\nthere are labcodes with null value\n\n\n\nValueError\nthere are labcodes with nan value\n\n\n\nValueError\nlabcodes are not unique\n\n\n\nValueError\nthe provided columns do no match to the defined domain\n\n\n\nValueError\nthe provided columns do no match to the defined domain\n\n\n\nValueError\nInput with null values\n\n\n\nValueError\nInput with nan values\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: The provided dataframe with experimental data\n\n\n\n\n\n\nvalidate_unique_feature_keys\ndata_models.domain.domain.Domain.validate_unique_feature_keys()\nValidates if provided input and output feature keys are unique\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nOutputs\nList of all output features of the domain.\nrequired\n\n\nvalue\nDict[str, Inputs]\nDict containing a list of input features as single entry.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nFeature keys are not unique.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nOutputs\n\nKeeps output features as given."
  },
  {
    "objectID": "docs/userguides/surrogates.html",
    "href": "docs/userguides/surrogates.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Surrogate models"
    ]
  },
  {
    "objectID": "docs/userguides/surrogates.html#surrogate-model-options",
    "href": "docs/userguides/surrogates.html#surrogate-model-options",
    "title": "",
    "section": "Surrogate model options",
    "text": "Surrogate model options\nBoFire offers the following classes of surrogate models.\n\n\n\nSurrogate\nOptimization of\nWhen to use\nType\n\n\n\n\nSingleTaskGPSurrogate\na single objective with real valued inputs\nLimited data and black-box function is smooth\nGaussian process\n\n\nRandomForestSurrogate\na single objective\nRich data; black-box function does not have to be smooth\nsklearn random forest implementation\n\n\nMLP\na single objective with real-valued inputs\nRich data and black-box function is smooth\nMulti layer perceptron\n\n\nMixedSingleTaskGPSurrogate\na single objective with categorical and real valued inputs\nLimited data and black-box function is smooth\nGaussian process\n\n\nTanimotoGP\na single objective\nAt least one input feature is a molecule represented as fingerprint\nGaussian process on a molecule space for which Tanimoto similarity determines the similarity between points\n\n\n\nAll of these are single-objective surrogate models. For optimization of multiple objectives at the same time, a suitable Strategy has to be chosen. Then for each objective a different surrogate model can be specified. By default the SingleTaskGPSurrogate is used.\nExample:\nsurrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n)\nsurrogate_data_1 = RandomForestSurrogate(\n    inputs=domain.inputs,\n    outputs=Outputs(features=[domain.outputs[1]]),\n)\nqparego_data_model = QparegoStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data_0, surrogate_data_1]\n    ),\n)\nNote:\n\nThe standard Kernel for all Gaussian Process (GP) surrogates is a 5/2 matern kernel with automated relevance detection and normalization of the input features.\nThe RandomForestSurrogate does not have kernels but quantifies uncertainty using the standard deviation of the predictions of its individual trees.\nMLP quantifies uncertainty using the standard deviation of multiple predictions that come from different dropout rates (randomly setting neural network weights to zero).",
    "crumbs": [
      "Surrogate models"
    ]
  },
  {
    "objectID": "docs/userguides/surrogates.html#customization",
    "href": "docs/userguides/surrogates.html#customization",
    "title": "",
    "section": "Customization",
    "text": "Customization\nBoFire also offers the option to customize surrogate models. In particular, it is possible to customize the SingleTaskGPSurrogate in the following ways.\n\nKernel customization\nSpecify the Kernel:\n\n\n\nKernel\nDescription\nTranslation invariant\nInput variable type\n\n\n\n\nRBFKernel\nBased on Gaussian distribution\nYes\nContinuous\n\n\nMaternKernel\nBased on Gamma function; allows setting a smoothness parameter\nYes\nContinuous\n\n\nPolynomialKernel\nBased on dot-product of two vectors of input points\nNo\nContinuous\n\n\nLinearKernel\nEqual to dot-product of two vectors of input points\nNo\nContinuous\n\n\nTanimotoKernel\nMeasures similarities between binary vectors using Tanimoto Similarity\nNot applicable\nCategoricalMolecularInput\n\n\nHammingDistanceKernel\nSimilarity is defined by the Hamming distance which considers the number of equal entries between two vectors (e.g., in One-Hot-encoding)\nNot applicable\nCategorical\n\n\n\nTranslational invariance means that the similarity between two input points is not affected by shifting both points by the same amount but only determined by their distance. Example: with a translationally invariant kernel, the values 10 and 20 are equally similar to each other as the values 20 and 30, while with a polynomial kernel the latter pair has potentially higher similarity. Polynomial kernels are often suitable for high-dimensional inputs while for low-dimensional inputs an RBF or Matérn kernel is recommended.\nNote:\n\nSingleTaskGPSurrogate with PolynomialKernel is equivalent to PolynomialSurrogate.\nSingleTaskGPSurrogate with LinearKernel is equivalent to LinearSurrogate.\nSingleTaskGPSurrogate with TanimotoKernel is equivalent to TanimotoGP.\nOne can combine two Kernels by using AdditiveKernel or MultiplicativeKernel.\n\nExample:\nsurrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2)\n)\n\n\nNoise model customization\nFor experimental data subject to noise, one can specify the distribution of this noise. The options are:\n\n\n\nNoise Model\nWhen to use\n\n\n\n\nNormalPrior\nNoise is Gaussian\n\n\nGammaPrior\nNoise has a Gamma distribution\n\n\n\nExample:\nsurrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2),\n        noise_prior=NormalPrior(loc=0, scale=1)\n)",
    "crumbs": [
      "Surrogate models"
    ]
  },
  {
    "objectID": "docs/userguides/domain.html",
    "href": "docs/userguides/domain.html",
    "title": "Domains",
    "section": "",
    "text": "The BoFire domain contains all information about the optimization problem. In general, the domain is defined by inputs, outputs, objectives and constraints. Different types for each of these elements are implemented in the BoFire framework. A brief description of the usage of domains and the individual elements is given in the following.\nA basic example of a domain consists of two continuous inputs \\(x_1\\), \\(x_2\\) and a continuous output \\(y\\). If no objective is given, a maximization objective for the output is assumed. If no constraints are defined, an empty set of constraints is assumed.\nA continuous input can be defined using the ContinuousInput class, which requires the variable name and its bounds. Please note that unbounded and partially bounded input variables are currently not supported. Here, we assume \\(x_1, x_2 \\in [0,1]\\).\nfrom bofire.data_models.features.api import ContinuousInput\nfrom bofire.data_models.domain.api import Inputs\n\ninputs = Inputs(features=[\n    ContinuousInput(key=\"x1\", bounds=[0,1]),\n    ContinuousInput(key=\"x2\", bounds=[0,1])\n]\n)\nAnalogously, the continuous output is defined using the ContinuousOutput class.\nfrom bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.domain.api import Outputs\n\noutputs = Outputs(features=[\n    ContinuousOutput(key=\"y\")\n    ]\n)\nIn this case, the domain definition is as follows\nfrom bofire.data_models.domain.api import Domain\n\ndomain = Domain(\n    inputs=inputs,\n    outputs=outputs\n)\nDomains can also be created more concisely using the from_lists method of the Domain class. This is especially useful if you want to define a domain without having to create separate lists for inputs and outputs. The following code achieves the same result as above:\ndomain = Domain.from_lists(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=[0,1]),\n        ContinuousInput(key=\"x2\", bounds=[0,1])\n    ],\n    outputs=[\n        ContinuousOutput(key=\"y\")\n    ]\n)\nThere are applications of domains, where no optimization of an objective is involved (e.g. randomly sampling from the design space). In such cases, we do not require to provide an output definition.\nLet us now assume, we have an additional linear equality constraint and we want to use a custom objective, e.g. a minimization objective for \\(y\\). The linear constraint can be defined using the LinearEqualityConstraint class\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint\n\nconstraints = [LinearEqualityConstraint(features=[\"x1\", \"x2\"], coefficients=[1,1], rhs=1)]\nThe code above corresponds to the linear equation\n\\[\nx_1 + x_2 = 1.\n\\]\nIn BoFire, the minimization objective can be selected by setting the objective-attribute of an output to an instance of MinimizeObjective\nfrom bofire.data_models.objectives.api import MinimizeObjective\n\noutputs = [ContinuousOutput(key=\"y\", objective=MinimizeObjective())]\nThe modified domain in this case becomes\ndomain = Domain(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints\n)",
    "crumbs": [
      "Domains"
    ]
  },
  {
    "objectID": "docs/userguides/domain.html#inputs",
    "href": "docs/userguides/domain.html#inputs",
    "title": "Domains",
    "section": "Inputs",
    "text": "Inputs\nBoFire allows for the following different user-facing input classes:\n\nContinuousInput\nDiscreteInput\nCategoricalInput\nCategoricalDescriptorInput\nCategoricalMolecularInput\nTaskInput \n\nEach of these input classes can be used for defining domains, however some strategies only support a subset of the available input types. You can call the is_feature_implemented() function of a given strategy and input class to check whether the input is supported by the strategy. For example, the following code checks whether the ContinuousInput is supported by the RandomSearch strategy. \n\nfrom bofire.data_models.strategies.api import RandomStrategy\n\nRandomStrategy.is_feature_implemented(ContinuousInput)\n\nTrue\n\n\nThe result will be True if the input type is supported by the strategy, otherwise it will return False. This is useful to check whether a certain input type can be used with a specific strategy before defining the domain.\n\nInput types\n\nContinuous inputs\nContinuous inputs are used to define real-valued input variables with finite upper and lower bounds (see example above).\n\n\nDiscrete inputs\nFor discrete inputs, only a finite set of values is allowed. The DiscreteInput class requires a list of permissible values as input. For example, the following code defines a new discrete input variable \\(x_3\\) with values \\(0, 0.1, 0.2\\).\n\nfrom bofire.data_models.features.api import DiscreteInput\n\nDiscreteInput(key=\"x3\", values=[0, 0.1, 0.2])\n\nDiscreteInput(type='DiscreteInput', key='x3', unit=None, values=[0.0, 0.1, 0.2], rtol=1e-07)\n\n\n\n\nCategorical inputs\nThis class of inputs is similar to the discrete inputs, but takes a list of strings as input. The following code defines a new categorical input variable \\(x_4\\) with categories “A”, “B”, “C”.\n\nfrom bofire.data_models.features.api import CategoricalInput\n\nCategoricalInput(key=\"x4\", categories=[\"A\", \"B\", \"C\"])\n\nCategoricalInput(type='CategoricalInput', key='x4', categories=['A', 'B', 'C'], allowed=[True, True, True])\n\n\n\n\n\nCategorical descriptor inputs\nVia the CategoricalDescriptorInput one can provide continuous encodings for the different categories via so called descriptors. Imagine, for example, having a categorial input with different categories, where each category corresponds to a specific material. Such descriptors could be e.g., density and hardness. Every material/category would get assigned a number for density and hardness in the hope that these two properties describe the material properly. In the context of fitting a GP, one can then use just these two dimensional vector for describing the material instead of a ten dimensional one-hot encoding, which results in a dimensionality reduction. Of course, this makes sense under the assumption that the descriptors actually correlate with the desired quantities.\nThe CategoricalDescriptorInput class requires a list of permissible values as input. For example, the following code defines a new categorical descriptor input variable \\(x_6\\) with values “A”, “B”, “C”.\n\nfrom bofire.data_models.features.api import CategoricalDescriptorInput\n\nCategoricalDescriptorInput(\n    key=\"x6\",\n    categories=[\"material_A\", \"material_B\", \"material_C\"],\n    descriptors=[\"density\", \"hardness\"],\n    values=[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n)\n\nCategoricalDescriptorInput(type='CategoricalDescriptorInput', key='x6', categories=['material_A', 'material_B', 'material_C'], allowed=[True, True, True], descriptors=['density', 'hardness'], values=[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n\n\n\n\nCategorical molecular inputs\nCategorical molecular inputs inherit from categorical inputs. Its list of categories has to be a list of valid SMILES strings. The following code defines a new categorical molecular input variable \\(x_7\\) where only the values “C1CCCCC1”, “O1CCOCC1” are allowed.\n\nfrom bofire.data_models.features.api import CategoricalMolecularInput\n\nCategoricalMolecularInput(key=\"x7\", categories=[\"C1CCCCC1\", \"O1CCOCC1\"])\n\nCategoricalMolecularInput(type='CategoricalMolecularInput', key='x7', categories=['C1CCCCC1', 'O1CCOCC1'], allowed=[True, True])\n\n\n\n\n\n\nInputs class\nThe Inputs class is used to collect multiple input variables. It is used to define the inputs of a domain. The following code defines a new input class with the above described input variables \\(x_1, x_2, x_3, x_4, x_5, x_6, x_7\\).\n\nfrom bofire.data_models.api import Inputs\n\ninputs = Inputs(\n    features=[ContinuousInput(key=\"x1\", bounds=[0,1]),\n    ContinuousInput(key=\"x2\", bounds=[0,1]),\n    DiscreteInput(key=\"x3\", values=[0, 0.1, 0.2]),\n    CategoricalInput(key=\"x4\", categories=[\"A\", \"B\", \"C\"]),\n    CategoricalDescriptorInput(key=\"x6\", categories=[\"material_A\", \"material_B\", \"material_C\"], descriptors=[\"density\", \"hardness\"], values=[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n    CategoricalMolecularInput(key=\"x7\", categories=[\"C1CCCCC1\", \"O1CCOCC1\"])])",
    "crumbs": [
      "Domains"
    ]
  },
  {
    "objectID": "docs/userguides/domain.html#outputs",
    "href": "docs/userguides/domain.html#outputs",
    "title": "Domains",
    "section": "Outputs",
    "text": "Outputs\nAt the moment only continuous and categorical outputs are supported. Those are similar to the continuous and categorical inputs, but they additionaly contain the objective attribute. The objective attribute is used to define the optimization objective for the output variable. Similar to the inputs the outputs can be also collected in an Outputs object. An example with a continuous and a categorical output is given below.\n\nfrom bofire.data_models.api import Outputs\nfrom bofire.data_models.objectives.api import ConstrainedCategoricalObjective\nfrom bofire.data_models.features.api import ContinuousOutput, CategoricalOutput\n\noutputs = Outputs(\n    features=[\n        ContinuousOutput(key=\"y1\", objective=MinimizeObjective()),\n        CategoricalOutput(\n            key=\"y2\",\n            categories=[\"a\", \"b\", \"c\"],  # Add the required categories\n            objective=ConstrainedCategoricalObjective(\n                categories=[\"a\", \"b\", \"c\"],\n                desirability=[True, False, True]\n            )\n        )\n    ]\n)\n\n\nObjectives\nDifferent classes for the objectives are implemented in BoFire. These are used to set the objective attribute of an output object. Note that each output variable can have its own objective. The following objectives are available:\n\nMaximizeObjective: This is the class of the default value. The objective is to maximize the output variable(s).\nMinimizeObjective: The objective is to minimize the output variable(s). Note that minimization objectives can be transformed into maximization objectives and vice versa just by multiplying the corresponding output by -1.\nMaximizeSigmoidObjective: The objective is to maximize the output variable(s) using a sigmoid transformation. This is useful to implement bounds on the output variable(s).\nMinimizeSigmoidObjective: Similar to MaximizeSigmoidObjective, but the objective is to minimize the output variable(s).\nTargetObjective: The objective is to reach a target value for the output variable(s). TargetObjective is of type ConstrainedObjective as MaximizeSigmoidObjective or MinimizeSigmoidObjective, i.e., it becomes one if the value is in the target region and falls asymptorically to zero outside that region. That means that once the target objective is met, e.g., objective value above target value in a MaximizeSigmoidObjective case, the optimization does not care about how close the objective value is compared to the target value.\nCloseToTargetObjective: This objective measures the difference to a target value. Such an objective is often meaningful to minimize in a multiobjective optimization, and thus be included in the pareto front. Note that also the objectives of type ConstrainedObjective can be used in multiobjective optimization, but for that at least two targets of type MaximizeSigmoidObjective,MinimizeSigmoidObjective or TargetObjective are needed. The main difference with respect to the TargetObjective is that here the goal is to stay as close as possible to the target value.\nConstrainedCategoricalObjective: Categorical objective where for each output variable a probability vector for the categories is given.\n\nIf many outputs are defined, then different weights can be attributed as optional argument w. More detailed descriptions for the individual objecrives can be found in the API documentation.",
    "crumbs": [
      "Domains"
    ]
  },
  {
    "objectID": "docs/userguides/domain.html#constraints",
    "href": "docs/userguides/domain.html#constraints",
    "title": "Domains",
    "section": "Constraints",
    "text": "Constraints\nThe optimization problem in BoFire can be solved subject to a variety of different constraint types in the input space. These constraints are currently categorized as linear, non-linear, n choose k and interpoint constraints.\n\nLinear constraints\nLinear constraints are used to define linear equalities and inequalities.\nEqualities can be defined using the LinearEqualityConstraint class. The features attribute is used to define the input variables, the coefficients attribute is used to define the coefficients of the linear equation and the rhs attribute is used to define the right-hand side of the equation.\nInequalities can be defined using the LinearInequalityConstraint class. The attributes are defined in the same way as for the LinearEqualityConstraint class.\nThe following code defines a new linear equality constraint \\(x_1 + x_2 = 1\\) and an inequality constraint \\(x_1 - x_3 \\leq 1\\).\n\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint, LinearInequalityConstraint\n\nconstraints = [LinearEqualityConstraint(features=[\"x1\", \"x2\"], coefficients=[1,1], rhs=1),\n               LinearInequalityConstraint(features=[\"x1\", \"x3\"], coefficients=[1,-1], rhs=1)]\n\nNote that the variables in the features attribute used in the constraints must be previously defined as inputs in the domain, in order to have a well defined optimization problem.\n\n\nNon-linear constraints\nNon-linear constraints can be used to define equalities and inequalities of the form:\n\\[\nc(\\mathbf{x}) = 0 \\quad \\text{and} \\quad c(\\mathbf{x}) \\leq 0,\n\\]\nwhere \\(c(\\mathbf{x})\\) can be an any differentiable function of the inputs x represented by a string attribute expression that can be evaluated via the eval() method of pandas dataframe. If sympy is installed, the derivate expressions are automatically calculated from the expression attribute. Otherwise, the user can provide additional expressions for the derivatives using the jacobian_expressions attribute. The features attrribute should contain the names of the input variables used in the expression attribute.\nThe following code defines a new non-linear inequality constraint \\(x1^2 + x2^2 - x3 \\leq 0\\).\n\nfrom bofire.data_models.constraints.api import NonlinearInequalityConstraint\n\nNonlinearInequalityConstraint(expression=\"x1**2 + x2**2 - x3\", features=[\"x1\",\"x2\",\"x3\"])\n\nNonlinearInequalityConstraint(type='NonlinearInequalityConstraint', features=['x1', 'x2', 'x3'], expression='x1**2 + x2**2 - x3', jacobian_expression='[2*x1, 2*x2, -1]', hessian_expression='[[2, 0, 0], [0, 2, 0], [0, 0, 0]]')\n\n\n\n\nN choose k constraints\nGiven a list of N features and attributes min_count, max_count, the NChooseKConstraint class is used to define constraints that require a minimum and maximum number of features to be nonzero. Note that one can put min_count==max_count. The following code defines a new NChooseKConstraint that requires at least 2 and at most 3 features to be selected from the list [“x1”, “x2”, “x3”, “x4”].\n\nfrom bofire.data_models.constraints.api import NChooseKConstraint\n\nNChooseKConstraint(features=[\"x1\", \"x2\", \"x3\", \"x4\"], min_count=2, max_count=3, none_also_valid=False)\n\nNChooseKConstraint(type='NChooseKConstraint', features=['x1', 'x2', 'x3', 'x4'], min_count=2, max_count=3, none_also_valid=False)\n\n\n\n\nInterpoint constraints\nThe InterpointEqualityConstraint forces that values of a certain feature of a set/batch of candidates should have the same value. The set is defined using the multiplicity attribute. The following code defines a new InterpointEqualityConstraint that forces the values of the feature “x1” to be the same for every 3 subsequent candidates.\n\nfrom bofire.data_models.constraints.api import InterpointEqualityConstraint\n\nInterpointEqualityConstraint(features=[\"x1\"], multiplicity=3)\n\nInterpointEqualityConstraint(type='InterpointEqualityConstraint', features=['x1'], multiplicity=3)",
    "crumbs": [
      "Domains"
    ]
  },
  {
    "objectID": "build/lib/docs/userguides/surrogates.html",
    "href": "build/lib/docs/userguides/surrogates.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/userguides/surrogates.html#surrogate-model-options",
    "href": "build/lib/docs/userguides/surrogates.html#surrogate-model-options",
    "title": "",
    "section": "Surrogate model options",
    "text": "Surrogate model options\nBoFire offers the following classes of surrogate models.\n\n\n\nSurrogate\nOptimization of\nWhen to use\nType\n\n\n\n\nSingleTaskGPSurrogate\na single objective with real valued inputs\nLimited data and black-box function is smooth\nGaussian process\n\n\nRandomForestSurrogate\na single objective\nRich data; black-box function does not have to be smooth\nsklearn random forest implementation\n\n\nMLP\na single objective with real-valued inputs\nRich data and black-box function is smooth\nMulti layer perceptron\n\n\nMixedSingleTaskGPSurrogate\na single objective with categorical and real valued inputs\nLimited data and black-box function is smooth\nGaussian process\n\n\nTanimotoGP\na single objective\nAt least one input feature is a molecule represented as fingerprint\nGaussian process on a molecule space for which Tanimoto similarity determines the similarity between points\n\n\n\nAll of these are single-objective surrogate models. For optimization of multiple objectives at the same time, a suitable Strategy has to be chosen. Then for each objective a different surrogate model can be specified. By default the SingleTaskGPSurrogate is used.\nExample:\nsurrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n)\nsurrogate_data_1 = RandomForestSurrogate(\n    inputs=domain.inputs,\n    outputs=Outputs(features=[domain.outputs[1]]),\n)\nqparego_data_model = QparegoStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data_0, surrogate_data_1]\n    ),\n)\nNote:\n\nThe standard Kernel for all Gaussian Process (GP) surrogates is a 5/2 matern kernel with automated relevance detection and normalization of the input features.\nThe RandomForestSurrogate does not have kernels but quantifies uncertainty using the standard deviation of the predictions of its individual trees.\nMLP quantifies uncertainty using the standard deviation of multiple predictions that come from different dropout rates (randomly setting neural network weights to zero)."
  },
  {
    "objectID": "build/lib/docs/userguides/surrogates.html#customization",
    "href": "build/lib/docs/userguides/surrogates.html#customization",
    "title": "",
    "section": "Customization",
    "text": "Customization\nBoFire also offers the option to customize surrogate models. In particular, it is possible to customize the SingleTaskGPSurrogate in the following ways.\n\nKernel customization\nSpecify the Kernel:\n\n\n\nKernel\nDescription\nTranslation invariant\nInput variable type\n\n\n\n\nRBFKernel\nBased on Gaussian distribution\nYes\nContinuous\n\n\nMaternKernel\nBased on Gamma function; allows setting a smoothness parameter\nYes\nContinuous\n\n\nPolynomialKernel\nBased on dot-product of two vectors of input points\nNo\nContinuous\n\n\nLinearKernel\nEqual to dot-product of two vectors of input points\nNo\nContinuous\n\n\nTanimotoKernel\nMeasures similarities between binary vectors using Tanimoto Similarity\nNot applicable\nCategoricalMolecularInput\n\n\nHammingDistanceKernel\nSimilarity is defined by the Hamming distance which considers the number of equal entries between two vectors (e.g., in One-Hot-encoding)\nNot applicable\nCategorical\n\n\n\nTranslational invariance means that the similarity between two input points is not affected by shifting both points by the same amount but only determined by their distance. Example: with a translationally invariant kernel, the values 10 and 20 are equally similar to each other as the values 20 and 30, while with a polynomial kernel the latter pair has potentially higher similarity. Polynomial kernels are often suitable for high-dimensional inputs while for low-dimensional inputs an RBF or Matérn kernel is recommended.\nNote:\n\nSingleTaskGPSurrogate with PolynomialKernel is equivalent to PolynomialSurrogate.\nSingleTaskGPSurrogate with LinearKernel is equivalent to LinearSurrogate.\nSingleTaskGPSurrogate with TanimotoKernel is equivalent to TanimotoGP.\nOne can combine two Kernels by using AdditiveKernel or MultiplicativeKernel.\n\nExample:\nsurrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2)\n)\n\n\nNoise model customization\nFor experimental data subject to noise, one can specify the distribution of this noise. The options are:\n\n\n\nNoise Model\nWhen to use\n\n\n\n\nNormalPrior\nNoise is Gaussian\n\n\nGammaPrior\nNoise has a Gamma distribution\n\n\n\nExample:\nsurrogate_data_0 = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=Outputs(features=[domain.outputs[0]]),\n        kernel=PolynomialKernel(power=2),\n        noise_prior=NormalPrior(loc=0, scale=1)\n)"
  },
  {
    "objectID": "build/lib/docs/userguides/domain.html",
    "href": "build/lib/docs/userguides/domain.html",
    "title": "Domains",
    "section": "",
    "text": "The BoFire domain contains all information about the optimization problem. In general, the domain is defined by inputs, outputs, objectives and constraints. Different types for each of these elements are implemented in the BoFire framework. A brief description of the usage of domains and the individual elements is given in the following.\nA basic example of a domain consists of two continuous inputs \\(x_1\\), \\(x_2\\) and a continuous output \\(y\\). If no objective is given, a maximization objective for the output is assumed. If no constraints are defined, an empty set of constraints is assumed.\nA continuous input can be defined using the ContinuousInput class, which requires the variable name and its bounds. Please note that unbounded and partially bounded input variables are currently not supported. Here, we assume \\(x_1, x_2 \\in [0,1]\\).\nfrom bofire.data_models.features.api import ContinuousInput\nfrom bofire.data_models.domain.api import Inputs\n\ninputs = Inputs(features=[\n    ContinuousInput(key=\"x1\", bounds=[0,1]),\n    ContinuousInput(key=\"x2\", bounds=[0,1])\n]\n)\nAnalogously, the continuous output is defined using the ContinuousOutput class.\nfrom bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.domain.api import Outputs\n\noutputs = Outputs(features=[\n    ContinuousOutput(key=\"y\")\n    ]\n)\nIn this case, the domain definition is as follows\nfrom bofire.data_models.domain.api import Domain\n\ndomain = Domain(\n    inputs=inputs,\n    outputs=outputs\n)\nDomains can also be created more concisely using the from_lists method of the Domain class. This is especially useful if you want to define a domain without having to create separate lists for inputs and outputs. The following code achieves the same result as above:\ndomain = Domain.from_lists(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=[0,1]),\n        ContinuousInput(key=\"x2\", bounds=[0,1])\n    ],\n    outputs=[\n        ContinuousOutput(key=\"y\")\n    ]\n)\nThere are applications of domains, where no optimization of an objective is involved (e.g. randomly sampling from the design space). In such cases, we do not require to provide an output definition.\nLet us now assume, we have an additional linear equality constraint and we want to use a custom objective, e.g. a minimization objective for \\(y\\). The linear constraint can be defined using the LinearEqualityConstraint class\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint\n\nconstraints = [LinearEqualityConstraint(features=[\"x1\", \"x2\"], coefficients=[1,1], rhs=1)]\nThe code above corresponds to the linear equation\n\\[\nx_1 + x_2 = 1.\n\\]\nIn BoFire, the minimization objective can be selected by setting the objective-attribute of an output to an instance of MinimizeObjective\nfrom bofire.data_models.objectives.api import MinimizeObjective\n\noutputs = [ContinuousOutput(key=\"y\", objective=MinimizeObjective())]\nThe modified domain in this case becomes\ndomain = Domain(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints\n)"
  },
  {
    "objectID": "build/lib/docs/userguides/domain.html#inputs",
    "href": "build/lib/docs/userguides/domain.html#inputs",
    "title": "Domains",
    "section": "Inputs",
    "text": "Inputs\nBoFire allows for the following different user-facing input classes:\n\nContinuousInput\nDiscreteInput\nCategoricalInput\nCategoricalDescriptorInput\nCategoricalMolecularInput\nTaskInput \n\nEach of these input classes can be used for defining domains, however some strategies only support a subset of the available input types. You can call the is_feature_implemented() function of a given strategy and input class to check whether the input is supported by the strategy. For example, the following code checks whether the ContinuousInput is supported by the RandomSearch strategy. \n\nfrom bofire.data_models.strategies.api import RandomStrategy\n\nRandomStrategy.is_feature_implemented(ContinuousInput)\n\nTrue\n\n\nThe result will be True if the input type is supported by the strategy, otherwise it will return False. This is useful to check whether a certain input type can be used with a specific strategy before defining the domain.\n\nInput types\n\nContinuous inputs\nContinuous inputs are used to define real-valued input variables with finite upper and lower bounds (see example above).\n\n\nDiscrete inputs\nFor discrete inputs, only a finite set of values is allowed. The DiscreteInput class requires a list of permissible values as input. For example, the following code defines a new discrete input variable \\(x_3\\) with values \\(0, 0.1, 0.2\\).\n\nfrom bofire.data_models.features.api import DiscreteInput\n\nDiscreteInput(key=\"x3\", values=[0, 0.1, 0.2])\n\nDiscreteInput(type='DiscreteInput', key='x3', unit=None, values=[0.0, 0.1, 0.2], rtol=1e-07)\n\n\n\n\nCategorical inputs\nThis class of inputs is similar to the discrete inputs, but takes a list of strings as input. The following code defines a new categorical input variable \\(x_4\\) with categories “A”, “B”, “C”.\n\nfrom bofire.data_models.features.api import CategoricalInput\n\nCategoricalInput(key=\"x4\", categories=[\"A\", \"B\", \"C\"])\n\nCategoricalInput(type='CategoricalInput', key='x4', categories=['A', 'B', 'C'], allowed=[True, True, True])\n\n\n\n\n\nCategorical descriptor inputs\nVia the CategoricalDescriptorInput one can provide continuous encodings for the different categories via so called descriptors. Imagine, for example, having a categorial input with different categories, where each category corresponds to a specific material. Such descriptors could be e.g., density and hardness. Every material/category would get assigned a number for density and hardness in the hope that these two properties describe the material properly. In the context of fitting a GP, one can then use just these two dimensional vector for describing the material instead of a ten dimensional one-hot encoding, which results in a dimensionality reduction. Of course, this makes sense under the assumption that the descriptors actually correlate with the desired quantities.\nThe CategoricalDescriptorInput class requires a list of permissible values as input. For example, the following code defines a new categorical descriptor input variable \\(x_6\\) with values “A”, “B”, “C”.\n\nfrom bofire.data_models.features.api import CategoricalDescriptorInput\n\nCategoricalDescriptorInput(\n    key=\"x6\",\n    categories=[\"material_A\", \"material_B\", \"material_C\"],\n    descriptors=[\"density\", \"hardness\"],\n    values=[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]\n)\n\nCategoricalDescriptorInput(type='CategoricalDescriptorInput', key='x6', categories=['material_A', 'material_B', 'material_C'], allowed=[True, True, True], descriptors=['density', 'hardness'], values=[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n\n\n\n\nCategorical molecular inputs\nCategorical molecular inputs inherit from categorical inputs. Its list of categories has to be a list of valid SMILES strings. The following code defines a new categorical molecular input variable \\(x_7\\) where only the values “C1CCCCC1”, “O1CCOCC1” are allowed.\n\nfrom bofire.data_models.features.api import CategoricalMolecularInput\n\nCategoricalMolecularInput(key=\"x7\", categories=[\"C1CCCCC1\", \"O1CCOCC1\"])\n\nCategoricalMolecularInput(type='CategoricalMolecularInput', key='x7', categories=['C1CCCCC1', 'O1CCOCC1'], allowed=[True, True])\n\n\n\n\n\n\nInputs class\nThe Inputs class is used to collect multiple input variables. It is used to define the inputs of a domain. The following code defines a new input class with the above described input variables \\(x_1, x_2, x_3, x_4, x_5, x_6, x_7\\).\n\nfrom bofire.data_models.api import Inputs\n\ninputs = Inputs(\n    features=[ContinuousInput(key=\"x1\", bounds=[0,1]),\n    ContinuousInput(key=\"x2\", bounds=[0,1]),\n    DiscreteInput(key=\"x3\", values=[0, 0.1, 0.2]),\n    CategoricalInput(key=\"x4\", categories=[\"A\", \"B\", \"C\"]),\n    CategoricalDescriptorInput(key=\"x6\", categories=[\"material_A\", \"material_B\", \"material_C\"], descriptors=[\"density\", \"hardness\"], values=[[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n    CategoricalMolecularInput(key=\"x7\", categories=[\"C1CCCCC1\", \"O1CCOCC1\"])])"
  },
  {
    "objectID": "build/lib/docs/userguides/domain.html#outputs",
    "href": "build/lib/docs/userguides/domain.html#outputs",
    "title": "Domains",
    "section": "Outputs",
    "text": "Outputs\nAt the moment only continuous and categorical outputs are supported. Those are similar to the continuous and categorical inputs, but they additionaly contain the objective attribute. The objective attribute is used to define the optimization objective for the output variable. Similar to the inputs the outputs can be also collected in an Outputs object. An example with a continuous and a categorical output is given below.\n\nfrom bofire.data_models.api import Outputs\nfrom bofire.data_models.objectives.api import ConstrainedCategoricalObjective\nfrom bofire.data_models.features.api import ContinuousOutput, CategoricalOutput\n\noutputs = Outputs(\n    features=[\n        ContinuousOutput(key=\"y1\", objective=MinimizeObjective()),\n        CategoricalOutput(\n            key=\"y2\",\n            categories=[\"a\", \"b\", \"c\"],  # Add the required categories\n            objective=ConstrainedCategoricalObjective(\n                categories=[\"a\", \"b\", \"c\"],\n                desirability=[True, False, True]\n            )\n        )\n    ]\n)\n\n\nObjectives\nDifferent classes for the objectives are implemented in BoFire. These are used to set the objective attribute of an output object. Note that each output variable can have its own objective. The following objectives are available:\n\nMaximizeObjective: This is the class of the default value. The objective is to maximize the output variable(s).\nMinimizeObjective: The objective is to minimize the output variable(s). Note that minimization objectives can be transformed into maximization objectives and vice versa just by multiplying the corresponding output by -1.\nMaximizeSigmoidObjective: The objective is to maximize the output variable(s) using a sigmoid transformation. This is useful to implement bounds on the output variable(s).\nMinimizeSigmoidObjective: Similar to MaximizeSigmoidObjective, but the objective is to minimize the output variable(s).\nTargetObjective: The objective is to reach a target value for the output variable(s). TargetObjective is of type ConstrainedObjective as MaximizeSigmoidObjective or MinimizeSigmoidObjective, i.e., it becomes one if the value is in the target region and falls asymptorically to zero outside that region. That means that once the target objective is met, e.g., objective value above target value in a MaximizeSigmoidObjective case, the optimization does not care about how close the objective value is compared to the target value.\nCloseToTargetObjective: This objective measures the difference to a target value. Such an objective is often meaningful to minimize in a multiobjective optimization, and thus be included in the pareto front. Note that also the objectives of type ConstrainedObjective can be used in multiobjective optimization, but for that at least two targets of type MaximizeSigmoidObjective,MinimizeSigmoidObjective or TargetObjective are needed. The main difference with respect to the TargetObjective is that here the goal is to stay as close as possible to the target value.\nConstrainedCategoricalObjective: Categorical objective where for each output variable a probability vector for the categories is given.\n\nIf many outputs are defined, then different weights can be attributed as optional argument w. More detailed descriptions for the individual objecrives can be found in the API documentation."
  },
  {
    "objectID": "build/lib/docs/userguides/domain.html#constraints",
    "href": "build/lib/docs/userguides/domain.html#constraints",
    "title": "Domains",
    "section": "Constraints",
    "text": "Constraints\nThe optimization problem in BoFire can be solved subject to a variety of different constraint types in the input space. These constraints are currently categorized as linear, non-linear, n choose k and interpoint constraints.\n\nLinear constraints\nLinear constraints are used to define linear equalities and inequalities.\nEqualities can be defined using the LinearEqualityConstraint class. The features attribute is used to define the input variables, the coefficients attribute is used to define the coefficients of the linear equation and the rhs attribute is used to define the right-hand side of the equation.\nInequalities can be defined using the LinearInequalityConstraint class. The attributes are defined in the same way as for the LinearEqualityConstraint class.\nThe following code defines a new linear equality constraint \\(x_1 + x_2 = 1\\) and an inequality constraint \\(x_1 - x_3 \\leq 1\\).\n\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint, LinearInequalityConstraint\n\nconstraints = [LinearEqualityConstraint(features=[\"x1\", \"x2\"], coefficients=[1,1], rhs=1),\n               LinearInequalityConstraint(features=[\"x1\", \"x3\"], coefficients=[1,-1], rhs=1)]\n\nNote that the variables in the features attribute used in the constraints must be previously defined as inputs in the domain, in order to have a well defined optimization problem.\n\n\nNon-linear constraints\nNon-linear constraints can be used to define equalities and inequalities of the form:\n\\[\nc(\\mathbf{x}) = 0 \\quad \\text{and} \\quad c(\\mathbf{x}) \\leq 0,\n\\]\nwhere \\(c(\\mathbf{x})\\) can be an any differentiable function of the inputs x represented by a string attribute expression that can be evaluated via the eval() method of pandas dataframe. If sympy is installed, the derivate expressions are automatically calculated from the expression attribute. Otherwise, the user can provide additional expressions for the derivatives using the jacobian_expressions attribute. The features attrribute should contain the names of the input variables used in the expression attribute.\nThe following code defines a new non-linear inequality constraint \\(x1^2 + x2^2 - x3 \\leq 0\\).\n\nfrom bofire.data_models.constraints.api import NonlinearInequalityConstraint\n\nNonlinearInequalityConstraint(expression=\"x1**2 + x2**2 - x3\", features=[\"x1\",\"x2\",\"x3\"])\n\nNonlinearInequalityConstraint(type='NonlinearInequalityConstraint', features=['x1', 'x2', 'x3'], expression='x1**2 + x2**2 - x3', jacobian_expression='[2*x1, 2*x2, -1]', hessian_expression='[[2, 0, 0], [0, 2, 0], [0, 0, 0]]')\n\n\n\n\nN choose k constraints\nGiven a list of N features and attributes min_count, max_count, the NChooseKConstraint class is used to define constraints that require a minimum and maximum number of features to be nonzero. Note that one can put min_count==max_count. The following code defines a new NChooseKConstraint that requires at least 2 and at most 3 features to be selected from the list [“x1”, “x2”, “x3”, “x4”].\n\nfrom bofire.data_models.constraints.api import NChooseKConstraint\n\nNChooseKConstraint(features=[\"x1\", \"x2\", \"x3\", \"x4\"], min_count=2, max_count=3, none_also_valid=False)\n\nNChooseKConstraint(type='NChooseKConstraint', features=['x1', 'x2', 'x3', 'x4'], min_count=2, max_count=3, none_also_valid=False)\n\n\n\n\nInterpoint constraints\nThe InterpointEqualityConstraint forces that values of a certain feature of a set/batch of candidates should have the same value. The set is defined using the multiplicity attribute. The following code defines a new InterpointEqualityConstraint that forces the values of the feature “x1” to be the same for every 3 subsequent candidates.\n\nfrom bofire.data_models.constraints.api import InterpointEqualityConstraint\n\nInterpointEqualityConstraint(features=[\"x1\"], multiplicity=3)\n\nInterpointEqualityConstraint(type='InterpointEqualityConstraint', features=['x1'], multiplicity=3)"
  },
  {
    "objectID": "docs/tutorials/serialization/strategies_serial.html",
    "href": "docs/tutorials/serialization/strategies_serial.html",
    "title": "Strategy Serialization with BoFire",
    "section": "",
    "text": "from pydantic import TypeAdapter\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.kernels.api import RBFKernel, ScaleKernel\nfrom bofire.data_models.strategies.api import AnyStrategy\nfrom bofire.data_models.strategies.api import MoboStrategy as MoboStrategyDataModel\nfrom bofire.data_models.strategies.api import RandomStrategy as RandomStrategyDataModel\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDataModel\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\nfrom bofire.surrogates.diagnostics import CvResults2CrossValidationValues\nfrom bofire.surrogates.trainable import TrainableSurrogate",
    "crumbs": [
      "Serialization",
      "Strategy Serialization with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/strategies_serial.html#imports",
    "href": "docs/tutorials/serialization/strategies_serial.html#imports",
    "title": "Strategy Serialization with BoFire",
    "section": "",
    "text": "from pydantic import TypeAdapter\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.kernels.api import RBFKernel, ScaleKernel\nfrom bofire.data_models.strategies.api import AnyStrategy\nfrom bofire.data_models.strategies.api import MoboStrategy as MoboStrategyDataModel\nfrom bofire.data_models.strategies.api import RandomStrategy as RandomStrategyDataModel\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDataModel\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\nfrom bofire.surrogates.diagnostics import CvResults2CrossValidationValues\nfrom bofire.surrogates.trainable import TrainableSurrogate",
    "crumbs": [
      "Serialization",
      "Strategy Serialization with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/strategies_serial.html#single-objective-problem-setup",
    "href": "docs/tutorials/serialization/strategies_serial.html#single-objective-problem-setup",
    "title": "Strategy Serialization with BoFire",
    "section": "Single Objective Problem Setup",
    "text": "Single Objective Problem Setup\n\nbenchmark = Himmelblau()\nsamples = benchmark.domain.inputs.sample(n=10)\n\n# this is the training data\nexperiments = benchmark.f(samples, return_complete=True)\n\n# this are the pending candidates\npending_candidates = benchmark.domain.inputs.sample(2)",
    "crumbs": [
      "Serialization",
      "Strategy Serialization with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/strategies_serial.html#random-strategy",
    "href": "docs/tutorials/serialization/strategies_serial.html#random-strategy",
    "title": "Strategy Serialization with BoFire",
    "section": "Random Strategy",
    "text": "Random Strategy\nThe random strategy and other strategies that just inherit from Strategy and not PredictiveStrategy are special as they do not need defined output features in the domain and they do not need a call to tell before the ask. Furthermore they online provide input features in the candidates and no predictions for output features.\n\n# setup the data model\ndomain = Domain(inputs=benchmark.domain.inputs)\nstrategy_data = RandomStrategyDataModel(domain=domain)\n\n# we generate the json spec\njspec = strategy_data.model_dump_json()\n\njspec\n\n'{\"type\":\"RandomStrategy\",\"domain\":{\"type\":\"Domain\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[]},\"constraints\":{\"type\":\"Constraints\",\"constraints\":[]}},\"seed\":null,\"fallback_sampling_method\":\"UNIFORM\",\"n_burnin\":1000,\"n_thinning\":32,\"num_base_samples\":null,\"max_iters\":1000,\"sampler_kwargs\":null}'\n\n\n\n# load it\nstrategy_data = TypeAdapter(AnyStrategy).validate_json(jspec)\n\n# map it\nstrategy = strategies.map(strategy_data)\n\n# ask it\ndf_candidates = strategy.ask(candidate_count=5)\n\n# transform to spec\ncandidates = strategy.to_candidates(df_candidates)\n\ncandidates\n\n[Candidate(inputValues={'x_1': InputValue(value='5.776783223334235'), 'x_2': InputValue(value='-2.5499200971168703')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='-4.748342241591863'), 'x_2': InputValue(value='-3.565823882713743')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='5.333651542457245'), 'x_2': InputValue(value='-1.4288140836844532')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='-1.7245610298138105'), 'x_2': InputValue(value='-4.2460927834957145')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='4.304029899204766'), 'x_2': InputValue(value='-4.789656489351125')}, outputValues=None)]",
    "crumbs": [
      "Serialization",
      "Strategy Serialization with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/strategies_serial.html#sobo-strategy",
    "href": "docs/tutorials/serialization/strategies_serial.html#sobo-strategy",
    "title": "Strategy Serialization with BoFire",
    "section": "SOBO Strategy",
    "text": "SOBO Strategy\nSetup the strategies data model.\n\n# setup the data model\nstrategy_data = SoboStrategyDataModel(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# we generate the json spec\njspec = strategy_data.model_dump_json()\n\njspec\n\n'{\"type\":\"SoboStrategy\",\"domain\":{\"type\":\"Domain\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"constraints\":{\"type\":\"Constraints\",\"constraints\":[]}},\"seed\":null,\"acquisition_optimizer\":{\"prefer_exhaustive_search_for_purely_categorical_domains\":true,\"type\":\"BotorchOptimizer\",\"n_restarts\":20,\"n_raw_samples\":1024,\"maxiter\":2000,\"batch_limit\":20,\"sequential\":false,\"local_search_config\":null},\"surrogate_specs\":{\"surrogates\":[{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}]},\"outlier_detection_specs\":null,\"min_experiments_before_outlier_check\":1,\"frequency_check\":1,\"frequency_hyperopt\":0,\"folds\":5,\"include_infeasible_exps_in_acqf_calc\":false,\"acquisition_function\":{\"type\":\"qLogNEI\",\"prune_baseline\":true,\"n_mc_samples\":512}}'\n\n\nAs SOBO is a predictive strategy, training data has to be provided before candidated can be requested.\n\n# load it\nstrategy_data = TypeAdapter(AnyStrategy).validate_json(jspec)\n\n# map it\nstrategy = strategies.map(strategy_data)\n\n# tell it the pending candidates if present\nif pending_candidates is not None:\n    strategy.add_candidates(pending_candidates)\n\n# tell it\nstrategy.tell(experiments=experiments)\n\n# ask it\ndf_candidates = strategy.ask(candidate_count=2)\n\n# transform to spec\ncandidates = strategy.to_candidates(df_candidates)\n\ncandidates\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n[Candidate(inputValues={'x_1': InputValue(value='-0.8846113257554975'), 'x_2': InputValue(value='3.0378378266377486')}, outputValues={'y': OutputValue(predictedValue='-0.6810595746728723', standardDeviation=101.1524625088662, objective=0.6810595746728723)}),\n Candidate(inputValues={'x_1': InputValue(value='5.175496966847776'), 'x_2': InputValue(value='-6.0')}, outputValues={'y': OutputValue(predictedValue='25.44144276740201', standardDeviation=169.46307767731298, objective=-25.44144276740201)})]\n\n\nWe can also save the trained models of the strategy, for more info look at the model_serial.ipynb notebook. It could be that the dumps command fails here. But this is already fixed in the main branch of the linear_operator package, and if not yet, it should be available in main soon.\n\njsurrogate_spec = strategy_data.surrogate_specs.surrogates[0].model_dump_json()\ndump = strategy.surrogates.surrogates[0].dumps()",
    "crumbs": [
      "Serialization",
      "Strategy Serialization with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/strategies_serial.html#mobo-strategy",
    "href": "docs/tutorials/serialization/strategies_serial.html#mobo-strategy",
    "title": "Strategy Serialization with BoFire",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\nAs example for a multiobjective strategy we are using here the MoboStrategy. Related strategies would be Qparego, MultiplicativeSobo etc. To use it, we have to first generate a multiobjective domain.\n\nbenchmark = DTLZ2(dim=6)\nsamples = benchmark.domain.inputs.sample(n=20)\nexperiments = benchmark.f(samples, return_complete=True)\npending_candidates = benchmark.domain.inputs.sample(2)\n\nNow the strategy spec is setup. Note that we can define there exactly which model to use.\n\n# setup the data model\nstrategy_data = MoboStrategyDataModel(\n    domain=benchmark.domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=benchmark.domain.inputs,\n                outputs=Outputs(features=[benchmark.domain.outputs[0]]),\n                kernel=ScaleKernel(base_kernel=RBFKernel(ard=False)),\n            ),\n        ],\n    ),\n)\n\n# we generate the json spec\njspec = strategy_data.model_dump_json()\n\njspec\n\n'{\"type\":\"MoboStrategy\",\"domain\":{\"type\":\"Domain\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_0\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_3\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_4\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_5\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"f_0\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}},{\"type\":\"ContinuousOutput\",\"key\":\"f_1\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"constraints\":{\"type\":\"Constraints\",\"constraints\":[]}},\"seed\":null,\"acquisition_optimizer\":{\"prefer_exhaustive_search_for_purely_categorical_domains\":true,\"type\":\"BotorchOptimizer\",\"n_restarts\":20,\"n_raw_samples\":1024,\"maxiter\":2000,\"batch_limit\":20,\"sequential\":false,\"local_search_config\":null},\"surrogate_specs\":{\"surrogates\":[{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_0\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_3\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_4\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_5\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"f_0\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"ScaleKernel\",\"base_kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":false,\"lengthscale_prior\":null,\"lengthscale_constraint\":null},\"outputscale_prior\":null,\"outputscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}},{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_0\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_3\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_4\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_5\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"f_1\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}]},\"outlier_detection_specs\":null,\"min_experiments_before_outlier_check\":1,\"frequency_check\":1,\"frequency_hyperopt\":0,\"folds\":5,\"include_infeasible_exps_in_acqf_calc\":false,\"ref_point\":{\"type\":\"ExplicitReferencePoint\",\"values\":{\"f_0\":{\"type\":\"AbsoluteMovingReferenceValue\",\"orient_at_best\":false,\"offset\":0.0},\"f_1\":{\"type\":\"AbsoluteMovingReferenceValue\",\"orient_at_best\":false,\"offset\":0.0}}},\"acquisition_function\":{\"type\":\"qLogNEHVI\",\"alpha\":0.0,\"prune_baseline\":true,\"n_mc_samples\":512}}'\n\n\nGenerate the candidates.\n\n# load it\nstrategy_data = TypeAdapter(AnyStrategy).validate_json(jspec)\n\n# map it\nstrategy = strategies.map(strategy_data)\n\n# tell it the pending candidates if available\nif pending_candidates is not None:\n    strategy.add_candidates(pending_candidates)\n\n# tell it\nstrategy.tell(experiments=experiments)\n\n# ask it\ndf_candidates = strategy.ask(candidate_count=1)\n\n# transform to spec\ncandidates = strategy.to_candidates(df_candidates)\n\ncandidates\n\n[Candidate(inputValues={'x_0': InputValue(value='0.14706907886235498'), 'x_1': InputValue(value='1.0'), 'x_2': InputValue(value='1.0'), 'x_3': InputValue(value='0.0'), 'x_4': InputValue(value='1.0'), 'x_5': InputValue(value='1.0')}, outputValues={'f_0': OutputValue(predictedValue='0.9032021462753871', standardDeviation=0.32373340750866353, objective=-0.9032021462753871), 'f_1': OutputValue(predictedValue='0.3358215583385029', standardDeviation=0.15161669049874651, objective=-0.3358215583385029)})]\n\n\nTo fill the model info section accordingly, the following snippet has to be executed for every surrogate, incldung saving the actual models.\n\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\n\nclass TestMethod(BaseModel):\n    type: str\n\n\nclass CrossValidation(TestMethod):\n    type: Literal[\"CrossValidation\"] = \"CrossValidation\"\n    foldCount: int\n\n\nfor i in range(len(strategy_data.surrogate_specs.surrogates)):\n    surrogate_data = strategy.surrogate_specs.surrogates[i]\n    surrogate = strategy.surrogates.surrogates[i]\n    # get the spec\n    jsurrogate_spec = surrogate_data.model_dump_json()\n    # get the dump\n    dump = surrogate.dumps()\n    # do the cross validation, only if we have a trainable model under the hood\n    if isinstance(surrogate, TrainableSurrogate):\n        cv_train, cv_test, _ = surrogate.cross_validate(strategy.experiments, folds=5)\n        # transform the bofire objects to the backend objects\n        testMethod = CrossValidation(foldCount=5)\n        cvResultsTrain = CvResults2CrossValidationValues(cv_train)\n        cvResultsTest = CvResults2CrossValidationValues(cv_test)\n        metricsTrain = {\n            surrogate.outputs[0].key: cv_train.get_metrics(combine_folds=False)\n            .describe()\n            .loc[\"mean\"]\n            .to_dict(),\n        }\n        metricsTest = {\n            surrogate.outputs[0].key: cv_test.get_metrics(combine_folds=True)\n            .describe()\n            .loc[\"mean\"]\n            .to_dict(),\n        }\n        # save to backend\n        # - jsurrogate_spec\n        # - dump\n        # - testMethod\n        # - cvResultsTrain\n        # - cvResultsTest\n        # - metricsTrain\n        # - metricsTest",
    "crumbs": [
      "Serialization",
      "Strategy Serialization with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/index.html",
    "href": "docs/tutorials/serialization/index.html",
    "title": "Serialization",
    "section": "",
    "text": "All classes in BoFire are serializable and can be saved to JSON formats. These tutorials demonstrate how to serialize and deserialize BoFire objects for storage, sharing, and integration with RESTful APIs.\n\n\n\n\nLearn how to serialize and deserialize surrogate models.\n\n\n\nSave and load optimization strategies for reproducibility and deployment.\n\n\n\n\nSerialization is crucial for:\n\nReproducibility: Save the exact configuration of your optimization runs\nCollaboration: Share strategies and models with team members\nAPI Integration: Seamlessly integrate BoFire into RESTful services\nVersion Control: Track changes in experimental designs over time\nDeployment: Move configurations from development to production environments",
    "crumbs": [
      "Serialization",
      "Serialization"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/index.html#available-tutorials",
    "href": "docs/tutorials/serialization/index.html#available-tutorials",
    "title": "Serialization",
    "section": "",
    "text": "Learn how to serialize and deserialize surrogate models.\n\n\n\nSave and load optimization strategies for reproducibility and deployment.",
    "crumbs": [
      "Serialization",
      "Serialization"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/index.html#why-serialization-matters",
    "href": "docs/tutorials/serialization/index.html#why-serialization-matters",
    "title": "Serialization",
    "section": "",
    "text": "Serialization is crucial for:\n\nReproducibility: Save the exact configuration of your optimization runs\nCollaboration: Share strategies and models with team members\nAPI Integration: Seamlessly integrate BoFire into RESTful services\nVersion Control: Track changes in experimental designs over time\nDeployment: Move configurations from development to production environments",
    "crumbs": [
      "Serialization",
      "Serialization"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/nchoosek_constraint.html",
    "href": "docs/tutorials/doe/nchoosek_constraint.html",
    "title": "Design with NChooseK constraint",
    "section": "",
    "text": "The doe subpackage also supports problems with NChooseK constraints. Since IPOPT has problems finding feasible solutions using the gradient of the NChooseK constraint violation, a closely related (but stricter) constraint that suffices to fulfill the NChooseK constraint is imposed onto the problem: For each experiment \\(j\\) N-K decision variables \\(x_{i_1,j},...,x_{i_{N-K,j}}\\) from the NChooseK constraints’ names attribute are picked that are forced to be zero. This is done by setting the upper and lower bounds of the picked variables are set to 0 in the corresponding experiments. This causes IPOPT to treat them as “fixed variables” (i.e. it will not optimize for them) and will always stick to the only feasible value (which is 0 here). However, this constraint is stricter than the original NChooseK constraint. In combination with other constraints on the same decision variables this can result in a situation where the constraints cannot be fulfilled even though the original constraints would allow for a solution. For example consider a problem with four decision variables \\(x_1, x_2, x_3, x_4\\), an NChooseK constraint on the first four variable that restricts the number of nonzero variables to two. Additionally, we have a linear constraint \\[\nx_3 + x_4 \\geq 0.1\n\\] We can easily find points that fulfill both constraints (e.g. \\((0,0,0,0.1)\\)). Now consider the stricter, linear constraint from above. Eventually, it will happen that \\(x_3\\) and \\(x_4\\) are chosen to be zero for one experiment. For this experiment it is impossible to fulfill the linear constraint \\(x_3 + x_4 \\geq 0.1\\) since \\(x_3 = x_4 = 0\\).\nTherefore one has to be very careful when imposing linear constraints upon decision variables that already show up in an NChooseK constraint.\nFor practical reasons it necessary that two NChooseK constraints of the same problem must not share any variables.\nYou can find an example for a problem with NChooseK constraints and additional linear constraints imposed on the same variables.\n\nimport numpy as np\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NChooseKConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(8)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[f\"x{i+1}\" for i in range(8)],\n            coefficients=[1, 1, 1, 1, 1, 1, 1, 1],\n            rhs=1,\n        ),\n        NChooseKConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            min_count=0,\n            max_count=1,\n            none_also_valid=True,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=0.7,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x7\", \"x8\"],\n            coefficients=[-1, -1],\n            rhs=-0.1,\n        ),\n        LinearInequalityConstraint(features=[\"x7\", \"x8\"], coefficients=[1, 1], rhs=0.9),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n    ipopt_options={\"max_iter\": 500},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\n\n\n\n\n0\n0.0\n0.0\n0.000\n0.507\n0.000\n0.0\n0.000\n0.493\n\n\n1\n0.7\n0.0\n0.000\n0.200\n0.000\n0.0\n0.100\n0.000\n\n\n2\n0.0\n0.0\n0.000\n0.000\n0.498\n0.0\n0.502\n0.000\n\n\n3\n0.0\n0.0\n0.000\n0.900\n0.000\n0.0\n0.100\n0.000\n\n\n4\n0.0\n0.0\n0.539\n0.000\n0.000\n0.0\n0.461\n0.000\n\n\n5\n0.0\n0.0\n0.000\n0.449\n0.000\n0.0\n0.551\n0.000\n\n\n6\n0.1\n0.0\n0.000\n0.000\n0.000\n0.0\n0.900\n0.000\n\n\n7\n0.0\n0.0\n0.700\n0.000\n0.200\n0.0\n0.000\n0.100\n\n\n8\n0.0\n0.0\n0.000\n0.000\n0.000\n0.9\n0.000\n0.100\n\n\n9\n0.0\n0.0\n0.100\n0.000\n0.000\n0.0\n0.000\n0.900\n\n\n10\n0.0\n0.0\n0.000\n0.000\n0.900\n0.0\n0.000\n0.100\n\n\n11\n0.0\n0.7\n0.000\n0.000\n0.000\n0.2\n0.000\n0.100",
    "crumbs": [
      "Design of Experiments",
      "Design with NChooseK constraint"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/fractional_factorial.html",
    "href": "docs/tutorials/doe/fractional_factorial.html",
    "title": "Full and Fractional Factorial Designs",
    "section": "",
    "text": "BoFire can be used to setup full (two level) and fractional factorial designs (https://en.wikipedia.org/wiki/Fractional_factorial_design). This tutorial notebook shows how.",
    "crumbs": [
      "Design of Experiments",
      "Full and Fractional Factorial Designs"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/fractional_factorial.html#imports-and-helper-functions",
    "href": "docs/tutorials/doe/fractional_factorial.html#imports-and-helper-functions",
    "title": "Full and Fractional Factorial Designs",
    "section": "Imports and helper functions",
    "text": "Imports and helper functions\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import CategoricalInput, ContinuousInput\nfrom bofire.data_models.strategies.api import FractionalFactorialStrategy\nfrom bofire.utils.doe import get_alias_structure, get_confounding_matrix, get_generator\n\n\ndef plot_design(design: pd.DataFrame):\n    # we do a plot with three subplots in one row in which the three degrees of freedom (temperature, time and ph) are plotted\n    _, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].scatter(design[\"temperature\"], design[\"time\"])\n    axs[0].set_xlabel(\"Temperature\")\n    axs[0].set_ylabel(\"Time\")\n    axs[1].scatter(design[\"temperature\"], design[\"ph\"])\n    axs[1].set_xlabel(\"Temperature\")\n    axs[1].set_ylabel(\"pH\")\n    axs[2].scatter(design[\"time\"], design[\"ph\"])\n    axs[2].set_xlabel(\"Time\")\n    axs[2].set_ylabel(\"pH\")\n    plt.show()",
    "crumbs": [
      "Design of Experiments",
      "Full and Fractional Factorial Designs"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/fractional_factorial.html#setup-the-problem-domain",
    "href": "docs/tutorials/doe/fractional_factorial.html#setup-the-problem-domain",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup the problem domain",
    "text": "Setup the problem domain\nThe designs are generated for a simple three dimensional problem comprised of three continuous factors/features.\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n    ],\n)",
    "crumbs": [
      "Design of Experiments",
      "Full and Fractional Factorial Designs"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design",
    "href": "docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup a full factorial design",
    "text": "Setup a full factorial design\nHere we setup a full two-level factorial design including a center point and plot it.\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\n\n\n\n\n0\n7.0\n20.0\n60.0\n\n\n1\n7.0\n20.0\n120.0\n\n\n2\n7.0\n80.0\n60.0\n\n\n3\n7.0\n80.0\n120.0\n\n\n4\n13.0\n20.0\n60.0\n\n\n5\n13.0\n20.0\n120.0\n\n\n6\n13.0\n80.0\n60.0\n\n\n7\n13.0\n80.0\n120.0\n\n\n8\n10.0\n50.0\n90.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confounding structure is shown below, as expected for a full factorial design, no confound is present.\n\nm = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()",
    "crumbs": [
      "Design of Experiments",
      "Full and Fractional Factorial Designs"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design-with-blocking",
    "href": "docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design-with-blocking",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup a full factorial design with blocking",
    "text": "Setup a full factorial design with blocking\nHere we setup a blocked full two-level factorial design including a center point and plot it.\n\nblocked_domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n        CategoricalInput(key=\"operator\", categories=[\"A\", \"B\", \"C\", \"D\"]),\n    ],\n)\n\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=blocked_domain,\n    n_center=1,  # number of center points per block\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    block_feature_key=\"operator\",\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\noperator\n\n\n\n\n0\n7.0\n20.0\n60.0\nA\n\n\n1\n13.0\n80.0\n120.0\nA\n\n\n2\n10.0\n50.0\n90.0\nA\n\n\n3\n7.0\n20.0\n120.0\nB\n\n\n4\n13.0\n80.0\n60.0\nB\n\n\n5\n10.0\n50.0\n90.0\nB\n\n\n6\n7.0\n80.0\n60.0\nC\n\n\n7\n13.0\n20.0\n120.0\nC\n\n\n8\n10.0\n50.0\n90.0\nC\n\n\n9\n7.0\n80.0\n120.0\nD\n\n\n10\n13.0\n20.0\n60.0\nD\n\n\n11\n10.0\n50.0\n90.0\nD",
    "crumbs": [
      "Design of Experiments",
      "Full and Fractional Factorial Designs"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/fractional_factorial.html#setup-a-fractional-factorial-design",
    "href": "docs/tutorials/doe/fractional_factorial.html#setup-a-fractional-factorial-design",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup a fractional factorial design",
    "text": "Setup a fractional factorial design\nHere a fractional factorial design of the form \\(2^{3-1}\\) is setup by specifying the number of generators (here 1). In comparison to the full factorial design with 9 candidates, it features only 5 experiments.\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    n_generators=1,  # number of generators, ie number of reducing factors\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\n\n\n\n\n0\n7.0\n20.0\n120.0\n\n\n1\n7.0\n80.0\n60.0\n\n\n2\n13.0\n20.0\n60.0\n\n\n3\n13.0\n80.0\n120.0\n\n\n4\n10.0\n50.0\n90.0\n\n\n\n\n\n\n\nThe generator string is automatically generated by making use of the method get_generator and specifying the total number of factors (here 3) and the number of generators (here 1).\n\nget_generator(n_factors=3, n_generators=1)\n\n'a b ab'\n\n\nAs expected for a type III design the main effects are confounded with the two factor interactions:\n\nm = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n\n\n\n\n\n\n\n\nThis can also be expressed by the so called alias structure that can be calculated as following:\n\nget_alias_structure(\"a b ab\")\n\n['a = bc', 'b = ac', 'c = ab', 'I = abc']\n\n\nHere again a fractional factorial design of the form \\(2^{3-1}\\) is setup by providing the complete generator string of the form a b -ab explicitly to the strategy.\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    generator=\"a b -ab\",  # the exact generator\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\n\n\n\n\n0\n7.0\n20.0\n60.0\n\n\n1\n7.0\n80.0\n120.0\n\n\n2\n13.0\n20.0\n120.0\n\n\n3\n13.0\n80.0\n60.0\n\n\n4\n10.0\n50.0\n90.0\n\n\n\n\n\n\n\nThe last two designs differ only in the last feature time, since the generator strings are different. In the first one it holds time=ph x temperature whereas in the second it holds time=-ph x temperature, which is also reflected in the confounding structure.\n\nm = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()",
    "crumbs": [
      "Design of Experiments",
      "Full and Fractional Factorial Designs"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/candidate_goodness_and_experiments.html",
    "href": "docs/tutorials/doe/candidate_goodness_and_experiments.html",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "",
    "text": "BoFire supports classical design of experiments where you can specify different models. If you have already started running experiments, say with a linear design, and you have resources to search more broadly in the same space (aka domain definition), you might want to ‘augment’ the original design to a larger design, say ‘fully quadratic’.\nAnother case is when you have existing experiments and want to see what can be (re)used in a design of experiments. This is different from Bayesian Optimization: with classical DoE, we do one set of experiments rather than iteratively asking for an experiment or two, then retraining the model with the acquired data before asking again. For DoE, outputs are not taken into account, unlike Bayesian Optimization.\nYou can set these as your candidate experiments.\nBoFire provides methods to evaluate design quality and plan additional experiments by calculating the Fisher Information Matrix (FIM) rank of your candidate experiments. The FIM rank tells you how many model parameters for the current design can be uniquely estimated from your candidates."
  },
  {
    "objectID": "docs/tutorials/doe/candidate_goodness_and_experiments.html#key-concepts",
    "href": "docs/tutorials/doe/candidate_goodness_and_experiments.html#key-concepts",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "Key Concepts",
    "text": "Key Concepts\nFor a linear model with 3 continuous inputs:\n\nModel: y = β₀ + β₁x₁ + β₂x₂ + β₃x₃\nNumber of parameters to estimate: 4 (intercept + 3 coefficients)\nMinimum experiments for parameter estimation: 4\nRecommended experiments (with buffer for error estimation): 7 (4 + 3)\n\nThe FIM rank of your candidates tells you how many parameters you can actually estimate. If rank &lt; number of parameters, your design is rank-deficient and cannot estimate all model coefficients uniquely."
  },
  {
    "objectID": "docs/tutorials/doe/candidate_goodness_and_experiments.html#example-checking-design-quality",
    "href": "docs/tutorials/doe/candidate_goodness_and_experiments.html#example-checking-design-quality",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "Example: Checking Design Quality",
    "text": "Example: Checking Design Quality\nLet’s create a simple domain and check the quality of different experimental designs:\n\nimport pandas as pd\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\nimport bofire.data_models.strategies.api as data_models\nfrom bofire.strategies.api import DoEStrategy\n\n# Create a simple domain with 3 continuous inputs\ndomain = Domain.from_lists(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n)\n\n# Create a DoE strategy with a linear model\ndata_model = data_models.DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\")\n)\nstrategy = DoEStrategy(data_model=data_model)\n\n\nHow many experiments are recommended?\n\nrequired = strategy.get_required_number_of_experiments()\nprint(f\"Recommended number of experiments: {required}\")\nprint(f\"Additional experiments needed (no candidates yet): {strategy.get_additional_experiments_needed()}\")\n\nRecommended number of experiments: 7\nAdditional experiments needed (no candidates yet): 7\n\n\nThe recommended number is 7 (4 parameters + 3 buffer for error estimation and validation).\n\n\nLooking at the quality of candidates towards that design\nLet’s say you’ve run 2 experiments. If they’re good, we expect to only need 5 more for our D-Optimal linear design (recall above the recommended number of experiments was 7).\n\n# Two experiments along one axis only\ncandidates_partial = pd.DataFrame({\n    \"x1\": [0.0, 1.0],\n    \"x2\": [0.0, 0.0],\n    \"x3\": [0.0, 0.0]\n})\n\nstrategy.set_candidates(candidates_partial)\nrank = strategy.get_candidate_rank()\nadditional = strategy.get_additional_experiments_needed()\n\nprint(f\"FIM rank of current design: {rank}\")\nprint(f\"Model parameters: 4 (intercept + 3 coefficients)\")\nprint(f\"Additional experiments needed: {additional}\")\n\nFIM rank of current design: 2\nModel parameters: 4 (intercept + 3 coefficients)\nAdditional experiments needed: 5\n\n\nWith only 2 experiments along one axis, the FIM rank is 2 - you can only estimate 2 parameters (intercept and x1 coefficient). You need 5 more experiments to reach the recommended count.\n\n\nGenerating and evaluating a full design\n\n# Generate a full D-optimal design\nfull_doe = strategy.ask(candidate_count=required)\nprint(f\"Generated {len(full_doe)} experiments:\")\nprint(full_doe.round(3))\n\n# Evaluate the design quality\nstrategy_fresh = DoEStrategy(data_model=data_model)\nstrategy_fresh.set_candidates(full_doe)\nrank_full = strategy_fresh.get_candidate_rank()\nadditional_full = strategy_fresh.get_additional_experiments_needed()\n\nprint(f\"\\nFIM rank: {rank_full}\")\nprint(f\"Additional experiments needed: {additional_full}\")\n\nGenerated 7 experiments:\n    x1   x2   x3\n0  0.0  1.0  0.0\n1  1.0  1.0  1.0\n2  1.0  0.0  1.0\n3  0.0  0.0  1.0\n4  0.0  1.0  1.0\n5  1.0  1.0  0.0\n6  0.0  1.0  1.0\n\nFIM rank: 4\nAdditional experiments needed: 3\n\n\nNotice that even though we generated 7 experiments, the FIM rank is 4 (the number of model parameters). This is correct! The D-optimal design places experiments at strategic locations (typically corners of the design space) to estimate all 4 parameters. The extra 3 experiments provide:\n\nDegrees of freedom for error estimation\nAbility to detect lack-of-fit\nRobustness against experimental errors"
  },
  {
    "objectID": "docs/tutorials/doe/candidate_goodness_and_experiments.html#practical-workflow",
    "href": "docs/tutorials/doe/candidate_goodness_and_experiments.html#practical-workflow",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "Practical Workflow",
    "text": "Practical Workflow\nWhen planning experiments:\n\nCheck requirements: Use get_required_number_of_experiments() to see the recommended count\nEvaluate existing data: Use get_candidate_rank() to assess your candidates, if you have any that you want to incorporate\nPlan additions: Use get_additional_experiments_needed() to determine how many more experiments to run\nGenerate design: Use ask() to get the additional experiments, which will be optimized to complement your existing data\n\nThis approach works for any model type (\"linear\", \"linear-and-quadratic\", \"linear-and-interactions\", \"fully-quadratic\") and automatically handles constraints, discrete inputs, and categorical inputs."
  },
  {
    "objectID": "docs/tutorials/benchmarks/index.html",
    "href": "docs/tutorials/benchmarks/index.html",
    "title": "Benchmark Examples",
    "section": "",
    "text": "These tutorials demonstrate how to recreate results from various papers and common studies using BoFire’s benchmarking capabilities. Benchmarks are useful for validating optimization strategies and comparing performance across different approaches.\n\n\n\n\nClassic optimization benchmark using the Himmelblau function.\n\n\n\nMulti-objective optimization using the DTLZ2 test problem.\n\n\n\nHartmann function optimization with n-choose-k constraints.\n\n\n\nHigh-dimensional optimization using an extended Branin function.\n\n\n\nRobust optimization in the presence of outliers.\n\n\n\nMolecular optimization using chemical fingerprints and specialized kernels.\n\n\n\nLocal search-based Bayesian optimization.\n\n\n\nMulti-objective optimization using the ZDT1 test problem.\n\n\n\nBinh and Korn multi-objective test function.\n\n\n\nTanaka multi-objective test function.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nHigh-dimensional Bayesian optimization using spherical linear kernels.",
    "crumbs": [
      "Benchmarks",
      "Benchmark Examples"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/index.html#available-benchmarks",
    "href": "docs/tutorials/benchmarks/index.html#available-benchmarks",
    "title": "Benchmark Examples",
    "section": "",
    "text": "Classic optimization benchmark using the Himmelblau function.\n\n\n\nMulti-objective optimization using the DTLZ2 test problem.\n\n\n\nHartmann function optimization with n-choose-k constraints.\n\n\n\nHigh-dimensional optimization using an extended Branin function.\n\n\n\nRobust optimization in the presence of outliers.\n\n\n\nMolecular optimization using chemical fingerprints and specialized kernels.\n\n\n\nLocal search-based Bayesian optimization.\n\n\n\nMulti-objective optimization using the ZDT1 test problem.\n\n\n\nBinh and Korn multi-objective test function.\n\n\n\nTanaka multi-objective test function.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nHigh-dimensional Bayesian optimization using spherical linear kernels.",
    "crumbs": [
      "Benchmarks",
      "Benchmark Examples"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/012-engineered_features.html",
    "href": "docs/tutorials/benchmarks/012-engineered_features.html",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "",
    "text": "Continuous descriptor inputs can be used in combination with engineered features to incorporate more domain knowledge into an optimization. Example could be a solvent of formulation optimization where per continuous input feature additional information about its properties is available.\nFor examples in the literature have a look at this paper.",
    "crumbs": [
      "Benchmarks",
      "Optimization with `ContinuousDescriptorInput`s and engineered Features"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/012-engineered_features.html#imports",
    "href": "docs/tutorials/benchmarks/012-engineered_features.html#imports",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousDescriptorInput, SumFeature, WeightedSumFeature, ContinuousOutput\nfrom bofire.data_models.domain.api import Inputs, EngineeredFeatures, Outputs, Domain, Constraints\nfrom bofire.benchmarks.api import Himmelblau, FormulationWrapper\nfrom bofire.runners.api import run\nfrom bofire.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.surrogates.api import BotorchSurrogates\nfrom typing import Optional\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Optimization with `ContinuousDescriptorInput`s and engineered Features"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/012-engineered_features.html#problem-setup",
    "href": "docs/tutorials/benchmarks/012-engineered_features.html#problem-setup",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "Problem Setup",
    "text": "Problem Setup\nFor demo purposes, the formulation wrapper is used around the two dimensional Himmelblau Benchmark to wrap it as a formulation problem with 7 features in total. Per original feature, three new features are introduced and one filler feature. The sum of each feature group is used as (normalized) input for the evaluation of the original Himmelblau function. A seventh feature is introduced as filler to always reach a total of 1 to fulfill the formulation/equality constraint. For more information on this setup have a look at the docstring of FormulationWrapper.\n\nbench = FormulationWrapper(benchmark=Himmelblau(), n_features_per_original_feature=3, n_filler_features=1)\n\nThe continuos descriptor features have the following descriptor values for the descriptors x_1 and x_2:\n\npd.concat([feat.to_df() for feat in bench.domain.inputs.get(ContinuousDescriptorInput)])\n\n\n\n\n\n\n\n\nx_1\nx_2\n\n\n\n\nx_1_0\n1.0\n0.0\n\n\nx_1_1\n1.0\n0.0\n\n\nx_1_2\n1.0\n0.0\n\n\nx_2_0\n0.0\n1.0\n\n\nx_2_1\n0.0\n1.0\n\n\nx_2_2\n0.0\n1.0",
    "crumbs": [
      "Benchmarks",
      "Optimization with `ContinuousDescriptorInput`s and engineered Features"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/012-engineered_features.html#benchmarks",
    "href": "docs/tutorials/benchmarks/012-engineered_features.html#benchmarks",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nRandom Strategy\nAs baseline, we run the random strategy on the problem.\n\ndef sample(domain):\n    strategy = RandomStrategy.make(domain=domain)\n    return strategy.ask(10)\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\nrandom_results = []\nfor _ in range(15 if not SMOKE_TEST else 1):\n    results = run(\n        benchmark=bench,\n        strategy_factory=lambda domain: RandomStrategy.make(domain=domain),\n        n_iterations=40 if not SMOKE_TEST else 2,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    random_results.append(results[0][0])\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:00&lt;?, ?it/s, Current Best:=27.645]Run 0:  50%|█████     | 1/2 [00:00&lt;00:00,  9.27it/s, Current Best:=27.645]Run 0:  50%|█████     | 1/2 [00:00&lt;00:00,  9.27it/s, Current Best:=27.645]Run 0:  50%|█████     | 1/2 [00:00&lt;00:00,  9.27it/s, Current Best:=27.645]Run 0: 100%|██████████| 2/2 [00:00&lt;00:00,  9.21it/s, Current Best:=27.645]Run 0: 100%|██████████| 2/2 [00:00&lt;00:00,  9.20it/s, Current Best:=27.645]\n\n\n\n\nSobo Strategy\nNext we run the SoboStrategy on the problem.\n\ndef sample(domain):\n    return initial_experiments[domain.inputs.get_keys()].copy()\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\nsobo_results = []\nfor i in range(15 if not SMOKE_TEST else 1):\n    initial_experiments = random_results[i][:10].copy()\n    results = run(\n        benchmark=bench,\n        strategy_factory=lambda domain: SoboStrategy.make(domain=domain),\n        n_iterations=40 if not SMOKE_TEST else 2,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    sobo_results.append(results[0][0])\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:05&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:05&lt;?, ?it/s, Current Best:=27.645]Run 0:  50%|█████     | 1/2 [00:05&lt;00:05,  5.45s/it, Current Best:=27.645]Run 0:  50%|█████     | 1/2 [00:11&lt;00:05,  5.45s/it, Current Best:=27.645]Run 0:  50%|█████     | 1/2 [00:11&lt;00:05,  5.45s/it, Current Best:=27.645]Run 0: 100%|██████████| 2/2 [00:11&lt;00:00,  5.87s/it, Current Best:=27.645]Run 0: 100%|██████████| 2/2 [00:11&lt;00:00,  5.81s/it, Current Best:=27.645]\n\n\nNow we run the SoboStrategy again, but with an engineered feature as additional input. The WeightedSumFeature computes the sum over the specified descriptors weighted by the values of the involved original features.\n\ndef create_strategy(domain):\n    surrogate_data = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=domain.outputs,\n        engineered_features=EngineeredFeatures(\n            features=[\n                WeightedSumFeature(\n                    key=\"WeightedSum\",\n                    features=bench.domain.inputs.get_keys(ContinuousDescriptorInput),\n                    descriptors=[\"x_1\", \"x_2\"]\n                    )]),\n        #kernel=RBFKernel(features=[\"x_1\", \"x_2\"], lengthscale_prior=HVARFNER_LENGTHSCALE_PRIOR())\n    )\n    return SoboStrategy.make(domain=domain, surrogate_specs=BotorchSurrogates(surrogates=[surrogate_data]))\n\nsobo_engineered_results_all = []\nfor i in range(15 if not SMOKE_TEST else 1):\n    initial_experiments = random_results[i][:10].copy()\n    results = run(\n        benchmark=bench,\n        strategy_factory=create_strategy,\n        n_iterations=40,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    sobo_engineered_results_all.append(results[0][0])\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/engineered_features.py:71: UserWarning:\n\nCreating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n\n  0%|          | 0/40 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/40 [00:06&lt;?, ?it/s]Run 0:   0%|          | 0/40 [00:06&lt;?, ?it/s, Current Best:=27.645]Run 0:   2%|▎         | 1/40 [00:06&lt;04:04,  6.28s/it, Current Best:=27.645]Run 0:   2%|▎         | 1/40 [00:12&lt;04:04,  6.28s/it, Current Best:=27.645]Run 0:   2%|▎         | 1/40 [00:12&lt;04:04,  6.28s/it, Current Best:=15.237]Run 0:   5%|▌         | 2/40 [00:12&lt;03:51,  6.08s/it, Current Best:=15.237]Run 0:   5%|▌         | 2/40 [00:18&lt;03:51,  6.08s/it, Current Best:=15.237]Run 0:   5%|▌         | 2/40 [00:18&lt;03:51,  6.08s/it, Current Best:=15.237]Run 0:   8%|▊         | 3/40 [00:18&lt;03:51,  6.26s/it, Current Best:=15.237]Run 0:   8%|▊         | 3/40 [00:24&lt;03:51,  6.26s/it, Current Best:=15.237]Run 0:   8%|▊         | 3/40 [00:24&lt;03:51,  6.26s/it, Current Best:=15.237]Run 0:  10%|█         | 4/40 [00:24&lt;03:38,  6.06s/it, Current Best:=15.237]Run 0:  10%|█         | 4/40 [00:29&lt;03:38,  6.06s/it, Current Best:=15.237]Run 0:  10%|█         | 4/40 [00:29&lt;03:38,  6.06s/it, Current Best:=15.237]Run 0:  12%|█▎        | 5/40 [00:29&lt;03:20,  5.74s/it, Current Best:=15.237]Run 0:  12%|█▎        | 5/40 [00:35&lt;03:20,  5.74s/it, Current Best:=15.237]Run 0:  12%|█▎        | 5/40 [00:35&lt;03:20,  5.74s/it, Current Best:=15.237]Run 0:  15%|█▌        | 6/40 [00:35&lt;03:16,  5.77s/it, Current Best:=15.237]Run 0:  15%|█▌        | 6/40 [00:41&lt;03:16,  5.77s/it, Current Best:=15.237]Run 0:  15%|█▌        | 6/40 [00:41&lt;03:16,  5.77s/it, Current Best:=15.237]Run 0:  18%|█▊        | 7/40 [00:41&lt;03:14,  5.89s/it, Current Best:=15.237]Run 0:  18%|█▊        | 7/40 [00:48&lt;03:14,  5.89s/it, Current Best:=15.237]Run 0:  18%|█▊        | 7/40 [00:48&lt;03:14,  5.89s/it, Current Best:=5.286] Run 0:  20%|██        | 8/40 [00:48&lt;03:16,  6.14s/it, Current Best:=5.286]Run 0:  20%|██        | 8/40 [00:54&lt;03:16,  6.14s/it, Current Best:=5.286]Run 0:  20%|██        | 8/40 [00:54&lt;03:16,  6.14s/it, Current Best:=5.286]Run 0:  22%|██▎       | 9/40 [00:54&lt;03:06,  6.03s/it, Current Best:=5.286]Run 0:  22%|██▎       | 9/40 [01:00&lt;03:06,  6.03s/it, Current Best:=5.286]Run 0:  22%|██▎       | 9/40 [01:00&lt;03:06,  6.03s/it, Current Best:=5.286]Run 0:  25%|██▌       | 10/40 [01:00&lt;03:05,  6.17s/it, Current Best:=5.286]Run 0:  25%|██▌       | 10/40 [01:05&lt;03:05,  6.17s/it, Current Best:=5.286]Run 0:  25%|██▌       | 10/40 [01:05&lt;03:05,  6.17s/it, Current Best:=5.286]Run 0:  28%|██▊       | 11/40 [01:05&lt;02:52,  5.93s/it, Current Best:=5.286]Run 0:  28%|██▊       | 11/40 [01:12&lt;02:52,  5.93s/it, Current Best:=5.286]Run 0:  28%|██▊       | 11/40 [01:12&lt;02:52,  5.93s/it, Current Best:=5.286]Run 0:  30%|███       | 12/40 [01:12&lt;02:51,  6.14s/it, Current Best:=5.286]Run 0:  30%|███       | 12/40 [01:18&lt;02:51,  6.14s/it, Current Best:=5.286]Run 0:  30%|███       | 12/40 [01:18&lt;02:51,  6.14s/it, Current Best:=5.286]Run 0:  32%|███▎      | 13/40 [01:18&lt;02:47,  6.19s/it, Current Best:=5.286]Run 0:  32%|███▎      | 13/40 [01:25&lt;02:47,  6.19s/it, Current Best:=5.286]Run 0:  32%|███▎      | 13/40 [01:25&lt;02:47,  6.19s/it, Current Best:=5.286]Run 0:  35%|███▌      | 14/40 [01:25&lt;02:45,  6.36s/it, Current Best:=5.286]Run 0:  35%|███▌      | 14/40 [01:32&lt;02:45,  6.36s/it, Current Best:=5.286]Run 0:  35%|███▌      | 14/40 [01:32&lt;02:45,  6.36s/it, Current Best:=5.286]Run 0:  38%|███▊      | 15/40 [01:32&lt;02:42,  6.49s/it, Current Best:=5.286]Run 0:  38%|███▊      | 15/40 [01:38&lt;02:42,  6.49s/it, Current Best:=5.286]Run 0:  38%|███▊      | 15/40 [01:38&lt;02:42,  6.49s/it, Current Best:=5.286]Run 0:  40%|████      | 16/40 [01:38&lt;02:35,  6.49s/it, Current Best:=5.286]Run 0:  40%|████      | 16/40 [01:45&lt;02:35,  6.49s/it, Current Best:=5.286]Run 0:  40%|████      | 16/40 [01:45&lt;02:35,  6.49s/it, Current Best:=5.286]Run 0:  42%|████▎     | 17/40 [01:45&lt;02:28,  6.45s/it, Current Best:=5.286]Run 0:  42%|████▎     | 17/40 [01:51&lt;02:28,  6.45s/it, Current Best:=5.286]Run 0:  42%|████▎     | 17/40 [01:51&lt;02:28,  6.45s/it, Current Best:=3.033]Run 0:  45%|████▌     | 18/40 [01:51&lt;02:20,  6.41s/it, Current Best:=3.033]Run 0:  45%|████▌     | 18/40 [01:57&lt;02:20,  6.41s/it, Current Best:=3.033]Run 0:  45%|████▌     | 18/40 [01:57&lt;02:20,  6.41s/it, Current Best:=1.915]Run 0:  48%|████▊     | 19/40 [01:57&lt;02:10,  6.22s/it, Current Best:=1.915]Run 0:  48%|████▊     | 19/40 [02:02&lt;02:10,  6.22s/it, Current Best:=1.915]Run 0:  48%|████▊     | 19/40 [02:02&lt;02:10,  6.22s/it, Current Best:=1.915]Run 0:  50%|█████     | 20/40 [02:02&lt;01:59,  5.99s/it, Current Best:=1.915]Run 0:  50%|█████     | 20/40 [02:08&lt;01:59,  5.99s/it, Current Best:=1.915]Run 0:  50%|█████     | 20/40 [02:08&lt;01:59,  5.99s/it, Current Best:=1.915]Run 0:  52%|█████▎    | 21/40 [02:08&lt;01:54,  6.03s/it, Current Best:=1.915]Run 0:  52%|█████▎    | 21/40 [02:14&lt;01:54,  6.03s/it, Current Best:=1.915]Run 0:  52%|█████▎    | 21/40 [02:14&lt;01:54,  6.03s/it, Current Best:=1.915]Run 0:  55%|█████▌    | 22/40 [02:14&lt;01:45,  5.83s/it, Current Best:=1.915]Run 0:  55%|█████▌    | 22/40 [02:20&lt;01:45,  5.83s/it, Current Best:=1.915]Run 0:  55%|█████▌    | 22/40 [02:20&lt;01:45,  5.83s/it, Current Best:=1.915]Run 0:  57%|█████▊    | 23/40 [02:20&lt;01:40,  5.92s/it, Current Best:=1.915]Run 0:  57%|█████▊    | 23/40 [02:26&lt;01:40,  5.92s/it, Current Best:=1.915]Run 0:  57%|█████▊    | 23/40 [02:26&lt;01:40,  5.92s/it, Current Best:=1.915]Run 0:  60%|██████    | 24/40 [02:26&lt;01:35,  5.96s/it, Current Best:=1.915]Run 0:  60%|██████    | 24/40 [02:33&lt;01:35,  5.96s/it, Current Best:=1.915]Run 0:  60%|██████    | 24/40 [02:33&lt;01:35,  5.96s/it, Current Best:=1.915]Run 0:  62%|██████▎   | 25/40 [02:33&lt;01:33,  6.23s/it, Current Best:=1.915]Run 0:  62%|██████▎   | 25/40 [02:39&lt;01:33,  6.23s/it, Current Best:=1.915]Run 0:  62%|██████▎   | 25/40 [02:39&lt;01:33,  6.23s/it, Current Best:=1.915]Run 0:  65%|██████▌   | 26/40 [02:39&lt;01:27,  6.28s/it, Current Best:=1.915]Run 0:  65%|██████▌   | 26/40 [02:45&lt;01:27,  6.28s/it, Current Best:=1.915]Run 0:  65%|██████▌   | 26/40 [02:45&lt;01:27,  6.28s/it, Current Best:=1.915]Run 0:  68%|██████▊   | 27/40 [02:45&lt;01:18,  6.04s/it, Current Best:=1.915]Run 0:  68%|██████▊   | 27/40 [02:51&lt;01:18,  6.04s/it, Current Best:=1.915]Run 0:  68%|██████▊   | 27/40 [02:51&lt;01:18,  6.04s/it, Current Best:=0.591]Run 0:  70%|███████   | 28/40 [02:51&lt;01:12,  6.03s/it, Current Best:=0.591]Run 0:  70%|███████   | 28/40 [02:57&lt;01:12,  6.03s/it, Current Best:=0.591]Run 0:  70%|███████   | 28/40 [02:57&lt;01:12,  6.03s/it, Current Best:=0.591]Run 0:  72%|███████▎  | 29/40 [02:57&lt;01:05,  5.97s/it, Current Best:=0.591]Run 0:  72%|███████▎  | 29/40 [03:02&lt;01:05,  5.97s/it, Current Best:=0.591]Run 0:  72%|███████▎  | 29/40 [03:02&lt;01:05,  5.97s/it, Current Best:=0.591]Run 0:  75%|███████▌  | 30/40 [03:02&lt;00:58,  5.87s/it, Current Best:=0.591]Run 0:  75%|███████▌  | 30/40 [03:09&lt;00:58,  5.87s/it, Current Best:=0.591]Run 0:  75%|███████▌  | 30/40 [03:09&lt;00:58,  5.87s/it, Current Best:=0.591]Run 0:  78%|███████▊  | 31/40 [03:09&lt;00:55,  6.15s/it, Current Best:=0.591]Run 0:  78%|███████▊  | 31/40 [03:13&lt;00:55,  6.15s/it, Current Best:=0.591]Run 0:  78%|███████▊  | 31/40 [03:14&lt;00:55,  6.15s/it, Current Best:=0.591]Run 0:  80%|████████  | 32/40 [03:14&lt;00:45,  5.66s/it, Current Best:=0.591]Run 0:  80%|████████  | 32/40 [03:19&lt;00:45,  5.66s/it, Current Best:=0.591]Run 0:  80%|████████  | 32/40 [03:19&lt;00:45,  5.66s/it, Current Best:=0.591]Run 0:  82%|████████▎ | 33/40 [03:19&lt;00:39,  5.67s/it, Current Best:=0.591]Run 0:  82%|████████▎ | 33/40 [03:25&lt;00:39,  5.67s/it, Current Best:=0.591]Run 0:  82%|████████▎ | 33/40 [03:25&lt;00:39,  5.67s/it, Current Best:=0.591]Run 0:  85%|████████▌ | 34/40 [03:25&lt;00:33,  5.66s/it, Current Best:=0.591]Run 0:  85%|████████▌ | 34/40 [03:32&lt;00:33,  5.66s/it, Current Best:=0.591]Run 0:  85%|████████▌ | 34/40 [03:32&lt;00:33,  5.66s/it, Current Best:=0.591]Run 0:  88%|████████▊ | 35/40 [03:32&lt;00:30,  6.13s/it, Current Best:=0.591]Run 0:  88%|████████▊ | 35/40 [03:38&lt;00:30,  6.13s/it, Current Best:=0.591]Run 0:  88%|████████▊ | 35/40 [03:38&lt;00:30,  6.13s/it, Current Best:=0.591]Run 0:  90%|█████████ | 36/40 [03:38&lt;00:23,  5.94s/it, Current Best:=0.591]Run 0:  90%|█████████ | 36/40 [03:43&lt;00:23,  5.94s/it, Current Best:=0.591]Run 0:  90%|█████████ | 36/40 [03:43&lt;00:23,  5.94s/it, Current Best:=0.591]Run 0:  92%|█████████▎| 37/40 [03:43&lt;00:17,  5.76s/it, Current Best:=0.591]Run 0:  92%|█████████▎| 37/40 [03:48&lt;00:17,  5.76s/it, Current Best:=0.591]Run 0:  92%|█████████▎| 37/40 [03:48&lt;00:17,  5.76s/it, Current Best:=0.591]Run 0:  95%|█████████▌| 38/40 [03:48&lt;00:11,  5.66s/it, Current Best:=0.591]Run 0:  95%|█████████▌| 38/40 [03:54&lt;00:11,  5.66s/it, Current Best:=0.591]Run 0:  95%|█████████▌| 38/40 [03:54&lt;00:11,  5.66s/it, Current Best:=0.591]Run 0:  98%|█████████▊| 39/40 [03:54&lt;00:05,  5.68s/it, Current Best:=0.591]Run 0:  98%|█████████▊| 39/40 [03:59&lt;00:05,  5.68s/it, Current Best:=0.591]Run 0:  98%|█████████▊| 39/40 [03:59&lt;00:05,  5.68s/it, Current Best:=0.591]Run 0: 100%|██████████| 40/40 [03:59&lt;00:00,  5.61s/it, Current Best:=0.591]Run 0: 100%|██████████| 40/40 [03:59&lt;00:00,  6.00s/it, Current Best:=0.591]\n\n\n\n\nPlot the performance\nNext we plot the performance. Using the engineered features shows a clear benefit. Using a SAAS model on it would lead to even better results, as only two features are needed and SAAS models are very good in figuring out the features of interest.\n\ndef get_padded_trajectories(dfs: list[pd.DataFrame]) -&gt; np.ndarray:\n    trajectories = []\n    for df in dfs:\n        y_values = [df.y[:10].max()] + df.y[10:].tolist()\n        cummin = np.minimum.accumulate(np.log10(y_values))\n        trajectories.append(np.array(cummin))\n    max_len = max(len(t) for t in trajectories)\n    return [np.pad(traj, (0, max_len-len(traj)), mode='edge') for traj in trajectories]\n\nfig, ax = plt.subplots()\n\nrandom_trajectories = get_padded_trajectories(random_results)\nstd_random = np.std(random_trajectories, axis=0)\nmean_random = np.mean(random_trajectories, axis=0)\nsem_random = std_random / np.sqrt(len(random_trajectories))\nax.plot(mean_random, label='Random', color='blue')\nax.fill_between(range(len(mean_random)), mean_random - sem_random, mean_random + sem_random, color='blue', alpha=0.2)\n\n\nsobo_trajectories = get_padded_trajectories(sobo_results)\nstd_sobo = np.std(sobo_trajectories, axis=0)\nmean_sobo = np.mean(sobo_trajectories, axis=0)\nsem_sobo = std_sobo / np.sqrt(len(sobo_trajectories))\nax.plot(mean_sobo, label='SOBO', color='green')\nax.fill_between(range(len(mean_sobo)), mean_sobo - sem_sobo, mean_sobo + sem_sobo, color='green', alpha=0.2)\n\nsobo_engineered_trajectories = get_padded_trajectories(sobo_engineered_results_all)\nstd_sobo_engineered = np.std(sobo_engineered_trajectories, axis=0)\nmean_sobo_engineered = np.mean(sobo_engineered_trajectories, axis=0)\nsem_sobo_engineered = std_sobo_engineered / np.sqrt(len(sobo_engineered_trajectories))\nax.plot(mean_sobo_engineered, label='SOBO Engineered', color='red')\nax.fill_between(range(len(mean_sobo_engineered)), mean_sobo_engineered - sem_sobo_engineered, mean_sobo_engineered + sem_sobo_engineered, color='red', alpha=0.2)\n\nax.set_title('Optimization Performance')\nax.legend()\nplt.show()",
    "crumbs": [
      "Benchmarks",
      "Optimization with `ContinuousDescriptorInput`s and engineered Features"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/010-TNK.html",
    "href": "docs/tutorials/benchmarks/010-TNK.html",
    "title": "TNK Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import TNK\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "TNK Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/010-TNK.html#imports",
    "href": "docs/tutorials/benchmarks/010-TNK.html#imports",
    "title": "TNK Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import TNK\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "TNK Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/010-TNK.html#random-strategy",
    "href": "docs/tutorials/benchmarks/010-TNK.html#random-strategy",
    "title": "TNK Benchmark",
    "section": "Random Strategy",
    "text": "Random Strategy\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return compute_hypervolume(\n        domain,\n        experiments.loc[(experiments.c1 &gt;= 0) & (experiments.c2 &lt;= 0.5)],\n        ref_point={\"f1\": 4, \"f2\": 4},\n    )\n\n\nrandom_results = run(\n    TNK(),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=100 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=0.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 46.85it/s, Current Best:=0.000]",
    "crumbs": [
      "Benchmarks",
      "TNK Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/010-TNK.html#mobo-strategy",
    "href": "docs/tutorials/benchmarks/010-TNK.html#mobo-strategy",
    "title": "TNK Benchmark",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(domain=domain, ref_point={\"f1\": 4.0, \"f2\": 4.0})\n    return strategies.map(data_model)\n\n\nresults = run(\n    TNK(),\n    strategy_factory=strategy_factory,\n    n_iterations=100 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:08&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:08&lt;?, ?it/s, Current Best:=11.285]Run 0: 100%|██████████| 1/1 [00:08&lt;00:00,  8.92s/it, Current Best:=11.285]Run 0: 100%|██████████| 1/1 [00:08&lt;00:00,  8.92s/it, Current Best:=11.285]\n\n\n\nif not SMOKE_TEST:\n    fig, ax = plt.subplots()\n    ax.plot(random_results[0][1], label=\"random\")\n    ax.plot(results[0][1], label=\"MOBO\")\n    ax.set_xlabel(\"iteration\")\n    ax.set_ylabel(\"hypervolume\")\n    ax.legend()\n    plt.show()",
    "crumbs": [
      "Benchmarks",
      "TNK Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/008-ZDT1.html",
    "href": "docs/tutorials/benchmarks/008-ZDT1.html",
    "title": "ZDT1",
    "section": "",
    "text": "import os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import ZDT1\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.kernels.api import RBFKernel\nfrom bofire.data_models.priors.api import (\n    HVARFNER_NOISE_PRIOR,\n    DimensionalityScaledLogNormalPrior,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    FullyBayesianSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume, get_pareto_front\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nN_ITERATIONS = 50 if not SMOKE_TEST else 1\nBATCH_SIZE = 5 if not SMOKE_TEST else 1\nWARMUP_STEPS = 256 if not SMOKE_TEST else 32\nNUM_SAMPLES = 128 if not SMOKE_TEST else 16\nTHINNING = 16",
    "crumbs": [
      "Benchmarks",
      "ZDT1"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/008-ZDT1.html#imports",
    "href": "docs/tutorials/benchmarks/008-ZDT1.html#imports",
    "title": "ZDT1",
    "section": "",
    "text": "import os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import ZDT1\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.kernels.api import RBFKernel\nfrom bofire.data_models.priors.api import (\n    HVARFNER_NOISE_PRIOR,\n    DimensionalityScaledLogNormalPrior,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    FullyBayesianSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume, get_pareto_front\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nN_ITERATIONS = 50 if not SMOKE_TEST else 1\nBATCH_SIZE = 5 if not SMOKE_TEST else 1\nWARMUP_STEPS = 256 if not SMOKE_TEST else 32\nNUM_SAMPLES = 128 if not SMOKE_TEST else 16\nTHINNING = 16",
    "crumbs": [
      "Benchmarks",
      "ZDT1"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/008-ZDT1.html#random-optimization",
    "href": "docs/tutorials/benchmarks/008-ZDT1.html#random-optimization",
    "title": "ZDT1",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return compute_hypervolume(domain, experiments, ref_point={\"y1\": 1.0, \"y2\": 5.0})\n\n\nrandom_results = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=1.115]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 18.62it/s, Current Best:=1.115]",
    "crumbs": [
      "Benchmarks",
      "ZDT1"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-hvarfner-priors",
    "href": "docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-hvarfner-priors",
    "title": "ZDT1",
    "section": "Optimization with Hvarfner priors",
    "text": "Optimization with Hvarfner priors\n\nbenchmark = ZDT1(n_inputs=30)\n\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(\n        domain=domain,\n        ref_point={\"y1\": 1.0, \"y2\": 5.0},\n        surrogate_specs=BotorchSurrogates(\n            surrogates=[\n                SingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y1\"]),\n                    # the following hyperparams do not need to be provided\n                    kernel=RBFKernel(\n                        ard=True,\n                        lengthscale_prior=DimensionalityScaledLogNormalPrior(),\n                    ),\n                    noise_prior=HVARFNER_NOISE_PRIOR(),\n                ),\n                SingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y2\"]),\n                    # the following hyperparams do not need to be provided\n                    kernel=RBFKernel(\n                        ard=True,\n                        lengthscale_prior=DimensionalityScaledLogNormalPrior(),\n                    ),\n                    noise_prior=HVARFNER_NOISE_PRIOR(),\n                ),\n            ],\n        ),\n    )\n    return strategies.map(data_model)\n\n\nresults = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:05&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:05&lt;?, ?it/s, Current Best:=1.652]Run 0: 100%|██████████| 1/1 [00:05&lt;00:00,  5.55s/it, Current Best:=1.652]Run 0: 100%|██████████| 1/1 [00:05&lt;00:00,  5.55s/it, Current Best:=1.652]",
    "crumbs": [
      "Benchmarks",
      "ZDT1"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-default-priors",
    "href": "docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-default-priors",
    "title": "ZDT1",
    "section": "Optimization with default priors",
    "text": "Optimization with default priors\n\nbenchmark = ZDT1(n_inputs=30)\n\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(\n        domain=domain,\n        ref_point={\"y1\": 1.0, \"y2\": 5.0},\n    )\n    return strategies.map(data_model)\n\n\nresults = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:08&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:08&lt;?, ?it/s, Current Best:=1.097]Run 0: 100%|██████████| 1/1 [00:08&lt;00:00,  8.06s/it, Current Best:=1.097]Run 0: 100%|██████████| 1/1 [00:08&lt;00:00,  8.06s/it, Current Best:=1.097]",
    "crumbs": [
      "Benchmarks",
      "ZDT1"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/008-ZDT1.html#saasbo-optimization",
    "href": "docs/tutorials/benchmarks/008-ZDT1.html#saasbo-optimization",
    "title": "ZDT1",
    "section": "SAASBO Optimization",
    "text": "SAASBO Optimization\n\nbenchmark = ZDT1(n_inputs=30)\n\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(\n        domain=domain,\n        ref_point={\"y1\": 1.0, \"y2\": 5.0},\n        surrogate_specs=BotorchSurrogates(\n            surrogates=[\n                FullyBayesianSingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y1\"]),\n                    # the following hyperparams do not need to be provided\n                    warmup_steps=WARMUP_STEPS,\n                    num_samples=NUM_SAMPLES,\n                    thinning=THINNING,\n                    model_type=\"saas\",\n                ),\n                FullyBayesianSingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y2\"]),\n                    # the following hyperparams do not need to be provided\n                    warmup_steps=WARMUP_STEPS,\n                    num_samples=NUM_SAMPLES,\n                    thinning=THINNING,\n                    model_type=\"saas\",\n                ),\n            ],\n        ),\n    )\n    return strategies.map(data_model)\n\n\nresults = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:34&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:34&lt;?, ?it/s, Current Best:=0.923]Run 0: 100%|██████████| 1/1 [00:34&lt;00:00, 34.67s/it, Current Best:=0.923]Run 0: 100%|██████████| 1/1 [00:34&lt;00:00, 34.67s/it, Current Best:=0.923]\n\n\nPlot the pareto front.\n\ntheoretical_front = benchmark.get_optima()\nfront = get_pareto_front(domain=benchmark.domain, experiments=results[0][0])\n\nfig, ax = plt.subplots()\nax.plot(theoretical_front.y1, theoretical_front.y2, label=\"theoretical Pareto front\")\n\nax.scatter(front.y1, front.y2, label=\"Mobo\")\nax.set_xlabel(\"f1\")\nax.set_ylabel(\"f2\")\n\n\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nShow the performance of the optimizer.\n\nfig, ax = plt.subplots()\n\nax.plot(results[0][1], label=\"Mobo, refpoint=(1, 5)\")\n\nax.set_ylabel(\"Hypervolume\")\nax.set_xlabel(\"Iteration\")\n\nax.legend()",
    "crumbs": [
      "Benchmarks",
      "ZDT1"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html",
    "href": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "",
    "text": "An example notebook for Bayesian optimisation on a molecular dataset using a Tanimoto fingerprint kernel and the photoswitch dataset.\\(\\newline\\) Paper: https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h \\(\\newline\\) Code: https://github.com/Ryan-Rhys/The-Photoswitch-Dataset \\(\\newline\\) This notebook is adapted from https://github.com/leojklarner/gauche/blob/main/notebooks/Bayesian%20Optimisation%20Over%20Molecules.ipynb \\(\\newline\\) The method of obtaining new data from a discrete dataset is explained in the notebook and the details of the dataset and the method are explained in the code and the paper respectively.",
    "crumbs": [
      "Benchmarks",
      "Bayesian Optimisation Over Molecules"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#imports",
    "href": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#imports",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport io\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.data.photoswitches import EXPERIMENTS\nfrom bofire.benchmarks.LookupTableBenchmark import LookupTableBenchmark\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import CategoricalMolecularInput, ContinuousOutput\nfrom bofire.data_models.molfeatures.api import FingerprintsFragments\nfrom bofire.data_models.objectives.api import MaximizeObjective\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, TanimotoGPSurrogate\nfrom bofire.runners.api import run\n\n\nwarnings.filterwarnings(\"ignore\")\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Bayesian Optimisation Over Molecules"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#benchmark",
    "href": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#benchmark",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Benchmark",
    "text": "Benchmark\ninput and output feature keys and extract them to get LookUpTable\n\nbenchmark = {\n    \"input\": \"SMILES\",\n    \"output\": \"E isomer pi-pi* wavelength in nm\",\n}\ndf = pd.read_json(io.StringIO(EXPERIMENTS))\nmain_file = pd.DataFrame(columns=[benchmark[\"input\"], benchmark[\"output\"]])\nnans = df[benchmark[\"output\"]].isnull().to_list()\nnan_indices = [nan for nan, x in enumerate(nans) if x]\nmain_file[benchmark[\"input\"]] = df[benchmark[\"input\"]].drop(nan_indices).to_list()\nmain_file[benchmark[\"output\"]] = (\n    df[benchmark[\"output\"]].dropna().to_numpy().reshape(-1, 1)\n)\ninput_feature = CategoricalMolecularInput(\n    key=benchmark[\"input\"],\n    categories=list(set(main_file[benchmark[\"input\"]].to_list())),\n)\nobjective = MaximizeObjective(\n    w=1.0,\n)\ninputs = Inputs(features=[input_feature])\noutput_feature = ContinuousOutput(key=benchmark[\"output\"], objective=objective)\noutputs = Outputs(features=[output_feature])\ndomain = Domain(inputs=inputs, outputs=outputs)",
    "crumbs": [
      "Benchmarks",
      "Bayesian Optimisation Over Molecules"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#random-vs-sobo-optimization",
    "href": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#random-vs-sobo-optimization",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Random vs SOBO optimization",
    "text": "Random vs SOBO optimization\nFor molecules, we use Tanimoto GP which has a Tanimoto kernel as default\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(20)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments[domain.outputs.get_keys()[0]].max()\n\n\nn_iter = 20 if not SMOKE_TEST else 1\nbo_results_set = []\nrandom_results_set = []\nn_iterations = 49 if not SMOKE_TEST else 1\n\nfor _ in range(n_iter):\n    Benchmark = LookupTableBenchmark(domain=domain, lookup_table=main_file)\n    sampled = sample(Benchmark.domain)\n    sampled_xy = Benchmark.f(sampled, return_complete=True)\n    random_results = run(\n        Benchmark,\n        strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n        n_iterations=n_iterations,\n        metric=best,\n        initial_sampler=sampled_xy,\n        n_runs=1,\n        n_procs=1,\n    )\n\n    specs = {Benchmark.domain.inputs.get_keys()[0]: FingerprintsFragments(n_bits=2048)}\n    surrogate = TanimotoGPSurrogate(\n        inputs=Benchmark.domain.inputs,\n        outputs=Benchmark.domain.outputs,\n        categorical_encodings=specs,\n    )\n\n    def sobo_factory(domain: Domain, surrogate=surrogate):\n        return strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qLogEI(),\n                surrogate_specs=BotorchSurrogates(surrogates=[surrogate]),\n            ),\n        )\n\n    qExpectedImprovement = qLogEI()\n    bo_results = run(\n        Benchmark,\n        strategy_factory=sobo_factory,\n        n_iterations=n_iterations,\n        metric=best,\n        initial_sampler=sampled_xy,\n        n_runs=1,\n        n_procs=1,\n    )\n    random_results_new = np.insert(\n        random_results[0][1].to_numpy(),\n        0,\n        best(Benchmark.domain, sampled_xy),\n    )\n    bo_results_new = np.insert(\n        bo_results[0][1].to_numpy(),\n        0,\n        best(Benchmark.domain, sampled_xy),\n    )\n    random_results_set.append(random_results_new)\n    bo_results_set.append(bo_results_new)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=503.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 69.22it/s, Current Best:=503.000]\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=503.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  4.52it/s, Current Best:=503.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  4.50it/s, Current Best:=503.000]",
    "crumbs": [
      "Benchmarks",
      "Bayesian Optimisation Over Molecules"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#performance",
    "href": "docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#performance",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Performance",
    "text": "Performance\nSOBO outperforms random search in terms of selecting molecules with high E isomer pi-pi* transition wavelength.\n\n# Define a confience interval function for plotting.\ndef ci(y):\n    return 1.96 * y.std(axis=0) / np.sqrt(n_iter)\n\n\nif not SMOKE_TEST:\n    iters = np.arange(n_iterations + 1)\n    y_rnd = np.asarray(random_results_set)\n    y_ei = np.asarray(bo_results_set)\n\n    y_rnd_mean = y_rnd.mean(axis=0)\n    y_ei_mean = y_ei.mean(axis=0)\n    y_rnd_std = y_rnd.std(axis=0)\n    y_ei_std = y_ei.std(axis=0)\n\n    lower_rnd = y_rnd_mean - y_rnd_std\n    upper_rnd = y_rnd_mean + y_rnd_std\n    lower_ei = y_ei_mean - y_ei_std\n    upper_ei = y_ei_mean + y_ei_std\n\n    plt.plot(iters, y_rnd_mean, label=\"Random\")\n    plt.fill_between(iters, lower_rnd, upper_rnd, alpha=0.2)\n    plt.plot(iters, y_ei_mean, label=\"SOBO\")\n    plt.fill_between(iters, lower_ei, upper_ei, alpha=0.2)\n    plt.xlabel(\"Number of Iterations\")\n    plt.ylabel(\"Best Objective Value\")\n    plt.legend(loc=\"lower right\")\n    plt.show()",
    "crumbs": [
      "Benchmarks",
      "Bayesian Optimisation Over Molecules"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/004-30dimBranin.html",
    "href": "docs/tutorials/benchmarks/004-30dimBranin.html",
    "title": "30dim Branin Benchmark with SAASBO",
    "section": "",
    "text": "This is a port from https://github.com/pytorch/botorch/blob/main/tutorials/saasbo.ipynb ## Imports\nimport os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Branin30\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    FullyBayesianSingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nN_ITERATIONS = 10 if not SMOKE_TEST else 1\nBATCH_SIZE = 5 if not SMOKE_TEST else 1\nWARMUP_STEPS = 256 if not SMOKE_TEST else 32\nNUM_SAMPLES = 128 if not SMOKE_TEST else 16\nTHINNING = 16",
    "crumbs": [
      "Benchmarks",
      "30dim Branin Benchmark with SAASBO"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/004-30dimBranin.html#random-optimization",
    "href": "docs/tutorials/benchmarks/004-30dimBranin.html#random-optimization",
    "title": "30dim Branin Benchmark with SAASBO",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nrandom_results = run(\n    Branin30(),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=N_ITERATIONS,\n    metric=best,\n    initial_sampler=sample,\n    n_candidates_per_proposal=5,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=12.855]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 18.66it/s, Current Best:=12.855]",
    "crumbs": [
      "Benchmarks",
      "30dim Branin Benchmark with SAASBO"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/004-30dimBranin.html#saasbo-optimization",
    "href": "docs/tutorials/benchmarks/004-30dimBranin.html#saasbo-optimization",
    "title": "30dim Branin Benchmark with SAASBO",
    "section": "SAASBO Optimization",
    "text": "SAASBO Optimization\n\nbenchmark = Branin30()\n\n\ndef strategy_factory(domain: Domain):\n    data_model = SoboStrategy(\n        domain=domain,\n        acquisition_function=qLogEI(),\n        surrogate_specs=BotorchSurrogates(\n            surrogates=[\n                FullyBayesianSingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs,\n                    model_type=\"saas\",\n                    # the following hyperparams do not need to be provided\n                    warmup_steps=WARMUP_STEPS,\n                    num_samples=NUM_SAMPLES,\n                    thinning=THINNING,\n                ),\n            ],\n        ),\n    )\n    return strategies.map(data_model)\n\n\nrandom_results = run(\n    Branin30(),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=best,\n    initial_sampler=sample,\n    n_candidates_per_proposal=5,\n    n_runs=1,\n    n_procs=1,\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:07&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:07&lt;?, ?it/s, Current Best:=4.460]Run 0: 100%|██████████| 1/1 [00:07&lt;00:00,  7.63s/it, Current Best:=4.460]Run 0: 100%|██████████| 1/1 [00:07&lt;00:00,  7.63s/it, Current Best:=4.460]",
    "crumbs": [
      "Benchmarks",
      "30dim Branin Benchmark with SAASBO"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html",
    "title": "DTLZ2 Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import MinimizeObjective\nfrom bofire.data_models.strategies.api import (\n    MoboStrategy,\n    QparegoStrategy,\n    RandomStrategy,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html#imports",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html#imports",
    "title": "DTLZ2 Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import MinimizeObjective\nfrom bofire.data_models.strategies.api import (\n    MoboStrategy,\n    QparegoStrategy,\n    RandomStrategy,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html#manual-setup-of-the-optimization-domain",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html#manual-setup-of-the-optimization-domain",
    "title": "DTLZ2 Benchmark",
    "section": "Manual setup of the optimization domain",
    "text": "Manual setup of the optimization domain\nThe following cell shows how to manually setup the optimization problem in BoFire for didactic purposes. In the following the implemented benchmark module is then used.\n\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i}\", bounds=(0, 1)) for i in range(6)],\n)\n# here the minimize objective is used, if you want to maximize you have to use the maximize objective.\noutput_features = Outputs(\n    features=[\n        ContinuousOutput(key=f\"f_{i}\", objective=MinimizeObjective(w=1.0))\n        for i in range(2)\n    ],\n)\n# no constraints are present so we can create the domain\ndomain = Domain(inputs=input_features, outputs=output_features)",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html#random-strategy",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html#random-strategy",
    "title": "DTLZ2 Benchmark",
    "section": "Random Strategy",
    "text": "Random Strategy\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return compute_hypervolume(domain, experiments, ref_point={\"f_0\": 1.1, \"f_1\": 1.1})\n\n\nrandom_results = run(\n    DTLZ2(dim=6),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=0.023]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 50.44it/s, Current Best:=0.023]",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html#mobo-strategy",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html#mobo-strategy",
    "title": "DTLZ2 Benchmark",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\n\nAutomatic run\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(domain=domain, ref_point={\"f_0\": 1.1, \"f_1\": 1.1})\n    return strategies.map(data_model)\n\n\nresults = run(\n    DTLZ2(dim=6),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:03&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:03&lt;?, ?it/s, Current Best:=0.022]Run 0: 100%|██████████| 1/1 [00:03&lt;00:00,  3.52s/it, Current Best:=0.022]Run 0: 100%|██████████| 1/1 [00:03&lt;00:00,  3.52s/it, Current Best:=0.022]\n\n\n\n\nManual setup\n\nUsing the default Models\n\n# we get the domain from the benchmark module, in real use case we have to build it on our own\n# make sure that the objective is set correctly\ndomain = DTLZ2(dim=6).domain\n# we generate training data\nexperiments = DTLZ2(dim=6).f(domain.inputs.sample(10), return_complete=True)\n# we setup the strategy\n# providing of a reference point is not mandatory but can help\n# the reference point has to be wrt to the assigned objective always worse than the points on the paretofront.\ndata_model = MoboStrategy(domain=domain, ref_point={\"f_0\": 1.1, \"f_1\": 1.1})\nrecommender = strategies.map(data_model=data_model)\n# we tell the strategy our historical data\nrecommender.tell(experiments=experiments)\n# we ask for a new point to evaluate\ncandidates = recommender.ask(candidate_count=1)\n# we show the candidate\ndisplay(candidates)\n# this candidate has to be then provided to the benchmark function and evaluated and then told back to the optimizer to get the next candidate\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nf_0_pred\nf_1_pred\nf_0_sd\nf_1_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n0.670926\n0.303248\n0.0\n0.0\n0.539207\n0.014902\n0.569709\n0.691875\n0.165453\n0.173311\n-0.569709\n-0.691875\n\n\n\n\n\n\n\n\n\nSetup specific models\n\nfrom bofire.data_models.kernels.api import RBFKernel, ScaleKernel\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\n\n\n# in this case you would use non default kernels for the different outputs\n# it is also possible to build the models for a subset of the complete features\ndata_model = MoboStrategy(\n    domain=domain,\n    ref_point={\"f_0\": 1.1, \"f_1\": 1.1},\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[0]]),\n                kernel=ScaleKernel(base_kernel=RBFKernel(ard=True)),\n            ),\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[1]]),\n                kernel=ScaleKernel(base_kernel=RBFKernel(ard=False)),\n            ),\n        ],\n    ),\n)\nrecommender = strategies.map(data_model=data_model)\n# we tell the strategy our historical data\nrecommender.tell(experiments=experiments)\n# we ask for a new point to evaluate\ncandidates = recommender.ask(candidate_count=1)\n# we show the candidate\ndisplay(candidates)\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nf_0_pred\nf_1_pred\nf_0_sd\nf_1_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n0.773635\n0.279132\n0.0\n0.0\n1.0\n0.0\n0.438232\n0.791216\n0.095055\n0.272711\n-0.438232\n-0.791216",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html#qparego-strategy",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html#qparego-strategy",
    "title": "DTLZ2 Benchmark",
    "section": "QPAREGO Strategy",
    "text": "QPAREGO Strategy\n\nresults_qparego = run(\n    DTLZ2(dim=6),\n    strategy_factory=lambda domain: strategies.map(QparegoStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/acquisition/monte_carlo.py:502: NumericsWarning:\n\nqNoisyExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n\n     qNoisyExpectedImprovement   --&gt;     qLogNoisyExpectedImprovement \n\ninstead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n\nRun 0:   0%|          | 0/1 [00:02&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:02&lt;?, ?it/s, Current Best:=0.124]Run 0: 100%|██████████| 1/1 [00:02&lt;00:00,  2.08s/it, Current Best:=0.124]Run 0: 100%|██████████| 1/1 [00:02&lt;00:00,  2.08s/it, Current Best:=0.124]",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/002-DTLZ2.html#performance-plot",
    "href": "docs/tutorials/benchmarks/002-DTLZ2.html#performance-plot",
    "title": "DTLZ2 Benchmark",
    "section": "Performance Plot",
    "text": "Performance Plot\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots()\n\nax.scatter(results[0][0].f_0, results[0][0].f_1, label=\"qehvi\")\nax.scatter(results_qparego[0][0].f_0, results_qparego[0][0].f_1, label=\"qparego\")\nax.scatter(random_results[0][0].f_0, random_results[0][0].f_1, label=\"random\")\n\nax.legend()\n\nax.set_xlabel(\"f_0\")\nax.set_ylabel(\"f_1\")\n\nfig.show()",
    "crumbs": [
      "Benchmarks",
      "DTLZ2 Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/index_kernel.html",
    "href": "docs/tutorials/basic_examples/index_kernel.html",
    "title": "Introduction to index kernel and positive index kernel.",
    "section": "",
    "text": "The Index kernel models categorical variables by assigning each category an index and learning a low-rank representation of the kernel matrix. This is particularly useful for ordered categorical variables or when categories have some inherent structure. Unlike Hamming distance kernel which assumes binary correlation (same or different), Index kernels try to learn the correlation between the categories while fitting GP.\nIn this tutorial, we show the steps to create a GP surrogate using Index and Positive Index kernels. One can extend the steps to incorporate the shown feature for Bayesian optimization.\nWe use the aniline_cn_crosscoupling data-set.\n\n# import basic python libraries\nimport numpy as np\nimport pandas as pd\nimport json\n\n# import bofire components\nfrom bofire.data_models.kernels.api import IndexKernel, RBFKernel, PositiveIndexKernel, AdditiveKernel, ScaleKernel\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.domain.api import Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput, CategoricalInput\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\nimport bofire.surrogates.diagnostics as diagnostics\nfrom bofire.data_models.priors.api import GreaterThan\n\n# import data\nfrom bofire.benchmarks.data.aniline_cn_crosscoupling import EXPERIMENTS\n\nLoad the data, get the basic properties out of the data and perform the train-test split\n\ndata_df = pd.DataFrame(json.loads(EXPERIMENTS))\ncategories_catalyst = list(data_df[\"catalyst\"].unique())\ncategories_base = list(data_df[\"base\"].unique())\nbounds_temperature = (data_df[\"temperature\"].min(), data_df[\"temperature\"].max())\nbounds_t_res = (data_df[\"t_res\"].min(), data_df[\"t_res\"].max())\nbounds_base_equivalents = (data_df[\"base_equivalents\"].min(), data_df[\"base_equivalents\"].max())\n\ntest_size = 0.3\ntrain_data = data_df.sample(frac=1 - test_size, random_state=42)\ntest_data = data_df.drop(train_data.index)\n\nDefine the input and output bofire variables\n\ninputs = Inputs(\n        features=[\n            ContinuousInput(key=\"temperature\", bounds=bounds_temperature),\n            ContinuousInput(key=\"t_res\", bounds=bounds_t_res),\n            ContinuousInput(key=\"base_equivalents\", bounds=bounds_base_equivalents),\n            CategoricalInput(key='catalyst', categories=categories_catalyst),\n            CategoricalInput(key='base', categories=categories_base)\n        ]\n    )\noutputs = Outputs(features=[ContinuousOutput(key=\"yld\")])\n\nIn this example, we will use RBF kernel for the continuous variables and Index Kernel for the categorical variables. The final kernel will be linear combination of each kernel. Users are free to combine the kernels according to their choice.\n\nkernel_list_index = [\n    ScaleKernel(base_kernel=RBFKernel(ard=True, lengthscale_constraint=GreaterThan(lower_bound=2.500e-02), features=['temperature', 't_res', 'base_equivalents'])),\n    ScaleKernel(base_kernel=IndexKernel(num_categories=len(categories_catalyst), rank=1, features=['catalyst'])),\n    ScaleKernel(base_kernel=IndexKernel(num_categories=len(categories_base), rank=1, features=['base']))\n]\nfinal_kernel_index = AdditiveKernel(kernels=kernel_list_index)\ndata_model_index = SingleTaskGPSurrogate(\n    inputs=inputs,\n    outputs=outputs,\n    kernel=final_kernel_index,\n    )\nsurrogate_index = surrogates.map(data_model_index)\nsurrogate_index.fit(train_data)\nprint(\"MAE:\", diagnostics.mean_absolute_error(surrogate_index.predict(test_data)[\"yld_pred\"], test_data[\"yld\"]))\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:1134: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nMAE: 0.2970088128835723\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/interpolation.py:71: UserWarning:\n\ntorch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:654.)\n\n\n\nMany a times the Index Kernel do not generate positive definite correlations matrices. Positive Index Kernel kernel addresses this by using Cholesky decomposition with positive elements only. So, off diagonal elements are always positive and the diagonal elements are normalized to 1 for a target task.\nNOTE: This kernel should only be used when the correlation between different categories is expected to be positive.\nOne can replace IndexKernel with PositiveIndexKernel to make use of Positive Index Kernels.",
    "crumbs": [
      "Basic Examples",
      "Introduction to index kernel and positive index kernel."
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/basic_terminology.html",
    "href": "docs/tutorials/basic_examples/basic_terminology.html",
    "title": "Basic terminology",
    "section": "",
    "text": "In the following it is showed how to setup optimization problems in BoFire and how to use strategies to solve them.",
    "crumbs": [
      "Basic Examples",
      "Basic terminology"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/basic_terminology.html#setting-up-the-optimization-problem",
    "href": "docs/tutorials/basic_examples/basic_terminology.html#setting-up-the-optimization-problem",
    "title": "Basic terminology",
    "section": "Setting up the optimization problem",
    "text": "Setting up the optimization problem\nIn BoFire, an optimization problem is defined by defining a domain containing input and output features, as well as optionally including constraints.\n\nFeatures\nInput features can be continuous, discrete, categorical.\nWe also support a range of specialized inputs that make defining your experiments easier, such as: - CategoricalMolecularInput allows transformations of molecules to featurizations (Fingerprints, Fragments and more). - TaskInput enables transfer learning and multi-fidelity methods, where you have access to similar experiments that can inform your optimization. - *DescriptorInput gives additional information about its value, combining the data with its significance.\n\nfrom bofire.data_models.features.api import (\n    CategoricalDescriptorInput,\n    CategoricalInput,\n    ContinuousInput,\n    DiscreteInput,\n)\n\n\nx1 = ContinuousInput(key=\"conc_A\", bounds=[0, 1])\nx2 = ContinuousInput(key=\"conc_B\", bounds=[0, 1])\nx3 = ContinuousInput(key=\"conc_C\", bounds=[0, 1])\nx4 = DiscreteInput(key=\"temperature\", values=[20, 50, 90], unit=\"°C\")\n\nx5 = CategoricalInput(\n    key=\"catalyst\",\n    categories=[\"cat_X\", \"cat_Y\", \"cat_Z\"],\n    allowed=[\n        True,\n        True,\n        False,\n    ],  # we have run out of catalyst Z, but still want to model past experiments\n)\n\nx6 = CategoricalDescriptorInput(\n    key=\"solvent\",\n    categories=[\"water\", \"methanol\", \"ethanol\"],\n    descriptors=[\"viscosity (mPa s)\", \"density (kg/m3)\"],\n    values=[[1.0, 997], [0.59, 792], [1.2, 789]],\n)\n\nWe can define both continuous and categorical outputs. Each output feature should have an objective, which determines if we aim to minimize, maximize, or drive the feature to a given value. Furthermore, we can define weights between 0 and 1 in case the objectives should not be weighted equally.\n\nfrom bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective\n\n\nobjective1 = MaximizeObjective(\n    w=1.0,\n    bounds=[0.0, 1.0],\n)\ny1 = ContinuousOutput(key=\"yield\", objective=objective1)\n\nobjective2 = MinimizeObjective(w=1.0)\ny2 = ContinuousOutput(key=\"time_taken\", objective=objective2)\n\nIn- and output features are collected in respective feature lists, which can be summarized with the get_reps_df method.\n\nfrom bofire.data_models.domain.api import Inputs, Outputs\n\n\ninput_features = Inputs(features=[x1, x2, x3, x4, x5, x6])\noutput_features = Outputs(features=[y1, y2])\n\ninput_features.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nconc_A\nContinuousInput\n[0.0,1.0]\n\n\nconc_B\nContinuousInput\n[0.0,1.0]\n\n\nconc_C\nContinuousInput\n[0.0,1.0]\n\n\ntemperature\nDiscreteInput\ntype='DiscreteInput' key='temperature' unit='°...\n\n\nsolvent\nCategoricalDescriptorInput\n3 categories\n\n\ncatalyst\nCategoricalInput\n3 categories\n\n\n\n\n\n\n\n\noutput_features.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntime_taken\nContinuousOutput\nContinuousOutputFeature\n\n\nyield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\nIndividual features can be retrieved by name, and a collection of features can be retrieved with a list of names.\n\ninput_features.get_by_key(\"catalyst\")\n\nCategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])\n\n\n\ninput_features.get_by_keys([\"catalyst\", \"conc_B\"])\n\nInputs(type='Inputs', features=[ContinuousInput(type='ContinuousInput', key='conc_B', unit=None, bounds=[0.0, 1.0], local_relative_bounds=None, stepsize=None, allow_zero=False), CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\nFeatures of a specific type can be returned by the get method. By using the exact argument, we can force the method to only return features that match the class exactly.\n\ninput_features.get(CategoricalInput)\n\nInputs(type='Inputs', features=[CategoricalDescriptorInput(type='CategoricalDescriptorInput', key='solvent', categories=['water', 'methanol', 'ethanol'], allowed=[True, True, True], descriptors=['viscosity (mPa s)', 'density (kg/m3)'], values=[[1.0, 997.0], [0.59, 792.0], [1.2, 789.0]]), CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\n\ninput_features.get(CategoricalInput, exact=True)\n\nInputs(type='Inputs', features=[CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\nThe get_keys method follows the same logic as the get method, but returns just the keys of the features instead of the features itself.\n\ninput_features.get_keys(CategoricalInput)\n\n['solvent', 'catalyst']\n\n\nThe input feature container further provides methods to return a feature container with only all fixed or all free features.\n\nfree_inputs = input_features.get_free()\nfixed_inputs = input_features.get_fixed()\n\nOne can uniformly sample from individual input features.\n\nx5.sample(2)\n\n0    cat_X\n1    cat_X\nName: catalyst, dtype: str\n\n\nOr directly from input feature containers, uniform, sobol and LHS sampling is possible. A default, uniform sampling is used.\n\nfrom bofire.data_models.enum import SamplingMethodEnum\n\n\nX = input_features.sample(n=10, method=SamplingMethodEnum.LHS)\n\nX\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\ntemperature\nsolvent\ncatalyst\n\n\n\n\n0\n0.044681\n0.039582\n0.703059\n90.0\nethanol\ncat_X\n\n\n1\n0.534625\n0.545594\n0.412853\n90.0\nwater\ncat_Y\n\n\n2\n0.830685\n0.775482\n0.929770\n90.0\nethanol\ncat_X\n\n\n3\n0.689623\n0.492656\n0.812667\n90.0\nmethanol\ncat_Y\n\n\n4\n0.781107\n0.835031\n0.599383\n50.0\nmethanol\ncat_X\n\n\n5\n0.364435\n0.647562\n0.649083\n50.0\nwater\ncat_X\n\n\n6\n0.965360\n0.165169\n0.088658\n50.0\nmethanol\ncat_Y\n\n\n7\n0.172554\n0.968882\n0.375345\n20.0\nmethanol\ncat_Y\n\n\n8\n0.471795\n0.292893\n0.119034\n20.0\nwater\ncat_X\n\n\n9\n0.212652\n0.328472\n0.226612\n20.0\nethanol\ncat_Y\n\n\n\n\n\n\n\n\n\nConstraints\nThe search space can be further defined by constraints on the input features. BoFire supports linear equality and inequality constraints, as well as non-linear equality and inequality constraints.\n\nLinear constraints\nLinearEqualityConstraint and LinearInequalityConstraint are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input features they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\).\n\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n)\n\n\n# A mixture: x1 + x2 + x3 = 1\nconstr1 = LinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    coefficients=[1, 1, 1],\n    rhs=1,\n)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = LinearInequalityConstraint(\n    features=[\"conc_A\", \"conc_C\"],\n    coefficients=[1, 2],\n    rhs=0.8,\n)\n\nLinear constraints can only operate on ContinuousInput features.\n\n\nNonlinear constraints\nNonlinearEqualityConstraint and NonlinearInequalityConstraint take any expression that can be evaluated by pandas.eval, including mathematical operators such as sin, exp, log10 or exponentiation. So far, they cannot be used in any optimizations.\n\nfrom bofire.data_models.constraints.api import NonlinearEqualityConstraint\n\n\n# The unit circle: x1**2 + x2**2 = 1\nconst3 = NonlinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\"], expression=\"conc_A**2 + conc_B**2 - 1\"\n)\nconst3\n\nNonlinearEqualityConstraint(type='NonlinearEqualityConstraint', features=['conc_A', 'conc_B'], expression='conc_A**2 + conc_B**2 - 1', jacobian_expression='[2*conc_A, 2*conc_B]', hessian_expression='[[2, 0], [0, 2]]')\n\n\n\n\nCombinatorial constraint\nUse NChooseKConstraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe.\n\nfrom bofire.data_models.constraints.api import NChooseKConstraint\n\n\n# Only 1 or 2 out of 3 compounds can be present (have non-zero concentration)\nconstr5 = NChooseKConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    min_count=1,\n    max_count=2,\n    none_also_valid=False,\n)\nconstr5\n\nNChooseKConstraint(type='NChooseKConstraint', features=['conc_A', 'conc_B', 'conc_C'], min_count=1, max_count=2, none_also_valid=False)\n\n\nNote that we have to set a boolean, if none is also a valid selection, e.g. if we want to have 0, 1, or 2 of the ingredients in our recipe.\n\n\nCategoricalExcludeConstraint\nThe CategoricalExcludeConstraint can be used to exclude certain combinations of categories between categorical features or exclude a combination between categories and numerical values. So far, this constraint is only supported by the RandomStrategy.\nIn the example below, it would be forbidden that cat_C is used together with one of the solvents methanol or ethanol.\n\nfrom bofire.data_models.constraints.api import (\n    CategoricalExcludeConstraint,\n    SelectionCondition,\n)\n\n\nfeat_cat = CategoricalInput(\n    key=\"cat1\",\n    categories=[\"cat_A\", \"cat_B\", \"cat_C\"],\n)\nfeat_solvent = CategoricalInput(\n    key=\"solvent\", categories=[\"water\", \"methanol\", \"ethanol\"]\n)\n\nconstr6 = CategoricalExcludeConstraint(\n    features=[\"cat1\", \"solvent\"],\n    conditions=[\n        SelectionCondition(selection=[\"cat_C\"]),\n        SelectionCondition(selection=[\"methanol\", \"ethanol\"]),\n    ],\n)\n\nThe next example shows how to forbid that solvent ethanol is used at a temperature higher than 40°C, this is achieved by using a ThresholdCondition.\n\nfrom bofire.data_models.constraints.api import ThresholdCondition\n\n\nfeat_temp = ContinuousInput(\n    key=\"temperature\",\n    bounds=[0, 100],\n    unit=\"°C\",\n)\nconstr7 = CategoricalExcludeConstraint(\n    features=[\"solvent\", \"temperature\"],\n    conditions=[\n        SelectionCondition(selection=[\"water\"]),\n        ThresholdCondition(\n            threshold=40,\n            operator=\"&gt;=\",\n        ),\n    ],\n)\n\nSimilar to the features, constraints can be grouped in a container which acts as the union constraints.\n\nfrom bofire.data_models.domain.api import Constraints\n\n\nconstraints = Constraints(constraints=[constr1, constr2])\n\nA summary of the constraints can be obtained by the method get_reps_df:\n\nconstraints.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n0\nLinearEqualityConstraint\ntype='LinearEqualityConstraint' features=['con...\n\n\n1\nLinearInequalityConstraint\ntype='LinearInequalityConstraint' features=['c...\n\n\n\n\n\n\n\nWe can check whether a point satisfies individual constraints or the list of constraints.\n\nconstr2.is_fulfilled(X).values\n\narray([False, False, False, False, False, False, False, False,  True,\n        True])\n\n\nOutput constraints can be setup via sigmoid-shaped objectives passed as argument to the respective feature, which can then also be plotted.\n\nfrom bofire.data_models.objectives.api import MinimizeSigmoidObjective\nfrom bofire.plot.api import plot_objective_plotly\n\n\noutput_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5)\ny3 = ContinuousOutput(key=\"y3\", objective=output_constraint)\n\noutput_features = Outputs(features=[y1, y2, y3])\n\nfig = plot_objective_plotly(feature=y3, lower=0, upper=1)\n\nfig.show()\n\n                            \n                                            \n\n\n\n\n\nThe domain\nThe domain holds then all information about an optimization problem and can be understood as a search space definition. A detailed description of the domain can be found in docs.\n\nfrom bofire.data_models.domain.api import Domain\n\n\ndomain = Domain(inputs=input_features, outputs=output_features, constraints=constraints)\n\nIn addition one can instantiate the domain also just from lists.\n\ndomain_single_objective = Domain.from_lists(\n    inputs=[x1, x2, x3, x4, x5, x6],\n    outputs=[y1],\n    constraints=[],\n)",
    "crumbs": [
      "Basic Examples",
      "Basic terminology"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/basic_terminology.html#optimization",
    "href": "docs/tutorials/basic_examples/basic_terminology.html#optimization",
    "title": "Basic terminology",
    "section": "Optimization",
    "text": "Optimization\nTo solve the optimization problem, we further need a solving strategy. BoFire supports strategies without a prediction model such as a random strategy and predictive strategies which are based on a prediction model.\nAll strategies contain an ask method returning a defined number of candidate experiments.\n\nRandom Strategy\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy\n\n\nstrategy_data_model = RandomStrategy(domain=domain)\n\nrandom_strategy = strategies.map(strategy_data_model)\nrandom_candidates = random_strategy.ask(2)\n\nrandom_candidates\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\ntemperature\nsolvent\ncatalyst\n\n\n\n\n0\n0.362893\n0.450306\n0.186801\n20.0\nwater\ncat_X\n\n\n1\n0.194838\n0.762846\n0.042316\n20.0\nethanol\ncat_Y\n\n\n\n\n\n\n\n\n\nSingle objective Bayesian Optimization strategy\nSince a predictive strategy includes a prediction model, we need to generate some historical data, which we can afterwards pass as training data to the strategy via the tell method.\nFor didactic purposes we just choose here from one of our benchmark methods.\n\nfrom bofire.benchmarks.single import Himmelblau\n\n\nbenchmark = Himmelblau()\n\n(benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nx_1\nContinuousInput\n[-6.0,6.0]\n\n\nx_2\nContinuousInput\n[-6.0,6.0]\n\n\ny\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\nGenerating some initial data works as follows:\n\nsamples = benchmark.domain.inputs.sample(10)\n\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n-2.722807\n-3.636431\n64.423899\n1\n\n\n1\n-3.773426\n-1.212723\n90.645545\n1\n\n\n2\n-5.871666\n2.737385\n716.092953\n1\n\n\n3\n-4.910945\n0.144689\n317.254840\n1\n\n\n4\n1.074801\n3.140154\n60.439431\n1\n\n\n5\n-1.079407\n-3.009936\n165.950315\n1\n\n\n6\n-3.460026\n-5.064775\n247.546963\n1\n\n\n7\n-3.171910\n-3.813293\n41.674895\n1\n\n\n8\n4.641882\n-3.438084\n140.072782\n1\n\n\n9\n4.257262\n0.521745\n64.565173\n1\n\n\n\n\n\n\n\nLet’s setup the SOBO strategy and ask for a candidate. First we need a serializable data model that contains the hyperparameters.\n\nfrom pprint import pprint\n\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM\n\n\nsobo_strategy_data_model = SoboStrategyDM(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# print information about hyperparameters\nprint(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function)\nprint()\nprint(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type)\nprint()\nprint(\"Surrogate's kernel:\")\npprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump())\n\nAcquisition function: type='qLogNEI' prune_baseline=True n_mc_samples=512\n\nSurrogate type: SingleTaskGPSurrogate\n\nSurrogate's kernel:\n{'ard': True,\n 'features': None,\n 'lengthscale_constraint': None,\n 'lengthscale_prior': {'loc': 1.4142135623730951,\n                       'loc_scaling': 0.5,\n                       'scale': 1.7320508075688772,\n                       'scale_scaling': 0.0,\n                       'type': 'DimensionalityScaledLogNormalPrior'},\n 'type': 'RBFKernel'}\n\n\nThe actual strategy can then be created via the mapper function.\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\nsobo_strategy.tell(experiments=experiments)\nsobo_strategy.ask(candidate_count=1)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n-4.561442\n-2.840763\n-44.071845\n78.057162\n44.071845\n\n\n\n\n\n\n\nAn alternative way is calling the strategy’s constructor directly.\n\nsobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model)\n\nThe latter way is helpful to keep type information.\n\n\nDesign of Experiments\nAs a simple example for the DoE functionalities we consider the task of finding a D-optimal design for a fully-quadratic model with three design variables with bounds (0,1) and a mixture constraint.\nWe define the design space including the constraint as a domain. Then we pass it to the optimization routine and specify the model. If the user does not indicate a number of experiments it will be chosen automatically based on the number of model terms.\n\nimport numpy as np\n\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1])\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\n\n\n\n\n0\n0.0\n0.5\n0.5\n\n\n1\n0.5\n0.0\n0.5\n\n\n2\n0.5\n0.5\n0.0\n\n\n3\n0.5\n0.0\n0.5\n\n\n4\n0.0\n0.5\n0.5\n\n\n5\n1.0\n0.0\n0.0\n\n\n6\n0.0\n0.0\n1.0\n\n\n7\n1.0\n0.0\n0.0\n\n\n8\n0.0\n1.0\n0.0\n\n\n9\n0.0\n0.5\n0.5\n\n\n10\n0.0\n1.0\n0.0\n\n\n11\n0.5\n0.5\n0.0\n\n\n\n\n\n\n\nThe resulting design looks like this:\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"fully-quadratic model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[x1.key],\n    ys=candidates[x2.key],\n    zs=candidates[x3.key],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n)",
    "crumbs": [
      "Basic Examples",
      "Basic terminology"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Reaction_Optimization_Example.html",
    "href": "docs/tutorials/basic_examples/Reaction_Optimization_Example.html",
    "title": "Getting started by Example: Optimization of Reaction Conditions",
    "section": "",
    "text": "In this example we take on a reaction condition optimization problem: Suppose you have some simple reaction where two ingredients A and B react to C.\nOur reactors can be temperature controlled, and we can use different solvents. Furthermore, we can dilute our reaction mixture by using a different solvent volume. parameters like the temperature or the solvent volume are continuous parameters, where we have to set lower and upper bounds for. The temperature can be controlled between 0 and 60°C and the solvent volume between 20 and 90 ml.\nParameters like the use of which solvent, where there’s a choice of either this or that, are categorical parameters. Here we can choose between MeOH, THF and Dioxane.\nFor now we only wish top optimize the Reaction yield, making this a single objective optimization problem.\nBelow we’ll see how to perform such an optimization using bofire, Utilizing a Single Objective Bayesian Optimization (SOBO) Strategy\n# python imports we'll need in this notebook\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nprint(f\"SMOKE_TEST: {SMOKE_TEST}\")\n\nSMOKE_TEST: 1",
    "crumbs": [
      "Basic Examples",
      "Getting started by Example: Optimization of Reaction Conditions"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Reaction_Optimization_Example.html#setting-up-the-optimization-problem-as-a-reaction-domain",
    "href": "docs/tutorials/basic_examples/Reaction_Optimization_Example.html#setting-up-the-optimization-problem-as-a-reaction-domain",
    "title": "Getting started by Example: Optimization of Reaction Conditions",
    "section": "Setting up the optimization problem as a Reaction Domain",
    "text": "Setting up the optimization problem as a Reaction Domain\n\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import (  # we won't need all of those.\n    CategoricalInput,\n    ContinuousInput,\n    ContinuousOutput,\n)\n\n\n# We wish the temperature of the reaction to be between 0 and 60 °C\ntemperature_feature = ContinuousInput(key=\"Temperature\", bounds=[0.0, 60.0], unit=\"°C\")\n\n# Solvent Amount\nsolvent_amount_feature = ContinuousInput(key=\"Solvent Volume\", bounds=[20, 90])\n\n# we have a couple of solvents in stock, which we'd like to use\nsolvent_type_feature = CategoricalInput(\n    key=\"Solvent Type\",\n    categories=[\"MeOH\", \"THF\", \"Dioxane\"],\n)\n\n\n# gather all individual features\ninput_features = Inputs(\n    features=[\n        temperature_feature,\n        solvent_type_feature,\n        solvent_amount_feature,\n    ],\n)\n\n\n# outputs: we wish to maximize the Yield\n# import Maximize Objective to tell the optimizer you wish to optimize\nfrom bofire.data_models.objectives.api import MaximizeObjective\n\n\nobjective = MaximizeObjective(\n    w=1.0,\n)\nyield_feature = ContinuousOutput(key=\"Yield\", objective=objective)\n# create an output feature\noutput_features = Outputs(features=[yield_feature])\n\n\nobjective\n\nMaximizeObjective(type='MaximizeObjective', w=1.0, bounds=[0, 1])\n\n\n\n# we now have\nprint(\"input_features:\", input_features)\nprint(\"output_features:\", output_features)\n\ninput_features: type='Inputs' features=[ContinuousInput(type='ContinuousInput', key='Temperature', unit='°C', bounds=[0.0, 60.0], local_relative_bounds=None, stepsize=None, allow_zero=False), CategoricalInput(type='CategoricalInput', key='Solvent Type', categories=['MeOH', 'THF', 'Dioxane'], allowed=[True, True, True]), ContinuousInput(type='ContinuousInput', key='Solvent Volume', unit=None, bounds=[20.0, 90.0], local_relative_bounds=None, stepsize=None, allow_zero=False)]\noutput_features: type='Outputs' features=[ContinuousOutput(type='ContinuousOutput', key='Yield', unit=None, objective=MaximizeObjective(type='MaximizeObjective', w=1.0, bounds=[0, 1]))]\n\n\n\n# The domain is now the object that holds the entire optimization problem / problem definition.\ndomain = Domain(\n    inputs=input_features,\n    outputs=output_features,\n)\n\n\n# you can now have a pretty printout of your domain via\n(domain.inputs + domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSolvent Volume\nContinuousInput\n[20.0,90.0]\n\n\nTemperature\nContinuousInput\n[0.0,60.0]\n\n\nSolvent Type\nCategoricalInput\n3 categories\n\n\nYield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\n\n# and you can access your domain features via\nfor (\n    feature_key\n) in domain.inputs.get_keys():  # this will get all the feature names and loop over them\n    input_feature = domain.inputs.get_by_key(\n        feature_key,\n    )  # we can extract the individual feature object by asking for it by name\n    print(feature_key, \"|\", input_feature)\n\nSolvent Volume | [20.0,90.0]\nTemperature | [0.0,60.0]\nSolvent Type | 3 categories\n\n\n\n# as well as the output features as\n# and you can access your domain features via\nfor feature_key in (\n    domain.outputs.get_keys()\n):  # this will get all the feature names and loop over them\n    output_feature = domain.outputs.get_by_key(\n        feature_key,\n    )  # we can extract the individual feature object by asking for it by name\n    print(feature_key, \" | \", output_feature.__repr__())\n\nYield  |  ContinuousOutput(type='ContinuousOutput', key='Yield', unit=None, objective=MaximizeObjective(type='MaximizeObjective', w=1.0, bounds=[0, 1]))\n\n\n\n(domain.inputs + domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSolvent Volume\nContinuousInput\n[20.0,90.0]\n\n\nTemperature\nContinuousInput\n[0.0,60.0]\n\n\nSolvent Type\nCategoricalInput\n3 categories\n\n\nYield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\n\nImport a toy Reaction to play around with\nWe’ve prepared a reaction emulator, which you can use to emulate a real experiment below.\n\n# Reaction Optimization Notebook util code\nT0 = 25\nT1 = 100\ne0 = np.exp((T1 + 0) / T0)\ne60 = np.exp((T1 + 60) / T0)\nde = e60 - e0\n\nboiling_points = {  # in °C\n    \"MeOH\": 64.7,\n    \"THF\": 66.0,\n    \"Dioxane\": 101.0,\n}\ndensity = {  # in kg/l\n    \"MeOH\": 0.792,\n    \"THF\": 0.886,\n    \"Dioxane\": 1.03,\n}\n# create dict from individual dicts\ndescs = {\n    \"boiling_points\": boiling_points,\n    \"density\": density,\n}\nsolvent_descriptors = pd.DataFrame(descs)\n\n\n# these functions are for faking real experimental data ;)\ndef calc_volume_fact(V):\n    # 20-90\n    # max at 75 = 1\n    # min at 20 = 0.7\n    # at 90=0.5\n    x = (V - 20) / 70\n    x = 0.5 + (x - 0.75) * 0.1 + (x - 0.4) ** 2\n    return x\n\n\ndef calc_rhofact(solvent_type, Tfact):\n    #  between 0.7 and 1.1\n    x = solvent_descriptors[\"density\"][solvent_type]\n    x = (1.5 - x) * (Tfact + 0.5) / 2\n    return x.values\n\n\ndef calc_Tfact(T):\n    x = np.exp((T1 + T) / T0)\n    return (x - e0) / de\n\n\n# this can be used to create a dataframe of experiments including yields\ndef create_experiments(domain, nsamples=100, A=25, B=90, candidates=None):\n    Tf = domain.inputs.get_by_key(\"Temperature\")\n    Vf = domain.inputs.get_by_key(\"Solvent Volume\")\n    typef = domain.inputs.get_by_key(\"Solvent Type\")\n    yf = domain.outputs.get_by_key(\"Yield\")\n    if candidates is None:\n        T = np.random.uniform(low=Tf.lower_bound, high=Tf.upper_bound, size=(nsamples,))\n        V = np.random.uniform(low=Vf.lower_bound, high=Vf.upper_bound, size=(nsamples,))\n        solvent_types = [\n            domain.inputs.get_by_key(\"Solvent Type\").categories[np.random.randint(0, 3)]\n            for i in range(nsamples)\n        ]\n    else:\n        nsamples = len(candidates)\n        T = candidates[\"Temperature\"].values\n        V = candidates[\"Solvent Volume\"].values\n        solvent_types = candidates[\"Solvent Type\"].values\n\n    Tfact = calc_Tfact(T)\n    rhofact = calc_rhofact(solvent_types, Tfact)\n    Vfact = calc_volume_fact(V)\n    y = A * Tfact + B * rhofact\n    y = 0.5 * y + 0.5 * y * Vfact\n    # y = y.values\n    samples = pd.DataFrame(\n        {\n            Tf.key: T,\n            Vf.key: V,\n            yf.key: y,\n            typef.key: solvent_types,\n            \"valid_\" + yf.key: np.ones(nsamples),\n        },\n        # index=pd.RangeIndex(nsamples),\n    )\n    samples.index = pd.RangeIndex(nsamples)\n    return samples\n\n\ndef create_candidates(domain, nsamples=4):\n    experiments = create_experiments(domain, nsamples=nsamples)\n    candidates = experiments.drop([\"Yield\", \"valid_Yield\"], axis=1)\n    return candidates\n\n\n# this is for evaluating candidates that do not yet have a yield attributed to it.\ndef evaluate_experiments(domain, candidates):\n    return create_experiments(domain, candidates=candidates)\n\n\n# create some trial experiments (at unitform random)\ncandidates = create_candidates(domain, nsamples=4)\n\n\ncandidates\n\n\n\n\n\n\n\n\nTemperature\nSolvent Volume\nSolvent Type\n\n\n\n\n0\n9.408577\n66.416088\nTHF\n\n\n1\n30.295425\n84.937867\nMeOH\n\n\n2\n33.387955\n44.388446\nDioxane\n\n\n3\n29.027364\n85.985314\nTHF\n\n\n\n\n\n\n\n\n# we can evaluate the yield of those candidates\nexperiments = evaluate_experiments(domain, candidates)\n\n\nexperiments\n\n\n\n\n\n\n\n\nTemperature\nSolvent Volume\nYield\nSolvent Type\nvalid_Yield\n\n\n\n\n0\n9.408577\n66.416088\n12.651437\nTHF\n1.0\n\n\n1\n30.295425\n84.937867\n26.328585\nMeOH\n1.0\n\n\n2\n33.387955\n44.388446\n17.166980\nDioxane\n1.0\n\n\n3\n29.027364\n85.985314\n22.972998\nTHF\n1.0",
    "crumbs": [
      "Basic Examples",
      "Getting started by Example: Optimization of Reaction Conditions"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Reaction_Optimization_Example.html#optimization-loop",
    "href": "docs/tutorials/basic_examples/Reaction_Optimization_Example.html#optimization-loop",
    "title": "Getting started by Example: Optimization of Reaction Conditions",
    "section": "Optimization Loop",
    "text": "Optimization Loop\nWith this strategy.ask() and strategy.tell() we can now do our optimization loop, where after each new proposal, the conditions obtained from ask are evaluated and added to the known datapoints via tell. This requires to refit the underling model in each step.\n\nexperimental_budget = 10\ni = 0\n# in case of smoke_test we don't run the actual optimization loop ...\ndone = False if not SMOKE_TEST else True\n\nwhile not done:\n    i += 1\n    t1 = time.time()\n    # ask for a new experiment\n    new_candidate = sobo_strategy.ask(1)\n    new_experiment = evaluate_experiments(domain, new_candidate)\n    sobo_strategy.tell(new_experiment)\n    print(f\"Iteration took {(time.time()-t1):.2f} seconds\")\n    # inform the strategy about the new experiment\n    # experiments = pd.concat([experiments,new_experiment],ignore_index=True)\n    if i &gt; experimental_budget:\n        done = True\n\n\ninvestigating results\n\n# you have access to the experiments here\nsobo_strategy.experiments\n\n\n\n\n\n\n\n\nTemperature\nSolvent Volume\nYield\nSolvent Type\nvalid_Yield\n\n\n\n\n0\n57.015233\n65.131031\n50.937407\nMeOH\nTrue\n\n\n1\n24.913476\n67.538008\n17.897954\nTHF\nTrue\n\n\n2\n5.937901\n72.942831\n9.610495\nDioxane\nTrue\n\n\n3\n25.591252\n73.243800\n15.321101\nDioxane\nTrue\n\n\n4\n10.532623\n66.895665\n14.789718\nMeOH\nTrue\n\n\n\n\n\n\n\n\n# quick plot of yield vs. Iteration\nsobo_strategy.experiments[\"Yield\"].plot()",
    "crumbs": [
      "Basic Examples",
      "Getting started by Example: Optimization of Reaction Conditions"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html",
    "href": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html",
    "title": "Model Building with BoFire",
    "section": "",
    "text": "This notebooks shows how to setup and analyze models trained with BoFire. It is still WIP.",
    "crumbs": [
      "Basic Examples",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#imports",
    "href": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#imports",
    "title": "Model Building with BoFire",
    "section": "Imports",
    "text": "Imports\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.domain.api import Inputs, Outputs\nfrom bofire.data_models.enum import RegressionMetricsEnum\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\nfrom bofire.plot.feature_importance import plot_feature_importance_by_feature_plotly\nfrom bofire.surrogates.feature_importance import (\n    combine_lengthscale_importances,\n    combine_permutation_importances,\n    lengthscale_importance_hook,\n    permutation_importance_hook,\n)",
    "crumbs": [
      "Basic Examples",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#problem-setup",
    "href": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#problem-setup",
    "title": "Model Building with BoFire",
    "section": "Problem Setup",
    "text": "Problem Setup\nFor didactic purposes, we sample data from a Himmelblau benchmark function and use them to train a SingleTaskGP.\n\n# TODO: replace this after JDs PR is ready.\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i+1}\", bounds=(-4, 4)) for i in range(3)],\n)\noutput_features = Outputs(features=[ContinuousOutput(key=\"y\")])\nexperiments = input_features.sample(n=50)\nexperiments.eval(\"y=((x_1**2 + x_2 - 11)**2+(x_1 + x_2**2 -7)**2)\", inplace=True)\nexperiments[\"valid_y\"] = 1",
    "crumbs": [
      "Basic Examples",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#cross-validation",
    "href": "docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#cross-validation",
    "title": "Model Building with BoFire",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nRun the cross validation\n\ndata_model = SingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n)\n\nmodel = surrogates.map(data_model=data_model)\ntrain_cv, test_cv, pi = model.cross_validate(\n    experiments,\n    folds=5,\n    hooks={\n        \"permutation_importance\": permutation_importance_hook,\n        \"lengthscale_importance\": lengthscale_importance_hook,\n    },\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\ncombined_importances = {\n    m.name: combine_permutation_importances(pi[\"permutation_importance\"], m).describe()\n    for m in RegressionMetricsEnum\n}\ncombined_importances[\"lengthscale\"] = combine_lengthscale_importances(\n    pi[\"lengthscale_importance\"],\n).describe()\nplot_feature_importance_by_feature_plotly(\n    combined_importances,\n    relative=False,\n    caption=\"Permutation Feature Importances\",\n    show_std=True,\n    importance_measure=\"Permutation Feature Importance\",\n)\n\n                            \n                                            \n\n\n\n\nAnalyze the cross validation\nPlots are added in a future PR.\n\n# Performance on test sets\ntest_cv.get_metrics(combine_folds=True)\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n5.706573\n78.510266\n0.982865\n1.345047\n0.991454\n0.976759\n7.169177e-10\n\n\n\n\n\n\n\n\ndisplay(test_cv.get_metrics(combine_folds=False))\ndisplay(test_cv.get_metrics(combine_folds=False).describe())\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n9.484380\n221.985315\n0.931322\n5.680236\n0.966071\n0.733333\n0.103175\n\n\n1\n4.490493\n39.408738\n0.993379\n0.095541\n0.996700\n0.951515\n0.003968\n\n\n2\n6.740898\n74.476750\n0.976259\n0.842794\n0.989124\n0.975758\n0.103175\n\n\n3\n3.862618\n20.473186\n0.996505\n0.045850\n0.999101\n1.000000\n0.003968\n\n\n4\n3.954475\n36.207340\n0.978523\n0.060816\n0.995085\n0.975758\n0.003968\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n5.000000\n5.000000\n5.000000\n5.000000\n\n\nmean\n5.706573\n78.510266\n0.975198\n1.345047\n0.989216\n0.927273\n0.043651\n\n\nstd\n2.413013\n82.595594\n0.026086\n2.446659\n0.013453\n0.109762\n0.054338\n\n\nmin\n3.862618\n20.473186\n0.931322\n0.045850\n0.966071\n0.733333\n0.003968\n\n\n25%\n3.954475\n36.207340\n0.976259\n0.060816\n0.989124\n0.951515\n0.003968\n\n\n50%\n4.490493\n39.408738\n0.978523\n0.095541\n0.995085\n0.975758\n0.003968\n\n\n75%\n6.740898\n74.476750\n0.993379\n0.842794\n0.996700\n0.975758\n0.103175\n\n\nmax\n9.484380\n221.985315\n0.996505\n5.680236\n0.999101\n1.000000\n0.103175",
    "crumbs": [
      "Basic Examples",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/random_forest_in_bofire.html",
    "href": "docs/tutorials/advanced_examples/random_forest_in_bofire.html",
    "title": "Random Forest in BoFire",
    "section": "",
    "text": "import bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.domain.api import Outputs\nfrom bofire.data_models.strategies.api import MoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, RandomForestSurrogate",
    "crumbs": [
      "Advanced Examples",
      "Random Forest in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/random_forest_in_bofire.html#imports",
    "href": "docs/tutorials/advanced_examples/random_forest_in_bofire.html#imports",
    "title": "Random Forest in BoFire",
    "section": "",
    "text": "import bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.domain.api import Outputs\nfrom bofire.data_models.strategies.api import MoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, RandomForestSurrogate",
    "crumbs": [
      "Advanced Examples",
      "Random Forest in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-a-rf",
    "href": "docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-a-rf",
    "title": "Random Forest in BoFire",
    "section": "Setup a RF",
    "text": "Setup a RF\n\nbenchmark = DTLZ2(dim=6)\n\nexperiments = benchmark.f(benchmark.domain.inputs.sample(20), return_complete=True)\n\n# you can use the hyperparams from sklearn\nrf_data_model = RandomForestSurrogate(\n    inputs=benchmark.domain.inputs,\n    outputs=Outputs(features=[benchmark.domain.outputs[0]]),\n    n_estimators=100,\n)\n\nrf = surrogates.map(rf_data_model)\n\ncv_train, cv_test, _ = rf.cross_validate(experiments)\n\ncv_test.get_metrics()\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n0.113916\n0.023784\n0.875155\n3.751605\n0.936654\n0.816541\n0.000005",
    "crumbs": [
      "Advanced Examples",
      "Random Forest in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-an-optimization",
    "href": "docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-an-optimization",
    "title": "Random Forest in BoFire",
    "section": "Setup an optimization",
    "text": "Setup an optimization\n\nbenchmark = DTLZ2(dim=6)\n\ndata_model = MoboStrategy(\n    domain=benchmark.domain,\n    ref_point={\"f_0\": 1.1, \"f_1\": 1.1},\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            RandomForestSurrogate(\n                inputs=benchmark.domain.inputs,\n                outputs=Outputs(features=[benchmark.domain.outputs[0]]),\n            ),\n            RandomForestSurrogate(\n                inputs=benchmark.domain.inputs,\n                outputs=Outputs(features=[benchmark.domain.outputs[1]]),\n            ),\n        ],\n    ),\n)\n\nrecommender = strategies.map(data_model=data_model)\n\nexperiments = benchmark.f(benchmark.domain.inputs.sample(10), return_complete=True)\nrecommender.tell(experiments=experiments)\n\n\n# currently not supported\n# for i in range(10):\n#     samples = benchmark.domain.inputs.sample(512, method=SamplingMethodEnum.SOBOL)\n#     candidates = recommender.ask(1, candidate_pool=samples)\n#     candidates = candidates.reset_index(drop=True)\n#     new_experiments = benchmark.f(candidates[benchmark.domain.inputs.get_keys().copy()], return_complete=True)\n#     recommender.tell(experiments=new_experiments)",
    "crumbs": [
      "Advanced Examples",
      "Random Forest in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/objectives_on_inputs.html",
    "href": "docs/tutorials/advanced_examples/objectives_on_inputs.html",
    "title": "Input Features as Output Objectives",
    "section": "",
    "text": "This notebook demonstrates how to put objectives on input features or a combination of input features. Possible usecases are favoring lower or higher amounts of an ingredient or to take into account a known (linear) cost function. In case of categorical inputs it can be used to penalize the optimizer for choosing specific categories.",
    "crumbs": [
      "Advanced Examples",
      "Input Features as Output Objectives"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/objectives_on_inputs.html#imports",
    "href": "docs/tutorials/advanced_examples/objectives_on_inputs.html#imports",
    "title": "Input Features as Output Objectives",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np\n\nimport bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.api import Himmelblau\nfrom bofire.data_models.features.api import CategoricalInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import (\n    MaximizeObjective,\n    MaximizeSigmoidObjective,\n)\nfrom bofire.data_models.strategies.api import MultiplicativeSoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    CategoricalDeterministicSurrogate,\n    LinearDeterministicSurrogate,\n)",
    "crumbs": [
      "Advanced Examples",
      "Input Features as Output Objectives"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/objectives_on_inputs.html#setup-an-example",
    "href": "docs/tutorials/advanced_examples/objectives_on_inputs.html#setup-an-example",
    "title": "Input Features as Output Objectives",
    "section": "Setup an Example",
    "text": "Setup an Example\nWe use Himmelblau as example with an additional objective on x_2 which pushes it to be larger 3 during the optimization. In addition, we introduce a categorical feature called x_cat which is mapped by an CategoricalDeterministicSurrogate to a continuous output called y_cat.\n\nbench = Himmelblau()\nexperiments = bench.f(bench.domain.inputs.sample(10), return_complete=True)\n\ndomain = bench.domain\n\n# setup extra feature `y_x2` that is the same as `x_2` and is taken into account in the optimization by a sigmoid objective\ndomain.outputs.features.append(\n    ContinuousOutput(key=\"y_x2\", objective=MaximizeSigmoidObjective(tp=3, steepness=10))\n)\nexperiments[\"y_x2\"] = experiments.x_2\n\n\n# add extra categorical input feature and corresponding output feature\ndomain.inputs.features.append(CategoricalInput(key=\"x_cat\", categories=[\"a\", \"b\", \"c\"]))\ndomain.outputs.features.append(\n    ContinuousOutput(key=\"y_cat\", objective=MaximizeObjective())\n)\n\n# generate random values for the new categorical feature\nexperiments[\"x_cat\"] = np.random.choice([\"a\", \"b\", \"c\"], size=experiments.shape[0])\n\nThe LinearDeterministicSurrogate can be used to model that y_x2 = x_2.\n\nsurrogate_data = LinearDeterministicSurrogate(\n    inputs=domain.inputs.get_by_keys([\"x_2\"]),\n    outputs=domain.outputs.get_by_keys([\"y_x2\"]),\n    coefficients={\"x_2\": 1},\n    intercept=0,\n)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.predict(experiments[domain.inputs.get_keys()].copy())\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:47: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\ny_x2_pred\ny_x2_sd\n\n\n\n\n0\n1.702353\n0.0\n\n\n1\n-2.398912\n0.0\n\n\n2\n2.688952\n0.0\n\n\n3\n-4.166523\n0.0\n\n\n4\n2.699384\n0.0\n\n\n5\n3.255820\n0.0\n\n\n6\n4.536737\n0.0\n\n\n7\n5.892275\n0.0\n\n\n8\n-5.827006\n0.0\n\n\n9\n5.854629\n0.0\n\n\n\n\n\n\n\nThe CategoricalDeterministicSurrogate can be used to map categories to specific continuous values.\n\ncategorical_surrogate_data = CategoricalDeterministicSurrogate(\n    inputs=domain.inputs.get_by_keys([\"x_cat\"]),\n    outputs=domain.outputs.get_by_keys([\"y_cat\"]),\n    mapping={\"a\": 1, \"b\": 0.2, \"c\": 0.3},\n)\n\nsurrogate = surrogates.map(categorical_surrogate_data)\n\nsurrogate.predict(experiments[domain.inputs.get_keys()].copy())\n\nexperiments[\"y_cat\"] = surrogate.predict(experiments[domain.inputs.get_keys()].copy())[\n    \"y_cat_pred\"\n]\n\nexperiments\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\ny_x2\nx_cat\ny_cat\n\n\n\n\n0\n0.729311\n1.702353\n88.213377\n1\n1.702353\nc\n0.3\n\n\n1\n-5.644815\n-2.398912\n388.429600\n1\n-2.398912\nb\n0.2\n\n\n2\n1.796116\n2.688952\n29.964410\n1\n2.688952\nb\n0.2\n\n\n3\n-1.614956\n-4.166523\n234.188617\n1\n-4.166523\na\n1.0\n\n\n4\n5.227497\n2.699384\n392.398834\n1\n2.699384\na\n1.0\n\n\n5\n0.158083\n3.255820\n73.711815\n1\n3.255820\nb\n0.2\n\n\n6\n-0.676444\n4.536737\n202.621260\n1\n4.536737\nc\n0.3\n\n\n7\n-3.180559\n5.892275\n627.212767\n1\n5.892275\na\n1.0\n\n\n8\n3.171716\n-5.827006\n953.354098\n1\n-5.827006\nc\n0.3\n\n\n9\n-0.937318\n5.854629\n711.967639\n1\n5.854629\na\n1.0\n\n\n\n\n\n\n\nNext we setup a SoboStrategy using the custom surrogates for outputs y_x2 and y_cat and ask for a candidate. Note that the surrogate specs for output y is automatically generated and defaulted to be a SingleTaskGPSurrogate.\n\nstrategy_data = MultiplicativeSoboStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data, categorical_surrogate_data]\n    ),\n)\nstrategy = strategies.map(strategy_data)\nstrategy.tell(experiments)\nstrategy.ask(4)\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_cat\ny_pred\ny_cat_pred\ny_x2_pred\ny_sd\ny_cat_sd\ny_x2_sd\ny_des\ny_x2_des\ny_cat_des\n\n\n\n\n0\n6.0\n4.065237\nb\n113.630420\n0.2\n4.065237\n184.922244\n0.0\n0.0\n-113.630420\n0.999976\n0.2\n\n\n1\n6.0\n3.396680\nc\n195.063416\n0.3\n3.396680\n206.498329\n0.0\n0.0\n-195.063416\n0.981418\n0.3\n\n\n2\n-6.0\n3.278942\nc\n225.475679\n0.3\n3.278942\n202.184764\n0.0\n0.0\n-225.475679\n0.942101\n0.3\n\n\n3\n6.0\n6.000000\nc\n310.804227\n0.3\n6.000000\n231.040001\n0.0\n0.0\n-310.804227\n1.000000\n0.3",
    "crumbs": [
      "Advanced Examples",
      "Input Features as Output Objectives"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/merging_objectives.html",
    "href": "docs/tutorials/advanced_examples/merging_objectives.html",
    "title": "Merging multiple objectives to a scalar target for single-target BO",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import DTLZ2\nfrom bofire.data_models.objectives import api as objectives_data_model\nfrom bofire.data_models.strategies import api as strategies_data_model",
    "crumbs": [
      "Advanced Examples",
      "Merging multiple objectives to a scalar target for single-target BO"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/merging_objectives.html#benchmark-problem",
    "href": "docs/tutorials/advanced_examples/merging_objectives.html#benchmark-problem",
    "title": "Merging multiple objectives to a scalar target for single-target BO",
    "section": "Benchmark Problem",
    "text": "Benchmark Problem\nOnly used for domain definition\n\nbench = DTLZ2(dim=2, num_objectives=2)\nexperiments = bench.f(bench.domain.inputs.sample(10), return_complete=True)\n\ndomain = bench.domain\n\n\nChange the objectives: Multiplication, only reasonable for objectives &gt; 0\n\noutputs = domain.outputs.get_by_objective()\n\noutputs[0].objective = objectives_data_model.MaximizeObjective(w=1.0, bounds=(0.0, 5.0))\noutputs[1].objective = objectives_data_model.MaximizeObjective(w=1.0, bounds=(0.0, 2.0))\n# outputs[1].objective = objectives_data_model.MaximizeSigmoidObjective(w = 0.5, tp=2.5, steepness=3.)",
    "crumbs": [
      "Advanced Examples",
      "Merging multiple objectives to a scalar target for single-target BO"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/merging_objectives.html#select-strategies",
    "href": "docs/tutorials/advanced_examples/merging_objectives.html#select-strategies",
    "title": "Merging multiple objectives to a scalar target for single-target BO",
    "section": "Select Strategies",
    "text": "Select Strategies\nWe will use pure multiplicative and additive Sobo strategies, as well as a mixed one for this example: - Multiplicative: \\(f = f_0^{w_0} \\cdot f_1^{w_1}\\) - Additive: \\(f = f_0 \\cdot w_0 + f_1 \\cdot w_1\\) - Mixed (with f1 being the additive objective): \\(f = f_0^{w_0} \\cdot (1 + w_1 \\cdot f_1)\\)\n\nstrategy_data_model = {\n    \"multiplicative\": strategies_data_model.MultiplicativeSoboStrategy(domain=domain),\n    \"additive\": strategies_data_model.AdditiveSoboStrategy(domain=domain),\n    \"mixed\": strategies_data_model.MultiplicativeAdditiveSoboStrategy(\n        domain=domain, additive_features=[\"f_1\"]\n    ),\n}\n\n\nWe will now create the strategies and evaluate them on a grid to visualize the objectives.\nWe see the following: - Multiplicative: The objective is a product of the objectives: If either \\(f_0\\) or \\(f_1\\) is low, the objective is low. - Additive: The objective is a sum of the objectives: We see a linear increase in the objective with increasing \\(f_0\\) and \\(f_1\\). This is useful for complementary objectives. - Mixed: The objective is more strict w.r.t. \\(f_0\\) than the additive objective \\(f_1\\). The overall desirability can also be high, if \\(f_1\\) is low.\nChanging the weights \\(w_i\\) in the objectives above will further change the preference of \\(f_0\\) and \\(f_1\\).\n\n# map from the strategy data-model to the actual strategy object instances\nstrategy = {\n    key: strategies.map(strategy_data_model)\n    for (key, strategy_data_model) in strategy_data_model.items()\n}\n\n\n# tell the strategies about the experiments. This is required to set up the models, but not for the objective evaluation\nfor _, strat in strategy.items():\n    strat.tell(experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n# get the objectives for evaluation as a torch executable\nobjectives = {\n    key: strategy._get_objective_and_constraints()[0]\n    for (key, strategy) in strategy.items()\n}\n\n\n# f_0 / f_1 coordinates for objctive evaluation\nmesh = np.meshgrid(np.linspace(0, 2, 100), np.linspace(0, 5, 100))\n# transform to matrix-form torch tensor\nmesh_tensor = torch.tensor([m.flatten() for m in mesh]).T\n\n/tmp/ipykernel_5993/1215888523.py:4: UserWarning:\n\nCreating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n\n\n\n\n# evaluate objectives\nobjectives_eval = {\n    key: obj(mesh_tensor).detach().numpy().reshape(mesh[0].shape)\n    for (key, obj) in objectives.items()\n}\n\n\n# plot the objectives as contour plots\nfor key, obj in objectives_eval.items():\n    plt.figure()\n    plt.contour(*mesh, obj, label=key)\n    plt.title(key)\n    plt.xlabel(\"f_0\")\n    plt.ylabel(\"f_1\")\n    plt.grid(True)\n    plt.colorbar()\n\nplt.show()\n\n/tmp/ipykernel_5993/261754645.py:4: UserWarning:\n\nThe following kwargs were not used by contour: 'label'",
    "crumbs": [
      "Advanced Examples",
      "Merging multiple objectives to a scalar target for single-target BO"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/genetic_algorithm.html",
    "href": "docs/tutorials/advanced_examples/genetic_algorithm.html",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "",
    "text": "Usage is possible in multiple ways: 1. As an alternative to the botorch optimizer in predictive strategies 2. To optimize custom function in the bofire domain. The utiliy function take care of the definition of the objective domain (variable types, constraints, etc.) and the handling of multiple experiment (\\(q\\) points).\nfrom copy import deepcopy\nfrom time import time\nfrom typing import List\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom bofire.benchmarks import api as benchmarks\nfrom bofire.data_models.constraints import api as constraints_data_models\nfrom bofire.data_models.domain import api as domains_data_models\nfrom bofire.data_models.features import api as features_data_models\nfrom bofire.data_models.strategies import api as strategies_data_models\nfrom bofire.strategies import api as strategies\nfrom bofire.strategies.utils import run_ga\nimport warnings\n\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", message=\".*A not p.d., added jitter\")\nwarnings.filterwarnings(\n    \"ignore\", message=\".*np.power((rand * alpha), (1.0 / (eta + 1.0)))[mask]\"\n)",
    "crumbs": [
      "Advanced Examples",
      "For optimizations in the bofire domain, a GA optimizer is available"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/genetic_algorithm.html#example-1-usage-for-acquisition-function-optimization",
    "href": "docs/tutorials/advanced_examples/genetic_algorithm.html#example-1-usage-for-acquisition-function-optimization",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "Example 1) Usage for Acquisition Function Optimization",
    "text": "Example 1) Usage for Acquisition Function Optimization\nJust pass the GeneticAlgorithmOptimizer to the acquisition_optimizer argument of a strategy. The optimizer will then be used to optimize the acquisition function.\n\nbenchmark = benchmarks.Himmelblau()\n# generate experiments\nexperiments = benchmark.f(benchmark.domain.inputs.sample(10), return_complete=True)\n\n\noptimizer = strategies_data_models.GeneticAlgorithmOptimizer(\n    population_size=100,\n    n_max_gen=100,\n    verbose=False,\n)\n\n\nbenchmark_grid = np.hstack(\n    [\n        x.reshape((-1, 1))\n        for x in np.meshgrid(np.linspace(-6, 6, 100), np.linspace(-6, 6, 100))\n    ]\n)\nbenchmark_grid = pd.DataFrame(\n    benchmark_grid, columns=benchmark.domain.inputs.get_keys()\n)\nbenchmark_grid[\"y\"] = benchmark.f(benchmark_grid)[\"y\"]\n\n\ndef get_proposals(domain, n: int = 10) -&gt; pd.DataFrame:\n    strategy = strategies_data_models.SoboStrategy(\n        domain=domain, acquisition_optimizer=optimizer\n    )\n    # map to strategy object, and train the model\n    strategy = strategies.map(strategy)\n    strategy.tell(experiments)\n    t0 = time()\n    proposals = strategy.ask(n, raise_validation_error=False)\n    print(f\"Generated {len(proposals)} experiments, Time taken: {time() - t0:.2f}s\")\n    return proposals\n\n\nLinear Equality and Inequality Constraints are handled by a repair function, using QP\n\n# generate different cases\ndomain = deepcopy(benchmark.domain)\ndomain.constraints.constraints += [\n    constraints_data_models.LinearEqualityConstraint(  # x_1 + x_2 = 3\n        features=[\"x_1\", \"x_2\"],\n        coefficients=[1, 1],\n        rhs=3,\n    ),\n    constraints_data_models.LinearInequalityConstraint(  # x_2 &lt;= x_1\n        features=[\"x_1\", \"x_2\"],\n        coefficients=[-1, 1],\n        rhs=0,\n    ),\n]\n\nexperiments = benchmark.f(\n    strategies.RandomStrategy.make(domain=domain).ask(10), return_complete=True\n)\n\n\nproposals = get_proposals(domain)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nGenerated 10 experiments, Time taken: 20.28s\n\n\n\nplt.figure(figsize=(4, 4))\nplt.contour(\n    benchmark_grid[\"x_1\"].values.reshape((100, 100)),\n    benchmark_grid[\"x_2\"].values.reshape((100, 100)),\n    benchmark_grid[\"y\"].values.reshape((100, 100)),\n    levels=20,\n    label=\"true system response\",\n)\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\nplt.plot((-6, 6), (-6, 6), \"r--\", label=\"x_1 &lt; x_2\")\nplt.plot((-6, 6), (9, -3), \"g--\", label=\"x_1 + x_2 = 3\")\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_5944/1104301492.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'label'\n\n\n\n\n\n\n\n\n\n\n\n\nNChooseK Constraints are also handled by a repair function, using QP\n\ndomain = deepcopy(benchmark.domain)\ndomain.inputs.get_by_key(\"x_1\").bounds = (0.0, 6.0)\ndomain.inputs.get_by_key(\"x_2\").bounds = (0.0, 6.0)\ndomain.constraints.constraints += [\n    constraints_data_models.NChooseKConstraint(\n        features=[\"x_1\", \"x_2\"],\n        min_count=1,\n        max_count=1,\n        none_also_valid=True,\n    ),\n]\n\nexperiments = benchmark.f(\n    strategies.RandomStrategy.make(domain=domain).ask(10), return_complete=True\n)\n\n\nproposals = get_proposals(domain, n=10)\n\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nGenerated 10 experiments, Time taken: 10.24s\n\n\n\nplt.figure(figsize=(4, 4))\nplt.contour(\n    benchmark_grid[\"x_1\"].values.reshape((100, 100)),\n    benchmark_grid[\"x_2\"].values.reshape((100, 100)),\n    benchmark_grid[\"y\"].values.reshape((100, 100)),\n    levels=20,\n    label=\"true system response\",\n)\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nplt.xlim(0, 6)\nplt.ylim(0, 6)\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_5944/2170543038.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'label'\n\n\n\n\n\n\n\n\n\n\n\n\nInequality Constraints are handled by the GA objctive function\n\ndomain = deepcopy(benchmark.domain)\ndomain.constraints.constraints += [\n    constraints_data_models.NonlinearInequalityConstraint(\n        expression=\"x_1**2 + x_2**2 - 16\",\n        features=[\"x_1\", \"x_2\"],\n    ),\n]\nproposals = get_proposals(domain, n=20)\n\nGenerated 20 experiments, Time taken: 10.71s\n\n\n\nplt.figure(figsize=(4, 4))\nplt.contour(\n    benchmark_grid[\"x_1\"].values.reshape((100, 100)),\n    benchmark_grid[\"x_2\"].values.reshape((100, 100)),\n    benchmark_grid[\"y\"].values.reshape((100, 100)),\n    levels=20,\n    label=\"true system response\",\n)\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nx = np.linspace(-4, 4, 100)\ny1 = np.sqrt(16 - x**2)\ny2 = -y1\nplt.plot(x, y1, \"r--\", label=\"x_1**2 + x_2**2 &lt;= 4\")\nplt.plot(x, y2, \"r--\")\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_5944/1844526682.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'label'",
    "crumbs": [
      "Advanced Examples",
      "For optimizations in the bofire domain, a GA optimizer is available"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/genetic_algorithm.html#example-2-usage-for-custom-function-optimization",
    "href": "docs/tutorials/advanced_examples/genetic_algorithm.html#example-2-usage-for-custom-function-optimization",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "Example 2) Usage for Custom Function Optimization",
    "text": "Example 2) Usage for Custom Function Optimization\nWe can define a domain with input features, and constraints. Output features are not required\n\ndomain = domains_data_models.Domain(\n    inputs=domains_data_models.Inputs(\n        features=[\n            features_data_models.ContinuousInput(\n                key=\"x_1\",\n                bounds=(-6, 6),\n            ),\n            features_data_models.ContinuousInput(\n                key=\"x_2\",\n                bounds=(-6, 6),\n            ),\n        ]\n    ),\n    constraints=[\n        constraints_data_models.NonlinearInequalityConstraint(\n            expression=\"x_1**2 + x_2**2 - 16\",\n            features=[\"x_1\", \"x_2\"],\n        ),\n    ],\n)\n\n\noptimizer = strategies_data_models.GeneticAlgorithmOptimizer(\n    population_size=100,\n    n_max_gen=100,\n    verbose=False,\n)\n\n\nDefine the optimization problem: a) Using evaluations on pd.DataFrame\nWe want to maximize the mean variance of the experiments dataframe. The objective function will be called with a list of dataframes, each representing a set of experiments. Each list entry is one individual in the population of the GA. The direction of the optimizer is a minimization of the objective function. So we minimize the negative mean variance of each 3 experiments in a batch in this case.\n\ndef objective_function(x: List[pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"assume we want to maximize the mean variance of the experiments dataframe\"\"\"\n    vars = [xi.var(numeric_only=True).mean() for xi in x]\n    return np.array(vars)\n\nRun the optimization with the utility function run_ga\n\nx_opt, f_opt = run_ga(\n    data_model=optimizer,\n    domain=domain,\n    objective_callables=[objective_function],  # list of objective functions to optimize\n    q=3,  # number of points to optimize\n    callable_format=\"pandas\",\n    optimization_direction=\"max\",  # maximize the objective function\n)\n\n\nproposals = x_opt[0]\nproposals\n\n\n\n\n\n\n\ncolumn\nx_1\nx_2\n\n\n\n\n0\n3.819944\n1.185709\n\n\n1\n-3.203541\n2.393232\n\n\n2\n-0.762119\n-3.925779\n\n\n\n\n\n\n\n\nplt.figure(figsize=(4, 4))\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nx = np.linspace(-4, 4, 100)\ny1 = np.sqrt(16 - x**2)\ny2 = -y1\nplt.plot(x, y1, \"r--\", label=\"x_1**2 + x_2**2 &lt;= 4\")\nplt.plot(x, y2, \"r--\")\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDefine the optimization problem: a) Using evaluations on torch.Tensor\nFor efficiency, we can also compute the objective function as a callable of type Tensor. In this case, the computation is in the numerical domain.\nThis means, that categorical columns etc. are encoded. A specification with input_preprocessing_specs can be passed to the objective function (otherwise, defaults are used)\nThe objective accepts a Tensor in shape (n, q, d) and should return a Tensor of shape (n,)\n\ndef objective_function(x: torch.Tensor) -&gt; torch.Tensor:\n    var = torch.var(x, dim=2)\n    var_mean = torch.mean(var, dim=1)\n    return var_mean\n\n\nx_opt, f_opt = run_ga(\n    data_model=optimizer,\n    domain=domain,\n    objective_callables=[objective_function],  # list of objective functions to optimize\n    q=3,  # number of points to optimize\n    callable_format=\"torch\",\n    optimization_direction=\"max\",  # maximize the objective function\n)\n\n\nx_opt\n\ntensor([[ 2.9480, -2.7007],\n        [ 2.8933, -2.7610],\n        [-2.7956,  2.8580]], dtype=torch.float64)",
    "crumbs": [
      "Advanced Examples",
      "For optimizations in the bofire domain, a GA optimizer is available"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/genetic_algorithm.html#example-3-multiobjective-optimization",
    "href": "docs/tutorials/advanced_examples/genetic_algorithm.html#example-3-multiobjective-optimization",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "Example 3: Multiobjective Optimization",
    "text": "Example 3: Multiobjective Optimization\nby giving multiple objective functions, or returning a 2D array from the objective, we will trigger multiobjective optimization\n\ndef objective_function_1(x: List[pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"assume we want to maximize the mean variance of the experiments dataframe\"\"\"\n    vars = [xi.var(numeric_only=True).mean() for xi in x]\n    return np.array(vars)\n\n\ndef objective_function_2(x: List[pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"Maximize the sum of all inputs\"\"\"\n    vars = [xi.sum().sum() for xi in x]\n    return np.array(vars)\n\n\nx_opt, f_opt = run_ga(\n    data_model=optimizer,\n    domain=domain,\n    objective_callables=[\n        objective_function_1,\n        objective_function_2,\n    ],  # list of objective functions to optimize\n    q=3,  # number of points to optimize\n    callable_format=\"pandas\",\n    optimization_direction=\"max\",  # maximize the objective function\n)\n\nIn the multiobjective-case, the result is a list of pd.DataFrame with different pareto-optimal solutions\n\nplt.scatter(f_opt[:, 0], f_opt[:, 1])\nplt.xlabel(\"objective function 1\")\nplt.ylabel(\"objective function 2\")\n\nText(0, 0.5, 'objective function 2')\n\n\n\n\n\n\n\n\n\n\nx_opt[0]\n\n\n\n\n\n\n\ncolumn\nx_1\nx_2\n\n\n\n\n0\n-0.186013\n-3.970158\n\n\n1\n-1.957514\n3.456513\n\n\n2\n3.492936\n1.872092\n\n\n\n\n\n\n\n\nx_opt[1]\n\n\n\n\n\n\n\ncolumn\nx_1\nx_2\n\n\n\n\n0\n3.295582\n2.067643\n\n\n1\n1.585728\n3.657291\n\n\n2\n3.265379\n2.308500",
    "crumbs": [
      "Advanced Examples",
      "For optimizations in the bofire domain, a GA optimizer is available"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/custom_sobo.html",
    "href": "docs/tutorials/advanced_examples/custom_sobo.html",
    "title": "Custom Sobo Strategy",
    "section": "",
    "text": "The CustomSoboStrategy can be used to design custom objectives or objective combinations for optimizations. In this tutorial notebook, it is shown how to use it to optimize a quantity that depends on a combination of an inferred quantity and one of the inputs.",
    "crumbs": [
      "Advanced Examples",
      "Custom Sobo Strategy"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/custom_sobo.html#imports",
    "href": "docs/tutorials/advanced_examples/custom_sobo.html#imports",
    "title": "Custom Sobo Strategy",
    "section": "Imports",
    "text": "Imports\n\nimport torch\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Himmelblau\nfrom bofire.data_models.strategies.api import CustomSoboStrategy\nfrom bofire.utils.torch_tools import tkwargs",
    "crumbs": [
      "Advanced Examples",
      "Custom Sobo Strategy"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/custom_sobo.html#setup-the-optimization",
    "href": "docs/tutorials/advanced_examples/custom_sobo.html#setup-the-optimization",
    "title": "Custom Sobo Strategy",
    "section": "Setup the optimization",
    "text": "Setup the optimization\nFor the optimization, we want to subtract the inferred quantity by the value of feature x_0.\n\nbenchmark = Himmelblau()\nexperiments = benchmark.f(benchmark.domain.inputs.sample(10), return_complete=True)\n\nstrategy_data = CustomSoboStrategy(domain=benchmark.domain)\nstrategy = strategies.map(strategy_data)\n\n\n# here we find out what is the index of the input feature in the input tensor `X`\n# in the manipulation function below\nfeature2index, _ = strategy.domain.inputs._get_transform_info(\n    strategy.input_preprocessing_specs\n)\nfeat_idx = feature2index[\"x_1\"][0]\n\n\n# we assign now a torch based function to the strategy which performs the custom manipulation of the objective\n# the signature has to be understood in the following way:\n# - samples: the samples to evaluate the objective on, these are the predicted Y/output values of the model(s)\n# - callables: the botorch callables associated to objectives associated to the features\n#   (have a look at `get_objective_callable` in `bofire/utils/torch_tools.py`)\n# - weights: the weights associated to the objectives\n#   (have a look here: `_callables_and_weights` in `bofire/utils/torch_tools.py`)\n# - X: a tensor of input values associated to the output values  samples, associated to the Y/output values (`samples`)\n\n\ndef f(samples, callables, weights, X):\n    val = torch.tensor(0.0).to(**tkwargs)\n    for c, w in zip(callables, weights):\n        val = val + c(samples, None) * w\n    # here, you have to implement the custom manipulation of the objective\n    # in this example, we subtract the value of the first feature from the objective\n    val = val - X[..., feat_idx]\n    return val\n\n\nstrategy.f = f\n\nstrategy.tell(experiments)\nstrategy.ask(1)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n3.213863\n0.74831\n-72.424975\n50.308762\n72.424975",
    "crumbs": [
      "Advanced Examples",
      "Custom Sobo Strategy"
    ]
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/strategies_serial.html",
    "href": "build/lib/docs/tutorials/serialization/strategies_serial.html",
    "title": "Strategy Serialization with BoFire",
    "section": "",
    "text": "from pydantic import TypeAdapter\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.kernels.api import RBFKernel, ScaleKernel\nfrom bofire.data_models.strategies.api import AnyStrategy\nfrom bofire.data_models.strategies.api import MoboStrategy as MoboStrategyDataModel\nfrom bofire.data_models.strategies.api import RandomStrategy as RandomStrategyDataModel\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDataModel\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\nfrom bofire.surrogates.diagnostics import CvResults2CrossValidationValues\nfrom bofire.surrogates.trainable import TrainableSurrogate"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/strategies_serial.html#imports",
    "href": "build/lib/docs/tutorials/serialization/strategies_serial.html#imports",
    "title": "Strategy Serialization with BoFire",
    "section": "",
    "text": "from pydantic import TypeAdapter\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.kernels.api import RBFKernel, ScaleKernel\nfrom bofire.data_models.strategies.api import AnyStrategy\nfrom bofire.data_models.strategies.api import MoboStrategy as MoboStrategyDataModel\nfrom bofire.data_models.strategies.api import RandomStrategy as RandomStrategyDataModel\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDataModel\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\nfrom bofire.surrogates.diagnostics import CvResults2CrossValidationValues\nfrom bofire.surrogates.trainable import TrainableSurrogate"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/strategies_serial.html#single-objective-problem-setup",
    "href": "build/lib/docs/tutorials/serialization/strategies_serial.html#single-objective-problem-setup",
    "title": "Strategy Serialization with BoFire",
    "section": "Single Objective Problem Setup",
    "text": "Single Objective Problem Setup\n\nbenchmark = Himmelblau()\nsamples = benchmark.domain.inputs.sample(n=10)\n\n# this is the training data\nexperiments = benchmark.f(samples, return_complete=True)\n\n# this are the pending candidates\npending_candidates = benchmark.domain.inputs.sample(2)"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/strategies_serial.html#random-strategy",
    "href": "build/lib/docs/tutorials/serialization/strategies_serial.html#random-strategy",
    "title": "Strategy Serialization with BoFire",
    "section": "Random Strategy",
    "text": "Random Strategy\nThe random strategy and other strategies that just inherit from Strategy and not PredictiveStrategy are special as they do not need defined output features in the domain and they do not need a call to tell before the ask. Furthermore they online provide input features in the candidates and no predictions for output features.\n\n# setup the data model\ndomain = Domain(inputs=benchmark.domain.inputs)\nstrategy_data = RandomStrategyDataModel(domain=domain)\n\n# we generate the json spec\njspec = strategy_data.model_dump_json()\n\njspec\n\n'{\"type\":\"RandomStrategy\",\"domain\":{\"type\":\"Domain\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[]},\"constraints\":{\"type\":\"Constraints\",\"constraints\":[]}},\"seed\":null,\"fallback_sampling_method\":\"UNIFORM\",\"n_burnin\":1000,\"n_thinning\":32,\"num_base_samples\":null,\"max_iters\":1000,\"sampler_kwargs\":null}'\n\n\n\n# load it\nstrategy_data = TypeAdapter(AnyStrategy).validate_json(jspec)\n\n# map it\nstrategy = strategies.map(strategy_data)\n\n# ask it\ndf_candidates = strategy.ask(candidate_count=5)\n\n# transform to spec\ncandidates = strategy.to_candidates(df_candidates)\n\ncandidates\n\n[Candidate(inputValues={'x_1': InputValue(value='4.646923269066914'), 'x_2': InputValue(value='-1.7657577604237646')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='-0.20476450154508363'), 'x_2': InputValue(value='0.3224240705362522')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='-0.2057644173914479'), 'x_2': InputValue(value='3.7690750334953016')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='4.893599024254943'), 'x_2': InputValue(value='5.273024313436766')}, outputValues=None),\n Candidate(inputValues={'x_1': InputValue(value='-5.701866807944429'), 'x_2': InputValue(value='1.4149964600573046')}, outputValues=None)]"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/strategies_serial.html#sobo-strategy",
    "href": "build/lib/docs/tutorials/serialization/strategies_serial.html#sobo-strategy",
    "title": "Strategy Serialization with BoFire",
    "section": "SOBO Strategy",
    "text": "SOBO Strategy\nSetup the strategies data model.\n\n# setup the data model\nstrategy_data = SoboStrategyDataModel(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# we generate the json spec\njspec = strategy_data.model_dump_json()\n\njspec\n\n'{\"type\":\"SoboStrategy\",\"domain\":{\"type\":\"Domain\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"constraints\":{\"type\":\"Constraints\",\"constraints\":[]}},\"seed\":null,\"acquisition_optimizer\":{\"prefer_exhaustive_search_for_purely_categorical_domains\":true,\"type\":\"BotorchOptimizer\",\"n_restarts\":20,\"n_raw_samples\":1024,\"maxiter\":2000,\"batch_limit\":20,\"sequential\":false,\"local_search_config\":null},\"surrogate_specs\":{\"surrogates\":[{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}]},\"outlier_detection_specs\":null,\"min_experiments_before_outlier_check\":1,\"frequency_check\":1,\"frequency_hyperopt\":0,\"folds\":5,\"include_infeasible_exps_in_acqf_calc\":false,\"acquisition_function\":{\"type\":\"qLogNEI\",\"prune_baseline\":true,\"n_mc_samples\":512}}'\n\n\nAs SOBO is a predictive strategy, training data has to be provided before candidated can be requested.\n\n# load it\nstrategy_data = TypeAdapter(AnyStrategy).validate_json(jspec)\n\n# map it\nstrategy = strategies.map(strategy_data)\n\n# tell it the pending candidates if present\nif pending_candidates is not None:\n    strategy.add_candidates(pending_candidates)\n\n# tell it\nstrategy.tell(experiments=experiments)\n\n# ask it\ndf_candidates = strategy.ask(candidate_count=2)\n\n# transform to spec\ncandidates = strategy.to_candidates(df_candidates)\n\ncandidates\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n[Candidate(inputValues={'x_1': InputValue(value='-0.5757905183575339'), 'x_2': InputValue(value='2.6096113762066615')}, outputValues={'y': OutputValue(predictedValue='-32.60307657623224', standardDeviation=120.52549680655194, objective=32.60307657623224)}),\n Candidate(inputValues={'x_1': InputValue(value='-5.577831506661323'), 'x_2': InputValue(value='3.2437318188278357')}, outputValues={'y': OutputValue(predictedValue='27.764528042335286', standardDeviation=200.40492525062734, objective=-27.764528042335286)})]\n\n\nWe can also save the trained models of the strategy, for more info look at the model_serial.ipynb notebook. It could be that the dumps command fails here. But this is already fixed in the main branch of the linear_operator package, and if not yet, it should be available in main soon.\n\njsurrogate_spec = strategy_data.surrogate_specs.surrogates[0].model_dump_json()\ndump = strategy.surrogates.surrogates[0].dumps()"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/strategies_serial.html#mobo-strategy",
    "href": "build/lib/docs/tutorials/serialization/strategies_serial.html#mobo-strategy",
    "title": "Strategy Serialization with BoFire",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\nAs example for a multiobjective strategy we are using here the MoboStrategy. Related strategies would be Qparego, MultiplicativeSobo etc. To use it, we have to first generate a multiobjective domain.\n\nbenchmark = DTLZ2(dim=6)\nsamples = benchmark.domain.inputs.sample(n=20)\nexperiments = benchmark.f(samples, return_complete=True)\npending_candidates = benchmark.domain.inputs.sample(2)\n\nNow the strategy spec is setup. Note that we can define there exactly which model to use.\n\n# setup the data model\nstrategy_data = MoboStrategyDataModel(\n    domain=benchmark.domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=benchmark.domain.inputs,\n                outputs=Outputs(features=[benchmark.domain.outputs[0]]),\n                kernel=ScaleKernel(base_kernel=RBFKernel(ard=False)),\n            ),\n        ],\n    ),\n)\n\n# we generate the json spec\njspec = strategy_data.model_dump_json()\n\njspec\n\n'{\"type\":\"MoboStrategy\",\"domain\":{\"type\":\"Domain\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_0\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_3\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_4\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_5\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"f_0\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}},{\"type\":\"ContinuousOutput\",\"key\":\"f_1\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"constraints\":{\"type\":\"Constraints\",\"constraints\":[]}},\"seed\":null,\"acquisition_optimizer\":{\"prefer_exhaustive_search_for_purely_categorical_domains\":true,\"type\":\"BotorchOptimizer\",\"n_restarts\":20,\"n_raw_samples\":1024,\"maxiter\":2000,\"batch_limit\":20,\"sequential\":false,\"local_search_config\":null},\"surrogate_specs\":{\"surrogates\":[{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_0\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_3\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_4\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_5\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"f_0\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"ScaleKernel\",\"base_kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":false,\"lengthscale_prior\":null,\"lengthscale_constraint\":null},\"outputscale_prior\":null,\"outputscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}},{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_0\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_3\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_4\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_5\",\"unit\":null,\"bounds\":[0.0,1.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"f_1\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}]},\"outlier_detection_specs\":null,\"min_experiments_before_outlier_check\":1,\"frequency_check\":1,\"frequency_hyperopt\":0,\"folds\":5,\"include_infeasible_exps_in_acqf_calc\":false,\"ref_point\":{\"type\":\"ExplicitReferencePoint\",\"values\":{\"f_0\":{\"type\":\"AbsoluteMovingReferenceValue\",\"orient_at_best\":false,\"offset\":0.0},\"f_1\":{\"type\":\"AbsoluteMovingReferenceValue\",\"orient_at_best\":false,\"offset\":0.0}}},\"acquisition_function\":{\"type\":\"qLogNEHVI\",\"alpha\":0.0,\"prune_baseline\":true,\"n_mc_samples\":512}}'\n\n\nGenerate the candidates.\n\n# load it\nstrategy_data = TypeAdapter(AnyStrategy).validate_json(jspec)\n\n# map it\nstrategy = strategies.map(strategy_data)\n\n# tell it the pending candidates if available\nif pending_candidates is not None:\n    strategy.add_candidates(pending_candidates)\n\n# tell it\nstrategy.tell(experiments=experiments)\n\n# ask it\ndf_candidates = strategy.ask(candidate_count=1)\n\n# transform to spec\ncandidates = strategy.to_candidates(df_candidates)\n\ncandidates\n\n[Candidate(inputValues={'x_0': InputValue(value='0.23289117828024516'), 'x_1': InputValue(value='0.0'), 'x_2': InputValue(value='0.02999688704860642'), 'x_3': InputValue(value='0.0'), 'x_4': InputValue(value='0.0'), 'x_5': InputValue(value='0.0')}, outputValues={'f_0': OutputValue(predictedValue='0.8947723278420385', standardDeviation=0.3586869943187399, objective=-0.8947723278420385), 'f_1': OutputValue(predictedValue='0.5012182785818518', standardDeviation=0.1224842487996122, objective=-0.5012182785818518)})]\n\n\nTo fill the model info section accordingly, the following snippet has to be executed for every surrogate, incldung saving the actual models.\n\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\n\nclass TestMethod(BaseModel):\n    type: str\n\n\nclass CrossValidation(TestMethod):\n    type: Literal[\"CrossValidation\"] = \"CrossValidation\"\n    foldCount: int\n\n\nfor i in range(len(strategy_data.surrogate_specs.surrogates)):\n    surrogate_data = strategy.surrogate_specs.surrogates[i]\n    surrogate = strategy.surrogates.surrogates[i]\n    # get the spec\n    jsurrogate_spec = surrogate_data.model_dump_json()\n    # get the dump\n    dump = surrogate.dumps()\n    # do the cross validation, only if we have a trainable model under the hood\n    if isinstance(surrogate, TrainableSurrogate):\n        cv_train, cv_test, _ = surrogate.cross_validate(strategy.experiments, folds=5)\n        # transform the bofire objects to the backend objects\n        testMethod = CrossValidation(foldCount=5)\n        cvResultsTrain = CvResults2CrossValidationValues(cv_train)\n        cvResultsTest = CvResults2CrossValidationValues(cv_test)\n        metricsTrain = {\n            surrogate.outputs[0].key: cv_train.get_metrics(combine_folds=False)\n            .describe()\n            .loc[\"mean\"]\n            .to_dict(),\n        }\n        metricsTest = {\n            surrogate.outputs[0].key: cv_test.get_metrics(combine_folds=True)\n            .describe()\n            .loc[\"mean\"]\n            .to_dict(),\n        }\n        # save to backend\n        # - jsurrogate_spec\n        # - dump\n        # - testMethod\n        # - cvResultsTrain\n        # - cvResultsTest\n        # - metricsTrain\n        # - metricsTest"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/index.html",
    "href": "build/lib/docs/tutorials/serialization/index.html",
    "title": "Serialization",
    "section": "",
    "text": "All classes in BoFire are serializable and can be saved to JSON formats. These tutorials demonstrate how to serialize and deserialize BoFire objects for storage, sharing, and integration with RESTful APIs.\n\n\n\n\nLearn how to serialize and deserialize surrogate models.\n\n\n\nSave and load optimization strategies for reproducibility and deployment.\n\n\n\n\nSerialization is crucial for:\n\nReproducibility: Save the exact configuration of your optimization runs\nCollaboration: Share strategies and models with team members\nAPI Integration: Seamlessly integrate BoFire into RESTful services\nVersion Control: Track changes in experimental designs over time\nDeployment: Move configurations from development to production environments"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/index.html#available-tutorials",
    "href": "build/lib/docs/tutorials/serialization/index.html#available-tutorials",
    "title": "Serialization",
    "section": "",
    "text": "Learn how to serialize and deserialize surrogate models.\n\n\n\nSave and load optimization strategies for reproducibility and deployment."
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/index.html#why-serialization-matters",
    "href": "build/lib/docs/tutorials/serialization/index.html#why-serialization-matters",
    "title": "Serialization",
    "section": "",
    "text": "Serialization is crucial for:\n\nReproducibility: Save the exact configuration of your optimization runs\nCollaboration: Share strategies and models with team members\nAPI Integration: Seamlessly integrate BoFire into RESTful services\nVersion Control: Track changes in experimental designs over time\nDeployment: Move configurations from development to production environments"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/nchoosek_constraint.html",
    "href": "build/lib/docs/tutorials/doe/nchoosek_constraint.html",
    "title": "Design with NChooseK constraint",
    "section": "",
    "text": "The doe subpackage also supports problems with NChooseK constraints. Since IPOPT has problems finding feasible solutions using the gradient of the NChooseK constraint violation, a closely related (but stricter) constraint that suffices to fulfill the NChooseK constraint is imposed onto the problem: For each experiment \\(j\\) N-K decision variables \\(x_{i_1,j},...,x_{i_{N-K,j}}\\) from the NChooseK constraints’ names attribute are picked that are forced to be zero. This is done by setting the upper and lower bounds of the picked variables are set to 0 in the corresponding experiments. This causes IPOPT to treat them as “fixed variables” (i.e. it will not optimize for them) and will always stick to the only feasible value (which is 0 here). However, this constraint is stricter than the original NChooseK constraint. In combination with other constraints on the same decision variables this can result in a situation where the constraints cannot be fulfilled even though the original constraints would allow for a solution. For example consider a problem with four decision variables \\(x_1, x_2, x_3, x_4\\), an NChooseK constraint on the first four variable that restricts the number of nonzero variables to two. Additionally, we have a linear constraint \\[\nx_3 + x_4 \\geq 0.1\n\\] We can easily find points that fulfill both constraints (e.g. \\((0,0,0,0.1)\\)). Now consider the stricter, linear constraint from above. Eventually, it will happen that \\(x_3\\) and \\(x_4\\) are chosen to be zero for one experiment. For this experiment it is impossible to fulfill the linear constraint \\(x_3 + x_4 \\geq 0.1\\) since \\(x_3 = x_4 = 0\\).\nTherefore one has to be very careful when imposing linear constraints upon decision variables that already show up in an NChooseK constraint.\nFor practical reasons it necessary that two NChooseK constraints of the same problem must not share any variables.\nYou can find an example for a problem with NChooseK constraints and additional linear constraints imposed on the same variables.\n\nimport numpy as np\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NChooseKConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(8)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[f\"x{i+1}\" for i in range(8)],\n            coefficients=[1, 1, 1, 1, 1, 1, 1, 1],\n            rhs=1,\n        ),\n        NChooseKConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            min_count=0,\n            max_count=1,\n            none_also_valid=True,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=0.7,\n        ),\n        LinearInequalityConstraint(\n            features=[\"x7\", \"x8\"],\n            coefficients=[-1, -1],\n            rhs=-0.1,\n        ),\n        LinearInequalityConstraint(features=[\"x7\", \"x8\"], coefficients=[1, 1], rhs=0.9),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n    ipopt_options={\"max_iter\": 500},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nx6\nx7\nx8\n\n\n\n\n0\n0.700\n0.000\n0.0\n0.000\n0.200\n0.000\n0.000\n0.100\n\n\n1\n0.000\n0.000\n0.7\n0.000\n0.000\n0.200\n0.000\n0.100\n\n\n2\n0.000\n0.000\n0.0\n0.000\n0.000\n0.900\n0.100\n0.000\n\n\n3\n0.527\n0.000\n0.0\n0.000\n0.000\n0.000\n0.473\n0.000\n\n\n4\n0.000\n0.527\n0.0\n0.000\n0.000\n0.000\n0.473\n0.000\n\n\n5\n0.000\n0.000\n0.1\n0.000\n0.000\n0.000\n0.900\n0.000\n\n\n6\n0.000\n0.700\n0.0\n0.200\n0.000\n0.000\n0.000\n0.100\n\n\n7\n0.000\n0.000\n0.0\n0.000\n0.900\n0.000\n0.100\n0.000\n\n\n8\n0.000\n0.000\n0.0\n0.000\n0.000\n0.497\n0.000\n0.503\n\n\n9\n0.000\n0.000\n0.0\n0.900\n0.000\n0.000\n0.100\n0.000\n\n\n10\n0.100\n0.000\n0.0\n0.000\n0.000\n0.000\n0.000\n0.900\n\n\n11\n0.000\n0.000\n0.0\n0.345\n0.346\n0.000\n0.000\n0.309"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/fractional_factorial.html",
    "href": "build/lib/docs/tutorials/doe/fractional_factorial.html",
    "title": "Full and Fractional Factorial Designs",
    "section": "",
    "text": "BoFire can be used to setup full (two level) and fractional factorial designs (https://en.wikipedia.org/wiki/Fractional_factorial_design). This tutorial notebook shows how."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/fractional_factorial.html#imports-and-helper-functions",
    "href": "build/lib/docs/tutorials/doe/fractional_factorial.html#imports-and-helper-functions",
    "title": "Full and Fractional Factorial Designs",
    "section": "Imports and helper functions",
    "text": "Imports and helper functions\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import CategoricalInput, ContinuousInput\nfrom bofire.data_models.strategies.api import FractionalFactorialStrategy\nfrom bofire.utils.doe import get_alias_structure, get_confounding_matrix, get_generator\n\n\ndef plot_design(design: pd.DataFrame):\n    # we do a plot with three subplots in one row in which the three degrees of freedom (temperature, time and ph) are plotted\n    _, axs = plt.subplots(1, 3, figsize=(15, 5))\n    axs[0].scatter(design[\"temperature\"], design[\"time\"])\n    axs[0].set_xlabel(\"Temperature\")\n    axs[0].set_ylabel(\"Time\")\n    axs[1].scatter(design[\"temperature\"], design[\"ph\"])\n    axs[1].set_xlabel(\"Temperature\")\n    axs[1].set_ylabel(\"pH\")\n    axs[2].scatter(design[\"time\"], design[\"ph\"])\n    axs[2].set_xlabel(\"Time\")\n    axs[2].set_ylabel(\"pH\")\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-the-problem-domain",
    "href": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-the-problem-domain",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup the problem domain",
    "text": "Setup the problem domain\nThe designs are generated for a simple three dimensional problem comprised of three continuous factors/features.\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n    ],\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design",
    "href": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup a full factorial design",
    "text": "Setup a full factorial design\nHere we setup a full two-level factorial design including a center point and plot it.\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\n\n\n\n\n0\n7.0\n20.0\n60.0\n\n\n1\n7.0\n20.0\n120.0\n\n\n2\n7.0\n80.0\n60.0\n\n\n3\n7.0\n80.0\n120.0\n\n\n4\n13.0\n20.0\n60.0\n\n\n5\n13.0\n20.0\n120.0\n\n\n6\n13.0\n80.0\n60.0\n\n\n7\n13.0\n80.0\n120.0\n\n\n8\n10.0\n50.0\n90.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe confounding structure is shown below, as expected for a full factorial design, no confound is present.\n\nm = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design-with-blocking",
    "href": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-a-full-factorial-design-with-blocking",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup a full factorial design with blocking",
    "text": "Setup a full factorial design with blocking\nHere we setup a blocked full two-level factorial design including a center point and plot it.\n\nblocked_domain = Domain(\n    inputs=[\n        ContinuousInput(key=\"temperature\", bounds=(20, 80)),\n        ContinuousInput(key=\"time\", bounds=(60, 120)),\n        ContinuousInput(key=\"ph\", bounds=(7, 13)),\n        CategoricalInput(key=\"operator\", categories=[\"A\", \"B\", \"C\", \"D\"]),\n    ],\n)\n\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=blocked_domain,\n    n_center=1,  # number of center points per block\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    block_feature_key=\"operator\",\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\nplot_design(design=design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\noperator\n\n\n\n\n0\n7.0\n20.0\n60.0\nA\n\n\n1\n13.0\n80.0\n120.0\nA\n\n\n2\n10.0\n50.0\n90.0\nA\n\n\n3\n7.0\n20.0\n120.0\nB\n\n\n4\n13.0\n80.0\n60.0\nB\n\n\n5\n10.0\n50.0\n90.0\nB\n\n\n6\n7.0\n80.0\n60.0\nC\n\n\n7\n13.0\n20.0\n120.0\nC\n\n\n8\n10.0\n50.0\n90.0\nC\n\n\n9\n7.0\n80.0\n120.0\nD\n\n\n10\n13.0\n20.0\n60.0\nD\n\n\n11\n10.0\n50.0\n90.0\nD"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-a-fractional-factorial-design",
    "href": "build/lib/docs/tutorials/doe/fractional_factorial.html#setup-a-fractional-factorial-design",
    "title": "Full and Fractional Factorial Designs",
    "section": "Setup a fractional factorial design",
    "text": "Setup a fractional factorial design\nHere a fractional factorial design of the form \\(2^{3-1}\\) is setup by specifying the number of generators (here 1). In comparison to the full factorial design with 9 candidates, it features only 5 experiments.\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    n_generators=1,  # number of generators, ie number of reducing factors\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\n\n\n\n\n0\n7.0\n20.0\n120.0\n\n\n1\n7.0\n80.0\n60.0\n\n\n2\n13.0\n20.0\n60.0\n\n\n3\n13.0\n80.0\n120.0\n\n\n4\n10.0\n50.0\n90.0\n\n\n\n\n\n\n\nThe generator string is automatically generated by making use of the method get_generator and specifying the total number of factors (here 3) and the number of generators (here 1).\n\nget_generator(n_factors=3, n_generators=1)\n\n'a b ab'\n\n\nAs expected for a type III design the main effects are confounded with the two factor interactions:\n\nm = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()\n\n\n\n\n\n\n\n\nThis can also be expressed by the so called alias structure that can be calculated as following:\n\nget_alias_structure(\"a b ab\")\n\n['a = bc', 'b = ac', 'c = ab', 'I = abc']\n\n\nHere again a fractional factorial design of the form \\(2^{3-1}\\) is setup by providing the complete generator string of the form a b -ab explicitly to the strategy.\n\nstrategy_data = FractionalFactorialStrategy(\n    domain=domain,\n    n_center=1,  # number of center points\n    n_repetitions=1,  # number of repetitions, we do only one round here\n    generator=\"a b -ab\",  # the exact generator\n)\nstrategy = strategies.map(strategy_data)\ndesign = strategy.ask()\ndisplay(design)\n\n\n\n\n\n\n\n\nph\ntemperature\ntime\n\n\n\n\n0\n7.0\n20.0\n60.0\n\n\n1\n7.0\n80.0\n120.0\n\n\n2\n13.0\n20.0\n120.0\n\n\n3\n13.0\n80.0\n60.0\n\n\n4\n10.0\n50.0\n90.0\n\n\n\n\n\n\n\nThe last two designs differ only in the last feature time, since the generator strings are different. In the first one it holds time=ph x temperature whereas in the second it holds time=-ph x temperature, which is also reflected in the confounding structure.\n\nm = get_confounding_matrix(domain.inputs, design=design, interactions=[2])\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html",
    "href": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "",
    "text": "BoFire supports classical design of experiments where you can specify different models. If you have already started running experiments, say with a linear design, and you have resources to search more broadly in the same space (aka domain definition), you might want to ‘augment’ the original design to a larger design, say ‘fully quadratic’.\nAnother case is when you have existing experiments and want to see what can be (re)used in a design of experiments. This is different from Bayesian Optimization: with classical DoE, we do one set of experiments rather than iteratively asking for an experiment or two, then retraining the model with the acquired data before asking again. For DoE, outputs are not taken into account, unlike Bayesian Optimization.\nYou can set these as your candidate experiments.\nBoFire provides methods to evaluate design quality and plan additional experiments by calculating the Fisher Information Matrix (FIM) rank of your candidate experiments. The FIM rank tells you how many model parameters for the current design can be uniquely estimated from your candidates."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html#key-concepts",
    "href": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html#key-concepts",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "Key Concepts",
    "text": "Key Concepts\nFor a linear model with 3 continuous inputs:\n\nModel: y = β₀ + β₁x₁ + β₂x₂ + β₃x₃\nNumber of parameters to estimate: 4 (intercept + 3 coefficients)\nMinimum experiments for parameter estimation: 4\nRecommended experiments (with buffer for error estimation): 7 (4 + 3)\n\nThe FIM rank of your candidates tells you how many parameters you can actually estimate. If rank &lt; number of parameters, your design is rank-deficient and cannot estimate all model coefficients uniquely."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html#example-checking-design-quality",
    "href": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html#example-checking-design-quality",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "Example: Checking Design Quality",
    "text": "Example: Checking Design Quality\nLet’s create a simple domain and check the quality of different experimental designs:\n\nimport pandas as pd\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\nimport bofire.data_models.strategies.api as data_models\nfrom bofire.strategies.api import DoEStrategy\n\n# Create a simple domain with 3 continuous inputs\ndomain = Domain.from_lists(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n)\n\n# Create a DoE strategy with a linear model\ndata_model = data_models.DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\")\n)\nstrategy = DoEStrategy(data_model=data_model)\n\n\nHow many experiments are recommended?\n\nrequired = strategy.get_required_number_of_experiments()\nprint(f\"Recommended number of experiments: {required}\")\nprint(f\"Additional experiments needed (no candidates yet): {strategy.get_additional_experiments_needed()}\")\n\nRecommended number of experiments: 7\nAdditional experiments needed (no candidates yet): 7\n\n\nThe recommended number is 7 (4 parameters + 3 buffer for error estimation and validation).\n\n\nLooking at the quality of candidates towards that design\nLet’s say you’ve run 2 experiments. If they’re good, we expect to only need 5 more for our D-Optimal linear design (recall above the recommended number of experiments was 7).\n\n# Two experiments along one axis only\ncandidates_partial = pd.DataFrame({\n    \"x1\": [0.0, 1.0],\n    \"x2\": [0.0, 0.0],\n    \"x3\": [0.0, 0.0]\n})\n\nstrategy.set_candidates(candidates_partial)\nrank = strategy.get_candidate_rank()\nadditional = strategy.get_additional_experiments_needed()\n\nprint(f\"FIM rank of current design: {rank}\")\nprint(f\"Model parameters: 4 (intercept + 3 coefficients)\")\nprint(f\"Additional experiments needed: {additional}\")\n\nFIM rank of current design: 2\nModel parameters: 4 (intercept + 3 coefficients)\nAdditional experiments needed: 5\n\n\nWith only 2 experiments along one axis, the FIM rank is 2 - you can only estimate 2 parameters (intercept and x1 coefficient). You need 5 more experiments to reach the recommended count.\n\n\nGenerating and evaluating a full design\n\n# Generate a full D-optimal design\nfull_doe = strategy.ask(candidate_count=required)\nprint(f\"Generated {len(full_doe)} experiments:\")\nprint(full_doe.round(3))\n\n# Evaluate the design quality\nstrategy_fresh = DoEStrategy(data_model=data_model)\nstrategy_fresh.set_candidates(full_doe)\nrank_full = strategy_fresh.get_candidate_rank()\nadditional_full = strategy_fresh.get_additional_experiments_needed()\n\nprint(f\"\\nFIM rank: {rank_full}\")\nprint(f\"Additional experiments needed: {additional_full}\")\n\nGenerated 7 experiments:\n    x1   x2   x3\n0  1.0  0.0  1.0\n1  0.0  1.0  0.0\n2  0.0  0.0  1.0\n3  1.0  1.0  1.0\n4  0.0  1.0  0.0\n5  1.0  1.0  0.0\n6  0.0  1.0  1.0\n\nFIM rank: 4\nAdditional experiments needed: 3\n\n\nNotice that even though we generated 7 experiments, the FIM rank is 4 (the number of model parameters). This is correct! The D-optimal design places experiments at strategic locations (typically corners of the design space) to estimate all 4 parameters. The extra 3 experiments provide:\n\nDegrees of freedom for error estimation\nAbility to detect lack-of-fit\nRobustness against experimental errors"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html#practical-workflow",
    "href": "build/lib/docs/tutorials/doe/candidate_goodness_and_experiments.html#practical-workflow",
    "title": "Evaluating Design Quality and Planning Additional Experiments",
    "section": "Practical Workflow",
    "text": "Practical Workflow\nWhen planning experiments:\n\nCheck requirements: Use get_required_number_of_experiments() to see the recommended count\nEvaluate existing data: Use get_candidate_rank() to assess your candidates, if you have any that you want to incorporate\nPlan additions: Use get_additional_experiments_needed() to determine how many more experiments to run\nGenerate design: Use ask() to get the additional experiments, which will be optimized to complement your existing data\n\nThis approach works for any model type (\"linear\", \"linear-and-quadratic\", \"linear-and-interactions\", \"fully-quadratic\") and automatically handles constraints, discrete inputs, and categorical inputs."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/index.html",
    "href": "build/lib/docs/tutorials/benchmarks/index.html",
    "title": "Benchmark Examples",
    "section": "",
    "text": "These tutorials demonstrate how to recreate results from various papers and common studies using BoFire’s benchmarking capabilities. Benchmarks are useful for validating optimization strategies and comparing performance across different approaches.\n\n\n\n\nClassic optimization benchmark using the Himmelblau function.\n\n\n\nMulti-objective optimization using the DTLZ2 test problem.\n\n\n\nHartmann function optimization with n-choose-k constraints.\n\n\n\nHigh-dimensional optimization using an extended Branin function.\n\n\n\nRobust optimization in the presence of outliers.\n\n\n\nMolecular optimization using chemical fingerprints and specialized kernels.\n\n\n\nLocal search-based Bayesian optimization.\n\n\n\nMulti-objective optimization using the ZDT1 test problem.\n\n\n\nBinh and Korn multi-objective test function.\n\n\n\nTanaka multi-objective test function.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nHigh-dimensional Bayesian optimization using spherical linear kernels."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/index.html#available-benchmarks",
    "href": "build/lib/docs/tutorials/benchmarks/index.html#available-benchmarks",
    "title": "Benchmark Examples",
    "section": "",
    "text": "Classic optimization benchmark using the Himmelblau function.\n\n\n\nMulti-objective optimization using the DTLZ2 test problem.\n\n\n\nHartmann function optimization with n-choose-k constraints.\n\n\n\nHigh-dimensional optimization using an extended Branin function.\n\n\n\nRobust optimization in the presence of outliers.\n\n\n\nMolecular optimization using chemical fingerprints and specialized kernels.\n\n\n\nLocal search-based Bayesian optimization.\n\n\n\nMulti-objective optimization using the ZDT1 test problem.\n\n\n\nBinh and Korn multi-objective test function.\n\n\n\nTanaka multi-objective test function.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nActive learning strategies for experimental design.\n\n\n\nHigh-dimensional Bayesian optimization using spherical linear kernels."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html",
    "href": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "",
    "text": "Continuous descriptor inputs can be used in combination with engineered features to incorporate more domain knowledge into an optimization. Example could be a solvent of formulation optimization where per continuous input feature additional information about its properties is available.\nFor examples in the literature have a look at this paper."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html#imports",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "Imports",
    "text": "Imports\n\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousDescriptorInput, SumFeature, WeightedSumFeature, ContinuousOutput\nfrom bofire.data_models.domain.api import Inputs, EngineeredFeatures, Outputs, Domain, Constraints\nfrom bofire.benchmarks.api import Himmelblau, FormulationWrapper\nfrom bofire.runners.api import run\nfrom bofire.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.surrogates.api import BotorchSurrogates\nfrom typing import Optional\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html#problem-setup",
    "href": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html#problem-setup",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "Problem Setup",
    "text": "Problem Setup\nFor demo purposes, the formulation wrapper is used around the two dimensional Himmelblau Benchmark to wrap it as a formulation problem with 7 features in total. Per original feature, three new features are introduced and one filler feature. The sum of each feature group is used as (normalized) input for the evaluation of the original Himmelblau function. A seventh feature is introduced as filler to always reach a total of 1 to fulfill the formulation/equality constraint. For more information on this setup have a look at the docstring of FormulationWrapper.\n\nbench = FormulationWrapper(benchmark=Himmelblau(), n_features_per_original_feature=3, n_filler_features=1)\n\nThe continuos descriptor features have the following descriptor values for the descriptors x_1 and x_2:\n\npd.concat([feat.to_df() for feat in bench.domain.inputs.get(ContinuousDescriptorInput)])\n\n\n\n\n\n\n\n\nx_1\nx_2\n\n\n\n\nx_1_0\n1.0\n0.0\n\n\nx_1_1\n1.0\n0.0\n\n\nx_1_2\n1.0\n0.0\n\n\nx_2_0\n0.0\n1.0\n\n\nx_2_1\n0.0\n1.0\n\n\nx_2_2\n0.0\n1.0"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html#benchmarks",
    "href": "build/lib/docs/tutorials/benchmarks/012-engineered_features.html#benchmarks",
    "title": "Optimization with ContinuousDescriptorInputs and engineered Features",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nRandom Strategy\nAs baseline, we run the random strategy on the problem.\n\ndef sample(domain):\n    strategy = RandomStrategy.make(domain=domain)\n    return strategy.ask(10)\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\nrandom_results = []\nfor _ in range(15 if not SMOKE_TEST else 1):\n    results = run(\n        benchmark=bench,\n        strategy_factory=lambda domain: RandomStrategy.make(domain=domain),\n        n_iterations=40 if not SMOKE_TEST else 2,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    random_results.append(results[0][0])\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:00&lt;?, ?it/s, Current Best:=40.516]Run 0:  50%|█████     | 1/2 [00:00&lt;00:00,  9.50it/s, Current Best:=40.516]Run 0:  50%|█████     | 1/2 [00:00&lt;00:00,  9.50it/s, Current Best:=40.516]Run 0:  50%|█████     | 1/2 [00:00&lt;00:00,  9.50it/s, Current Best:=40.516]Run 0: 100%|██████████| 2/2 [00:00&lt;00:00,  9.41it/s, Current Best:=40.516]Run 0: 100%|██████████| 2/2 [00:00&lt;00:00,  9.38it/s, Current Best:=40.516]\n\n\n\n\nSobo Strategy\nNext we run the SoboStrategy on the problem.\n\ndef sample(domain):\n    return initial_experiments[domain.inputs.get_keys()].copy()\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\nsobo_results = []\nfor i in range(15 if not SMOKE_TEST else 1):\n    initial_experiments = random_results[i][:10].copy()\n    results = run(\n        benchmark=bench,\n        strategy_factory=lambda domain: SoboStrategy.make(domain=domain),\n        n_iterations=40 if not SMOKE_TEST else 2,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    sobo_results.append(results[0][0])\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:04&lt;?, ?it/s]Run 0:   0%|          | 0/2 [00:04&lt;?, ?it/s, Current Best:=40.516]Run 0:  50%|█████     | 1/2 [00:04&lt;00:04,  5.00s/it, Current Best:=40.516]Run 0:  50%|█████     | 1/2 [00:10&lt;00:04,  5.00s/it, Current Best:=40.516]Run 0:  50%|█████     | 1/2 [00:10&lt;00:04,  5.00s/it, Current Best:=40.516]Run 0: 100%|██████████| 2/2 [00:10&lt;00:00,  5.46s/it, Current Best:=40.516]Run 0: 100%|██████████| 2/2 [00:10&lt;00:00,  5.39s/it, Current Best:=40.516]\n\n\nNow we run the SoboStrategy again, but with an engineered feature as additional input. The WeightedSumFeature computes the sum over the specified descriptors weighted by the values of the involved original features.\n\ndef create_strategy(domain):\n    surrogate_data = SingleTaskGPSurrogate(\n        inputs=domain.inputs,\n        outputs=domain.outputs,\n        engineered_features=EngineeredFeatures(\n            features=[\n                WeightedSumFeature(\n                    key=\"WeightedSum\",\n                    features=bench.domain.inputs.get_keys(ContinuousDescriptorInput),\n                    descriptors=[\"x_1\", \"x_2\"]\n                    )]),\n        #kernel=RBFKernel(features=[\"x_1\", \"x_2\"], lengthscale_prior=HVARFNER_LENGTHSCALE_PRIOR())\n    )\n    return SoboStrategy.make(domain=domain, surrogate_specs=BotorchSurrogates(surrogates=[surrogate_data]))\n\nsobo_engineered_results_all = []\nfor i in range(15 if not SMOKE_TEST else 1):\n    initial_experiments = random_results[i][:10].copy()\n    results = run(\n        benchmark=bench,\n        strategy_factory=create_strategy,\n        n_iterations=40,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    sobo_engineered_results_all.append(results[0][0])\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/engineered_features.py:71: UserWarning:\n\nCreating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n\n  0%|          | 0/40 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/40 [00:05&lt;?, ?it/s]Run 0:   0%|          | 0/40 [00:05&lt;?, ?it/s, Current Best:=40.516]Run 0:   2%|▎         | 1/40 [00:05&lt;03:40,  5.65s/it, Current Best:=40.516]Run 0:   2%|▎         | 1/40 [00:10&lt;03:40,  5.65s/it, Current Best:=40.516]Run 0:   2%|▎         | 1/40 [00:10&lt;03:40,  5.65s/it, Current Best:=40.516]Run 0:   5%|▌         | 2/40 [00:10&lt;03:25,  5.41s/it, Current Best:=40.516]Run 0:   5%|▌         | 2/40 [00:16&lt;03:25,  5.41s/it, Current Best:=40.516]Run 0:   5%|▌         | 2/40 [00:16&lt;03:25,  5.41s/it, Current Best:=40.516]Run 0:   8%|▊         | 3/40 [00:16&lt;03:28,  5.63s/it, Current Best:=40.516]Run 0:   8%|▊         | 3/40 [00:23&lt;03:28,  5.63s/it, Current Best:=40.516]Run 0:   8%|▊         | 3/40 [00:23&lt;03:28,  5.63s/it, Current Best:=40.516]Run 0:  10%|█         | 4/40 [00:23&lt;03:31,  5.88s/it, Current Best:=40.516]Run 0:  10%|█         | 4/40 [00:29&lt;03:31,  5.88s/it, Current Best:=40.516]Run 0:  10%|█         | 4/40 [00:29&lt;03:31,  5.88s/it, Current Best:=40.516]Run 0:  12%|█▎        | 5/40 [00:29&lt;03:28,  5.96s/it, Current Best:=40.516]Run 0:  12%|█▎        | 5/40 [00:35&lt;03:28,  5.96s/it, Current Best:=40.516]Run 0:  12%|█▎        | 5/40 [00:35&lt;03:28,  5.96s/it, Current Best:=40.516]Run 0:  15%|█▌        | 6/40 [00:35&lt;03:21,  5.94s/it, Current Best:=40.516]Run 0:  15%|█▌        | 6/40 [00:41&lt;03:21,  5.94s/it, Current Best:=40.516]Run 0:  15%|█▌        | 6/40 [00:41&lt;03:21,  5.94s/it, Current Best:=40.516]Run 0:  18%|█▊        | 7/40 [00:41&lt;03:24,  6.20s/it, Current Best:=40.516]Run 0:  18%|█▊        | 7/40 [00:47&lt;03:24,  6.20s/it, Current Best:=40.516]Run 0:  18%|█▊        | 7/40 [00:47&lt;03:24,  6.20s/it, Current Best:=40.516]Run 0:  20%|██        | 8/40 [00:47&lt;03:13,  6.05s/it, Current Best:=40.516]Run 0:  20%|██        | 8/40 [00:53&lt;03:13,  6.05s/it, Current Best:=40.516]Run 0:  20%|██        | 8/40 [00:53&lt;03:13,  6.05s/it, Current Best:=36.573]Run 0:  22%|██▎       | 9/40 [00:53&lt;03:11,  6.17s/it, Current Best:=36.573]Run 0:  22%|██▎       | 9/40 [01:00&lt;03:11,  6.17s/it, Current Best:=36.573]Run 0:  22%|██▎       | 9/40 [01:00&lt;03:11,  6.17s/it, Current Best:=36.573]Run 0:  25%|██▌       | 10/40 [01:00&lt;03:08,  6.28s/it, Current Best:=36.573]Run 0:  25%|██▌       | 10/40 [01:06&lt;03:08,  6.28s/it, Current Best:=36.573]Run 0:  25%|██▌       | 10/40 [01:06&lt;03:08,  6.28s/it, Current Best:=36.573]Run 0:  28%|██▊       | 11/40 [01:06&lt;03:00,  6.23s/it, Current Best:=36.573]Run 0:  28%|██▊       | 11/40 [01:12&lt;03:00,  6.23s/it, Current Best:=36.573]Run 0:  28%|██▊       | 11/40 [01:12&lt;03:00,  6.23s/it, Current Best:=7.587] Run 0:  30%|███       | 12/40 [01:12&lt;02:52,  6.17s/it, Current Best:=7.587]Run 0:  30%|███       | 12/40 [01:19&lt;02:52,  6.17s/it, Current Best:=7.587]Run 0:  30%|███       | 12/40 [01:19&lt;02:52,  6.17s/it, Current Best:=7.587]Run 0:  32%|███▎      | 13/40 [01:19&lt;02:51,  6.36s/it, Current Best:=7.587]Run 0:  32%|███▎      | 13/40 [01:25&lt;02:51,  6.36s/it, Current Best:=7.587]Run 0:  32%|███▎      | 13/40 [01:25&lt;02:51,  6.36s/it, Current Best:=7.587]Run 0:  35%|███▌      | 14/40 [01:25&lt;02:45,  6.36s/it, Current Best:=7.587]Run 0:  35%|███▌      | 14/40 [01:32&lt;02:45,  6.36s/it, Current Best:=7.587]Run 0:  35%|███▌      | 14/40 [01:32&lt;02:45,  6.36s/it, Current Best:=7.587]Run 0:  38%|███▊      | 15/40 [01:32&lt;02:39,  6.37s/it, Current Best:=7.587]Run 0:  38%|███▊      | 15/40 [01:38&lt;02:39,  6.37s/it, Current Best:=7.587]Run 0:  38%|███▊      | 15/40 [01:38&lt;02:39,  6.37s/it, Current Best:=7.587]Run 0:  40%|████      | 16/40 [01:38&lt;02:31,  6.30s/it, Current Best:=7.587]Run 0:  40%|████      | 16/40 [01:45&lt;02:31,  6.30s/it, Current Best:=7.587]Run 0:  40%|████      | 16/40 [01:45&lt;02:31,  6.30s/it, Current Best:=7.587]Run 0:  42%|████▎     | 17/40 [01:45&lt;02:29,  6.52s/it, Current Best:=7.587]Run 0:  42%|████▎     | 17/40 [01:52&lt;02:29,  6.52s/it, Current Best:=7.587]Run 0:  42%|████▎     | 17/40 [01:52&lt;02:29,  6.52s/it, Current Best:=7.587]Run 0:  45%|████▌     | 18/40 [01:52&lt;02:25,  6.59s/it, Current Best:=7.587]Run 0:  45%|████▌     | 18/40 [01:57&lt;02:25,  6.59s/it, Current Best:=7.587]Run 0:  45%|████▌     | 18/40 [01:57&lt;02:25,  6.59s/it, Current Best:=7.587]Run 0:  48%|████▊     | 19/40 [01:57&lt;02:13,  6.35s/it, Current Best:=7.587]Run 0:  48%|████▊     | 19/40 [02:03&lt;02:13,  6.35s/it, Current Best:=7.587]Run 0:  48%|████▊     | 19/40 [02:03&lt;02:13,  6.35s/it, Current Best:=7.587]Run 0:  50%|█████     | 20/40 [02:03&lt;01:59,  6.00s/it, Current Best:=7.587]Run 0:  50%|█████     | 20/40 [02:10&lt;01:59,  6.00s/it, Current Best:=7.587]Run 0:  50%|█████     | 20/40 [02:10&lt;01:59,  6.00s/it, Current Best:=7.587]Run 0:  52%|█████▎    | 21/40 [02:10&lt;01:59,  6.31s/it, Current Best:=7.587]Run 0:  52%|█████▎    | 21/40 [02:17&lt;01:59,  6.31s/it, Current Best:=7.587]Run 0:  52%|█████▎    | 21/40 [02:17&lt;01:59,  6.31s/it, Current Best:=7.587]Run 0:  55%|█████▌    | 22/40 [02:17&lt;01:57,  6.51s/it, Current Best:=7.587]Run 0:  55%|█████▌    | 22/40 [02:23&lt;01:57,  6.51s/it, Current Best:=7.587]Run 0:  55%|█████▌    | 22/40 [02:23&lt;01:57,  6.51s/it, Current Best:=7.587]Run 0:  57%|█████▊    | 23/40 [02:23&lt;01:49,  6.45s/it, Current Best:=7.587]Run 0:  57%|█████▊    | 23/40 [02:28&lt;01:49,  6.45s/it, Current Best:=7.587]Run 0:  57%|█████▊    | 23/40 [02:28&lt;01:49,  6.45s/it, Current Best:=7.587]Run 0:  60%|██████    | 24/40 [02:28&lt;01:38,  6.15s/it, Current Best:=7.587]Run 0:  60%|██████    | 24/40 [02:34&lt;01:38,  6.15s/it, Current Best:=7.587]Run 0:  60%|██████    | 24/40 [02:34&lt;01:38,  6.15s/it, Current Best:=0.799]Run 0:  62%|██████▎   | 25/40 [02:34&lt;01:30,  6.05s/it, Current Best:=0.799]Run 0:  62%|██████▎   | 25/40 [02:40&lt;01:30,  6.05s/it, Current Best:=0.799]Run 0:  62%|██████▎   | 25/40 [02:40&lt;01:30,  6.05s/it, Current Best:=0.799]Run 0:  65%|██████▌   | 26/40 [02:40&lt;01:23,  5.97s/it, Current Best:=0.799]Run 0:  65%|██████▌   | 26/40 [02:45&lt;01:23,  5.97s/it, Current Best:=0.799]Run 0:  65%|██████▌   | 26/40 [02:45&lt;01:23,  5.97s/it, Current Best:=0.799]Run 0:  68%|██████▊   | 27/40 [02:45&lt;01:15,  5.81s/it, Current Best:=0.799]Run 0:  68%|██████▊   | 27/40 [02:51&lt;01:15,  5.81s/it, Current Best:=0.799]Run 0:  68%|██████▊   | 27/40 [02:51&lt;01:15,  5.81s/it, Current Best:=0.799]Run 0:  70%|███████   | 28/40 [02:51&lt;01:08,  5.72s/it, Current Best:=0.799]Run 0:  70%|███████   | 28/40 [02:57&lt;01:08,  5.72s/it, Current Best:=0.799]Run 0:  70%|███████   | 28/40 [02:57&lt;01:08,  5.72s/it, Current Best:=0.799]Run 0:  72%|███████▎  | 29/40 [02:57&lt;01:03,  5.81s/it, Current Best:=0.799]Run 0:  72%|███████▎  | 29/40 [03:02&lt;01:03,  5.81s/it, Current Best:=0.799]Run 0:  72%|███████▎  | 29/40 [03:02&lt;01:03,  5.81s/it, Current Best:=0.799]Run 0:  75%|███████▌  | 30/40 [03:02&lt;00:57,  5.73s/it, Current Best:=0.799]Run 0:  75%|███████▌  | 30/40 [03:08&lt;00:57,  5.73s/it, Current Best:=0.799]Run 0:  75%|███████▌  | 30/40 [03:08&lt;00:57,  5.73s/it, Current Best:=0.799]Run 0:  78%|███████▊  | 31/40 [03:08&lt;00:52,  5.82s/it, Current Best:=0.799]Run 0:  78%|███████▊  | 31/40 [03:14&lt;00:52,  5.82s/it, Current Best:=0.799]Run 0:  78%|███████▊  | 31/40 [03:14&lt;00:52,  5.82s/it, Current Best:=0.799]Run 0:  80%|████████  | 32/40 [03:14&lt;00:46,  5.82s/it, Current Best:=0.799]Run 0:  80%|████████  | 32/40 [03:22&lt;00:46,  5.82s/it, Current Best:=0.799]Run 0:  80%|████████  | 32/40 [03:22&lt;00:46,  5.82s/it, Current Best:=0.799]Run 0:  82%|████████▎ | 33/40 [03:22&lt;00:45,  6.46s/it, Current Best:=0.799]Run 0:  82%|████████▎ | 33/40 [03:28&lt;00:45,  6.46s/it, Current Best:=0.799]Run 0:  82%|████████▎ | 33/40 [03:28&lt;00:45,  6.46s/it, Current Best:=0.799]Run 0:  85%|████████▌ | 34/40 [03:28&lt;00:38,  6.36s/it, Current Best:=0.799]Run 0:  85%|████████▌ | 34/40 [03:34&lt;00:38,  6.36s/it, Current Best:=0.799]Run 0:  85%|████████▌ | 34/40 [03:34&lt;00:38,  6.36s/it, Current Best:=0.799]Run 0:  88%|████████▊ | 35/40 [03:34&lt;00:30,  6.20s/it, Current Best:=0.799]Run 0:  88%|████████▊ | 35/40 [03:41&lt;00:30,  6.20s/it, Current Best:=0.799]Run 0:  88%|████████▊ | 35/40 [03:41&lt;00:30,  6.20s/it, Current Best:=0.799]Run 0:  90%|█████████ | 36/40 [03:41&lt;00:25,  6.32s/it, Current Best:=0.799]Run 0:  90%|█████████ | 36/40 [03:47&lt;00:25,  6.32s/it, Current Best:=0.799]Run 0:  90%|█████████ | 36/40 [03:47&lt;00:25,  6.32s/it, Current Best:=0.799]Run 0:  92%|█████████▎| 37/40 [03:47&lt;00:18,  6.25s/it, Current Best:=0.799]Run 0:  92%|█████████▎| 37/40 [03:53&lt;00:18,  6.25s/it, Current Best:=0.799]Run 0:  92%|█████████▎| 37/40 [03:53&lt;00:18,  6.25s/it, Current Best:=0.799]Run 0:  95%|█████████▌| 38/40 [03:53&lt;00:12,  6.18s/it, Current Best:=0.799]Run 0:  95%|█████████▌| 38/40 [03:59&lt;00:12,  6.18s/it, Current Best:=0.799]Run 0:  95%|█████████▌| 38/40 [03:59&lt;00:12,  6.18s/it, Current Best:=0.630]Run 0:  98%|█████████▊| 39/40 [03:59&lt;00:06,  6.08s/it, Current Best:=0.630]Run 0:  98%|█████████▊| 39/40 [04:05&lt;00:06,  6.08s/it, Current Best:=0.630]Run 0:  98%|█████████▊| 39/40 [04:05&lt;00:06,  6.08s/it, Current Best:=0.333]Run 0: 100%|██████████| 40/40 [04:05&lt;00:00,  6.10s/it, Current Best:=0.333]Run 0: 100%|██████████| 40/40 [04:05&lt;00:00,  6.13s/it, Current Best:=0.333]\n\n\n\n\nPlot the performance\nNext we plot the performance. Using the engineered features shows a clear benefit. Using a SAAS model on it would lead to even better results, as only two features are needed and SAAS models are very good in figuring out the features of interest.\n\ndef get_padded_trajectories(dfs: list[pd.DataFrame]) -&gt; np.ndarray:\n    trajectories = []\n    for df in dfs:\n        y_values = [df.y[:10].max()] + df.y[10:].tolist()\n        cummin = np.minimum.accumulate(np.log10(y_values))\n        trajectories.append(np.array(cummin))\n    max_len = max(len(t) for t in trajectories)\n    return [np.pad(traj, (0, max_len-len(traj)), mode='edge') for traj in trajectories]\n\nfig, ax = plt.subplots()\n\nrandom_trajectories = get_padded_trajectories(random_results)\nstd_random = np.std(random_trajectories, axis=0)\nmean_random = np.mean(random_trajectories, axis=0)\nsem_random = std_random / np.sqrt(len(random_trajectories))\nax.plot(mean_random, label='Random', color='blue')\nax.fill_between(range(len(mean_random)), mean_random - sem_random, mean_random + sem_random, color='blue', alpha=0.2)\n\n\nsobo_trajectories = get_padded_trajectories(sobo_results)\nstd_sobo = np.std(sobo_trajectories, axis=0)\nmean_sobo = np.mean(sobo_trajectories, axis=0)\nsem_sobo = std_sobo / np.sqrt(len(sobo_trajectories))\nax.plot(mean_sobo, label='SOBO', color='green')\nax.fill_between(range(len(mean_sobo)), mean_sobo - sem_sobo, mean_sobo + sem_sobo, color='green', alpha=0.2)\n\nsobo_engineered_trajectories = get_padded_trajectories(sobo_engineered_results_all)\nstd_sobo_engineered = np.std(sobo_engineered_trajectories, axis=0)\nmean_sobo_engineered = np.mean(sobo_engineered_trajectories, axis=0)\nsem_sobo_engineered = std_sobo_engineered / np.sqrt(len(sobo_engineered_trajectories))\nax.plot(mean_sobo_engineered, label='SOBO Engineered', color='red')\nax.fill_between(range(len(mean_sobo_engineered)), mean_sobo_engineered - sem_sobo_engineered, mean_sobo_engineered + sem_sobo_engineered, color='red', alpha=0.2)\n\nax.set_title('Optimization Performance')\nax.legend()\nplt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/010-TNK.html",
    "href": "build/lib/docs/tutorials/benchmarks/010-TNK.html",
    "title": "TNK Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import TNK\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/010-TNK.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/010-TNK.html#imports",
    "title": "TNK Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import TNK\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/010-TNK.html#random-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/010-TNK.html#random-strategy",
    "title": "TNK Benchmark",
    "section": "Random Strategy",
    "text": "Random Strategy\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return compute_hypervolume(\n        domain,\n        experiments.loc[(experiments.c1 &gt;= 0) & (experiments.c2 &lt;= 0.5)],\n        ref_point={\"f1\": 4, \"f2\": 4},\n    )\n\n\nrandom_results = run(\n    TNK(),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=100 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=10.160]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 45.73it/s, Current Best:=10.160]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/010-TNK.html#mobo-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/010-TNK.html#mobo-strategy",
    "title": "TNK Benchmark",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(domain=domain, ref_point={\"f1\": 4.0, \"f2\": 4.0})\n    return strategies.map(data_model)\n\n\nresults = run(\n    TNK(),\n    strategy_factory=strategy_factory,\n    n_iterations=100 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s, Current Best:=10.892]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.94s/it, Current Best:=10.892]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.94s/it, Current Best:=10.892]\n\n\n\nif not SMOKE_TEST:\n    fig, ax = plt.subplots()\n    ax.plot(random_results[0][1], label=\"random\")\n    ax.plot(results[0][1], label=\"MOBO\")\n    ax.set_xlabel(\"iteration\")\n    ax.set_ylabel(\"hypervolume\")\n    ax.legend()\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html",
    "href": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html",
    "title": "ZDT1",
    "section": "",
    "text": "import os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import ZDT1\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.kernels.api import RBFKernel\nfrom bofire.data_models.priors.api import (\n    HVARFNER_NOISE_PRIOR,\n    DimensionalityScaledLogNormalPrior,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    FullyBayesianSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume, get_pareto_front\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nN_ITERATIONS = 50 if not SMOKE_TEST else 1\nBATCH_SIZE = 5 if not SMOKE_TEST else 1\nWARMUP_STEPS = 256 if not SMOKE_TEST else 32\nNUM_SAMPLES = 128 if not SMOKE_TEST else 16\nTHINNING = 16"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#imports",
    "title": "ZDT1",
    "section": "",
    "text": "import os\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import ZDT1\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.kernels.api import RBFKernel\nfrom bofire.data_models.priors.api import (\n    HVARFNER_NOISE_PRIOR,\n    DimensionalityScaledLogNormalPrior,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    FullyBayesianSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume, get_pareto_front\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nN_ITERATIONS = 50 if not SMOKE_TEST else 1\nBATCH_SIZE = 5 if not SMOKE_TEST else 1\nWARMUP_STEPS = 256 if not SMOKE_TEST else 32\nNUM_SAMPLES = 128 if not SMOKE_TEST else 16\nTHINNING = 16"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#random-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#random-optimization",
    "title": "ZDT1",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return compute_hypervolume(domain, experiments, ref_point={\"y1\": 1.0, \"y2\": 5.0})\n\n\nrandom_results = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=1.020]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 18.09it/s, Current Best:=1.020]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-hvarfner-priors",
    "href": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-hvarfner-priors",
    "title": "ZDT1",
    "section": "Optimization with Hvarfner priors",
    "text": "Optimization with Hvarfner priors\n\nbenchmark = ZDT1(n_inputs=30)\n\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(\n        domain=domain,\n        ref_point={\"y1\": 1.0, \"y2\": 5.0},\n        surrogate_specs=BotorchSurrogates(\n            surrogates=[\n                SingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y1\"]),\n                    # the following hyperparams do not need to be provided\n                    kernel=RBFKernel(\n                        ard=True,\n                        lengthscale_prior=DimensionalityScaledLogNormalPrior(),\n                    ),\n                    noise_prior=HVARFNER_NOISE_PRIOR(),\n                ),\n                SingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y2\"]),\n                    # the following hyperparams do not need to be provided\n                    kernel=RBFKernel(\n                        ard=True,\n                        lengthscale_prior=DimensionalityScaledLogNormalPrior(),\n                    ),\n                    noise_prior=HVARFNER_NOISE_PRIOR(),\n                ),\n            ],\n        ),\n    )\n    return strategies.map(data_model)\n\n\nresults = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:09&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:09&lt;?, ?it/s, Current Best:=1.186]Run 0: 100%|██████████| 1/1 [00:09&lt;00:00,  9.65s/it, Current Best:=1.186]Run 0: 100%|██████████| 1/1 [00:09&lt;00:00,  9.65s/it, Current Best:=1.186]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-default-priors",
    "href": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#optimization-with-default-priors",
    "title": "ZDT1",
    "section": "Optimization with default priors",
    "text": "Optimization with default priors\n\nbenchmark = ZDT1(n_inputs=30)\n\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(\n        domain=domain,\n        ref_point={\"y1\": 1.0, \"y2\": 5.0},\n    )\n    return strategies.map(data_model)\n\n\nresults = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:06&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:06&lt;?, ?it/s, Current Best:=1.201]Run 0: 100%|██████████| 1/1 [00:06&lt;00:00,  6.91s/it, Current Best:=1.201]Run 0: 100%|██████████| 1/1 [00:06&lt;00:00,  6.91s/it, Current Best:=1.201]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#saasbo-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/008-ZDT1.html#saasbo-optimization",
    "title": "ZDT1",
    "section": "SAASBO Optimization",
    "text": "SAASBO Optimization\n\nbenchmark = ZDT1(n_inputs=30)\n\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(\n        domain=domain,\n        ref_point={\"y1\": 1.0, \"y2\": 5.0},\n        surrogate_specs=BotorchSurrogates(\n            surrogates=[\n                FullyBayesianSingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y1\"]),\n                    # the following hyperparams do not need to be provided\n                    warmup_steps=WARMUP_STEPS,\n                    num_samples=NUM_SAMPLES,\n                    thinning=THINNING,\n                    model_type=\"saas\",\n                ),\n                FullyBayesianSingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs.get_by_keys([\"y2\"]),\n                    # the following hyperparams do not need to be provided\n                    warmup_steps=WARMUP_STEPS,\n                    num_samples=NUM_SAMPLES,\n                    thinning=THINNING,\n                    model_type=\"saas\",\n                ),\n            ],\n        ),\n    )\n    return strategies.map(data_model)\n\n\nresults = run(\n    ZDT1(n_inputs=30),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:14&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:14&lt;?, ?it/s, Current Best:=1.137]Run 0: 100%|██████████| 1/1 [00:14&lt;00:00, 14.75s/it, Current Best:=1.137]Run 0: 100%|██████████| 1/1 [00:14&lt;00:00, 14.75s/it, Current Best:=1.137]\n\n\nPlot the pareto front.\n\ntheoretical_front = benchmark.get_optima()\nfront = get_pareto_front(domain=benchmark.domain, experiments=results[0][0])\n\nfig, ax = plt.subplots()\nax.plot(theoretical_front.y1, theoretical_front.y2, label=\"theoretical Pareto front\")\n\nax.scatter(front.y1, front.y2, label=\"Mobo\")\nax.set_xlabel(\"f1\")\nax.set_ylabel(\"f2\")\n\n\nax.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nShow the performance of the optimizer.\n\nfig, ax = plt.subplots()\n\nax.plot(results[0][1], label=\"Mobo, refpoint=(1, 5)\")\n\nax.set_ylabel(\"Hypervolume\")\nax.set_xlabel(\"Iteration\")\n\nax.legend()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html",
    "href": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "",
    "text": "An example notebook for Bayesian optimisation on a molecular dataset using a Tanimoto fingerprint kernel and the photoswitch dataset.\\(\\newline\\) Paper: https://pubs.rsc.org/en/content/articlelanding/2022/sc/d2sc04306h \\(\\newline\\) Code: https://github.com/Ryan-Rhys/The-Photoswitch-Dataset \\(\\newline\\) This notebook is adapted from https://github.com/leojklarner/gauche/blob/main/notebooks/Bayesian%20Optimisation%20Over%20Molecules.ipynb \\(\\newline\\) The method of obtaining new data from a discrete dataset is explained in the notebook and the details of the dataset and the method are explained in the code and the paper respectively."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#imports",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Imports",
    "text": "Imports\n\nimport os\nimport io\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.data.photoswitches import EXPERIMENTS\nfrom bofire.benchmarks.LookupTableBenchmark import LookupTableBenchmark\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import CategoricalMolecularInput, ContinuousOutput\nfrom bofire.data_models.molfeatures.api import FingerprintsFragments\nfrom bofire.data_models.objectives.api import MaximizeObjective\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, TanimotoGPSurrogate\nfrom bofire.runners.api import run\n\n\nwarnings.filterwarnings(\"ignore\")\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#benchmark",
    "href": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#benchmark",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Benchmark",
    "text": "Benchmark\ninput and output feature keys and extract them to get LookUpTable\n\nbenchmark = {\n    \"input\": \"SMILES\",\n    \"output\": \"E isomer pi-pi* wavelength in nm\",\n}\ndf = pd.read_json(io.StringIO(EXPERIMENTS))\nmain_file = pd.DataFrame(columns=[benchmark[\"input\"], benchmark[\"output\"]])\nnans = df[benchmark[\"output\"]].isnull().to_list()\nnan_indices = [nan for nan, x in enumerate(nans) if x]\nmain_file[benchmark[\"input\"]] = df[benchmark[\"input\"]].drop(nan_indices).to_list()\nmain_file[benchmark[\"output\"]] = (\n    df[benchmark[\"output\"]].dropna().to_numpy().reshape(-1, 1)\n)\ninput_feature = CategoricalMolecularInput(\n    key=benchmark[\"input\"],\n    categories=list(set(main_file[benchmark[\"input\"]].to_list())),\n)\nobjective = MaximizeObjective(\n    w=1.0,\n)\ninputs = Inputs(features=[input_feature])\noutput_feature = ContinuousOutput(key=benchmark[\"output\"], objective=objective)\noutputs = Outputs(features=[output_feature])\ndomain = Domain(inputs=inputs, outputs=outputs)"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#random-vs-sobo-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#random-vs-sobo-optimization",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Random vs SOBO optimization",
    "text": "Random vs SOBO optimization\nFor molecules, we use Tanimoto GP which has a Tanimoto kernel as default\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(20)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments[domain.outputs.get_keys()[0]].max()\n\n\nn_iter = 20 if not SMOKE_TEST else 1\nbo_results_set = []\nrandom_results_set = []\nn_iterations = 49 if not SMOKE_TEST else 1\n\nfor _ in range(n_iter):\n    Benchmark = LookupTableBenchmark(domain=domain, lookup_table=main_file)\n    sampled = sample(Benchmark.domain)\n    sampled_xy = Benchmark.f(sampled, return_complete=True)\n    random_results = run(\n        Benchmark,\n        strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n        n_iterations=n_iterations,\n        metric=best,\n        initial_sampler=sampled_xy,\n        n_runs=1,\n        n_procs=1,\n    )\n\n    specs = {Benchmark.domain.inputs.get_keys()[0]: FingerprintsFragments(n_bits=2048)}\n    surrogate = TanimotoGPSurrogate(\n        inputs=Benchmark.domain.inputs,\n        outputs=Benchmark.domain.outputs,\n        categorical_encodings=specs,\n    )\n\n    def sobo_factory(domain: Domain, surrogate=surrogate):\n        return strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qLogEI(),\n                surrogate_specs=BotorchSurrogates(surrogates=[surrogate]),\n            ),\n        )\n\n    qExpectedImprovement = qLogEI()\n    bo_results = run(\n        Benchmark,\n        strategy_factory=sobo_factory,\n        n_iterations=n_iterations,\n        metric=best,\n        initial_sampler=sampled_xy,\n        n_runs=1,\n        n_procs=1,\n    )\n    random_results_new = np.insert(\n        random_results[0][1].to_numpy(),\n        0,\n        best(Benchmark.domain, sampled_xy),\n    )\n    bo_results_new = np.insert(\n        bo_results[0][1].to_numpy(),\n        0,\n        best(Benchmark.domain, sampled_xy),\n    )\n    random_results_set.append(random_results_new)\n    bo_results_set.append(bo_results_new)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=518.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 75.89it/s, Current Best:=518.000]\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=518.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  5.20it/s, Current Best:=518.000]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  5.19it/s, Current Best:=518.000]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#performance",
    "href": "build/lib/docs/tutorials/benchmarks/006-Bayesian_optimization_over_molecules.html#performance",
    "title": "Bayesian Optimisation Over Molecules",
    "section": "Performance",
    "text": "Performance\nSOBO outperforms random search in terms of selecting molecules with high E isomer pi-pi* transition wavelength.\n\n# Define a confience interval function for plotting.\ndef ci(y):\n    return 1.96 * y.std(axis=0) / np.sqrt(n_iter)\n\n\nif not SMOKE_TEST:\n    iters = np.arange(n_iterations + 1)\n    y_rnd = np.asarray(random_results_set)\n    y_ei = np.asarray(bo_results_set)\n\n    y_rnd_mean = y_rnd.mean(axis=0)\n    y_ei_mean = y_ei.mean(axis=0)\n    y_rnd_std = y_rnd.std(axis=0)\n    y_ei_std = y_ei.std(axis=0)\n\n    lower_rnd = y_rnd_mean - y_rnd_std\n    upper_rnd = y_rnd_mean + y_rnd_std\n    lower_ei = y_ei_mean - y_ei_std\n    upper_ei = y_ei_mean + y_ei_std\n\n    plt.plot(iters, y_rnd_mean, label=\"Random\")\n    plt.fill_between(iters, lower_rnd, upper_rnd, alpha=0.2)\n    plt.plot(iters, y_ei_mean, label=\"SOBO\")\n    plt.fill_between(iters, lower_ei, upper_ei, alpha=0.2)\n    plt.xlabel(\"Number of Iterations\")\n    plt.ylabel(\"Best Objective Value\")\n    plt.legend(loc=\"lower right\")\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/004-30dimBranin.html",
    "href": "build/lib/docs/tutorials/benchmarks/004-30dimBranin.html",
    "title": "30dim Branin Benchmark with SAASBO",
    "section": "",
    "text": "This is a port from https://github.com/pytorch/botorch/blob/main/tutorials/saasbo.ipynb ## Imports\nimport os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Branin30\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    FullyBayesianSingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\nN_ITERATIONS = 10 if not SMOKE_TEST else 1\nBATCH_SIZE = 5 if not SMOKE_TEST else 1\nWARMUP_STEPS = 256 if not SMOKE_TEST else 32\nNUM_SAMPLES = 128 if not SMOKE_TEST else 16\nTHINNING = 16"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/004-30dimBranin.html#random-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/004-30dimBranin.html#random-optimization",
    "title": "30dim Branin Benchmark with SAASBO",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nrandom_results = run(\n    Branin30(),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=N_ITERATIONS,\n    metric=best,\n    initial_sampler=sample,\n    n_candidates_per_proposal=5,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=2.276]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 18.89it/s, Current Best:=2.276]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/004-30dimBranin.html#saasbo-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/004-30dimBranin.html#saasbo-optimization",
    "title": "30dim Branin Benchmark with SAASBO",
    "section": "SAASBO Optimization",
    "text": "SAASBO Optimization\n\nbenchmark = Branin30()\n\n\ndef strategy_factory(domain: Domain):\n    data_model = SoboStrategy(\n        domain=domain,\n        acquisition_function=qLogEI(),\n        surrogate_specs=BotorchSurrogates(\n            surrogates=[\n                FullyBayesianSingleTaskGPSurrogate(\n                    inputs=benchmark.domain.inputs,\n                    outputs=benchmark.domain.outputs,\n                    model_type=\"saas\",\n                    # the following hyperparams do not need to be provided\n                    warmup_steps=WARMUP_STEPS,\n                    num_samples=NUM_SAMPLES,\n                    thinning=THINNING,\n                ),\n            ],\n        ),\n    )\n    return strategies.map(data_model)\n\n\nrandom_results = run(\n    Branin30(),\n    strategy_factory=strategy_factory,\n    n_iterations=N_ITERATIONS,\n    metric=best,\n    initial_sampler=sample,\n    n_candidates_per_proposal=5,\n    n_runs=1,\n    n_procs=1,\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:15&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:15&lt;?, ?it/s, Current Best:=0.888]Run 0: 100%|██████████| 1/1 [00:15&lt;00:00, 15.81s/it, Current Best:=0.888]Run 0: 100%|██████████| 1/1 [00:15&lt;00:00, 15.81s/it, Current Best:=0.888]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html",
    "title": "DTLZ2 Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import MinimizeObjective\nfrom bofire.data_models.strategies.api import (\n    MoboStrategy,\n    QparegoStrategy,\n    RandomStrategy,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#imports",
    "title": "DTLZ2 Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import MinimizeObjective\nfrom bofire.data_models.strategies.api import (\n    MoboStrategy,\n    QparegoStrategy,\n    RandomStrategy,\n)\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#manual-setup-of-the-optimization-domain",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#manual-setup-of-the-optimization-domain",
    "title": "DTLZ2 Benchmark",
    "section": "Manual setup of the optimization domain",
    "text": "Manual setup of the optimization domain\nThe following cell shows how to manually setup the optimization problem in BoFire for didactic purposes. In the following the implemented benchmark module is then used.\n\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i}\", bounds=(0, 1)) for i in range(6)],\n)\n# here the minimize objective is used, if you want to maximize you have to use the maximize objective.\noutput_features = Outputs(\n    features=[\n        ContinuousOutput(key=f\"f_{i}\", objective=MinimizeObjective(w=1.0))\n        for i in range(2)\n    ],\n)\n# no constraints are present so we can create the domain\ndomain = Domain(inputs=input_features, outputs=output_features)"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#random-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#random-strategy",
    "title": "DTLZ2 Benchmark",
    "section": "Random Strategy",
    "text": "Random Strategy\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return compute_hypervolume(domain, experiments, ref_point={\"f_0\": 1.1, \"f_1\": 1.1})\n\n\nrandom_results = run(\n    DTLZ2(dim=6),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=0.042]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 51.74it/s, Current Best:=0.042]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#mobo-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#mobo-strategy",
    "title": "DTLZ2 Benchmark",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\n\nAutomatic run\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(domain=domain, ref_point={\"f_0\": 1.1, \"f_1\": 1.1})\n    return strategies.map(data_model)\n\n\nresults = run(\n    DTLZ2(dim=6),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s, Current Best:=0.069]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.43s/it, Current Best:=0.069]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.43s/it, Current Best:=0.069]\n\n\n\n\nManual setup\n\nUsing the default Models\n\n# we get the domain from the benchmark module, in real use case we have to build it on our own\n# make sure that the objective is set correctly\ndomain = DTLZ2(dim=6).domain\n# we generate training data\nexperiments = DTLZ2(dim=6).f(domain.inputs.sample(10), return_complete=True)\n# we setup the strategy\n# providing of a reference point is not mandatory but can help\n# the reference point has to be wrt to the assigned objective always worse than the points on the paretofront.\ndata_model = MoboStrategy(domain=domain, ref_point={\"f_0\": 1.1, \"f_1\": 1.1})\nrecommender = strategies.map(data_model=data_model)\n# we tell the strategy our historical data\nrecommender.tell(experiments=experiments)\n# we ask for a new point to evaluate\ncandidates = recommender.ask(candidate_count=1)\n# we show the candidate\ndisplay(candidates)\n# this candidate has to be then provided to the benchmark function and evaluated and then told back to the optimizer to get the next candidate\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nf_0_pred\nf_1_pred\nf_0_sd\nf_1_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.025945\n0.925247\n0.246039\n0.321472\n-0.025945\n-0.925247\n\n\n\n\n\n\n\n\n\nSetup specific models\n\nfrom bofire.data_models.kernels.api import RBFKernel, ScaleKernel\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\n\n\n# in this case you would use non default kernels for the different outputs\n# it is also possible to build the models for a subset of the complete features\ndata_model = MoboStrategy(\n    domain=domain,\n    ref_point={\"f_0\": 1.1, \"f_1\": 1.1},\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[0]]),\n                kernel=ScaleKernel(base_kernel=RBFKernel(ard=True)),\n            ),\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[1]]),\n                kernel=ScaleKernel(base_kernel=RBFKernel(ard=False)),\n            ),\n        ],\n    ),\n)\nrecommender = strategies.map(data_model=data_model)\n# we tell the strategy our historical data\nrecommender.tell(experiments=experiments)\n# we ask for a new point to evaluate\ncandidates = recommender.ask(candidate_count=1)\n# we show the candidate\ndisplay(candidates)\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_2\nx_3\nx_4\nx_5\nf_0_pred\nf_1_pred\nf_0_sd\nf_1_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n1.0\n1.0\n-0.433217\n0.732053\n0.146893\n0.545934\n0.433217\n-0.732053"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#qparego-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#qparego-strategy",
    "title": "DTLZ2 Benchmark",
    "section": "QPAREGO Strategy",
    "text": "QPAREGO Strategy\n\nresults_qparego = run(\n    DTLZ2(dim=6),\n    strategy_factory=lambda domain: strategies.map(QparegoStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/acquisition/monte_carlo.py:502: NumericsWarning:\n\nqNoisyExpectedImprovement has known numerical issues that lead to suboptimal optimization performance. It is strongly recommended to simply replace\n\n     qNoisyExpectedImprovement   --&gt;     qLogNoisyExpectedImprovement \n\ninstead, which fixes the issues and has the same API. See https://arxiv.org/abs/2310.20708 for details.\n\nRun 0:   0%|          | 0/1 [00:02&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:02&lt;?, ?it/s, Current Best:=0.004]Run 0: 100%|██████████| 1/1 [00:02&lt;00:00,  2.74s/it, Current Best:=0.004]Run 0: 100%|██████████| 1/1 [00:02&lt;00:00,  2.74s/it, Current Best:=0.004]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#performance-plot",
    "href": "build/lib/docs/tutorials/benchmarks/002-DTLZ2.html#performance-plot",
    "title": "DTLZ2 Benchmark",
    "section": "Performance Plot",
    "text": "Performance Plot\n\nimport matplotlib.pyplot as plt\n\n\nfig, ax = plt.subplots()\n\nax.scatter(results[0][0].f_0, results[0][0].f_1, label=\"qehvi\")\nax.scatter(results_qparego[0][0].f_0, results_qparego[0][0].f_1, label=\"qparego\")\nax.scatter(random_results[0][0].f_0, random_results[0][0].f_1, label=\"random\")\n\nax.legend()\n\nax.set_xlabel(\"f_0\")\nax.set_ylabel(\"f_1\")\n\nfig.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/index_kernel.html",
    "href": "build/lib/docs/tutorials/basic_examples/index_kernel.html",
    "title": "Introduction to index kernel and positive index kernel.",
    "section": "",
    "text": "The Index kernel models categorical variables by assigning each category an index and learning a low-rank representation of the kernel matrix. This is particularly useful for ordered categorical variables or when categories have some inherent structure. Unlike Hamming distance kernel which assumes binary correlation (same or different), Index kernels try to learn the correlation between the categories while fitting GP.\nIn this tutorial, we show the steps to create a GP surrogate using Index and Positive Index kernels. One can extend the steps to incorporate the shown feature for Bayesian optimization.\nWe use the aniline_cn_crosscoupling data-set.\n\n# import basic python libraries\nimport numpy as np\nimport pandas as pd\nimport json\n\n# import bofire components\nfrom bofire.data_models.kernels.api import IndexKernel, RBFKernel, PositiveIndexKernel, AdditiveKernel, ScaleKernel\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.domain.api import Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput, CategoricalInput\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\nimport bofire.surrogates.diagnostics as diagnostics\nfrom bofire.data_models.priors.api import GreaterThan\n\n# import data\nfrom bofire.benchmarks.data.aniline_cn_crosscoupling import EXPERIMENTS\n\nLoad the data, get the basic properties out of the data and perform the train-test split\n\ndata_df = pd.DataFrame(json.loads(EXPERIMENTS))\ncategories_catalyst = list(data_df[\"catalyst\"].unique())\ncategories_base = list(data_df[\"base\"].unique())\nbounds_temperature = (data_df[\"temperature\"].min(), data_df[\"temperature\"].max())\nbounds_t_res = (data_df[\"t_res\"].min(), data_df[\"t_res\"].max())\nbounds_base_equivalents = (data_df[\"base_equivalents\"].min(), data_df[\"base_equivalents\"].max())\n\ntest_size = 0.3\ntrain_data = data_df.sample(frac=1 - test_size, random_state=42)\ntest_data = data_df.drop(train_data.index)\n\nDefine the input and output bofire variables\n\ninputs = Inputs(\n        features=[\n            ContinuousInput(key=\"temperature\", bounds=bounds_temperature),\n            ContinuousInput(key=\"t_res\", bounds=bounds_t_res),\n            ContinuousInput(key=\"base_equivalents\", bounds=bounds_base_equivalents),\n            CategoricalInput(key='catalyst', categories=categories_catalyst),\n            CategoricalInput(key='base', categories=categories_base)\n        ]\n    )\noutputs = Outputs(features=[ContinuousOutput(key=\"yld\")])\n\nIn this example, we will use RBF kernel for the continuous variables and Index Kernel for the categorical variables. The final kernel will be linear combination of each kernel. Users are free to combine the kernels according to their choice.\n\nkernel_list_index = [\n    ScaleKernel(base_kernel=RBFKernel(ard=True, lengthscale_constraint=GreaterThan(lower_bound=2.500e-02), features=['temperature', 't_res', 'base_equivalents'])),\n    ScaleKernel(base_kernel=IndexKernel(num_categories=len(categories_catalyst), rank=1, features=['catalyst'])),\n    ScaleKernel(base_kernel=IndexKernel(num_categories=len(categories_base), rank=1, features=['base']))\n]\nfinal_kernel_index = AdditiveKernel(kernels=kernel_list_index)\ndata_model_index = SingleTaskGPSurrogate(\n    inputs=inputs,\n    outputs=outputs,\n    kernel=final_kernel_index,\n    )\nsurrogate_index = surrogates.map(data_model_index)\nsurrogate_index.fit(train_data)\nprint(\"MAE:\", diagnostics.mean_absolute_error(surrogate_index.predict(test_data)[\"yld_pred\"], test_data[\"yld\"]))\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:1134: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nMAE: 0.29700600101703495\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/interpolation.py:71: UserWarning:\n\ntorch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:654.)\n\n\n\nMany a times the Index Kernel do not generate positive definite correlations matrices. Positive Index Kernel kernel addresses this by using Cholesky decomposition with positive elements only. So, off diagonal elements are always positive and the diagonal elements are normalized to 1 for a target task.\nNOTE: This kernel should only be used when the correlation between different categories is expected to be positive.\nOne can replace IndexKernel with PositiveIndexKernel to make use of Positive Index Kernels."
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/basic_terminology.html",
    "href": "build/lib/docs/tutorials/basic_examples/basic_terminology.html",
    "title": "Basic terminology",
    "section": "",
    "text": "In the following it is showed how to setup optimization problems in BoFire and how to use strategies to solve them."
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/basic_terminology.html#setting-up-the-optimization-problem",
    "href": "build/lib/docs/tutorials/basic_examples/basic_terminology.html#setting-up-the-optimization-problem",
    "title": "Basic terminology",
    "section": "Setting up the optimization problem",
    "text": "Setting up the optimization problem\nIn BoFire, an optimization problem is defined by defining a domain containing input and output features, as well as optionally including constraints.\n\nFeatures\nInput features can be continuous, discrete, categorical.\nWe also support a range of specialized inputs that make defining your experiments easier, such as: - CategoricalMolecularInput allows transformations of molecules to featurizations (Fingerprints, Fragments and more). - TaskInput enables transfer learning and multi-fidelity methods, where you have access to similar experiments that can inform your optimization. - *DescriptorInput gives additional information about its value, combining the data with its significance.\n\nfrom bofire.data_models.features.api import (\n    CategoricalDescriptorInput,\n    CategoricalInput,\n    ContinuousInput,\n    DiscreteInput,\n)\n\n\nx1 = ContinuousInput(key=\"conc_A\", bounds=[0, 1])\nx2 = ContinuousInput(key=\"conc_B\", bounds=[0, 1])\nx3 = ContinuousInput(key=\"conc_C\", bounds=[0, 1])\nx4 = DiscreteInput(key=\"temperature\", values=[20, 50, 90], unit=\"°C\")\n\nx5 = CategoricalInput(\n    key=\"catalyst\",\n    categories=[\"cat_X\", \"cat_Y\", \"cat_Z\"],\n    allowed=[\n        True,\n        True,\n        False,\n    ],  # we have run out of catalyst Z, but still want to model past experiments\n)\n\nx6 = CategoricalDescriptorInput(\n    key=\"solvent\",\n    categories=[\"water\", \"methanol\", \"ethanol\"],\n    descriptors=[\"viscosity (mPa s)\", \"density (kg/m3)\"],\n    values=[[1.0, 997], [0.59, 792], [1.2, 789]],\n)\n\nWe can define both continuous and categorical outputs. Each output feature should have an objective, which determines if we aim to minimize, maximize, or drive the feature to a given value. Furthermore, we can define weights between 0 and 1 in case the objectives should not be weighted equally.\n\nfrom bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective\n\n\nobjective1 = MaximizeObjective(\n    w=1.0,\n    bounds=[0.0, 1.0],\n)\ny1 = ContinuousOutput(key=\"yield\", objective=objective1)\n\nobjective2 = MinimizeObjective(w=1.0)\ny2 = ContinuousOutput(key=\"time_taken\", objective=objective2)\n\nIn- and output features are collected in respective feature lists, which can be summarized with the get_reps_df method.\n\nfrom bofire.data_models.domain.api import Inputs, Outputs\n\n\ninput_features = Inputs(features=[x1, x2, x3, x4, x5, x6])\noutput_features = Outputs(features=[y1, y2])\n\ninput_features.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nconc_A\nContinuousInput\n[0.0,1.0]\n\n\nconc_B\nContinuousInput\n[0.0,1.0]\n\n\nconc_C\nContinuousInput\n[0.0,1.0]\n\n\ntemperature\nDiscreteInput\ntype='DiscreteInput' key='temperature' unit='°...\n\n\nsolvent\nCategoricalDescriptorInput\n3 categories\n\n\ncatalyst\nCategoricalInput\n3 categories\n\n\n\n\n\n\n\n\noutput_features.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntime_taken\nContinuousOutput\nContinuousOutputFeature\n\n\nyield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\nIndividual features can be retrieved by name, and a collection of features can be retrieved with a list of names.\n\ninput_features.get_by_key(\"catalyst\")\n\nCategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])\n\n\n\ninput_features.get_by_keys([\"catalyst\", \"conc_B\"])\n\nInputs(type='Inputs', features=[ContinuousInput(type='ContinuousInput', key='conc_B', unit=None, bounds=[0.0, 1.0], local_relative_bounds=None, stepsize=None, allow_zero=False), CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\nFeatures of a specific type can be returned by the get method. By using the exact argument, we can force the method to only return features that match the class exactly.\n\ninput_features.get(CategoricalInput)\n\nInputs(type='Inputs', features=[CategoricalDescriptorInput(type='CategoricalDescriptorInput', key='solvent', categories=['water', 'methanol', 'ethanol'], allowed=[True, True, True], descriptors=['viscosity (mPa s)', 'density (kg/m3)'], values=[[1.0, 997.0], [0.59, 792.0], [1.2, 789.0]]), CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\n\ninput_features.get(CategoricalInput, exact=True)\n\nInputs(type='Inputs', features=[CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\nThe get_keys method follows the same logic as the get method, but returns just the keys of the features instead of the features itself.\n\ninput_features.get_keys(CategoricalInput)\n\n['solvent', 'catalyst']\n\n\nThe input feature container further provides methods to return a feature container with only all fixed or all free features.\n\nfree_inputs = input_features.get_free()\nfixed_inputs = input_features.get_fixed()\n\nOne can uniformly sample from individual input features.\n\nx5.sample(2)\n\n0    cat_Y\n1    cat_X\nName: catalyst, dtype: str\n\n\nOr directly from input feature containers, uniform, sobol and LHS sampling is possible. A default, uniform sampling is used.\n\nfrom bofire.data_models.enum import SamplingMethodEnum\n\n\nX = input_features.sample(n=10, method=SamplingMethodEnum.LHS)\n\nX\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\ntemperature\nsolvent\ncatalyst\n\n\n\n\n0\n0.295064\n0.007757\n0.074675\n20.0\nethanol\ncat_Y\n\n\n1\n0.766505\n0.806243\n0.219208\n90.0\nmethanol\ncat_Y\n\n\n2\n0.530636\n0.998497\n0.575783\n50.0\nmethanol\ncat_X\n\n\n3\n0.466877\n0.669573\n0.129770\n90.0\nethanol\ncat_X\n\n\n4\n0.114104\n0.144586\n0.435287\n20.0\nwater\ncat_X\n\n\n5\n0.603533\n0.487666\n0.859594\n20.0\nmethanol\ncat_Y\n\n\n6\n0.805909\n0.283091\n0.762382\n20.0\nethanol\ncat_X\n\n\n7\n0.368794\n0.770660\n0.310131\n90.0\nmethanol\ncat_X\n\n\n8\n0.069577\n0.514006\n0.613640\n90.0\nwater\ncat_Y\n\n\n9\n0.945684\n0.360959\n0.971725\n50.0\nwater\ncat_Y\n\n\n\n\n\n\n\n\n\nConstraints\nThe search space can be further defined by constraints on the input features. BoFire supports linear equality and inequality constraints, as well as non-linear equality and inequality constraints.\n\nLinear constraints\nLinearEqualityConstraint and LinearInequalityConstraint are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input features they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\).\n\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n)\n\n\n# A mixture: x1 + x2 + x3 = 1\nconstr1 = LinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    coefficients=[1, 1, 1],\n    rhs=1,\n)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = LinearInequalityConstraint(\n    features=[\"conc_A\", \"conc_C\"],\n    coefficients=[1, 2],\n    rhs=0.8,\n)\n\nLinear constraints can only operate on ContinuousInput features.\n\n\nNonlinear constraints\nNonlinearEqualityConstraint and NonlinearInequalityConstraint take any expression that can be evaluated by pandas.eval, including mathematical operators such as sin, exp, log10 or exponentiation. So far, they cannot be used in any optimizations.\n\nfrom bofire.data_models.constraints.api import NonlinearEqualityConstraint\n\n\n# The unit circle: x1**2 + x2**2 = 1\nconst3 = NonlinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\"], expression=\"conc_A**2 + conc_B**2 - 1\"\n)\nconst3\n\nNonlinearEqualityConstraint(type='NonlinearEqualityConstraint', features=['conc_A', 'conc_B'], expression='conc_A**2 + conc_B**2 - 1', jacobian_expression='[2*conc_A, 2*conc_B]', hessian_expression='[[2, 0], [0, 2]]')\n\n\n\n\nCombinatorial constraint\nUse NChooseKConstraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe.\n\nfrom bofire.data_models.constraints.api import NChooseKConstraint\n\n\n# Only 1 or 2 out of 3 compounds can be present (have non-zero concentration)\nconstr5 = NChooseKConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    min_count=1,\n    max_count=2,\n    none_also_valid=False,\n)\nconstr5\n\nNChooseKConstraint(type='NChooseKConstraint', features=['conc_A', 'conc_B', 'conc_C'], min_count=1, max_count=2, none_also_valid=False)\n\n\nNote that we have to set a boolean, if none is also a valid selection, e.g. if we want to have 0, 1, or 2 of the ingredients in our recipe.\n\n\nCategoricalExcludeConstraint\nThe CategoricalExcludeConstraint can be used to exclude certain combinations of categories between categorical features or exclude a combination between categories and numerical values. So far, this constraint is only supported by the RandomStrategy.\nIn the example below, it would be forbidden that cat_C is used together with one of the solvents methanol or ethanol.\n\nfrom bofire.data_models.constraints.api import (\n    CategoricalExcludeConstraint,\n    SelectionCondition,\n)\n\n\nfeat_cat = CategoricalInput(\n    key=\"cat1\",\n    categories=[\"cat_A\", \"cat_B\", \"cat_C\"],\n)\nfeat_solvent = CategoricalInput(\n    key=\"solvent\", categories=[\"water\", \"methanol\", \"ethanol\"]\n)\n\nconstr6 = CategoricalExcludeConstraint(\n    features=[\"cat1\", \"solvent\"],\n    conditions=[\n        SelectionCondition(selection=[\"cat_C\"]),\n        SelectionCondition(selection=[\"methanol\", \"ethanol\"]),\n    ],\n)\n\nThe next example shows how to forbid that solvent ethanol is used at a temperature higher than 40°C, this is achieved by using a ThresholdCondition.\n\nfrom bofire.data_models.constraints.api import ThresholdCondition\n\n\nfeat_temp = ContinuousInput(\n    key=\"temperature\",\n    bounds=[0, 100],\n    unit=\"°C\",\n)\nconstr7 = CategoricalExcludeConstraint(\n    features=[\"solvent\", \"temperature\"],\n    conditions=[\n        SelectionCondition(selection=[\"water\"]),\n        ThresholdCondition(\n            threshold=40,\n            operator=\"&gt;=\",\n        ),\n    ],\n)\n\nSimilar to the features, constraints can be grouped in a container which acts as the union constraints.\n\nfrom bofire.data_models.domain.api import Constraints\n\n\nconstraints = Constraints(constraints=[constr1, constr2])\n\nA summary of the constraints can be obtained by the method get_reps_df:\n\nconstraints.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n0\nLinearEqualityConstraint\ntype='LinearEqualityConstraint' features=['con...\n\n\n1\nLinearInequalityConstraint\ntype='LinearInequalityConstraint' features=['c...\n\n\n\n\n\n\n\nWe can check whether a point satisfies individual constraints or the list of constraints.\n\nconstr2.is_fulfilled(X).values\n\narray([ True, False, False,  True, False, False, False, False, False,\n       False])\n\n\nOutput constraints can be setup via sigmoid-shaped objectives passed as argument to the respective feature, which can then also be plotted.\n\nfrom bofire.data_models.objectives.api import MinimizeSigmoidObjective\nfrom bofire.plot.api import plot_objective_plotly\n\n\noutput_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5)\ny3 = ContinuousOutput(key=\"y3\", objective=output_constraint)\n\noutput_features = Outputs(features=[y1, y2, y3])\n\nfig = plot_objective_plotly(feature=y3, lower=0, upper=1)\n\nfig.show()\n\n                            \n                                            \n\n\n\n\n\nThe domain\nThe domain holds then all information about an optimization problem and can be understood as a search space definition. A detailed description of the domain can be found in docs.\n\nfrom bofire.data_models.domain.api import Domain\n\n\ndomain = Domain(inputs=input_features, outputs=output_features, constraints=constraints)\n\nIn addition one can instantiate the domain also just from lists.\n\ndomain_single_objective = Domain.from_lists(\n    inputs=[x1, x2, x3, x4, x5, x6],\n    outputs=[y1],\n    constraints=[],\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/basic_terminology.html#optimization",
    "href": "build/lib/docs/tutorials/basic_examples/basic_terminology.html#optimization",
    "title": "Basic terminology",
    "section": "Optimization",
    "text": "Optimization\nTo solve the optimization problem, we further need a solving strategy. BoFire supports strategies without a prediction model such as a random strategy and predictive strategies which are based on a prediction model.\nAll strategies contain an ask method returning a defined number of candidate experiments.\n\nRandom Strategy\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy\n\n\nstrategy_data_model = RandomStrategy(domain=domain)\n\nrandom_strategy = strategies.map(strategy_data_model)\nrandom_candidates = random_strategy.ask(2)\n\nrandom_candidates\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\ntemperature\nsolvent\ncatalyst\n\n\n\n\n0\n0.601671\n0.358884\n0.039445\n20.0\nwater\ncat_Y\n\n\n1\n0.477287\n0.479047\n0.043667\n90.0\nethanol\ncat_Y\n\n\n\n\n\n\n\n\n\nSingle objective Bayesian Optimization strategy\nSince a predictive strategy includes a prediction model, we need to generate some historical data, which we can afterwards pass as training data to the strategy via the tell method.\nFor didactic purposes we just choose here from one of our benchmark methods.\n\nfrom bofire.benchmarks.single import Himmelblau\n\n\nbenchmark = Himmelblau()\n\n(benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nx_1\nContinuousInput\n[-6.0,6.0]\n\n\nx_2\nContinuousInput\n[-6.0,6.0]\n\n\ny\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\nGenerating some initial data works as follows:\n\nsamples = benchmark.domain.inputs.sample(10)\n\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n2.999298\n2.548082\n6.505968\n1\n\n\n1\n5.614150\n4.615099\n1028.245582\n1\n\n\n2\n2.919880\n1.732363\n1.714803\n1\n\n\n3\n5.840556\n0.668628\n566.030251\n1\n\n\n4\n5.985390\n1.450827\n691.601934\n1\n\n\n5\n-0.009895\n3.031301\n68.246168\n1\n\n\n6\n2.822379\n2.120672\n0.936648\n1\n\n\n7\n-0.816111\n-0.921571\n175.223595\n1\n\n\n8\n-0.747522\n-5.781106\n922.300768\n1\n\n\n9\n-4.597166\n5.041975\n421.420803\n1\n\n\n\n\n\n\n\nLet’s setup the SOBO strategy and ask for a candidate. First we need a serializable data model that contains the hyperparameters.\n\nfrom pprint import pprint\n\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM\n\n\nsobo_strategy_data_model = SoboStrategyDM(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# print information about hyperparameters\nprint(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function)\nprint()\nprint(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type)\nprint()\nprint(\"Surrogate's kernel:\")\npprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump())\n\nAcquisition function: type='qLogNEI' prune_baseline=True n_mc_samples=512\n\nSurrogate type: SingleTaskGPSurrogate\n\nSurrogate's kernel:\n{'ard': True,\n 'features': None,\n 'lengthscale_constraint': None,\n 'lengthscale_prior': {'loc': 1.4142135623730951,\n                       'loc_scaling': 0.5,\n                       'scale': 1.7320508075688772,\n                       'scale_scaling': 0.0,\n                       'type': 'DimensionalityScaledLogNormalPrior'},\n 'type': 'RBFKernel'}\n\n\nThe actual strategy can then be created via the mapper function.\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\nsobo_strategy.tell(experiments=experiments)\nsobo_strategy.ask(candidate_count=1)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n1.795729\n2.679825\n-24.710745\n124.998949\n24.710745\n\n\n\n\n\n\n\nAn alternative way is calling the strategy’s constructor directly.\n\nsobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model)\n\nThe latter way is helpful to keep type information.\n\n\nDesign of Experiments\nAs a simple example for the DoE functionalities we consider the task of finding a D-optimal design for a fully-quadratic model with three design variables with bounds (0,1) and a mixture constraint.\nWe define the design space including the constraint as a domain. Then we pass it to the optimization routine and specify the model. If the user does not indicate a number of experiments it will be chosen automatically based on the number of model terms.\n\nimport numpy as np\n\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1])\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\n\n\n\n\n0\n0.0\n0.0\n1.0\n\n\n1\n1.0\n0.0\n0.0\n\n\n2\n0.5\n0.5\n0.0\n\n\n3\n0.0\n0.5\n0.5\n\n\n4\n0.5\n0.0\n0.5\n\n\n5\n0.0\n1.0\n0.0\n\n\n6\n0.5\n0.0\n0.5\n\n\n7\n1.0\n0.0\n0.0\n\n\n8\n0.0\n0.0\n1.0\n\n\n9\n1.0\n0.0\n0.0\n\n\n10\n0.5\n0.5\n0.0\n\n\n11\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nThe resulting design looks like this:\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"fully-quadratic model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[x1.key],\n    ys=candidates[x2.key],\n    zs=candidates[x3.key],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Reaction_Optimization_Example.html",
    "href": "build/lib/docs/tutorials/basic_examples/Reaction_Optimization_Example.html",
    "title": "Getting started by Example: Optimization of Reaction Conditions",
    "section": "",
    "text": "In this example we take on a reaction condition optimization problem: Suppose you have some simple reaction where two ingredients A and B react to C.\nOur reactors can be temperature controlled, and we can use different solvents. Furthermore, we can dilute our reaction mixture by using a different solvent volume. parameters like the temperature or the solvent volume are continuous parameters, where we have to set lower and upper bounds for. The temperature can be controlled between 0 and 60°C and the solvent volume between 20 and 90 ml.\nParameters like the use of which solvent, where there’s a choice of either this or that, are categorical parameters. Here we can choose between MeOH, THF and Dioxane.\nFor now we only wish top optimize the Reaction yield, making this a single objective optimization problem.\nBelow we’ll see how to perform such an optimization using bofire, Utilizing a Single Objective Bayesian Optimization (SOBO) Strategy\n# python imports we'll need in this notebook\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nprint(f\"SMOKE_TEST: {SMOKE_TEST}\")\n\nSMOKE_TEST: 1"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Reaction_Optimization_Example.html#setting-up-the-optimization-problem-as-a-reaction-domain",
    "href": "build/lib/docs/tutorials/basic_examples/Reaction_Optimization_Example.html#setting-up-the-optimization-problem-as-a-reaction-domain",
    "title": "Getting started by Example: Optimization of Reaction Conditions",
    "section": "Setting up the optimization problem as a Reaction Domain",
    "text": "Setting up the optimization problem as a Reaction Domain\n\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import (  # we won't need all of those.\n    CategoricalInput,\n    ContinuousInput,\n    ContinuousOutput,\n)\n\n\n# We wish the temperature of the reaction to be between 0 and 60 °C\ntemperature_feature = ContinuousInput(key=\"Temperature\", bounds=[0.0, 60.0], unit=\"°C\")\n\n# Solvent Amount\nsolvent_amount_feature = ContinuousInput(key=\"Solvent Volume\", bounds=[20, 90])\n\n# we have a couple of solvents in stock, which we'd like to use\nsolvent_type_feature = CategoricalInput(\n    key=\"Solvent Type\",\n    categories=[\"MeOH\", \"THF\", \"Dioxane\"],\n)\n\n\n# gather all individual features\ninput_features = Inputs(\n    features=[\n        temperature_feature,\n        solvent_type_feature,\n        solvent_amount_feature,\n    ],\n)\n\n\n# outputs: we wish to maximize the Yield\n# import Maximize Objective to tell the optimizer you wish to optimize\nfrom bofire.data_models.objectives.api import MaximizeObjective\n\n\nobjective = MaximizeObjective(\n    w=1.0,\n)\nyield_feature = ContinuousOutput(key=\"Yield\", objective=objective)\n# create an output feature\noutput_features = Outputs(features=[yield_feature])\n\n\nobjective\n\nMaximizeObjective(type='MaximizeObjective', w=1.0, bounds=[0, 1])\n\n\n\n# we now have\nprint(\"input_features:\", input_features)\nprint(\"output_features:\", output_features)\n\ninput_features: type='Inputs' features=[ContinuousInput(type='ContinuousInput', key='Temperature', unit='°C', bounds=[0.0, 60.0], local_relative_bounds=None, stepsize=None, allow_zero=False), CategoricalInput(type='CategoricalInput', key='Solvent Type', categories=['MeOH', 'THF', 'Dioxane'], allowed=[True, True, True]), ContinuousInput(type='ContinuousInput', key='Solvent Volume', unit=None, bounds=[20.0, 90.0], local_relative_bounds=None, stepsize=None, allow_zero=False)]\noutput_features: type='Outputs' features=[ContinuousOutput(type='ContinuousOutput', key='Yield', unit=None, objective=MaximizeObjective(type='MaximizeObjective', w=1.0, bounds=[0, 1]))]\n\n\n\n# The domain is now the object that holds the entire optimization problem / problem definition.\ndomain = Domain(\n    inputs=input_features,\n    outputs=output_features,\n)\n\n\n# you can now have a pretty printout of your domain via\n(domain.inputs + domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSolvent Volume\nContinuousInput\n[20.0,90.0]\n\n\nTemperature\nContinuousInput\n[0.0,60.0]\n\n\nSolvent Type\nCategoricalInput\n3 categories\n\n\nYield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\n\n# and you can access your domain features via\nfor (\n    feature_key\n) in domain.inputs.get_keys():  # this will get all the feature names and loop over them\n    input_feature = domain.inputs.get_by_key(\n        feature_key,\n    )  # we can extract the individual feature object by asking for it by name\n    print(feature_key, \"|\", input_feature)\n\nSolvent Volume | [20.0,90.0]\nTemperature | [0.0,60.0]\nSolvent Type | 3 categories\n\n\n\n# as well as the output features as\n# and you can access your domain features via\nfor feature_key in (\n    domain.outputs.get_keys()\n):  # this will get all the feature names and loop over them\n    output_feature = domain.outputs.get_by_key(\n        feature_key,\n    )  # we can extract the individual feature object by asking for it by name\n    print(feature_key, \" | \", output_feature.__repr__())\n\nYield  |  ContinuousOutput(type='ContinuousOutput', key='Yield', unit=None, objective=MaximizeObjective(type='MaximizeObjective', w=1.0, bounds=[0, 1]))\n\n\n\n(domain.inputs + domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSolvent Volume\nContinuousInput\n[20.0,90.0]\n\n\nTemperature\nContinuousInput\n[0.0,60.0]\n\n\nSolvent Type\nCategoricalInput\n3 categories\n\n\nYield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\n\nImport a toy Reaction to play around with\nWe’ve prepared a reaction emulator, which you can use to emulate a real experiment below.\n\n# Reaction Optimization Notebook util code\nT0 = 25\nT1 = 100\ne0 = np.exp((T1 + 0) / T0)\ne60 = np.exp((T1 + 60) / T0)\nde = e60 - e0\n\nboiling_points = {  # in °C\n    \"MeOH\": 64.7,\n    \"THF\": 66.0,\n    \"Dioxane\": 101.0,\n}\ndensity = {  # in kg/l\n    \"MeOH\": 0.792,\n    \"THF\": 0.886,\n    \"Dioxane\": 1.03,\n}\n# create dict from individual dicts\ndescs = {\n    \"boiling_points\": boiling_points,\n    \"density\": density,\n}\nsolvent_descriptors = pd.DataFrame(descs)\n\n\n# these functions are for faking real experimental data ;)\ndef calc_volume_fact(V):\n    # 20-90\n    # max at 75 = 1\n    # min at 20 = 0.7\n    # at 90=0.5\n    x = (V - 20) / 70\n    x = 0.5 + (x - 0.75) * 0.1 + (x - 0.4) ** 2\n    return x\n\n\ndef calc_rhofact(solvent_type, Tfact):\n    #  between 0.7 and 1.1\n    x = solvent_descriptors[\"density\"][solvent_type]\n    x = (1.5 - x) * (Tfact + 0.5) / 2\n    return x.values\n\n\ndef calc_Tfact(T):\n    x = np.exp((T1 + T) / T0)\n    return (x - e0) / de\n\n\n# this can be used to create a dataframe of experiments including yields\ndef create_experiments(domain, nsamples=100, A=25, B=90, candidates=None):\n    Tf = domain.inputs.get_by_key(\"Temperature\")\n    Vf = domain.inputs.get_by_key(\"Solvent Volume\")\n    typef = domain.inputs.get_by_key(\"Solvent Type\")\n    yf = domain.outputs.get_by_key(\"Yield\")\n    if candidates is None:\n        T = np.random.uniform(low=Tf.lower_bound, high=Tf.upper_bound, size=(nsamples,))\n        V = np.random.uniform(low=Vf.lower_bound, high=Vf.upper_bound, size=(nsamples,))\n        solvent_types = [\n            domain.inputs.get_by_key(\"Solvent Type\").categories[np.random.randint(0, 3)]\n            for i in range(nsamples)\n        ]\n    else:\n        nsamples = len(candidates)\n        T = candidates[\"Temperature\"].values\n        V = candidates[\"Solvent Volume\"].values\n        solvent_types = candidates[\"Solvent Type\"].values\n\n    Tfact = calc_Tfact(T)\n    rhofact = calc_rhofact(solvent_types, Tfact)\n    Vfact = calc_volume_fact(V)\n    y = A * Tfact + B * rhofact\n    y = 0.5 * y + 0.5 * y * Vfact\n    # y = y.values\n    samples = pd.DataFrame(\n        {\n            Tf.key: T,\n            Vf.key: V,\n            yf.key: y,\n            typef.key: solvent_types,\n            \"valid_\" + yf.key: np.ones(nsamples),\n        },\n        # index=pd.RangeIndex(nsamples),\n    )\n    samples.index = pd.RangeIndex(nsamples)\n    return samples\n\n\ndef create_candidates(domain, nsamples=4):\n    experiments = create_experiments(domain, nsamples=nsamples)\n    candidates = experiments.drop([\"Yield\", \"valid_Yield\"], axis=1)\n    return candidates\n\n\n# this is for evaluating candidates that do not yet have a yield attributed to it.\ndef evaluate_experiments(domain, candidates):\n    return create_experiments(domain, candidates=candidates)\n\n\n# create some trial experiments (at unitform random)\ncandidates = create_candidates(domain, nsamples=4)\n\n\ncandidates\n\n\n\n\n\n\n\n\nTemperature\nSolvent Volume\nSolvent Type\n\n\n\n\n0\n4.433746\n64.781922\nTHF\n\n\n1\n45.681404\n66.486651\nMeOH\n\n\n2\n4.685283\n87.804149\nDioxane\n\n\n3\n17.722548\n40.331706\nTHF\n\n\n\n\n\n\n\n\n# we can evaluate the yield of those candidates\nexperiments = evaluate_experiments(domain, candidates)\n\n\nexperiments\n\n\n\n\n\n\n\n\nTemperature\nSolvent Volume\nYield\nSolvent Type\nvalid_Yield\n\n\n\n\n0\n4.433746\n64.781922\n11.469957\nTHF\n1.0\n\n\n1\n45.681404\n66.486651\n35.535155\nMeOH\n1.0\n\n\n2\n4.685283\n87.804149\n10.632120\nDioxane\n1.0\n\n\n3\n17.722548\n40.331706\n14.097931\nTHF\n1.0"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Reaction_Optimization_Example.html#optimization-loop",
    "href": "build/lib/docs/tutorials/basic_examples/Reaction_Optimization_Example.html#optimization-loop",
    "title": "Getting started by Example: Optimization of Reaction Conditions",
    "section": "Optimization Loop",
    "text": "Optimization Loop\nWith this strategy.ask() and strategy.tell() we can now do our optimization loop, where after each new proposal, the conditions obtained from ask are evaluated and added to the known datapoints via tell. This requires to refit the underling model in each step.\n\nexperimental_budget = 10\ni = 0\n# in case of smoke_test we don't run the actual optimization loop ...\ndone = False if not SMOKE_TEST else True\n\nwhile not done:\n    i += 1\n    t1 = time.time()\n    # ask for a new experiment\n    new_candidate = sobo_strategy.ask(1)\n    new_experiment = evaluate_experiments(domain, new_candidate)\n    sobo_strategy.tell(new_experiment)\n    print(f\"Iteration took {(time.time()-t1):.2f} seconds\")\n    # inform the strategy about the new experiment\n    # experiments = pd.concat([experiments,new_experiment],ignore_index=True)\n    if i &gt; experimental_budget:\n        done = True\n\n\ninvestigating results\n\n# you have access to the experiments here\nsobo_strategy.experiments\n\n\n\n\n\n\n\n\nTemperature\nSolvent Volume\nYield\nSolvent Type\nvalid_Yield\n\n\n\n\n0\n3.257861\n51.360015\n10.706498\nTHF\nTrue\n\n\n1\n16.366422\n30.100258\n11.159435\nDioxane\nTrue\n\n\n2\n32.883936\n89.717937\n29.507498\nMeOH\nTrue\n\n\n3\n54.530566\n33.388233\n40.964851\nTHF\nTrue\n\n\n4\n42.569744\n68.876292\n24.742334\nDioxane\nTrue\n\n\n\n\n\n\n\n\n# quick plot of yield vs. Iteration\nsobo_strategy.experiments[\"Yield\"].plot()"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html",
    "href": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html",
    "title": "Model Building with BoFire",
    "section": "",
    "text": "This notebooks shows how to setup and analyze models trained with BoFire. It is still WIP."
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#imports",
    "href": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#imports",
    "title": "Model Building with BoFire",
    "section": "Imports",
    "text": "Imports\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.domain.api import Inputs, Outputs\nfrom bofire.data_models.enum import RegressionMetricsEnum\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\nfrom bofire.plot.feature_importance import plot_feature_importance_by_feature_plotly\nfrom bofire.surrogates.feature_importance import (\n    combine_lengthscale_importances,\n    combine_permutation_importances,\n    lengthscale_importance_hook,\n    permutation_importance_hook,\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#problem-setup",
    "href": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#problem-setup",
    "title": "Model Building with BoFire",
    "section": "Problem Setup",
    "text": "Problem Setup\nFor didactic purposes, we sample data from a Himmelblau benchmark function and use them to train a SingleTaskGP.\n\n# TODO: replace this after JDs PR is ready.\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i+1}\", bounds=(-4, 4)) for i in range(3)],\n)\noutput_features = Outputs(features=[ContinuousOutput(key=\"y\")])\nexperiments = input_features.sample(n=50)\nexperiments.eval(\"y=((x_1**2 + x_2 - 11)**2+(x_1 + x_2**2 -7)**2)\", inplace=True)\nexperiments[\"valid_y\"] = 1"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#cross-validation",
    "href": "build/lib/docs/tutorials/basic_examples/Model_Fitting_and_analysis.html#cross-validation",
    "title": "Model Building with BoFire",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nRun the cross validation\n\ndata_model = SingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n)\n\nmodel = surrogates.map(data_model=data_model)\ntrain_cv, test_cv, pi = model.cross_validate(\n    experiments,\n    folds=5,\n    hooks={\n        \"permutation_importance\": permutation_importance_hook,\n        \"lengthscale_importance\": lengthscale_importance_hook,\n    },\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\ncombined_importances = {\n    m.name: combine_permutation_importances(pi[\"permutation_importance\"], m).describe()\n    for m in RegressionMetricsEnum\n}\ncombined_importances[\"lengthscale\"] = combine_lengthscale_importances(\n    pi[\"lengthscale_importance\"],\n).describe()\nplot_feature_importance_by_feature_plotly(\n    combined_importances,\n    relative=False,\n    caption=\"Permutation Feature Importances\",\n    show_std=True,\n    importance_measure=\"Permutation Feature Importance\",\n)\n\n                            \n                                            \n\n\n\n\nAnalyze the cross validation\nPlots are added in a future PR.\n\n# Performance on test sets\ntest_cv.get_metrics(combine_folds=True)\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n8.8791\n546.366434\n0.872988\n0.837155\n0.93925\n0.935366\n7.169177e-10\n\n\n\n\n\n\n\n\ndisplay(test_cv.get_metrics(combine_folds=False))\ndisplay(test_cv.get_metrics(combine_folds=False).describe())\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n5.692695\n78.446321\n0.975235\n0.243104\n0.988485\n0.987879\n0.003968\n\n\n1\n3.645310\n22.608308\n0.996135\n3.458256\n0.998706\n0.987879\n0.003968\n\n\n2\n24.497721\n2436.755075\n0.024625\n0.328916\n0.625461\n0.418182\n0.500000\n\n\n3\n8.003899\n185.813081\n0.972885\n0.110298\n0.989986\n0.987879\n0.003968\n\n\n4\n2.555875\n8.209386\n0.996073\n0.045199\n0.998824\n0.975758\n0.003968\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\ncount\n5.000000\n5.000000\n5.000000\n5.000000\n5.000000\n5.000000\n5.000000\n\n\nmean\n8.879100\n546.366434\n0.792991\n0.837155\n0.920292\n0.871515\n0.103175\n\n\nstd\n8.975469\n1059.062410\n0.429671\n1.469429\n0.164885\n0.253475\n0.221832\n\n\nmin\n2.555875\n8.209386\n0.024625\n0.045199\n0.625461\n0.418182\n0.003968\n\n\n25%\n3.645310\n22.608308\n0.972885\n0.110298\n0.988485\n0.975758\n0.003968\n\n\n50%\n5.692695\n78.446321\n0.975235\n0.243104\n0.989986\n0.987879\n0.003968\n\n\n75%\n8.003899\n185.813081\n0.996073\n0.328916\n0.998706\n0.987879\n0.003968\n\n\nmax\n24.497721\n2436.755075\n0.996135\n3.458256\n0.998824\n0.987879\n0.500000"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html",
    "href": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html",
    "title": "Random Forest in BoFire",
    "section": "",
    "text": "import bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.domain.api import Outputs\nfrom bofire.data_models.strategies.api import MoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, RandomForestSurrogate"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html#imports",
    "href": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html#imports",
    "title": "Random Forest in BoFire",
    "section": "",
    "text": "import bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.multi import DTLZ2\nfrom bofire.data_models.domain.api import Outputs\nfrom bofire.data_models.strategies.api import MoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, RandomForestSurrogate"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-a-rf",
    "href": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-a-rf",
    "title": "Random Forest in BoFire",
    "section": "Setup a RF",
    "text": "Setup a RF\n\nbenchmark = DTLZ2(dim=6)\n\nexperiments = benchmark.f(benchmark.domain.inputs.sample(20), return_complete=True)\n\n# you can use the hyperparams from sklearn\nrf_data_model = RandomForestSurrogate(\n    inputs=benchmark.domain.inputs,\n    outputs=Outputs(features=[benchmark.domain.outputs[0]]),\n    n_estimators=100,\n)\n\nrf = surrogates.map(rf_data_model)\n\ncv_train, cv_test, _ = rf.cross_validate(experiments)\n\ncv_test.get_metrics()\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n0.124829\n0.028392\n0.776402\n0.164528\n0.885282\n0.786466\n0.000005"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-an-optimization",
    "href": "build/lib/docs/tutorials/advanced_examples/random_forest_in_bofire.html#setup-an-optimization",
    "title": "Random Forest in BoFire",
    "section": "Setup an optimization",
    "text": "Setup an optimization\n\nbenchmark = DTLZ2(dim=6)\n\ndata_model = MoboStrategy(\n    domain=benchmark.domain,\n    ref_point={\"f_0\": 1.1, \"f_1\": 1.1},\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            RandomForestSurrogate(\n                inputs=benchmark.domain.inputs,\n                outputs=Outputs(features=[benchmark.domain.outputs[0]]),\n            ),\n            RandomForestSurrogate(\n                inputs=benchmark.domain.inputs,\n                outputs=Outputs(features=[benchmark.domain.outputs[1]]),\n            ),\n        ],\n    ),\n)\n\nrecommender = strategies.map(data_model=data_model)\n\nexperiments = benchmark.f(benchmark.domain.inputs.sample(10), return_complete=True)\nrecommender.tell(experiments=experiments)\n\n\n# currently not supported\n# for i in range(10):\n#     samples = benchmark.domain.inputs.sample(512, method=SamplingMethodEnum.SOBOL)\n#     candidates = recommender.ask(1, candidate_pool=samples)\n#     candidates = candidates.reset_index(drop=True)\n#     new_experiments = benchmark.f(candidates[benchmark.domain.inputs.get_keys().copy()], return_complete=True)\n#     recommender.tell(experiments=new_experiments)"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/objectives_on_inputs.html",
    "href": "build/lib/docs/tutorials/advanced_examples/objectives_on_inputs.html",
    "title": "Input Features as Output Objectives",
    "section": "",
    "text": "This notebook demonstrates how to put objectives on input features or a combination of input features. Possible usecases are favoring lower or higher amounts of an ingredient or to take into account a known (linear) cost function. In case of categorical inputs it can be used to penalize the optimizer for choosing specific categories."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/objectives_on_inputs.html#imports",
    "href": "build/lib/docs/tutorials/advanced_examples/objectives_on_inputs.html#imports",
    "title": "Input Features as Output Objectives",
    "section": "Imports",
    "text": "Imports\n\nimport numpy as np\n\nimport bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.api import Himmelblau\nfrom bofire.data_models.features.api import CategoricalInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import (\n    MaximizeObjective,\n    MaximizeSigmoidObjective,\n)\nfrom bofire.data_models.strategies.api import MultiplicativeSoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    CategoricalDeterministicSurrogate,\n    LinearDeterministicSurrogate,\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/objectives_on_inputs.html#setup-an-example",
    "href": "build/lib/docs/tutorials/advanced_examples/objectives_on_inputs.html#setup-an-example",
    "title": "Input Features as Output Objectives",
    "section": "Setup an Example",
    "text": "Setup an Example\nWe use Himmelblau as example with an additional objective on x_2 which pushes it to be larger 3 during the optimization. In addition, we introduce a categorical feature called x_cat which is mapped by an CategoricalDeterministicSurrogate to a continuous output called y_cat.\n\nbench = Himmelblau()\nexperiments = bench.f(bench.domain.inputs.sample(10), return_complete=True)\n\ndomain = bench.domain\n\n# setup extra feature `y_x2` that is the same as `x_2` and is taken into account in the optimization by a sigmoid objective\ndomain.outputs.features.append(\n    ContinuousOutput(key=\"y_x2\", objective=MaximizeSigmoidObjective(tp=3, steepness=10))\n)\nexperiments[\"y_x2\"] = experiments.x_2\n\n\n# add extra categorical input feature and corresponding output feature\ndomain.inputs.features.append(CategoricalInput(key=\"x_cat\", categories=[\"a\", \"b\", \"c\"]))\ndomain.outputs.features.append(\n    ContinuousOutput(key=\"y_cat\", objective=MaximizeObjective())\n)\n\n# generate random values for the new categorical feature\nexperiments[\"x_cat\"] = np.random.choice([\"a\", \"b\", \"c\"], size=experiments.shape[0])\n\nThe LinearDeterministicSurrogate can be used to model that y_x2 = x_2.\n\nsurrogate_data = LinearDeterministicSurrogate(\n    inputs=domain.inputs.get_by_keys([\"x_2\"]),\n    outputs=domain.outputs.get_by_keys([\"y_x2\"]),\n    coefficients={\"x_2\": 1},\n    intercept=0,\n)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.predict(experiments[domain.inputs.get_keys()].copy())\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:47: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\ny_x2_pred\ny_x2_sd\n\n\n\n\n0\n5.708936\n0.0\n\n\n1\n1.814728\n0.0\n\n\n2\n1.215325\n0.0\n\n\n3\n0.386853\n0.0\n\n\n4\n5.875414\n0.0\n\n\n5\n-5.104080\n0.0\n\n\n6\n-2.910256\n0.0\n\n\n7\n2.052575\n0.0\n\n\n8\n-0.080082\n0.0\n\n\n9\n4.808561\n0.0\n\n\n\n\n\n\n\nThe CategoricalDeterministicSurrogate can be used to map categories to specific continuous values.\n\ncategorical_surrogate_data = CategoricalDeterministicSurrogate(\n    inputs=domain.inputs.get_by_keys([\"x_cat\"]),\n    outputs=domain.outputs.get_by_keys([\"y_cat\"]),\n    mapping={\"a\": 1, \"b\": 0.2, \"c\": 0.3},\n)\n\nsurrogate = surrogates.map(categorical_surrogate_data)\n\nsurrogate.predict(experiments[domain.inputs.get_keys()].copy())\n\nexperiments[\"y_cat\"] = surrogate.predict(experiments[domain.inputs.get_keys()].copy())[\n    \"y_cat_pred\"\n]\n\nexperiments\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\ny_x2\nx_cat\ny_cat\n\n\n\n\n0\n-2.093059\n5.708936\n553.026366\n1\n5.708936\na\n1.0\n\n\n1\n0.826491\n1.814728\n80.583126\n1\n1.814728\nc\n0.3\n\n\n2\n4.315232\n1.215325\n79.543252\n1\n1.215325\nb\n0.2\n\n\n3\n-3.017285\n0.386853\n99.647607\n1\n0.386853\na\n1.0\n\n\n4\n-4.693269\n5.875414\n806.766157\n1\n5.875414\nc\n0.3\n\n\n5\n-4.265076\n-5.104080\n222.997001\n1\n-5.104080\nb\n0.2\n\n\n6\n-4.992604\n-2.910256\n133.760292\n1\n-2.910256\na\n1.0\n\n\n7\n3.864543\n2.052575\n37.008624\n1\n2.052575\na\n1.0\n\n\n8\n-2.391234\n-0.080082\n116.826800\n1\n-0.080082\nc\n0.3\n\n\n9\n1.473531\n4.808561\n325.773284\n1\n4.808561\nb\n0.2\n\n\n\n\n\n\n\nNext we setup a SoboStrategy using the custom surrogates for outputs y_x2 and y_cat and ask for a candidate. Note that the surrogate specs for output y is automatically generated and defaulted to be a SingleTaskGPSurrogate.\n\nstrategy_data = MultiplicativeSoboStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data, categorical_surrogate_data]\n    ),\n)\nstrategy = strategies.map(strategy_data)\nstrategy.tell(experiments)\nstrategy.ask(4)\n\n\n\n\n\n\n\n\nx_1\nx_2\nx_cat\ny_pred\ny_cat_pred\ny_x2_pred\ny_sd\ny_cat_sd\ny_x2_sd\ny_des\ny_x2_des\ny_cat_des\n\n\n\n\n0\n6.000000\n3.249110\na\n146.344878\n1.0\n3.249110\n57.541911\n0.0\n0.0\n-146.344878\n0.923515\n1.0\n\n\n1\n6.000000\n3.128724\na\n140.522007\n1.0\n3.128724\n56.078159\n0.0\n0.0\n-140.522007\n0.783679\n1.0\n\n\n2\n2.031583\n2.267782\na\n40.308071\n1.0\n2.267782\n23.489846\n0.0\n0.0\n-40.308071\n0.000660\n1.0\n\n\n3\n6.000000\n6.000000\na\n323.529671\n1.0\n6.000000\n102.712249\n0.0\n0.0\n-323.529671\n1.000000\n1.0"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/merging_objectives.html",
    "href": "build/lib/docs/tutorials/advanced_examples/merging_objectives.html",
    "title": "Merging multiple objectives to a scalar target for single-target BO",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import DTLZ2\nfrom bofire.data_models.objectives import api as objectives_data_model\nfrom bofire.data_models.strategies import api as strategies_data_model"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/merging_objectives.html#benchmark-problem",
    "href": "build/lib/docs/tutorials/advanced_examples/merging_objectives.html#benchmark-problem",
    "title": "Merging multiple objectives to a scalar target for single-target BO",
    "section": "Benchmark Problem",
    "text": "Benchmark Problem\nOnly used for domain definition\n\nbench = DTLZ2(dim=2, num_objectives=2)\nexperiments = bench.f(bench.domain.inputs.sample(10), return_complete=True)\n\ndomain = bench.domain\n\n\nChange the objectives: Multiplication, only reasonable for objectives &gt; 0\n\noutputs = domain.outputs.get_by_objective()\n\noutputs[0].objective = objectives_data_model.MaximizeObjective(w=1.0, bounds=(0.0, 5.0))\noutputs[1].objective = objectives_data_model.MaximizeObjective(w=1.0, bounds=(0.0, 2.0))\n# outputs[1].objective = objectives_data_model.MaximizeSigmoidObjective(w = 0.5, tp=2.5, steepness=3.)"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/merging_objectives.html#select-strategies",
    "href": "build/lib/docs/tutorials/advanced_examples/merging_objectives.html#select-strategies",
    "title": "Merging multiple objectives to a scalar target for single-target BO",
    "section": "Select Strategies",
    "text": "Select Strategies\nWe will use pure multiplicative and additive Sobo strategies, as well as a mixed one for this example: - Multiplicative: \\(f = f_0^{w_0} \\cdot f_1^{w_1}\\) - Additive: \\(f = f_0 \\cdot w_0 + f_1 \\cdot w_1\\) - Mixed (with f1 being the additive objective): \\(f = f_0^{w_0} \\cdot (1 + w_1 \\cdot f_1)\\)\n\nstrategy_data_model = {\n    \"multiplicative\": strategies_data_model.MultiplicativeSoboStrategy(domain=domain),\n    \"additive\": strategies_data_model.AdditiveSoboStrategy(domain=domain),\n    \"mixed\": strategies_data_model.MultiplicativeAdditiveSoboStrategy(\n        domain=domain, additive_features=[\"f_1\"]\n    ),\n}\n\n\nWe will now create the strategies and evaluate them on a grid to visualize the objectives.\nWe see the following: - Multiplicative: The objective is a product of the objectives: If either \\(f_0\\) or \\(f_1\\) is low, the objective is low. - Additive: The objective is a sum of the objectives: We see a linear increase in the objective with increasing \\(f_0\\) and \\(f_1\\). This is useful for complementary objectives. - Mixed: The objective is more strict w.r.t. \\(f_0\\) than the additive objective \\(f_1\\). The overall desirability can also be high, if \\(f_1\\) is low.\nChanging the weights \\(w_i\\) in the objectives above will further change the preference of \\(f_0\\) and \\(f_1\\).\n\n# map from the strategy data-model to the actual strategy object instances\nstrategy = {\n    key: strategies.map(strategy_data_model)\n    for (key, strategy_data_model) in strategy_data_model.items()\n}\n\n\n# tell the strategies about the experiments. This is required to set up the models, but not for the objective evaluation\nfor _, strat in strategy.items():\n    strat.tell(experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n# get the objectives for evaluation as a torch executable\nobjectives = {\n    key: strategy._get_objective_and_constraints()[0]\n    for (key, strategy) in strategy.items()\n}\n\n\n# f_0 / f_1 coordinates for objctive evaluation\nmesh = np.meshgrid(np.linspace(0, 2, 100), np.linspace(0, 5, 100))\n# transform to matrix-form torch tensor\nmesh_tensor = torch.tensor([m.flatten() for m in mesh]).T\n\n/tmp/ipykernel_2860/1215888523.py:4: UserWarning:\n\nCreating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n\n\n\n\n# evaluate objectives\nobjectives_eval = {\n    key: obj(mesh_tensor).detach().numpy().reshape(mesh[0].shape)\n    for (key, obj) in objectives.items()\n}\n\n\n# plot the objectives as contour plots\nfor key, obj in objectives_eval.items():\n    plt.figure()\n    plt.contour(*mesh, obj, label=key)\n    plt.title(key)\n    plt.xlabel(\"f_0\")\n    plt.ylabel(\"f_1\")\n    plt.grid(True)\n    plt.colorbar()\n\nplt.show()\n\n/tmp/ipykernel_2860/261754645.py:4: UserWarning:\n\nThe following kwargs were not used by contour: 'label'"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html",
    "href": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "",
    "text": "Usage is possible in multiple ways: 1. As an alternative to the botorch optimizer in predictive strategies 2. To optimize custom function in the bofire domain. The utiliy function take care of the definition of the objective domain (variable types, constraints, etc.) and the handling of multiple experiment (\\(q\\) points).\nfrom copy import deepcopy\nfrom time import time\nfrom typing import List\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom bofire.benchmarks import api as benchmarks\nfrom bofire.data_models.constraints import api as constraints_data_models\nfrom bofire.data_models.domain import api as domains_data_models\nfrom bofire.data_models.features import api as features_data_models\nfrom bofire.data_models.strategies import api as strategies_data_models\nfrom bofire.strategies import api as strategies\nfrom bofire.strategies.utils import run_ga\nimport warnings\n\n\n# Suppress specific warnings\nwarnings.filterwarnings(\"ignore\", message=\".*A not p.d., added jitter\")\nwarnings.filterwarnings(\n    \"ignore\", message=\".*np.power((rand * alpha), (1.0 / (eta + 1.0)))[mask]\"\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html#example-1-usage-for-acquisition-function-optimization",
    "href": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html#example-1-usage-for-acquisition-function-optimization",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "Example 1) Usage for Acquisition Function Optimization",
    "text": "Example 1) Usage for Acquisition Function Optimization\nJust pass the GeneticAlgorithmOptimizer to the acquisition_optimizer argument of a strategy. The optimizer will then be used to optimize the acquisition function.\n\nbenchmark = benchmarks.Himmelblau()\n# generate experiments\nexperiments = benchmark.f(benchmark.domain.inputs.sample(10), return_complete=True)\n\n\noptimizer = strategies_data_models.GeneticAlgorithmOptimizer(\n    population_size=100,\n    n_max_gen=100,\n    verbose=False,\n)\n\n\nbenchmark_grid = np.hstack(\n    [\n        x.reshape((-1, 1))\n        for x in np.meshgrid(np.linspace(-6, 6, 100), np.linspace(-6, 6, 100))\n    ]\n)\nbenchmark_grid = pd.DataFrame(\n    benchmark_grid, columns=benchmark.domain.inputs.get_keys()\n)\nbenchmark_grid[\"y\"] = benchmark.f(benchmark_grid)[\"y\"]\n\n\ndef get_proposals(domain, n: int = 10) -&gt; pd.DataFrame:\n    strategy = strategies_data_models.SoboStrategy(\n        domain=domain, acquisition_optimizer=optimizer\n    )\n    # map to strategy object, and train the model\n    strategy = strategies.map(strategy)\n    strategy.tell(experiments)\n    t0 = time()\n    proposals = strategy.ask(n, raise_validation_error=False)\n    print(f\"Generated {len(proposals)} experiments, Time taken: {time() - t0:.2f}s\")\n    return proposals\n\n\nLinear Equality and Inequality Constraints are handled by a repair function, using QP\n\n# generate different cases\ndomain = deepcopy(benchmark.domain)\ndomain.constraints.constraints += [\n    constraints_data_models.LinearEqualityConstraint(  # x_1 + x_2 = 3\n        features=[\"x_1\", \"x_2\"],\n        coefficients=[1, 1],\n        rhs=3,\n    ),\n    constraints_data_models.LinearInequalityConstraint(  # x_2 &lt;= x_1\n        features=[\"x_1\", \"x_2\"],\n        coefficients=[-1, 1],\n        rhs=0,\n    ),\n]\n\nexperiments = benchmark.f(\n    strategies.RandomStrategy.make(domain=domain).ask(10), return_complete=True\n)\n\n\nproposals = get_proposals(domain)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nGenerated 10 experiments, Time taken: 20.02s\n\n\n\nplt.figure(figsize=(4, 4))\nplt.contour(\n    benchmark_grid[\"x_1\"].values.reshape((100, 100)),\n    benchmark_grid[\"x_2\"].values.reshape((100, 100)),\n    benchmark_grid[\"y\"].values.reshape((100, 100)),\n    levels=20,\n    label=\"true system response\",\n)\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\nplt.plot((-6, 6), (-6, 6), \"r--\", label=\"x_1 &lt; x_2\")\nplt.plot((-6, 6), (9, -3), \"g--\", label=\"x_1 + x_2 = 3\")\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_2802/1104301492.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'label'\n\n\n\n\n\n\n\n\n\n\n\n\nNChooseK Constraints are also handled by a repair function, using QP\n\ndomain = deepcopy(benchmark.domain)\ndomain.inputs.get_by_key(\"x_1\").bounds = (0.0, 6.0)\ndomain.inputs.get_by_key(\"x_2\").bounds = (0.0, 6.0)\ndomain.constraints.constraints += [\n    constraints_data_models.NChooseKConstraint(\n        features=[\"x_1\", \"x_2\"],\n        min_count=1,\n        max_count=1,\n        none_also_valid=True,\n    ),\n]\n\nexperiments = benchmark.f(\n    strategies.RandomStrategy.make(domain=domain).ask(10), return_complete=True\n)\n\n\nproposals = get_proposals(domain, n=10)\n\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nPolishing not needed - no active set detected at optimal point\nGenerated 10 experiments, Time taken: 10.99s\n\n\n\nplt.figure(figsize=(4, 4))\nplt.contour(\n    benchmark_grid[\"x_1\"].values.reshape((100, 100)),\n    benchmark_grid[\"x_2\"].values.reshape((100, 100)),\n    benchmark_grid[\"y\"].values.reshape((100, 100)),\n    levels=20,\n    label=\"true system response\",\n)\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nplt.xlim(0, 6)\nplt.ylim(0, 6)\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_2802/2170543038.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'label'\n\n\n\n\n\n\n\n\n\n\n\n\nInequality Constraints are handled by the GA objctive function\n\ndomain = deepcopy(benchmark.domain)\ndomain.constraints.constraints += [\n    constraints_data_models.NonlinearInequalityConstraint(\n        expression=\"x_1**2 + x_2**2 - 16\",\n        features=[\"x_1\", \"x_2\"],\n    ),\n]\nproposals = get_proposals(domain, n=20)\n\nGenerated 20 experiments, Time taken: 10.51s\n\n\n\nplt.figure(figsize=(4, 4))\nplt.contour(\n    benchmark_grid[\"x_1\"].values.reshape((100, 100)),\n    benchmark_grid[\"x_2\"].values.reshape((100, 100)),\n    benchmark_grid[\"y\"].values.reshape((100, 100)),\n    levels=20,\n    label=\"true system response\",\n)\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nx = np.linspace(-4, 4, 100)\ny1 = np.sqrt(16 - x**2)\ny2 = -y1\nplt.plot(x, y1, \"r--\", label=\"x_1**2 + x_2**2 &lt;= 4\")\nplt.plot(x, y2, \"r--\")\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\nplt.legend()\nplt.show()\n\n/tmp/ipykernel_2802/1844526682.py:2: UserWarning:\n\nThe following kwargs were not used by contour: 'label'"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html#example-2-usage-for-custom-function-optimization",
    "href": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html#example-2-usage-for-custom-function-optimization",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "Example 2) Usage for Custom Function Optimization",
    "text": "Example 2) Usage for Custom Function Optimization\nWe can define a domain with input features, and constraints. Output features are not required\n\ndomain = domains_data_models.Domain(\n    inputs=domains_data_models.Inputs(\n        features=[\n            features_data_models.ContinuousInput(\n                key=\"x_1\",\n                bounds=(-6, 6),\n            ),\n            features_data_models.ContinuousInput(\n                key=\"x_2\",\n                bounds=(-6, 6),\n            ),\n        ]\n    ),\n    constraints=[\n        constraints_data_models.NonlinearInequalityConstraint(\n            expression=\"x_1**2 + x_2**2 - 16\",\n            features=[\"x_1\", \"x_2\"],\n        ),\n    ],\n)\n\n\noptimizer = strategies_data_models.GeneticAlgorithmOptimizer(\n    population_size=100,\n    n_max_gen=100,\n    verbose=False,\n)\n\n\nDefine the optimization problem: a) Using evaluations on pd.DataFrame\nWe want to maximize the mean variance of the experiments dataframe. The objective function will be called with a list of dataframes, each representing a set of experiments. Each list entry is one individual in the population of the GA. The direction of the optimizer is a minimization of the objective function. So we minimize the negative mean variance of each 3 experiments in a batch in this case.\n\ndef objective_function(x: List[pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"assume we want to maximize the mean variance of the experiments dataframe\"\"\"\n    vars = [xi.var(numeric_only=True).mean() for xi in x]\n    return np.array(vars)\n\nRun the optimization with the utility function run_ga\n\nx_opt, f_opt = run_ga(\n    data_model=optimizer,\n    domain=domain,\n    objective_callables=[objective_function],  # list of objective functions to optimize\n    q=3,  # number of points to optimize\n    callable_format=\"pandas\",\n    optimization_direction=\"max\",  # maximize the objective function\n)\n\n\nproposals = x_opt[0]\nproposals\n\n\n\n\n\n\n\ncolumn\nx_1\nx_2\n\n\n\n\n0\n-1.170759\n3.824630\n\n\n1\n-2.449343\n-3.162042\n\n\n2\n3.894348\n-0.912181\n\n\n\n\n\n\n\n\nplt.figure(figsize=(4, 4))\nplt.plot(proposals[\"x_1\"], proposals[\"x_2\"], \"o\", label=\"Proposals\")\nx = np.linspace(-4, 4, 100)\ny1 = np.sqrt(16 - x**2)\ny2 = -y1\nplt.plot(x, y1, \"r--\", label=\"x_1**2 + x_2**2 &lt;= 4\")\nplt.plot(x, y2, \"r--\")\nplt.xlim(-6, 6)\nplt.ylim(-6, 6)\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDefine the optimization problem: a) Using evaluations on torch.Tensor\nFor efficiency, we can also compute the objective function as a callable of type Tensor. In this case, the computation is in the numerical domain.\nThis means, that categorical columns etc. are encoded. A specification with input_preprocessing_specs can be passed to the objective function (otherwise, defaults are used)\nThe objective accepts a Tensor in shape (n, q, d) and should return a Tensor of shape (n,)\n\ndef objective_function(x: torch.Tensor) -&gt; torch.Tensor:\n    var = torch.var(x, dim=2)\n    var_mean = torch.mean(var, dim=1)\n    return var_mean\n\n\nx_opt, f_opt = run_ga(\n    data_model=optimizer,\n    domain=domain,\n    objective_callables=[objective_function],  # list of objective functions to optimize\n    q=3,  # number of points to optimize\n    callable_format=\"torch\",\n    optimization_direction=\"max\",  # maximize the objective function\n)\n\n\nx_opt\n\ntensor([[ 2.5593, -3.0686],\n        [-2.9094,  2.7443],\n        [-3.2219,  2.3696]], dtype=torch.float64)"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html#example-3-multiobjective-optimization",
    "href": "build/lib/docs/tutorials/advanced_examples/genetic_algorithm.html#example-3-multiobjective-optimization",
    "title": "For optimizations in the bofire domain, a GA optimizer is available",
    "section": "Example 3: Multiobjective Optimization",
    "text": "Example 3: Multiobjective Optimization\nby giving multiple objective functions, or returning a 2D array from the objective, we will trigger multiobjective optimization\n\ndef objective_function_1(x: List[pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"assume we want to maximize the mean variance of the experiments dataframe\"\"\"\n    vars = [xi.var(numeric_only=True).mean() for xi in x]\n    return np.array(vars)\n\n\ndef objective_function_2(x: List[pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"Maximize the sum of all inputs\"\"\"\n    vars = [xi.sum().sum() for xi in x]\n    return np.array(vars)\n\n\nx_opt, f_opt = run_ga(\n    data_model=optimizer,\n    domain=domain,\n    objective_callables=[\n        objective_function_1,\n        objective_function_2,\n    ],  # list of objective functions to optimize\n    q=3,  # number of points to optimize\n    callable_format=\"pandas\",\n    optimization_direction=\"max\",  # maximize the objective function\n)\n\nIn the multiobjective-case, the result is a list of pd.DataFrame with different pareto-optimal solutions\n\nplt.scatter(f_opt[:, 0], f_opt[:, 1])\nplt.xlabel(\"objective function 1\")\nplt.ylabel(\"objective function 2\")\n\nText(0, 0.5, 'objective function 2')\n\n\n\n\n\n\n\n\n\n\nx_opt[0]\n\n\n\n\n\n\n\ncolumn\nx_1\nx_2\n\n\n\n\n0\n3.524850\n-1.890168\n\n\n1\n-3.939380\n-0.634087\n\n\n2\n2.555411\n3.067334\n\n\n\n\n\n\n\n\nx_opt[1]\n\n\n\n\n\n\n\ncolumn\nx_1\nx_2\n\n\n\n\n0\n1.239148\n3.702147\n\n\n1\n2.656452\n2.977783\n\n\n2\n2.527303\n3.074758"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/custom_sobo.html",
    "href": "build/lib/docs/tutorials/advanced_examples/custom_sobo.html",
    "title": "Custom Sobo Strategy",
    "section": "",
    "text": "The CustomSoboStrategy can be used to design custom objectives or objective combinations for optimizations. In this tutorial notebook, it is shown how to use it to optimize a quantity that depends on a combination of an inferred quantity and one of the inputs."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/custom_sobo.html#imports",
    "href": "build/lib/docs/tutorials/advanced_examples/custom_sobo.html#imports",
    "title": "Custom Sobo Strategy",
    "section": "Imports",
    "text": "Imports\n\nimport torch\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Himmelblau\nfrom bofire.data_models.strategies.api import CustomSoboStrategy\nfrom bofire.utils.torch_tools import tkwargs"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/custom_sobo.html#setup-the-optimization",
    "href": "build/lib/docs/tutorials/advanced_examples/custom_sobo.html#setup-the-optimization",
    "title": "Custom Sobo Strategy",
    "section": "Setup the optimization",
    "text": "Setup the optimization\nFor the optimization, we want to subtract the inferred quantity by the value of feature x_0.\n\nbenchmark = Himmelblau()\nexperiments = benchmark.f(benchmark.domain.inputs.sample(10), return_complete=True)\n\nstrategy_data = CustomSoboStrategy(domain=benchmark.domain)\nstrategy = strategies.map(strategy_data)\n\n\n# here we find out what is the index of the input feature in the input tensor `X`\n# in the manipulation function below\nfeature2index, _ = strategy.domain.inputs._get_transform_info(\n    strategy.input_preprocessing_specs\n)\nfeat_idx = feature2index[\"x_1\"][0]\n\n\n# we assign now a torch based function to the strategy which performs the custom manipulation of the objective\n# the signature has to be understood in the following way:\n# - samples: the samples to evaluate the objective on, these are the predicted Y/output values of the model(s)\n# - callables: the botorch callables associated to objectives associated to the features\n#   (have a look at `get_objective_callable` in `bofire/utils/torch_tools.py`)\n# - weights: the weights associated to the objectives\n#   (have a look here: `_callables_and_weights` in `bofire/utils/torch_tools.py`)\n# - X: a tensor of input values associated to the output values  samples, associated to the Y/output values (`samples`)\n\n\ndef f(samples, callables, weights, X):\n    val = torch.tensor(0.0).to(**tkwargs)\n    for c, w in zip(callables, weights):\n        val = val + c(samples, None) * w\n    # here, you have to implement the custom manipulation of the objective\n    # in this example, we subtract the value of the first feature from the objective\n    val = val - X[..., feat_idx]\n    return val\n\n\nstrategy.f = f\n\nstrategy.tell(experiments)\nstrategy.ask(1)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n0.23433\n-0.547178\n88.377362\n143.721316\n-88.377362"
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Basic terminology",
    "section": "",
    "text": "In the following it is showed how to setup optimization problems in BoFire and how to use strategies to solve them.",
    "crumbs": [
      "Home",
      "Basic terminology"
    ]
  },
  {
    "objectID": "getting_started.html#setting-up-the-optimization-problem",
    "href": "getting_started.html#setting-up-the-optimization-problem",
    "title": "Basic terminology",
    "section": "Setting up the optimization problem",
    "text": "Setting up the optimization problem\nIn BoFire, an optimization problem is defined by defining a domain containing input and output features, as well as optionally including constraints.\n\nFeatures\nInput features can be continuous, discrete, categorical.\nWe also support a range of specialized inputs that make defining your experiments easier, such as: - CategoricalMolecularInput allows transformations of molecules to featurizations (Fingerprints, Fragments and more). - TaskInput enables transfer learning and multi-fidelity methods, where you have access to similar experiments that can inform your optimization. - *DescriptorInput gives additional information about its value, combining the data with its significance.\n\nfrom bofire.data_models.features.api import (\n    CategoricalDescriptorInput,\n    CategoricalInput,\n    ContinuousInput,\n    DiscreteInput,\n)\n\n\nx1 = ContinuousInput(key=\"conc_A\", bounds=[0, 1])\nx2 = ContinuousInput(key=\"conc_B\", bounds=[0, 1])\nx3 = ContinuousInput(key=\"conc_C\", bounds=[0, 1])\nx4 = DiscreteInput(key=\"temperature\", values=[20, 50, 90], unit=\"°C\")\n\nx5 = CategoricalInput(\n    key=\"catalyst\",\n    categories=[\"cat_X\", \"cat_Y\", \"cat_Z\"],\n    allowed=[\n        True,\n        True,\n        False,\n    ],  # we have run out of catalyst Z, but still want to model past experiments\n)\n\nx6 = CategoricalDescriptorInput(\n    key=\"solvent\",\n    categories=[\"water\", \"methanol\", \"ethanol\"],\n    descriptors=[\"viscosity (mPa s)\", \"density (kg/m3)\"],\n    values=[[1.0, 997], [0.59, 792], [1.2, 789]],\n)\n\nWe can define both continuous and categorical outputs. Each output feature should have an objective, which determines if we aim to minimize, maximize, or drive the feature to a given value. Furthermore, we can define weights between 0 and 1 in case the objectives should not be weighted equally.\n\nfrom bofire.data_models.features.api import ContinuousOutput\nfrom bofire.data_models.objectives.api import MaximizeObjective, MinimizeObjective\n\n\nobjective1 = MaximizeObjective(\n    w=1.0,\n    bounds=[0.0, 1.0],\n)\ny1 = ContinuousOutput(key=\"yield\", objective=objective1)\n\nobjective2 = MinimizeObjective(w=1.0)\ny2 = ContinuousOutput(key=\"time_taken\", objective=objective2)\n\nIn- and output features are collected in respective feature lists, which can be summarized with the get_reps_df method.\n\nfrom bofire.data_models.domain.api import Inputs, Outputs\n\n\ninput_features = Inputs(features=[x1, x2, x3, x4, x5, x6])\noutput_features = Outputs(features=[y1, y2])\n\ninput_features.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nconc_A\nContinuousInput\n[0.0,1.0]\n\n\nconc_B\nContinuousInput\n[0.0,1.0]\n\n\nconc_C\nContinuousInput\n[0.0,1.0]\n\n\ntemperature\nDiscreteInput\ntype='DiscreteInput' key='temperature' unit='°...\n\n\nsolvent\nCategoricalDescriptorInput\n3 categories\n\n\ncatalyst\nCategoricalInput\n3 categories\n\n\n\n\n\n\n\n\noutput_features.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntime_taken\nContinuousOutput\nContinuousOutputFeature\n\n\nyield\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\nIndividual features can be retrieved by name, and a collection of features can be retrieved with a list of names.\n\ninput_features.get_by_key(\"catalyst\")\n\nCategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])\n\n\n\ninput_features.get_by_keys([\"catalyst\", \"conc_B\"])\n\nInputs(type='Inputs', features=[ContinuousInput(type='ContinuousInput', key='conc_B', unit=None, bounds=[0.0, 1.0], local_relative_bounds=None, stepsize=None, allow_zero=False), CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\nFeatures of a specific type can be returned by the get method. By using the exact argument, we can force the method to only return features that match the class exactly.\n\ninput_features.get(CategoricalInput)\n\nInputs(type='Inputs', features=[CategoricalDescriptorInput(type='CategoricalDescriptorInput', key='solvent', categories=['water', 'methanol', 'ethanol'], allowed=[True, True, True], descriptors=['viscosity (mPa s)', 'density (kg/m3)'], values=[[1.0, 997.0], [0.59, 792.0], [1.2, 789.0]]), CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\n\ninput_features.get(CategoricalInput, exact=True)\n\nInputs(type='Inputs', features=[CategoricalInput(type='CategoricalInput', key='catalyst', categories=['cat_X', 'cat_Y', 'cat_Z'], allowed=[True, True, False])])\n\n\nThe get_keys method follows the same logic as the get method, but returns just the keys of the features instead of the features itself.\n\ninput_features.get_keys(CategoricalInput)\n\n['solvent', 'catalyst']\n\n\nThe input feature container further provides methods to return a feature container with only all fixed or all free features.\n\nfree_inputs = input_features.get_free()\nfixed_inputs = input_features.get_fixed()\n\nOne can uniformly sample from individual input features.\n\nx5.sample(2)\n\n0    cat_X\n1    cat_X\nName: catalyst, dtype: str\n\n\nOr directly from input feature containers, uniform, sobol and LHS sampling is possible. A default, uniform sampling is used.\n\nfrom bofire.data_models.enum import SamplingMethodEnum\n\n\nX = input_features.sample(n=10, method=SamplingMethodEnum.LHS)\n\nX\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\ntemperature\nsolvent\ncatalyst\n\n\n\n\n0\n0.293507\n0.606473\n0.342847\n90.0\nmethanol\ncat_X\n\n\n1\n0.123719\n0.294153\n0.693531\n20.0\nethanol\ncat_Y\n\n\n2\n0.936970\n0.923832\n0.926881\n50.0\nwater\ncat_X\n\n\n3\n0.746237\n0.368344\n0.129886\n50.0\nmethanol\ncat_X\n\n\n4\n0.833210\n0.456737\n0.765468\n50.0\nmethanol\ncat_Y\n\n\n5\n0.090276\n0.051011\n0.056346\n20.0\nwater\ncat_Y\n\n\n6\n0.489508\n0.861146\n0.854561\n20.0\nwater\ncat_Y\n\n\n7\n0.586552\n0.173421\n0.225377\n50.0\nmethanol\ncat_Y\n\n\n8\n0.342624\n0.511642\n0.599030\n90.0\nethanol\ncat_X\n\n\n9\n0.693139\n0.734631\n0.449846\n90.0\nethanol\ncat_X\n\n\n\n\n\n\n\n\n\nConstraints\nThe search space can be further defined by constraints on the input features. BoFire supports linear equality and inequality constraints, as well as non-linear equality and inequality constraints.\n\nLinear constraints\nLinearEqualityConstraint and LinearInequalityConstraint are expressions of the form \\(\\sum_i a_i x_i = b\\) or \\(\\leq b\\) for equality and inequality constraints respectively. They take a list of names of the input features they are operating on, a list of left-hand-side coefficients \\(a_i\\) and a right-hand-side constant \\(b\\).\n\nfrom bofire.data_models.constraints.api import (\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n)\n\n\n# A mixture: x1 + x2 + x3 = 1\nconstr1 = LinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    coefficients=[1, 1, 1],\n    rhs=1,\n)\n\n# x1 + 2 * x3 &lt; 0.8\nconstr2 = LinearInequalityConstraint(\n    features=[\"conc_A\", \"conc_C\"],\n    coefficients=[1, 2],\n    rhs=0.8,\n)\n\nLinear constraints can only operate on ContinuousInput features.\n\n\nNonlinear constraints\nNonlinearEqualityConstraint and NonlinearInequalityConstraint take any expression that can be evaluated by pandas.eval, including mathematical operators such as sin, exp, log10 or exponentiation. So far, they cannot be used in any optimizations.\n\nfrom bofire.data_models.constraints.api import NonlinearEqualityConstraint\n\n\n# The unit circle: x1**2 + x2**2 = 1\nconst3 = NonlinearEqualityConstraint(\n    features=[\"conc_A\", \"conc_B\"], expression=\"conc_A**2 + conc_B**2 - 1\"\n)\nconst3\n\nNonlinearEqualityConstraint(type='NonlinearEqualityConstraint', features=['conc_A', 'conc_B'], expression='conc_A**2 + conc_B**2 - 1', jacobian_expression='[2*conc_A, 2*conc_B]', hessian_expression='[[2, 0], [0, 2]]')\n\n\n\n\nCombinatorial constraint\nUse NChooseKConstraint to express that we only want to have \\(k\\) out of the \\(n\\) parameters to take positive values. Think of a mixture, where we have long list of possible ingredients, but want to limit number of ingredients in any given recipe.\n\nfrom bofire.data_models.constraints.api import NChooseKConstraint\n\n\n# Only 1 or 2 out of 3 compounds can be present (have non-zero concentration)\nconstr5 = NChooseKConstraint(\n    features=[\"conc_A\", \"conc_B\", \"conc_C\"],\n    min_count=1,\n    max_count=2,\n    none_also_valid=False,\n)\nconstr5\n\nNChooseKConstraint(type='NChooseKConstraint', features=['conc_A', 'conc_B', 'conc_C'], min_count=1, max_count=2, none_also_valid=False)\n\n\nNote that we have to set a boolean, if none is also a valid selection, e.g. if we want to have 0, 1, or 2 of the ingredients in our recipe.\n\n\nCategoricalExcludeConstraint\nThe CategoricalExcludeConstraint can be used to exclude certain combinations of categories between categorical features or exclude a combination between categories and numerical values. So far, this constraint is only supported by the RandomStrategy.\nIn the example below, it would be forbidden that cat_C is used together with one of the solvents methanol or ethanol.\n\nfrom bofire.data_models.constraints.api import (\n    CategoricalExcludeConstraint,\n    SelectionCondition,\n)\n\n\nfeat_cat = CategoricalInput(\n    key=\"cat1\",\n    categories=[\"cat_A\", \"cat_B\", \"cat_C\"],\n)\nfeat_solvent = CategoricalInput(\n    key=\"solvent\", categories=[\"water\", \"methanol\", \"ethanol\"]\n)\n\nconstr6 = CategoricalExcludeConstraint(\n    features=[\"cat1\", \"solvent\"],\n    conditions=[\n        SelectionCondition(selection=[\"cat_C\"]),\n        SelectionCondition(selection=[\"methanol\", \"ethanol\"]),\n    ],\n)\n\nThe next example shows how to forbid that solvent ethanol is used at a temperature higher than 40°C, this is achieved by using a ThresholdCondition.\n\nfrom bofire.data_models.constraints.api import ThresholdCondition\n\n\nfeat_temp = ContinuousInput(\n    key=\"temperature\",\n    bounds=[0, 100],\n    unit=\"°C\",\n)\nconstr7 = CategoricalExcludeConstraint(\n    features=[\"solvent\", \"temperature\"],\n    conditions=[\n        SelectionCondition(selection=[\"water\"]),\n        ThresholdCondition(\n            threshold=40,\n            operator=\"&gt;=\",\n        ),\n    ],\n)\n\nSimilar to the features, constraints can be grouped in a container which acts as the union constraints.\n\nfrom bofire.data_models.domain.api import Constraints\n\n\nconstraints = Constraints(constraints=[constr1, constr2])\n\nA summary of the constraints can be obtained by the method get_reps_df:\n\nconstraints.get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\n0\nLinearEqualityConstraint\ntype='LinearEqualityConstraint' features=['con...\n\n\n1\nLinearInequalityConstraint\ntype='LinearInequalityConstraint' features=['c...\n\n\n\n\n\n\n\nWe can check whether a point satisfies individual constraints or the list of constraints.\n\nconstr2.is_fulfilled(X).values\n\narray([False, False, False, False, False,  True, False, False, False,\n       False])\n\n\nOutput constraints can be setup via sigmoid-shaped objectives passed as argument to the respective feature, which can then also be plotted.\n\nfrom bofire.data_models.objectives.api import MinimizeSigmoidObjective\nfrom bofire.plot.api import plot_objective_plotly\n\n\noutput_constraint = MinimizeSigmoidObjective(w=1.0, steepness=10, tp=0.5)\ny3 = ContinuousOutput(key=\"y3\", objective=output_constraint)\n\noutput_features = Outputs(features=[y1, y2, y3])\n\nfig = plot_objective_plotly(feature=y3, lower=0, upper=1)\n\nfig.show()\n\n                            \n                                            \n\n\n\n\n\nThe domain\nThe domain holds then all information about an optimization problem and can be understood as a search space definition. A detailed description of the domain can be found in docs.\n\nfrom bofire.data_models.domain.api import Domain\n\n\ndomain = Domain(inputs=input_features, outputs=output_features, constraints=constraints)\n\nIn addition one can instantiate the domain also just from lists.\n\ndomain_single_objective = Domain.from_lists(\n    inputs=[x1, x2, x3, x4, x5, x6],\n    outputs=[y1],\n    constraints=[],\n)",
    "crumbs": [
      "Home",
      "Basic terminology"
    ]
  },
  {
    "objectID": "getting_started.html#optimization",
    "href": "getting_started.html#optimization",
    "title": "Basic terminology",
    "section": "Optimization",
    "text": "Optimization\nTo solve the optimization problem, we further need a solving strategy. BoFire supports strategies without a prediction model such as a random strategy and predictive strategies which are based on a prediction model.\nAll strategies contain an ask method returning a defined number of candidate experiments.\n\nRandom Strategy\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy\n\n\nstrategy_data_model = RandomStrategy(domain=domain)\n\nrandom_strategy = strategies.map(strategy_data_model)\nrandom_candidates = random_strategy.ask(2)\n\nrandom_candidates\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\ntemperature\nsolvent\ncatalyst\n\n\n\n\n0\n0.382520\n0.465995\n0.151485\n50.0\nmethanol\ncat_X\n\n\n1\n0.218504\n0.650163\n0.131333\n20.0\nethanol\ncat_X\n\n\n\n\n\n\n\n\n\nSingle objective Bayesian Optimization strategy\nSince a predictive strategy includes a prediction model, we need to generate some historical data, which we can afterwards pass as training data to the strategy via the tell method.\nFor didactic purposes we just choose here from one of our benchmark methods.\n\nfrom bofire.benchmarks.single import Himmelblau\n\n\nbenchmark = Himmelblau()\n\n(benchmark.domain.inputs + benchmark.domain.outputs).get_reps_df()\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nx_1\nContinuousInput\n[-6.0,6.0]\n\n\nx_2\nContinuousInput\n[-6.0,6.0]\n\n\ny\nContinuousOutput\nContinuousOutputFeature\n\n\n\n\n\n\n\nGenerating some initial data works as follows:\n\nsamples = benchmark.domain.inputs.sample(10)\n\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n-5.517433\n3.383867\n522.161237\n1\n\n\n1\n3.922258\n0.023964\n28.900091\n1\n\n\n2\n2.269354\n5.049552\n431.922850\n1\n\n\n3\n4.096685\n0.916366\n49.137626\n1\n\n\n4\n-0.821480\n-1.354872\n172.253229\n1\n\n\n5\n3.327462\n5.282895\n616.080234\n1\n\n\n6\n4.475601\n5.734728\n1139.920897\n1\n\n\n7\n5.191045\n4.633330\n810.016234\n1\n\n\n8\n-4.366370\n0.084978\n195.455368\n1\n\n\n9\n0.693734\n-3.914350\n289.599752\n1\n\n\n\n\n\n\n\nLet’s setup the SOBO strategy and ask for a candidate. First we need a serializable data model that contains the hyperparameters.\n\nfrom pprint import pprint\n\nfrom bofire.data_models.acquisition_functions.api import qLogNEI\nfrom bofire.data_models.strategies.api import SoboStrategy as SoboStrategyDM\n\n\nsobo_strategy_data_model = SoboStrategyDM(\n    domain=benchmark.domain,\n    acquisition_function=qLogNEI(),\n)\n\n# print information about hyperparameters\nprint(\"Acquisition function:\", sobo_strategy_data_model.acquisition_function)\nprint()\nprint(\"Surrogate type:\", sobo_strategy_data_model.surrogate_specs.surrogates[0].type)\nprint()\nprint(\"Surrogate's kernel:\")\npprint(sobo_strategy_data_model.surrogate_specs.surrogates[0].kernel.model_dump())\n\nAcquisition function: type='qLogNEI' prune_baseline=True n_mc_samples=512\n\nSurrogate type: SingleTaskGPSurrogate\n\nSurrogate's kernel:\n{'ard': True,\n 'features': None,\n 'lengthscale_constraint': None,\n 'lengthscale_prior': {'loc': 1.4142135623730951,\n                       'loc_scaling': 0.5,\n                       'scale': 1.7320508075688772,\n                       'scale_scaling': 0.0,\n                       'type': 'DimensionalityScaledLogNormalPrior'},\n 'type': 'RBFKernel'}\n\n\nThe actual strategy can then be created via the mapper function.\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\nsobo_strategy.tell(experiments=experiments)\nsobo_strategy.ask(candidate_count=1)\n\n/home/runner/work/bofire/bofire/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n4.906945\n-0.409012\n133.55612\n267.596044\n-133.55612\n\n\n\n\n\n\n\nAn alternative way is calling the strategy’s constructor directly.\n\nsobo_strategy = strategies.SoboStrategy(sobo_strategy_data_model)\n\nThe latter way is helpful to keep type information.\n\n\nDesign of Experiments\nAs a simple example for the DoE functionalities we consider the task of finding a D-optimal design for a fully-quadratic model with three design variables with bounds (0,1) and a mixture constraint.\nWe define the design space including the constraint as a domain. Then we pass it to the optimization routine and specify the model. If the user does not indicate a number of experiments it will be chosen automatically based on the number of model terms.\n\nimport numpy as np\n\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\n\n\ndomain = Domain.from_lists(inputs=[x1, x2, x3], outputs=[y1], constraints=[constr1])\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"fully-quadratic\"),\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\nnp.round(candidates, 3)\n\n\n\n\n\n\n\n\nconc_A\nconc_B\nconc_C\n\n\n\n\n0\n0.0\n0.0\n1.0\n\n\n1\n0.5\n0.5\n0.0\n\n\n2\n0.0\n0.5\n0.5\n\n\n3\n0.5\n0.0\n0.5\n\n\n4\n1.0\n0.0\n0.0\n\n\n5\n0.5\n0.5\n0.0\n\n\n6\n1.0\n0.0\n0.0\n\n\n7\n0.0\n1.0\n0.0\n\n\n8\n1.0\n0.0\n0.0\n\n\n9\n0.0\n1.0\n0.0\n\n\n10\n0.5\n0.0\n0.5\n\n\n11\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nThe resulting design looks like this:\n\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"fully-quadratic model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(xs=[1, 0, 0, 1], ys=[0, 1, 0, 0], zs=[0, 0, 1, 0], linewidth=2)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[x1.key],\n    ys=candidates[x2.key],\n    zs=candidates[x3.key],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n)",
    "crumbs": [
      "Home",
      "Basic terminology"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#experimental-design",
    "href": "index.html#experimental-design",
    "title": "",
    "section": "Experimental design",
    "text": "Experimental design\nIn the context of experimental design BoFire allows to define a design space\n\\[\n\\mathbb{X} = x_1 \\otimes x_2 \\ldots \\otimes x_D\n\\]\nwhere the design parameters may take values depending on their type and domain, e.g.\n\ncontinuous: \\(x_1 \\in [0, 1]\\)\ndiscrete: \\(x_2 \\in \\{1, 2, 5, 7.5\\}\\)\ncategorical: \\(x_3 \\in \\{A, B, C\\}\\)\n\nand a set of equations define additional experimental constraints, e.g.\n\nlinear equality: \\(\\sum x_i = 1\\)\nlinear inequality: \\(2 x_1 \\leq x_2\\)\nnon-linear inequality: \\(\\sum x_i^2 \\leq 1\\)\nn-choose-k: only \\(k\\) out of \\(n\\) parameters can take non-zero values.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#multi-objective-optimization",
    "href": "index.html#multi-objective-optimization",
    "title": "",
    "section": "Multi-objective optimization",
    "text": "Multi-objective optimization\nIn the context of multi-objective optimization BoFire allows to define a vector-valued optimization problem\n\\[\n\\mathrm{argmax}_{x \\in \\mathbb{X}} s(y(x))\n\\]\nwhere\n\n\\(\\mathbb{X}\\) is again the experimental design space\n\\(y = \\{y_1, \\ldots y_M\\}\\) are known functions describing your experimental outputs and\n\\(s = \\{s_1, \\ldots s_M\\}\\) are the objectives to be maximized. For instance, \\(s_1\\) is the identity function if \\(y_1\\) is to be maximized.\n\nSince the objectives are usually conflicting, there is no point \\(x\\) that simultaneously optimizes all objectives. Instead the goal is to find the Pareto front of all optimal compromises.\nA decision maker can then explore these compromises to get a deep understanding of the problem and make the best informed decision.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#bayesian-optimization",
    "href": "index.html#bayesian-optimization",
    "title": "",
    "section": "Bayesian optimization",
    "text": "Bayesian optimization\nIn the context of Bayesian optimization we want to simultaneously learn the unknown function \\(y(x)\\) (exploration), while focusing the experimental effort on promising regions (exploitation). This is done by using the experimental data to fit a probabilistic model \\(p(y|x, \\mathrm{data})\\) that estimates the distribution of possible outcomes for \\(y\\). An acquisition function \\(a\\) then formulates the desired trade-off between exploration and exploitation\n\\[\n\\mathrm{argmax}_{x \\in \\mathbb{X}} a(s(p_y(x)))\n\\]\nand the maximizer \\(x_\\mathrm{opt}\\) of this acquisition function determines the next experiment \\(y(x)\\) to run.\nWhen there are multiple competing objectives, the task is again to find a suitable approximation of the Pareto front.",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#design-of-experiments",
    "href": "index.html#design-of-experiments",
    "title": "",
    "section": "Design of Experiments",
    "text": "Design of Experiments\nBoFire can be used to generate optimal experimental designs with respect to various optimality criteria like D-optimality, A-optimality or uniform space filling.\nFor this, the user specifies a design space and a model formula, then chooses an optimality criterion and the desired number of experiments in the design. The resulting optimization problem is then solved by IPOPT.\nThe doe subpackage also supports a wide range of constraints on the design space including linear and nonlinear equalities and inequalities as well a (limited) use of NChooseK constraints. The user can provide fixed experiments that will be treated as part of the design but remain fixed during the optimization process. While some of the optimization algorithms support non-continuous design variables, the doe subpackage only supports those that are continuous.\nBy default IPOPT uses the freely available linear solver MUMPS. For large models choosing a different linear solver (e.g. ma57 from Coin-HSL) can vastly reduce optimization time. A free academic license for Coin-HSL can be obtained here. Instructions on how to install additional linear solvers for IPOPT are given in the IPOPT documentation. For choosing a specific (HSL) linear solver in BoFire you can just pass the name of the solver to find_local_max_ipopt() with the linear_solver option together with the library’s name in the option hsllib, e.g.\nfind_local_max_ipopt(domain, \"fully-quadratic\", ipopt_options={\"linear_solver\":\"ma57\", \"hsllib\":\"libcoinhsl.so\"})",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reference",
    "href": "index.html#reference",
    "title": "",
    "section": "Reference",
    "text": "Reference\nWe would love for you to use BoFire in your work! If you do, please cite our paper:\n@misc{durholt2024bofire,\n  title={BoFire: Bayesian Optimization Framework Intended for Real Experiments},\n  author={Johannes P. D{\\\"{u}}rholt and Thomas S. Asche and Johanna Kleinekorte and Gabriel Mancino-Ball and Benjamin Schiller and Simon Sung and Julian Keupp and Aaron Osburg and Toby Boyne and Ruth Misener and Rosona Eldred and Wagner Steuer Costa and Chrysoula Kappatou and Robert M. Lee and Dominik Linzner and David Walz and Niklas Wulkow and Behrang Shafei},\n  year={2024},\n  eprint={2408.05040},\n  archivePrefix={arXiv},\n  primaryClass={cs.LG},\n  url={https://arxiv.org/abs/2408.05040},\n}",
    "crumbs": [
      "Home",
      "Introduction"
    ]
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "",
    "section": "",
    "text": "Code",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "install.html#installation-from-python-package-index-pypi",
    "href": "install.html#installation-from-python-package-index-pypi",
    "title": "",
    "section": "Installation from Python Package Index (PyPI)",
    "text": "Installation from Python Package Index (PyPI)\nBoFire can be installed to your Python environment by using pip. It can be done by executing\npip install bofire\n\n\n\n\n\n\nTip\n\n\n\nThe command from above will install a minimal BoFire version, consisting only of the data models. To install BoFire’s including its core optimization features, execute:\npip install 'bofire[optimization]'\n\n\n\nAdditional optional dependencies\nIn BoFire, there are several optional dependencies that can be selected during installation via pip, like\npip install 'bofire[optimization, cheminfo] # will install bofire with additional dependencies `optimization` and `cheminfo`\nTo get the most our of BoFire, it is recommended to install at least\npip install 'bofire[optimization]'\nThe available dependencies are:\n\noptimization: Core Bayesian optimization features.\ncheminfo: Cheminformatics utilities.\nentmoot: Entmoot functionality.\ntests: Required for running the test suite.\ndocs: Required for building the documentation.\ntutorials: Required for running the tutorials.\nall: Install all possible options (except DoE)\n\n\n\n\n\n\n\nWarning\n\n\n\nBoFire has the functionalities for creating D, E, A, G, K and I-optimal experimental designs via the DoEStrategy. This feature depends on cyipopt which is a python interface to ipopt. Unfortunately, it is not possible to install cyipopt including ipopt via pip. A solution is to install cyipopt and its dependencies via conda:\nconda install -c conda-forge cyipopt\nWe are working on a solution that makes BoFire’s model based DoE functionalities also accessible to users which do not have cyipopt available.",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "install.html#development-installation",
    "href": "install.html#development-installation",
    "title": "",
    "section": "Development Installation",
    "text": "Development Installation\nIf you want to contribute to BoFire, it is recommended to install the repository in editable mode (-e).\nAfter cloning the repository via\ngit clone https://github.com/experimental-design/bofire.git\nand navigating to the repositories root folder (cd bofire), you can proceed with\npip install -e \".[optimization, tests]\" # include optional dependencies as you wish",
    "crumbs": [
      "Home",
      "Installation Guide"
    ]
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/conditional_features_bo.html",
    "href": "build/lib/docs/tutorials/advanced_examples/conditional_features_bo.html",
    "title": "Conditional Features",
    "section": "",
    "text": "When optimizing chemical processes, we often have some inputs that are dependent on others. For example, the value of a catalyst_concentration input feature is only relevant depending on another feature use_catalyst==True. Whilst it may seem that use_catalyst==False is equivalent to just setting catalyst_concentration==0, there may be some limitations to this approach: - If a catalyst is used, there may be some minimum amount required. It is difficult to model the disjoint bounds of a continuous feature. - It may be the case that some tiny presence of catalyst enables a side reaction that completely changes the reaction. We therefore have a step change at 0, with smoother behaviour everywhere else in the domain, which Gaussian process surrogates cannot model well.\nFor a some examples of literature on these problems, see [Swersky2014Arc] and [Horn2019Wedge].\n[Swersky2014Arc] Swersky et al. 2014, “Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces” arXiv [Horn2019Wedge] Horn et al. “Surrogates for hierarchical search spaces: the wedge-kernel and an automated analysis”, GECCO\nWe consider a test problem as described above, where we wish to optimize the yield of a reaction by controlling the temperature and catalyst concentration.\nfrom bofire.data_models.constraints.api import NonZeroCondition\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.kernels.api import RBFKernel, WedgeKernel\n\n\ncatalyst_domain = Domain.from_lists(\n    inputs=[\n        ContinuousInput(key=\"temperature\", unit=\"°C\", bounds=(50.0, 100.0)),\n        ContinuousInput(key=\"catalyst_concentration\", unit=\"M\", bounds=(0.2, 1.0), allow_zero=True),\n    ],\n    outputs=[ContinuousOutput(key=\"yield\")],\n)\n\nindicator_kernel = WedgeKernel(\n    base_kernel=RBFKernel(),\n    conditions=[\n        (\"catalyst_concentration\", \"catalyst_concentration\", NonZeroCondition())\n    ],\n)\nAfter defining the domain, we can then build the wedge kernel for our GP surrogate.\nfrom bofire.data_models.kernels.api import RBFKernel, WedgeKernel\n# here, we manually build the list of conditions\n# in future we will automatically build them from any inputs with allow_zero==True\nconditions = [\n    (\n        \"catalyst_concentration\",\n        \"catalyst_concentration\",\n        NonZeroCondition(),\n    )\n]\n\nwedge_kernel_data_model = WedgeKernel(\n    base_kernel=RBFKernel(),\n    conditions=conditions,\n)\nimport torch\n\nimport bofire.kernels.api as kernels\n\n\ndef features_to_idx_mapper(feats: list[str]) -&gt; list[int]:\n    return catalyst_domain.inputs.get_feature_indices({}, feats)\n\n\nwedge_kernel = kernels.map(\n    wedge_kernel_data_model,\n    batch_shape=torch.Size([]),\n    active_dims=list(range(2)),\n    features_to_idx_mapper=features_to_idx_mapper,\n)\n# cast the return type from `kernels.map` to the wedge kernel\n# this fixes syntax highlighting in this notebook\nfrom typing import cast\n\nfrom bofire.kernels.conditional import WedgeKernel as WedgeKernelFunctional\n\nwedge_kernel = cast(WedgeKernelFunctional, wedge_kernel)\nwedge_kernel\n\nWedgeKernel(\n  (raw_lengthscale_constraint): Positive()\n  (raw_angle_constraint): Interval(1.000E-04, 9.999E-01)\n  (raw_radius_constraint): Positive()\n  (base_kernel): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\nWe check below that the kernel behaves as expected. Specifically, we want to check that the indicator function correctly masks the conditional feature to be inactive when the condition is not met.\n# check the order of dimensions in X\nassert features_to_idx_mapper([\"catalyst_concentration\", \"temperature\"]) == [0, 1]\n\nX = torch.tensor(\n    [\n        [0.0, 0.0],\n        [0.05, 0.0],\n        [0.50, 0.6],\n    ]\n)\n\n# the indicator function shows which dimensions are active\n# X[:, 1] should always be active\n# X[:, 0] should only be active if X[:, 0] != 0\nwedge_kernel.indicator_func(X)\n\ntensor([[False,  True],\n        [ True,  True],\n        [ True,  True]])"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/conditional_features_bo.html#using-the-wedge-kernel-for-bayesian-optimizaiton",
    "href": "build/lib/docs/tutorials/advanced_examples/conditional_features_bo.html#using-the-wedge-kernel-for-bayesian-optimizaiton",
    "title": "Conditional Features",
    "section": "Using the wedge kernel for Bayesian optimizaiton",
    "text": "Using the wedge kernel for Bayesian optimizaiton\nUsing the domain defined at the start of the notebook, we now create a benchmark that we can use to evaluate how well this kernel works for Bayesian optimization.\nWe set up our objective function such that adding a catalyst enables a side reaction, and so any small prescence of the catalyst hurts yield. However, at higher concentration of catalyst, we see improved performance.\nIncreasing temperature increases the yield, however it also causes decomposition of the catalyst at high temperatures.\n\nimport numpy as np\nimport pandas as pd\n\nfrom bofire.benchmarks.api import Benchmark\n\n\nclass ReactionOptimizationBenchmark(Benchmark):\n    _domain = catalyst_domain\n\n    def _f(self, X: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:  # type: ignore\n        cat_conc = X[\"catalyst_concentration\"]\n        temp = X[\"temperature\"]\n        norm_temp = (temp - 50) / 50\n\n        side_product = np.where(cat_conc &gt; 0.0, 10.0, 0.0)\n        effective_catalyst = np.clip(cat_conc - 1.5 * (norm_temp - 0.5), 0.0, cat_conc)\n\n        catalyst_effect = 10 * np.exp(effective_catalyst)\n        temperature_effect = 10 * np.exp(norm_temp)\n        y = temperature_effect + catalyst_effect - side_product\n\n        Y = pd.DataFrame({\"yield\": y, \"valid_yield\": 1})\n        return Y\n\n\nbenchmark = ReactionOptimizationBenchmark()\n\nFirst, we plot the objective function:\n\nimport matplotlib.pyplot as plt\n\n\nN_grid_pts = 50\n# plot active points\nX_c, X_t = np.meshgrid(\n    np.linspace(0.5, 1.0, num=N_grid_pts), np.linspace(50, 100, num=N_grid_pts)\n)\ngrid_X = np.stack((X_c.flatten(), X_t.flatten()), axis=-1)\ngrid_Y = benchmark.f(\n    pd.DataFrame(data=grid_X, columns=[\"catalyst_concentration\", \"temperature\"])\n)[\"yield\"].to_numpy()\nplt.contourf(X_c, X_t, grid_Y.reshape(N_grid_pts, N_grid_pts), vmin=10, vmax=40)\n\n# plot inactive points\nX_c, X_t = np.meshgrid(\n    np.linspace(-0.05, 0.05, num=2), np.linspace(50, 100, num=N_grid_pts)\n)\ngrid_X = np.stack((0.0 * X_c.flatten(), X_t.flatten()), axis=-1)\ngrid_Y = benchmark.f(\n    pd.DataFrame(data=grid_X, columns=[\"catalyst_concentration\", \"temperature\"])\n)[\"yield\"].to_numpy()\nplt.contourf(X_c, X_t, grid_Y.reshape(N_grid_pts, 2), vmin=10, vmax=40)\n\nplt.colorbar()\nplt.xlabel(\"catalyst_concentration\")\nplt.ylabel(\"temperature\")\n\nText(0, 0.5, 'temperature')\n\n\n\n\n\n\n\n\n\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\n\n\nsamples = strategies.map(RandomStrategy(domain=catalyst_domain, seed=0)).ask(10)\n# RandomStrategy doesn't currently support randomly sampling with allow_zero\n# So we manually set the first 2 reactions to have no catalyst.\nsamples.loc[:1, \"catalyst_concentration\"] = 0.0\nexperiments = benchmark.f(samples, return_complete=True)\nexperiments.head(4)\n\n\n\n\n\n\n\n\ncatalyst_concentration\ntemperature\nyield\nvalid_yield\n\n\n\n\n0\n0.000000\n59.171060\n22.013203\n1\n\n\n1\n0.000000\n74.864291\n26.442524\n1\n\n\n2\n0.297314\n92.215550\n23.263744\n1\n\n\n3\n0.406463\n75.872417\n21.404513\n1\n\n\n\n\n\n\n\n\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\n\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=catalyst_domain,\n    seed=1,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=catalyst_domain.inputs,\n                outputs=catalyst_domain.outputs,\n                kernel=wedge_kernel_data_model,\n            )\n        ]\n    ),\n)\nstrategy = strategies.map(sobo_strategy_data_model)\n\n\nstrategy.tell(experiments, replace=True)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nWe would expect the proposed batch to include some experiments with no catalyst, and some with a catalyst. Note that the volume of the search space with catalyst is larger, so we may also expect more experiments with the feature active.\n\nstrategy.ask(5)\n\n\n\n\n\n\n\n\ncatalyst_concentration\ntemperature\nyield_pred\nyield_sd\nyield_des\n\n\n\n\n0\n0.920084\n55.793790\n29.725369\n2.410661\n29.725369\n\n\n1\n0.000000\n97.313192\n27.228390\n3.424580\n27.228390\n\n\n2\n1.000000\n72.165219\n31.275936\n0.585717\n31.275936\n\n\n3\n0.616450\n50.000000\n27.636364\n3.372072\n27.636364\n\n\n4\n1.000000\n50.000000\n28.600987\n3.072161\n28.600987"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/desirability_objectives.html",
    "href": "build/lib/docs/tutorials/advanced_examples/desirability_objectives.html",
    "title": "Desirability Functions for Multi-Objective Optimization",
    "section": "",
    "text": "This notebook demonstrates the use of desirability functions for multi-objective optimization. The desirability function is a scalar function that maps a vector of objective values to a scalar value, most often in the range [0, 1]. The desirability function is used to aggregate multiple objectives into a single objective value, e.g. by the multiplicative Sobo strategy.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom bofire.data_models.objectives import api as objectives_data_model\n\n\nDesirability Functions map from the input space to the range [0, 1], also by clipping after the bounds\n\nobjectives = {\n    \"Increasing\": objectives_data_model.IncreasingDesirabilityObjective(\n        bounds=(0.0, 5.0)\n    ),\n    \"Decreasing\": objectives_data_model.DecreasingDesirabilityObjective(\n        bounds=(0.0, 5.0)\n    ),\n    \"Peak\": objectives_data_model.PeakDesirabilityObjective(\n        bounds=(0.0, 5.0), peak_position=2.5\n    ),\n    \"In-Range\": objectives_data_model.InRangeDesirability(bounds=(1.0, 3.0)),\n}\n\n\nfor key, objective in objectives.items():\n    x = np.linspace(-2.0, 7.0, 100)\n    y = objective(x, None)\n    plt.plot(x, y, label=key)\nplt.grid(True)\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nClipping is optional, but leads to values outside the [0, 1] range\n\nobjectives = {\n    \"Increasing\": objectives_data_model.IncreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), clip=False\n    ),\n    \"Decreasing\": objectives_data_model.DecreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), clip=False\n    ),\n    \"Peak\": objectives_data_model.PeakDesirabilityObjective(\n        bounds=(0.0, 5.0), peak_position=2.5, clip=False\n    ),\n}\nfor key, objective in objectives.items():\n    x = np.linspace(-2.0, 7.0, 100)\n    y = objective(x, None)\n    plt.plot(x, y, label=key)\nplt.grid(True)\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nA concave or convex desirability function can be created by setting the log_shape_factor\n\nobjectives = {\n    \"Increasing\": objectives_data_model.IncreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), log_shape_factor=1.0\n    ),\n    \"Decreasing\": objectives_data_model.DecreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), log_shape_factor=-1.0\n    ),\n    \"Peak\": objectives_data_model.PeakDesirabilityObjective(\n        bounds=(0.0, 5.0),\n        peak_position=2.5,\n        log_shape_factor=-1.0,\n        log_shape_factor_decreasing=1.0,\n    ),\n}\nfor key, objective in objectives.items():\n    x = np.linspace(-2.0, 7.0, 100)\n    y = objective(x, None)\n    plt.plot(x, y, label=key)\nplt.grid(True)\nplt.legend()"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/index.html",
    "href": "build/lib/docs/tutorials/advanced_examples/index.html",
    "title": "Advanced Examples",
    "section": "",
    "text": "These notebooks showcase more specialized and advanced use cases in BoFire. These examples are not necessarily better strategies, but represent more complex uses of components within the library.\n\n\n\n\nCreate custom single-objective Bayesian optimization strategies.\n\n\n\nWorking with desirability functions for multi-criteria optimization.\n\n\n\nUsing genetic algorithms for optimization in BoFire.\n\n\n\nTechniques for combining multiple objectives in optimization.\n\n\n\nLeveraging multiple fidelity levels for efficient optimization.\n\n\n\nDefining optimization objectives directly on input parameters.\n\n\n\nUsing Random Forest as a surrogate model instead of Gaussian Processes.\n\n\n\nApplying transfer learning techniques to Bayesian optimization.\n\n\n\nDefine input features that are conditionally active."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/index.html#available-tutorials",
    "href": "build/lib/docs/tutorials/advanced_examples/index.html#available-tutorials",
    "title": "Advanced Examples",
    "section": "",
    "text": "Create custom single-objective Bayesian optimization strategies.\n\n\n\nWorking with desirability functions for multi-criteria optimization.\n\n\n\nUsing genetic algorithms for optimization in BoFire.\n\n\n\nTechniques for combining multiple objectives in optimization.\n\n\n\nLeveraging multiple fidelity levels for efficient optimization.\n\n\n\nDefining optimization objectives directly on input parameters.\n\n\n\nUsing Random Forest as a surrogate model instead of Gaussian Processes.\n\n\n\nApplying transfer learning techniques to Bayesian optimization.\n\n\n\nDefine input features that are conditionally active."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/multifidelity_bo.html",
    "href": "build/lib/docs/tutorials/advanced_examples/multifidelity_bo.html",
    "title": "",
    "section": "",
    "text": "Code\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Ackley, Benchmark, Branin\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import TaskInput\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, MultiTaskGPSurrogate\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_INIT_HF = 4\nNUM_INIT_LF = 10\nif SMOKE_TEST:\n    num_runs = 5\n    num_iters = 2\n    verbose = False\nelse:\n    num_runs = 10\n    num_iters = 10\n    verbose = True\nThis notebook is a sequel to “Transfer Learning in BO”."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/multifidelity_bo.html#problem-definition",
    "href": "build/lib/docs/tutorials/advanced_examples/multifidelity_bo.html#problem-definition",
    "title": "",
    "section": "Problem definition",
    "text": "Problem definition\nWe use the same problem as the transfer learning notebook; optimizing the Branin benchmark, with a low-fidelity function biased by the Ackley function (with fewer initial points, to demonstrate the strength of being able to query low fidelities). Below, we define the problem domain, and the strategies we will use to optimize.\nAs a baseline, we use the SoboStrategy with the MultiTaskSurrogate, as in the previous notebook. We also introduce the MultiFidelityStrategy here, which uses the same surrogate, but is able to query the lower fidelity functions using a variance-based acquisition function [Kandasamy et al. 2016, Folch et al. 2023].\nBoth strategies first select a design point \\(x\\) by optimizing the target fidelity. The MultiFidelityStrategy then selects the fidelity, \\(m\\), by selecting the lowest fidelity that has a variance over a fixed threshold. This means that the strategy will explore the cheapest fidelities first, and only query the expensive fidelities when there is no information to be gained by the cheap approximations.\n\nclass BraninMultiTask(Benchmark):\n    def __init__(self, low_fidelity_allowed=False, **kwargs):\n        super().__init__(**kwargs)\n        self._branin = Branin()\n        self._ackley = Ackley()\n        task_input = TaskInput(\n            key=\"task\",\n            categories=[\"task_hf\", \"task_lf\"],\n            allowed=[True, low_fidelity_allowed],\n            fidelities=[0, 1],\n        )\n        self._domain = Domain(\n            inputs=self._branin.domain.inputs + (task_input,),\n            outputs=self._branin.domain.outputs,\n        )\n\n    def _f(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:\n        candidates_no_task = candidates.drop(columns=[\"task\"])\n        f_branin = self._branin.f(candidates_no_task)\n        f_ackley = self._ackley.f(candidates_no_task)\n        bias_scale = np.where(candidates[\"task\"] == \"task_hf\", 0.0, 0.15).reshape(-1, 1)\n        bias_scale = pd.DataFrame(bias_scale, columns=self._domain.outputs.get_keys())\n        bias_scale[\"valid_y\"] = 0.0\n        return f_branin + bias_scale * f_ackley\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        optima = self._branin.get_optima()\n        optima[\"task\"] = \"task_hf\"\n        return optima\n\n\nmf_benchmark = BraninMultiTask(low_fidelity_allowed=True)\ntl_benchmark = BraninMultiTask(low_fidelity_allowed=False)\n\n\ndef create_data_set(seed: int):\n    # use the tl_benchmark to sample without the low fidelity\n    experiments = tl_benchmark.domain.inputs.sample(\n        NUM_INIT_HF + NUM_INIT_LF, seed=seed\n    )\n    experiments[\"task\"] = np.where(\n        experiments.index &lt; NUM_INIT_LF, \"task_lf\", \"task_hf\"\n    )\n\n    # then use the ml_benchmark to evaluate the low fidelity\n    return mf_benchmark.f(experiments, return_complete=True)\n\n\ncreate_data_set(0)\n\n\n\n\n\n\n\n\nx_1\nx_2\ntask\ny\nvalid_y\n\n\n\n\n0\n-2.236178\n5.665713\ntask_lf\n26.409237\n1.0\n\n\n1\n-2.139889\n8.469326\ntask_lf\n9.288236\n1.0\n\n\n2\n1.124986\n10.645193\ntask_lf\n55.820241\n1.0\n\n\n3\n5.262999\n13.213141\ntask_lf\n161.890497\n1.0\n\n\n4\n-1.920241\n6.790379\ntask_lf\n16.125827\n1.0\n\n\n5\n0.962901\n2.647478\ntask_lf\n20.232801\n1.0\n\n\n6\n8.704473\n8.641214\ntask_lf\n50.234493\n1.0\n\n\n7\n7.233487\n7.940435\ntask_lf\n62.729505\n1.0\n\n\n8\n3.697975\n1.518759\ntask_lf\n3.272981\n1.0\n\n\n9\n3.066951\n9.750911\ntask_lf\n57.729416\n1.0\n\n\n10\n1.859665\n14.993475\ntask_hf\n139.663235\n1.0\n\n\n11\n6.712335\n7.141519\ntask_hf\n54.780215\n1.0\n\n\n12\n-1.488379\n4.954397\ntask_hf\n24.485001\n1.0\n\n\n13\n9.025726\n1.709098\ntask_hf\n1.354707\n1.0\n\n\n\n\n\n\n\n\nfrom bofire.data_models.strategies.api import MultiFidelityStrategy\n\n\n# It isn't necessary to define the surrogate specs here, as the MFStrategy\n# will use a MultiTaskGP by default.\n\nmf_data_model = MultiFidelityStrategy(\n    domain=mf_benchmark.domain,\n    acquisition_function=qLogEI(),\n    fidelity_thresholds=0.1,\n)\nmf_data_model.surrogate_specs.surrogates[0].inputs\n\nInputs(type='Inputs', features=[ContinuousInput(type='ContinuousInput', key='x_1', unit=None, bounds=[-5.0, 10.0], local_relative_bounds=None, stepsize=None, allow_zero=False), ContinuousInput(type='ContinuousInput', key='x_2', unit=None, bounds=[0.0, 15.0], local_relative_bounds=None, stepsize=None, allow_zero=False), TaskInput(type='TaskInput', key='task', categories=['task_hf', 'task_lf'], allowed=[True, True], fidelities=[0, 1])])\n\n\n\nfrom bofire.data_models.strategies.api import SoboStrategy\n\n\nsurrogate_specs = BotorchSurrogates(\n    surrogates=[\n        MultiTaskGPSurrogate(\n            inputs=tl_benchmark.domain.inputs,\n            outputs=tl_benchmark.domain.outputs,\n        )\n    ]\n)\n\ntl_data_model = SoboStrategy(\n    domain=tl_benchmark.domain,\n    acquisition_function=qLogEI(),\n    surrogate_specs=surrogate_specs,\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/octane_number.html",
    "href": "build/lib/docs/tutorials/advanced_examples/octane_number.html",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "",
    "text": "This tutorial shows how to predict the Motor Octane Number (MON) of a hydrocarbon mixture. The dataset stems from the paper from Chew et al.. It contains 722 experiments of up to 121 componet mixtures of 423 individual molecules. Each molecule is represented by its SMILES string."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/octane_number.html#imports",
    "href": "build/lib/docs/tutorials/advanced_examples/octane_number.html#imports",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "Imports",
    "text": "Imports\n\nimport os\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.data.octane_number import get_octane_data\nfrom bofire.data_models.domain.api import EngineeredFeatures, Inputs, Outputs\nfrom bofire.data_models.features.api import (\n    ContinuousMolecularInput,\n    ContinuousOutput,\n    MolecularWeightedSumFeature,\n)\nfrom bofire.data_models.molfeatures.api import MordredDescriptors\nfrom bofire.data_models.molfeatures.names import mordred as mordred_names\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/octane_number.html#setup-data",
    "href": "build/lib/docs/tutorials/advanced_examples/octane_number.html#setup-data",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "Setup Data",
    "text": "Setup Data\n\ndf_experiments = get_octane_data()\ndf_experiments[\"valid_MON\"] = 1\n\noutput_key = \"MON\"\n\ninputs = Inputs(\n    features = [\n        ContinuousMolecularInput(key=col, molecule=col, bounds=(0,1))\n        for col in df_experiments.columns\n        if col not in [\"MON\", \"Label\", \"valid_MON\"]\n    ]\n)\noutputs = Outputs(features=[ContinuousOutput(key=output_key)])\n\n/tmp/ipykernel_3049/4204084361.py:2: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`"
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/octane_number.html#setup-surrogate-and-perform-cv",
    "href": "build/lib/docs/tutorials/advanced_examples/octane_number.html#setup-surrogate-and-perform-cv",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "Setup Surrogate and perform CV",
    "text": "Setup Surrogate and perform CV\nWe model the high-dimensional problem by using an engineered feature called MolecularWeightedSumFeature. In computes the weighted sum of the molecular descriptors of the original ContinuousMolecularInputs that make up the engineered feature. Here we use Mordred descriptors with a correlation cutoff of 0.9.\n\nsurrogate_data = SingleTaskGPSurrogate(\n    inputs=inputs,\n    outputs=outputs,\n    engineered_features = EngineeredFeatures(\n        features=[\n                MolecularWeightedSumFeature(\n                    key=\"mixture\",\n                    features=inputs.get_keys(),\n                    molfeatures=MordredDescriptors(descriptors=mordred_names,ignore_3D=False, correlation_cutoff=0.9),\n                    keep_features=False\n                )\n            ]\n        )\n)\n\nprint(\"Number of molecular features before correlation filtering: \", len(surrogate_data.engineered_features[0].molfeatures.get_descriptor_names()))\nsurrogate = surrogates.map(surrogate_data)\ncv_train, cv_test, _ = surrogate.cross_validate(df_experiments, folds=10 if not SMOKE_TEST else 3)\n\ndisplay(cv_test.get_metrics())\n\nprint(\"Number of molecular features before correlation filtering: \", len(surrogate_data.engineered_features[0].molfeatures.get_descriptor_names()))\n\nNumber of molecular features before correlation filtering:  1826\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n4.184636\n67.906546\n0.786504\n5.589400e+14\n0.888642\n0.893881\n3.222481e-115\n\n\n\n\n\n\n\nNumber of molecular features before correlation filtering:  394\n\n\nEven better performance can be achieved by using SAAS based surrogates, like AdditiveMapSaasSingleTaskGPSurrogate or EnsembleMapSaasSingleTaskGPSurrogate. Drawback are higher computational costs."
  },
  {
    "objectID": "build/lib/docs/tutorials/advanced_examples/transfer_learning_bo.html",
    "href": "build/lib/docs/tutorials/advanced_examples/transfer_learning_bo.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.api import Ackley, Branin\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput, TaskInput\nfrom bofire.data_models.objectives.api import MaximizeObjective\nfrom bofire.data_models.strategies.api import SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    MultiTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\n\n\nimport matplotlib.pyplot as plt\n\nIn this notebook we show how to use BoFire for the purposes of transfer learning Bayesian optimization. In particular, we assume we have a task \\(f_2\\) with data that is relevant to the optimization of our current task \\(f_1\\). The procedure is simple, we fit a MultiTask GP to both data-sets, however only carry out the BO on \\(f_1\\), i.e., we optimize the acquisition functions on on the task \\(f_1\\).\nWe build a small data-set using the target task:\n\\[ f_1(x) = \\sin(2 \\pi x) \\]\nAnd we will have data the second related task:\n\\[ f_2 = 0.9 \\sin(2 \\pi x) + 0.2 \\cos(3 \\pi x) - 0.2 \\]\nWe begin by defining the functions, generating some data, and plotting it. We generate 15 data-points for Task 2 and just 4 data-points for Task 1, all the data-points in Task 1 will be in a restricted area of the space.\n\ndef task_1_f(x):\n    return np.sin(x * 2 * np.pi)\n\n\ndef task_2_f(x):\n    return 0.9 * np.sin(x * 2 * np.pi) - 0.2 + 0.2 * np.cos(x * 3 * np.pi)\n\n\nx = np.linspace(0, 1, 101)\n\n# generate lots of low fidelity data and a few high fidelity data\n\ntask_1_x = np.linspace(0.6, 1, 4)\ntask_1_y = task_1_f(task_1_x)\n\ntask_2_x = np.linspace(0, 1, 15)\ntask_2_y = task_2_f(task_2_x)\n\n# set the data in the pandas format\nexperiments = pd.DataFrame(\n    {\n        \"x\": np.concatenate([task_1_x, task_2_x]),\n        \"y\": np.concatenate([task_1_y, task_2_y]),\n        \"task\": [\"task_1\"] * len(task_1_x) + [\"task_2\"] * len(task_2_x),\n    },\n)\n\nplt.figure(figsize=(6, 4))\n\nplt.scatter(task_1_x, task_1_y, label=\"Task 1 data\", color=\"red\")\nplt.scatter(task_2_x, task_2_y, label=\"Task 2 data\", color=\"blue\")\n\nplt.plot(x, task_1_f(x), label=\"Task 1\", color=\"red\")\nplt.plot(x, task_2_f(x), label=\"Task 2\", color=\"blue\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nInference\nAt first we will show to do inference with the model and see make predictions using multiple data-sets.\nWe first set-up the model according to BoFire’s API, by defining the set of input and output features and the corresponding bounds, and create a surrogate data model:\n\nTo define the task we choose the TaskInput feature, everything else follows standard BoFire procedure.\n\n\n# set-up the task model with allowed variable as [\"True\"] for the target task and [\"False\"] for the other task\ntask_input = TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"])\n# define the input features\ninput_features = [ContinuousInput(key=\"x\", bounds=(0, 1)), task_input]\n\nobjective = MaximizeObjective(w=1)\noutput_features = [ContinuousOutput(key=\"y\", objective=objective)]\n\ninputs = Inputs(features=input_features)\noutputs = Outputs(features=output_features)\n\nsurrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\n\nWe map from the surrogate data into the surrogate model and fit the data.\n\nsurrogate = surrogates.map(surrogate_data)\n\nsurrogate.fit(experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/interpolation.py:71: UserWarning:\n\ntorch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:654.)\n\n\n\nPlot to see how we are able to predict outside of the region where there is data for Task 1, since we can use the data from Task 2 and the learnt correlations:\n\n# predict the high fidelity data\nx_predict = np.linspace(0, 1, 101)\ny_predict = surrogate.predict(\n    pd.DataFrame({\"x\": x_predict, \"task\": [\"task_1\"] * len(x_predict)}),\n)\n\n# plot data and predictions\nplt.plot(x_predict, y_predict[\"y_pred\"], label=\"Predictions\", color=\"green\")\nplt.fill_between(\n    x_predict,\n    y_predict[\"y_pred\"] - 2 * y_predict[\"y_sd\"],\n    y_predict[\"y_pred\"] + 2 * y_predict[\"y_sd\"],\n    color=\"green\",\n    alpha=0.2,\n)\n\n# plot the high fidelity function\nplt.plot(x, task_1_f(x), label=\"Task 1\", color=\"red\")\n\n# plot the data too\nplt.scatter(\n    experiments[experiments[\"task\"] == \"task_1\"][\"x\"],\n    experiments[experiments[\"task\"] == \"task_1\"][\"y\"],\n    label=\"Task 1 data\",\n    color=\"red\",\n)\nplt.scatter(\n    experiments[experiments[\"task\"] == \"task_2\"][\"x\"],\n    experiments[experiments[\"task\"] == \"task_2\"][\"y\"],\n    label=\"Task 2 data\",\n    color=\"blue\",\n)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nTransfer Learning Bayesian Optimisation\nLet us now integrate this into BoFire’s SOBO strategy. This can be done by following the standard BoFire syntax with a small modification.\n\nFor TaskInput we must set the variable allowed as a list, with each element in the list corresponding to one of the categories such that all auxiliary tasks have False and target task has True. For example, we have categories = [\"task_1, task_2\"] and the goal of our optimization is to optimize task_1 therefore we set allowed = [True, False]:\n\n\ninput_features = [\n    ContinuousInput(key=\"x\", bounds=(0, 1)),\n    TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"], allowed=[True, False]),\n]\n\nobjective = MaximizeObjective(w=1)\noutput_features = [ContinuousOutput(key=\"y\", objective=objective)]\n\ninputs = Inputs(features=input_features)\noutputs = Outputs(features=output_features)\n\nsurrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\nsurrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n# define the acquisition function\nacquisition = qLogEI()\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=Domain(\n        inputs=inputs,\n        outputs=outputs,\n    ),\n    acquisition_function=acquisition,\n    surrogate_specs=surrogate_specs,\n)\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\n\nsobo_strategy.tell(experiments)\n\nWe can now generate experimental candidates:\n\ncandidates = sobo_strategy.ask(3)\n\ncandidates\n\n\n\n\n\n\n\n\nx\ntask\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n0.214442\ntask_1\n0.905666\n0.192970\n0.905666\n\n\n1\n0.167622\ntask_1\n0.907404\n0.198502\n0.907404\n\n\n2\n0.189097\ntask_1\n0.916256\n0.196217\n0.916256\n\n\n\n\n\n\n\nIf we instead wanted to optimize task_2 instead of task_1, we simply change allowed = [False, True]:\n\ninput_features = [\n    ContinuousInput(key=\"x\", bounds=(0, 1)),\n    TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"], allowed=[False, True]),\n]\n\nobjective = MaximizeObjective(w=1)\noutput_features = [ContinuousOutput(key=\"y\", objective=objective)]\n\ninputs = Inputs(features=input_features)\noutputs = Outputs(features=output_features)\n\nsurrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\nsurrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n# define the acquisition function\nacquisition = qLogEI()\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=Domain(\n        inputs=inputs,\n        outputs=outputs,\n    ),\n    acquisition_function=acquisition,\n    surrogate_specs=surrogate_specs,\n)\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\n\nsobo_strategy.tell(experiments)\n\nWe now obtain candidates for task_2:\n\ncandidate = sobo_strategy.ask(1)\n\ncandidate\n\n\n\n\n\n\n\n\nx\ntask\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n0.197146\ntask_2\n0.594692\n0.026346\n0.594692\n\n\n\n\n\n\n\nLet us now run a Bayesian optimization loop on the Branin benchmark to show the usefulness of transfer learning Bayesian optimization in a practical setting. We create a small data-set composed of the Branin benchmark itself, and a large one composed of the Branin function with a small amount of bias added by summing the Branin and Ackley functions.\nWe begin by defining a function that creates random initial data-sets, and create as many data-sets as the number of runs we want to average over:\n\nbenchmark = Branin()\nbias = Ackley()\n\n\ndef create_data_set():\n    # choose the initial data-sets\n    low_fidelity_x = benchmark.domain.inputs.sample(25)\n    high_fidelity_x = benchmark.domain.inputs.sample(4)\n\n    # create the observations\n    high_fidelity_data = benchmark.f(high_fidelity_x, return_complete=True)\n    low_fidelity_bias = bias.f(low_fidelity_x, return_complete=True)\n\n    low_fidelity_data = benchmark.f(low_fidelity_x, return_complete=True)\n    low_fidelity_data[\"y\"] = low_fidelity_data[\"y\"] + 0.15 * low_fidelity_bias[\"y\"]\n\n    # create a joint data-set, with the task variable\n    high_fidelity_data[\"task\"] = \"task_1\"\n    low_fidelity_data[\"task\"] = \"task_2\"\n\n    experiments_joint = pd.concat([low_fidelity_data, high_fidelity_data])\n\n    return high_fidelity_data, experiments_joint\n\n\nsingle_task_all_regrets = []\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nif SMOKE_TEST:\n    num_runs = 5\n    num_iters = 2\n    verbose = False\nelse:\n    num_runs = 10\n    num_iters = 10\n    verbose = True\n\n# create the initial data-sets for each run\n\nhigh_fidelity_datasets = []\nexperiments_joint_datasets = []\n\nfor _ in range(num_runs):\n    high_fidelity_data, experiments_joint = create_data_set()\n    high_fidelity_datasets.append(high_fidelity_data)\n    experiments_joint_datasets.append(experiments_joint)\n\nLet us now run a Bayesian optimization loop only using the high-fidelity data:\n\nfor run in range(num_runs):\n    high_fidelity_data = high_fidelity_datasets[run]\n\n    inputs = benchmark.domain.inputs\n    outputs = benchmark.domain.outputs\n\n    surrogate_data = SingleTaskGPSurrogate(inputs=inputs, outputs=outputs)\n    surrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n    acquisition = qLogEI()\n\n    sobo_strategy_data_model = SoboStrategy(\n        domain=Domain(\n            inputs=inputs,\n            outputs=outputs,\n        ),\n        acquisition_function=acquisition,\n        surrogate_specs=surrogate_specs,\n    )\n\n    sobo_strategy = strategies.map(sobo_strategy_data_model)\n\n    dataset = high_fidelity_data.drop(columns=[\"task\"])\n\n    sobo_strategy.tell(dataset)\n\n    regrets_single_task = []\n\n    init_regret = (\n        sobo_strategy.experiments[\"y\"][sobo_strategy.experiments[\"y\"].argmin()]\n        - benchmark.get_optima()[\"y\"][0].item()\n    )\n    regrets_single_task.append(init_regret)\n\n    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n    for _iter in pbar:\n        candidate = sobo_strategy.ask(1)\n        y = benchmark.f(candidate, return_complete=True)\n        sobo_strategy.tell(y)\n\n        regret = (\n            sobo_strategy.experiments[\"y\"][sobo_strategy.experiments[\"y\"].argmin()]\n            - benchmark.get_optima()[\"y\"][0].item()\n        )\n        regrets_single_task.append(regret)\n\n        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n\n    single_task_all_regrets.append(regrets_single_task)\n\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=24.3633]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.81it/s, Regret=24.3633]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:00,  1.81it/s, Regret=24.3633]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.37it/s, Regret=24.3633]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.42it/s, Regret=24.3633]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=15.3041]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.00it/s, Regret=15.3041]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:00,  1.00it/s, Regret=15.3041]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.32it/s, Regret=15.3041]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.26it/s, Regret=15.3041]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=18.9881]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.55it/s, Regret=18.9881]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:00,  1.55it/s, Regret=18.9881]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.26it/s, Regret=18.9881]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.30it/s, Regret=18.9881]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=27.3195]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.39it/s, Regret=27.3195]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:00,  1.39it/s, Regret=27.3195]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.74it/s, Regret=27.3195]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.68it/s, Regret=27.3195]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=4.6112]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.81it/s, Regret=4.6112]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:00,  1.81it/s, Regret=4.6112]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.36it/s, Regret=4.6112]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.41it/s, Regret=4.6112]\n\n\nWe now repeat the experiment but using transfer learning BO:\n\nmultitask_all_regrets = []\n\nfor run in range(num_runs):\n    experiments_joint = experiments_joint_datasets[run]\n\n    input_features = benchmark.domain.inputs.features + [\n        TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"], allowed=[True, False]),\n    ]\n    inputs = Inputs(features=input_features)\n    outputs = benchmark.domain.outputs\n\n    surrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\n    surrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n    acquisition = qLogEI()\n\n    sobo_strategy_data_model = SoboStrategy(\n        domain=Domain(\n            inputs=inputs,\n            outputs=outputs,\n        ),\n        acquisition_function=acquisition,\n        surrogate_specs=surrogate_specs,\n    )\n\n    sobo_strategy = strategies.map(sobo_strategy_data_model)\n\n    dataset = experiments_joint.copy()\n\n    sobo_strategy.tell(dataset)\n\n    regrets_transfer_learning = []\n\n    # obtain experiments at the highest fidelity\n    experiments = sobo_strategy.experiments[\n        sobo_strategy.experiments[\"task\"] == \"task_1\"\n    ][\"y\"]\n    init_regret = (\n        experiments[experiments.argmin()] - benchmark.get_optima()[\"y\"][0].item()\n    )\n    regrets_transfer_learning.append(init_regret)\n\n    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n    for _iter in pbar:\n        candidate = sobo_strategy.ask(1)\n        candidate = candidate.drop(columns=[\"task\"])\n        y = benchmark.f(candidate, return_complete=True)\n        y[\"task\"] = \"task_1\"\n        sobo_strategy.tell(y)\n\n        experiments = sobo_strategy.experiments[\n            sobo_strategy.experiments[\"task\"] == \"task_1\"\n        ][\"y\"].reset_index(drop=True)\n        regret = (\n            experiments[experiments.argmin()] - benchmark.get_optima()[\"y\"][0].item()\n        )\n        regrets_transfer_learning.append(regret)\n\n        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n\n    multitask_all_regrets.append(regrets_transfer_learning)\n\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:02&lt;?, ?it/s, Regret=1.6021]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:02,  2.15s/it, Regret=1.6021]Optimizing:  50%|█████     | 1/2 [00:04&lt;00:02,  2.15s/it, Regret=1.6021]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.26s/it, Regret=1.6021]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.24s/it, Regret=1.6021]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:02&lt;?, ?it/s, Regret=1.6659]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:02,  2.91s/it, Regret=1.6659]Optimizing:  50%|█████     | 1/2 [00:06&lt;00:02,  2.91s/it, Regret=1.6659]Optimizing: 100%|██████████| 2/2 [00:06&lt;00:00,  3.05s/it, Regret=1.6659]Optimizing: 100%|██████████| 2/2 [00:06&lt;00:00,  3.03s/it, Regret=1.6659]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:01&lt;?, ?it/s, Regret=0.6765]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:01,  1.91s/it, Regret=0.6765]Optimizing:  50%|█████     | 1/2 [00:03&lt;00:01,  1.91s/it, Regret=0.6765]Optimizing: 100%|██████████| 2/2 [00:03&lt;00:00,  1.92s/it, Regret=0.6765]Optimizing: 100%|██████████| 2/2 [00:03&lt;00:00,  1.91s/it, Regret=0.6765]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:02&lt;?, ?it/s, Regret=1.6959]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:02,  2.42s/it, Regret=1.6959]Optimizing:  50%|█████     | 1/2 [00:04&lt;00:02,  2.42s/it, Regret=1.6134]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.10s/it, Regret=1.6134]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.15s/it, Regret=1.6134]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:03&lt;?, ?it/s, Regret=1.6253]Optimizing:  50%|█████     | 1/2 [00:03&lt;00:03,  3.09s/it, Regret=1.6253]Optimizing:  50%|█████     | 1/2 [00:06&lt;00:03,  3.09s/it, Regret=1.6253]Optimizing: 100%|██████████| 2/2 [00:06&lt;00:00,  3.32s/it, Regret=1.6253]Optimizing: 100%|██████████| 2/2 [00:06&lt;00:00,  3.28s/it, Regret=1.6253]\n\n\nWe now plot the quantiles and median simple regret against iteration:\n\n# plot the results\nplt.figure(figsize=(6, 4))\n\nregrets_single_task_median = np.median(np.array(single_task_all_regrets), axis=0)\nregrets_transfer_learning_median = np.median(np.array(multitask_all_regrets), axis=0)\n\n# get the 25 and 75 percentiles\nregrets_single_task_upper_quantile = np.quantile(\n    np.array(single_task_all_regrets),\n    0.75,\n    axis=0,\n)\nregrets_single_task_lower_quantile = np.quantile(\n    np.array(single_task_all_regrets),\n    0.25,\n    axis=0,\n)\n\nregrets_transfer_learning_upper_quantile = np.quantile(\n    np.array(multitask_all_regrets),\n    0.75,\n    axis=0,\n)\nregrets_transfer_learning_lower_quantile = np.quantile(\n    np.array(multitask_all_regrets),\n    0.25,\n    axis=0,\n)\n\nplt.plot(regrets_single_task_median, label=\"Single task\", color=\"red\")\nplt.plot(regrets_transfer_learning_median, label=\"Transfer learning\", color=\"blue\")\n\nplt.fill_between(\n    np.arange(num_iters + 1),\n    regrets_single_task_upper_quantile,\n    regrets_single_task_lower_quantile,\n    color=\"red\",\n    alpha=0.2,\n)\nplt.fill_between(\n    np.arange(num_iters + 1),\n    regrets_transfer_learning_upper_quantile,\n    regrets_transfer_learning_lower_quantile,\n    color=\"blue\",\n    alpha=0.2,\n)\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Regret\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that using transfer learning leads to significant improvement."
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Outlier_Detection_and_Robust_GP.html",
    "href": "build/lib/docs/tutorials/basic_examples/Outlier_Detection_and_Robust_GP.html",
    "title": "Outlier Detection and Robust GP",
    "section": "",
    "text": "This notebook shows how to use RobustSingleTaskGPSurrogate in Bofire to autmomatically detect outliers in your data and/or fit Gaussian process models robust to outliers.\nIt is based on the Robust Gaussian Processes via Relevance Pursuit paper and is based on the accompanying implementation and tutorial in BoTorch\nIn this approach, the typical GP observation noise \\(\\sigma^2\\) is extended with data-point-specific noise variances \\(\\rho\\). A prior distribution is placed over the number of outliers \\(S\\), and through a sequential greedy optimization algorithm (see for details the links above) a list of models with variying sparsity levels \\(|S|\\) are obtained. The most promising model can then be selected through Bayesian model selection.\nThis tutorial will show how to use the model, how to obtain the data-point specific noise levels, and how to obtain the full model trace.\n\nImports\n\n# Model imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import Tensor\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.domain.api import Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.surrogates.api import (\n    RobustSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\n\n\n\nSetting up a Synthetic example\n\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i+1}\", bounds=(-4, 4)) for i in range(1)],\n)\noutput_features = Outputs(features=[ContinuousOutput(key=\"y_1\")])\n\nexperiments = input_features.sample(n=10)\nexperiments[\"y_1\"] = np.sin(experiments[\"x_1\"])\n\nexperiments[\"valid_y_1\"] = 1\nexperiments[\"valid_y_2\"] = 1\n\n# prediction grid\nx = pd.DataFrame(pd.Series(np.linspace(-4, 4, 100), name=\"x_1\"))\n\n\n\nLet’s add two clear outliers\n\nexperiments.loc[0, \"y_1\"] = 5\nexperiments.loc[1, \"y_1\"] = -5\n\n\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\")\n\n# plot the last two points in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTesting a SingleTaskGP\n\nstgp_data_model = SingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n)\n\nstgp_model = surrogates.map(data_model=stgp_data_model)\nstgp_model.fit(experiments)\nstgp_predictions = stgp_model.predict(x)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:179: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n# plot the surrogate\nplt.figure(figsize=(10, 5))\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\", label=\"observations\")\n\n# plot the outliers in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\nplt.plot(x[\"x_1\"], stgp_predictions[\"y_1_pred\"], label=\"predictions\")\nplt.fill_between(\n    x[\"x_1\"],\n    stgp_predictions[\"y_1_pred\"] - 2 * stgp_predictions[\"y_1_sd\"],\n    stgp_predictions[\"y_1_pred\"] + 2 * stgp_predictions[\"y_1_sd\"],\n    alpha=0.2,\n    label=\"95% confidence interval\",\n)\nplt.xlabel(\"x_1\")\nplt.ylabel(\"y_1\")\nplt.title(\"Single Task GP Predictions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTesting the Robust GP\n\ndata_model = RobustSingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n)\n\nmodel = surrogates.map(data_model=data_model)\nmodel.fit(experiments)\npredictions = model.predict(x)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/relevance_pursuit.py:156: UserWarning:\n\nConverting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\nConsider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/fit.py:214: OptimizationWarning:\n\n`scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL: \n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/likelihoods/sparse_outlier_noise.py:104: InputDataWarning:\n\nSparseOutlierNoise: Robust rho not applied because the last dimension of the base noise covariance (100) is not compatible with the last dimension of rho (10). This can happen when the model posterior is evaluated on test data.\n\n\n\n\n# plot the surrogate\nplt.figure(figsize=(10, 5))\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\", label=\"observations\")\nplt.plot(x[\"x_1\"], predictions[\"y_1_pred\"], label=\"predictions\")\n\n# plot the outliers in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\nplt.fill_between(\n    x[\"x_1\"],\n    predictions[\"y_1_pred\"] - 2 * predictions[\"y_1_sd\"],\n    predictions[\"y_1_pred\"] + 2 * predictions[\"y_1_sd\"],\n    alpha=0.2,\n    label=\"95% confidence interval\",\n)\nplt.xlabel(\"x_1\")\nplt.ylabel(\"y_1\")\nplt.title(\"Robust Single Task GP Predictions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nReturning data point specific noise values \\(\\rho\\)\n\nmodel.predict_outliers(experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/fit.py:214: OptimizationWarning:\n\n`scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL: \n\n\n\n\n\n\n\n\n\n\ny_1_pred\ny_1_sd\ny_1_rho\n\n\n\n\n0\n-0.306048\n0.213447\n4.611599\n\n\n1\n-0.850602\n0.762216\n2.843414\n\n\n2\n-0.203744\n0.089320\n0.000000\n\n\n3\n1.009482\n0.094738\n0.000000\n\n\n4\n0.640685\n0.099127\n0.000000\n\n\n5\n-0.024140\n0.089344\n0.000000\n\n\n6\n1.005839\n0.093556\n0.000000\n\n\n7\n-0.601878\n0.093834\n0.000000\n\n\n8\n-1.001488\n0.093194\n0.000000\n\n\n9\n-1.010764\n0.095201\n0.000000\n\n\n\n\n\n\n\n\n\nPlotting the data point specific noise values\n\n# Get the outlier scores (rho) from the robust GP model\noutlier_scores = model.predict_outliers(experiments)\n\n# Plot the surrogate predictions\nplt.figure(figsize=(10, 5))\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\", label=\"observations\")\nplt.plot(x[\"x_1\"], predictions[\"y_1_pred\"], label=\"predictions\")\n\n# plot the outliers in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\n# Plot sqrt(rho) as error bars on the observations\nplt.errorbar(\n    experiments[\"x_1\"],\n    experiments[\"y_1\"],\n    yerr=outlier_scores[\"y_1_rho\"].values,\n    fmt=\"none\",\n    ecolor=\"orange\",\n    alpha=0.7,\n    label=\"rho\",\n)\n\nplt.fill_between(\n    x[\"x_1\"],\n    predictions[\"y_1_pred\"] - 2 * predictions[\"y_1_sd\"],\n    predictions[\"y_1_pred\"] + 2 * predictions[\"y_1_sd\"],\n    alpha=0.2,\n    label=\"95% confidence interval\",\n)\nplt.xlabel(\"x_1\")\nplt.ylabel(\"y_1\")\nplt.title(\"Robust Single Task GP Predictions with sqrt(rho) errorbars\")\nplt.legend()\nplt.show()\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/fit.py:214: OptimizationWarning:\n\n`scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL: \n\n\n\n\n\n\n\n\n\n\n\n\nHyperparameter optimization\nThe RobustSingleTaskGPSurrogate also support hyperparameter optimization over different kernels, priors, etc.\n\n# opt_surrogate_data, perf = hyperoptimize(surrogate_data=data_model, training_data=experiments, folds=4)\n# surrogate = surrogates.map(opt_surrogate_data)\n\n\n\nObtaining the full model trace\n\ndef bmc_plot(bmc_support_sizes: Tensor, bmc_probabilities: Tensor) -&gt; None:\n    cmap = plt.colormaps[\"viridis\"]\n    bar_width = 1\n    plt.title(\"Model Evidence\")\n    for i, ss in enumerate(bmc_support_sizes):\n        color = cmap((1 - (len(bmc_support_sizes) - i) / (2 * len(bmc_support_sizes))))\n        plt.bar(ss, bmc_probabilities[i], color=color, width=bar_width)\n\n    i = bmc_probabilities.argmax()\n    map_color = cmap((1 - (len(bmc_support_sizes) - i) / (2 * len(bmc_support_sizes))))\n    plt.bar(\n        bmc_support_sizes[i],\n        bmc_probabilities[i],\n        color=map_color,\n        label=\"MAP\",\n        edgecolor=\"black\",\n        linewidth=1.5,\n        width=bar_width,\n    )\n\n    support_prior = torch.exp(-bmc_support_sizes / model.model.prior_mean_of_support)\n    support_prior = support_prior / support_prior.sum()\n    plt.plot(bmc_support_sizes, support_prior, \"D\", color=\"black\", label=\"Prior\", ms=2)\n\n    plt.xlabel(\n        f\"Support Size (Prior = Exponential(mean={model.model.prior_mean_of_support:.1f}))\"\n    )\n    plt.ylabel(\"Posterior Marginal Likelihood (%)\")\n    plt.legend(loc=\"upper right\")\n\n\ndata_model = RobustSingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n    cache_model_trace=True,\n)\n\nmodel = surrogates.map(data_model=data_model)\nmodel.fit(experiments)\npredictions = model.predict(x)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/fit.py:214: OptimizationWarning:\n\n`scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL: \n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/fit.py:214: OptimizationWarning:\n\n`scipy_minimize` terminated with status OptimizationStatus.FAILURE, displaying original message from `scipy.optimize.minimize`: ABNORMAL: \n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/likelihoods/sparse_outlier_noise.py:104: InputDataWarning:\n\nSparseOutlierNoise: Robust rho not applied because the last dimension of the base noise covariance (100) is not compatible with the last dimension of rho (10). This can happen when the model posterior is evaluated on test data.\n\n\n\n\nfigsize = (6, 4)\nfig = plt.figure(dpi=100, figsize=figsize)\nbmc_plot(model.model.bmc_support_sizes.detach(), model.model.bmc_probabilities.detach())"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html",
    "href": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html",
    "title": "Classification Surrogate Tests",
    "section": "",
    "text": "We are interested in testing whether or not a surrogate model can correctly identify unknown constraints based on categorical criteria with classification surrogates. Essentially, we want to account for scenarios where specialists can look at a set of experiments and label outcomes as ‘acceptable’, ‘unacceptable’, ‘ideal’, etc.\nThis involves new models that produce CategoricalOutput’s rather than continuous outputs. Mathematically, if \\(g_{\\theta}:\\mathbb{R}^d\\to[0,1]^c\\) represents the function governed by learnable parameters \\(\\theta\\) which outputs a probability vector over \\(c\\) potential classes (i.e. for input \\(x\\in\\mathbb{R}^d\\), \\(g_{\\theta}(x)^\\top\\mathbf{1}=1\\) where \\(\\mathbf{1}\\) is the vector of all 1’s) and we have acceptibility criteria for the corresponding classes given by \\(a\\in\\{0,1\\}^c\\), we can compute the scalar output \\(g_{\\theta}(x)^\\top a\\in[0,1]\\) which represents the expected value of acceptance as an objective value to be passed in as a constrained function.\nIn this script, we look at the Rosenbrock function constrained to a disk which attains a global minima at \\((x_0^*,x_1^*)=(1.0, 1.0)\\). To facilitate testing the functionality offered by BoFire, we label all points inside of the circle \\(x_0^2+x_1^2\\le2\\) as ‘acceptable’ and further label anything inside of the intersection of this circle and the circle \\((x_0-1)^2+(x_1-1)^2\\le1.0\\) as ‘ideal’; points lying outside of these two locations are labeled as “unacceptable.”\n# Import packages\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import (\n    CategoricalInput,\n    CategoricalOutput,\n    ContinuousInput,\n    ContinuousOutput,\n)\nfrom bofire.data_models.objectives.api import (\n    ConstrainedCategoricalObjective,\n    MinimizeObjective,\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#manual-setup-of-the-optimization-domain",
    "href": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#manual-setup-of-the-optimization-domain",
    "title": "Classification Surrogate Tests",
    "section": "Manual setup of the optimization domain",
    "text": "Manual setup of the optimization domain\nThe following cells show how to manually setup the optimization problem in BoFire for didactic purposes.\n\n# Write helper functions which give the objective and the constraints\ndef rosenbrock(x: pd.Series) -&gt; pd.Series:\n    assert \"x_0\" in x.columns\n    assert \"x_1\" in x.columns\n    return (1 - x[\"x_0\"]) ** 2 + 100 * (x[\"x_1\"] - x[\"x_0\"] ** 2) ** 2\n\n\ndef constraints(x: pd.Series) -&gt; pd.Series:\n    assert \"x_0\" in x.columns\n    assert \"x_1\" in x.columns\n    feasiblity_vector = []\n    for _, row in x.iterrows():\n        if (row[\"x_0\"] ** 2 + row[\"x_1\"] ** 2 &lt;= 2.0) and (\n            (row[\"x_0\"] - 1.0) ** 2 + (row[\"x_1\"] - 1.0) ** 2 &lt;= 1.0\n        ):\n            feasiblity_vector.append(\"ideal\")\n        elif row[\"x_0\"] ** 2 + row[\"x_1\"] ** 2 &lt;= 2.0:\n            feasiblity_vector.append(\"acceptable\")\n        else:\n            feasiblity_vector.append(\"unacceptable\")\n    return feasiblity_vector\n\n\n# Set-up the inputs and outputs, use categorical domain just as an example\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i}\", bounds=(-1.75, 1.75)) for i in range(2)]\n    + [CategoricalInput(key=\"x_3\", categories=[\"0\", \"1\"], allowed=[True, True])],\n)\n\n# here the minimize objective is used, if you want to maximize you have to use the maximize objective.\noutput_features = Outputs(\n    features=[\n        ContinuousOutput(key=\"f_0\", objective=MinimizeObjective(w=1.0)),\n        CategoricalOutput(\n            key=\"f_1\",\n            categories=[\"unacceptable\", \"acceptable\", \"ideal\"],\n            objective=ConstrainedCategoricalObjective(\n                categories=[\"unacceptable\", \"acceptable\", \"ideal\"],\n                desirability=[False, True, True],\n            ),\n        ),  # This function will be associated with learning the categories\n    ],\n)\n\n# Create domain\ndomain1 = Domain(inputs=input_features, outputs=output_features)\n\n# Sample random points\nsample_df = domain1.inputs.sample(100)\n\n# Write a function which outputs one continuous variable and another discrete based on some logic\nsample_df[\"f_0\"] = rosenbrock(x=sample_df)\nsample_df[\"f_1\"] = constraints(x=sample_df)\n\nsample_df.head(5)\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_3\nf_0\nf_1\n\n\n\n\n0\n0.460766\n1.267506\n1\n111.635797\nideal\n\n\n1\n-0.980450\n-1.339360\n1\n533.217327\nunacceptable\n\n\n2\n0.728379\n0.073996\n1\n20.916646\nideal\n\n\n3\n-1.501883\n-1.617023\n0\n1506.022360\nunacceptable\n\n\n4\n1.596677\n-1.271649\n1\n1460.379006\nunacceptable\n\n\n\n\n\n\n\n\n# Plot the sample df\nimport math\n\nimport plotly.express as px\n\n\nfig = px.scatter(\n    sample_df,\n    x=\"x_0\",\n    y=\"x_1\",\n    color=\"f_1\",\n    width=550,\n    height=525,\n    title=\"Samples with labels\",\n)\nfig.add_shape(\n    type=\"circle\",\n    xref=\"x\",\n    yref=\"y\",\n    opacity=0.1,\n    fillcolor=\"red\",\n    x0=-math.sqrt(2),\n    y0=-math.sqrt(2),\n    x1=math.sqrt(2),\n    y1=math.sqrt(2),\n    line_color=\"red\",\n)\nfig.add_shape(\n    type=\"circle\",\n    xref=\"x\",\n    yref=\"y\",\n    opacity=0.2,\n    fillcolor=\"LightSeaGreen\",\n    x0=0,\n    y0=0,\n    x1=2,\n    y1=2,\n    line_color=\"LightSeaGreen\",\n)\nfig.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#evaluate-the-classification-model-performance-outside-of-the-optimization-procedure",
    "href": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#evaluate-the-classification-model-performance-outside-of-the-optimization-procedure",
    "title": "Classification Surrogate Tests",
    "section": "Evaluate the classification model performance (outside of the optimization procedure)",
    "text": "Evaluate the classification model performance (outside of the optimization procedure)\n\n# Import packages\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.surrogates.api import ClassificationMLPEnsemble\nfrom bofire.surrogates.diagnostics import ClassificationMetricsEnum\n\n\n# Instantiate the surrogate data model\nsurrogate_data = ClassificationMLPEnsemble(\n    inputs=domain1.inputs,\n    outputs=Outputs(features=[domain1.outputs.get_by_key(\"f_1\")]),\n    lr=0.03,\n    n_epochs=100,\n    hidden_layer_sizes=(\n        4,\n        2,\n    ),\n    weight_decay=0.0,\n    batch_size=10,\n    activation=\"tanh\",\n)\nsurrogate = surrogates.map(surrogate_data)\n\n# Fit the surrogate to the classification data\ncv_df = sample_df.drop([\"f_0\"], axis=1)\ncv_df[\"valid_f_1\"] = 1\ncv_train, cv_test, _ = surrogate.cross_validate(cv_df, folds=3)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:1134: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\n# Print training performance\ncv_train.get_metrics(\n    metrics=ClassificationMetricsEnum,\n    combine_folds=True,\n)\n\n\n\n\n\n\n\n\nACCURACY\nF1\n\n\n\n\n0\n0.85\n0.85\n\n\n\n\n\n\n\n\n# Print test performance\ncv_test.get_metrics(\n    metrics=ClassificationMetricsEnum,\n    combine_folds=True,\n)\n\n\n\n\n\n\n\n\nACCURACY\nF1\n\n\n\n\n0\n0.6\n0.6"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#setup-strategy-and-ask-for-candidates",
    "href": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#setup-strategy-and-ask-for-candidates",
    "title": "Classification Surrogate Tests",
    "section": "Setup strategy and ask for candidates",
    "text": "Setup strategy and ask for candidates\nNow we setup a SoboStrategy for generating candidates, the categorical output is modelled using the surrogate from above. The categorical output is modelled as an output constraint in the acquistion function optimization (constrained expected improvement). For more details have a look at this notebook: https://github.com/pytorch/botorch/blob/main/notebooks_community/clf_constrained_bo.ipynb and/or this paper: https://arxiv.org/abs/2402.07692.\n\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.strategies.api import SoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates\n\n\nstrategy_data = SoboStrategy(\n    domain=domain1,\n    acquisition_function=qLogEI(),\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data],\n    ),\n)\n\nstrategy = strategies.map(strategy_data)\n\nstrategy.tell(sample_df)\n\n\ncandidates = strategy.ask(10)\ncandidates\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-06 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_3\nf_1_pred\nf_1_sd\nf_1_unacceptable_prob\nf_1_acceptable_prob\nf_1_ideal_prob\nf_0_pred\nf_1_unacceptable_sd\nf_1_acceptable_sd\nf_1_ideal_sd\nf_0_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n-1.320985\n1.750000\n0\nunacceptable\n0.0\n2.884419\n0.200074\n0.799721\n0.000205\n7.689263\n0.446709\n0.446594\n0.000238\n-0.000205\n0.999795\n\n\n1\n1.321946\n1.750000\n1\nunacceptable\n0.0\n3.118302\n0.199472\n0.000714\n0.799814\n7.469858\n0.446010\n0.000399\n0.445614\n-0.799814\n0.200186\n\n\n2\n1.079564\n1.154037\n0\nacceptable\n0.0\n-0.568152\n0.199009\n0.000716\n0.800275\n4.852546\n0.444976\n0.000397\n0.444582\n-0.800275\n0.199725\n\n\n3\n-1.064780\n1.121446\n0\nunacceptable\n0.0\n1.978272\n0.003340\n0.996409\n0.000251\n5.104331\n0.006801\n0.006793\n0.000208\n-0.000251\n0.999749\n\n\n4\n0.995272\n0.981078\n1\nunacceptable\n0.0\n0.101942\n0.045769\n0.000925\n0.953305\n4.749159\n0.102320\n0.000070\n0.102364\n-0.953305\n0.046695\n\n\n5\n0.186865\n0.033685\n1\nunacceptable\n0.0\n1.042861\n0.000422\n0.999309\n0.000269\n4.601469\n0.000326\n0.000319\n0.000260\n-0.000269\n0.999731\n\n\n6\n1.717822\n-0.193580\n1\nunacceptable\n0.0\n987.314977\n0.200291\n0.207422\n0.592288\n5.691810\n0.446498\n0.430145\n0.530679\n-0.592288\n0.407712\n\n\n7\n0.867524\n0.742066\n0\nunacceptable\n0.0\n0.183276\n0.018265\n0.000948\n0.980787\n4.732750\n0.040818\n0.000092\n0.040903\n-0.980787\n0.019213\n\n\n8\n0.069643\n0.003609\n0\nideal\n0.0\n0.906431\n0.000429\n0.999279\n0.000292\n4.604541\n0.000325\n0.000333\n0.000239\n-0.000292\n0.999708\n\n\n9\n1.117661\n1.750000\n1\nunacceptable\n0.0\n25.898467\n0.197048\n0.000719\n0.802233\n6.110429\n0.440589\n0.000389\n0.440203\n-0.802233\n0.197767"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#check-classification-of-proposed-candidates",
    "href": "build/lib/docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#check-classification-of-proposed-candidates",
    "title": "Classification Surrogate Tests",
    "section": "Check classification of proposed candidates",
    "text": "Check classification of proposed candidates\nUse the logic from above to verify the classification values\n\n# Append to the candidates\ncandidates[\"f_1_true\"] = constraints(x=candidates)\n\n\n# Print results\ncandidates[[\"x_0\", \"x_1\", \"f_1_pred\", \"f_1_true\"]]\n\n\n\n\n\n\n\n\nx_0\nx_1\nf_1_pred\nf_1_true\n\n\n\n\n0\n-1.320985\n1.750000\nunacceptable\nunacceptable\n\n\n1\n1.321946\n1.750000\nunacceptable\nunacceptable\n\n\n2\n1.079564\n1.154037\nacceptable\nunacceptable\n\n\n3\n-1.064780\n1.121446\nunacceptable\nunacceptable\n\n\n4\n0.995272\n0.981078\nunacceptable\nideal\n\n\n5\n0.186865\n0.033685\nunacceptable\nacceptable\n\n\n6\n1.717822\n-0.193580\nunacceptable\nunacceptable\n\n\n7\n0.867524\n0.742066\nunacceptable\nideal\n\n\n8\n0.069643\n0.003609\nideal\nacceptable\n\n\n9\n1.117661\n1.750000\nunacceptable\nunacceptable"
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/index.html",
    "href": "build/lib/docs/tutorials/basic_examples/index.html",
    "title": "Basic Examples",
    "section": "",
    "text": "These notebooks demonstrate the basic functionality of BoFire, including:\n\nSetting up reaction domains\nDefining objectives\nRunning Bayesian optimization loops\nModel fitting and analysis\nOutlier detection and robust Gaussian Processes\nUnknown constraint classification\n\n\n\n\n\nIntroduction to fundamental concepts and terminology used throughout BoFire.\n\n\n\nA comprehensive example showing how to optimize chemical reactions using BoFire.\n\n\n\nLearn how to fit surrogate models and analyze their performance.\n\n\n\nTechniques for detecting outliers and building robust Gaussian Process models.\n\n\n\nHandling and classifying unknown constraints in optimization problems.\n\n\n\nLearn how to use Index Kernel and Positive Index Kernel for categorical variables."
  },
  {
    "objectID": "build/lib/docs/tutorials/basic_examples/index.html#available-tutorials",
    "href": "build/lib/docs/tutorials/basic_examples/index.html#available-tutorials",
    "title": "Basic Examples",
    "section": "",
    "text": "Introduction to fundamental concepts and terminology used throughout BoFire.\n\n\n\nA comprehensive example showing how to optimize chemical reactions using BoFire.\n\n\n\nLearn how to fit surrogate models and analyze their performance.\n\n\n\nTechniques for detecting outliers and building robust Gaussian Process models.\n\n\n\nHandling and classifying unknown constraints in optimization problems.\n\n\n\nLearn how to use Index Kernel and Positive Index Kernel for categorical variables."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html",
    "href": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html#imports",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html#random-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html#random-optimization",
    "title": "Himmelblau Benchmark",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nrandom_results = run(\n    Himmelblau(),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=14.157]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 83.36it/s, Current Best:=14.157]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html#sobo-gpei-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/001-Himmelblau.html#sobo-gpei-optimization",
    "title": "Himmelblau Benchmark",
    "section": "SOBO (GPEI) Optimization",
    "text": "SOBO (GPEI) Optimization\n\ndef strategy_factory(domain: Domain):\n    data_model = SoboStrategy(domain=domain, acquisition_function=qLogEI())\n    return strategies.map(data_model)\n\n\nbo_results = run(\n    Himmelblau(),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=25.942]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.74it/s, Current Best:=25.942]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.73it/s, Current Best:=25.942]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html",
    "href": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Hartmann\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import (\n    BotorchOptimizer,\n    RandomStrategy,\n    SoboStrategy,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#imports",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Hartmann\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import (\n    BotorchOptimizer,\n    RandomStrategy,\n    SoboStrategy,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#random-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#random-optimization",
    "title": "Himmelblau Benchmark",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nrandom_results = run(\n    Hartmann(dim=6, allowed_k=4),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=-1.166]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 54.09it/s, Current Best:=-1.166]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#sobo-gpei-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#sobo-gpei-optimization",
    "title": "Himmelblau Benchmark",
    "section": "SOBO (GPEI) Optimization",
    "text": "SOBO (GPEI) Optimization\n\ndef strategy_factory(domain: Domain):\n    data_model = SoboStrategy(\n        domain=domain,\n        acquisition_function=qLogEI(),\n        acquisition_optimizer=BotorchOptimizer(n_raw_samples=512, n_restarts=24),\n    )\n    return strategies.map(data_model)\n\n\nbo_results = run(\n    Hartmann(dim=6, allowed_k=4),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:09&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:09&lt;?, ?it/s, Current Best:=-0.647]Run 0: 100%|██████████| 1/1 [00:09&lt;00:00,  9.84s/it, Current Best:=-0.647]Run 0: 100%|██████████| 1/1 [00:09&lt;00:00,  9.84s/it, Current Best:=-0.647]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html",
    "href": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html",
    "title": "Himmelblau Benchmark with outliers",
    "section": "",
    "text": "import os\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.benchmark import UniformOutlierPrior\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.features.api import CategoricalInput\nfrom bofire.data_models.outlier_detection.api import (\n    IterativeTrimming,\n    OutlierDetections,\n)\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    MixedSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#imports",
    "title": "Himmelblau Benchmark with outliers",
    "section": "",
    "text": "import os\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.benchmark import UniformOutlierPrior\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.features.api import CategoricalInput\nfrom bofire.data_models.outlier_detection.api import (\n    IterativeTrimming,\n    OutlierDetections,\n)\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    MixedSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#sample-set-of-himmelblau-example-to-start-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#sample-set-of-himmelblau-example-to-start-optimization",
    "title": "Himmelblau Benchmark with outliers",
    "section": "sample set of Himmelblau example to start optimization",
    "text": "sample set of Himmelblau example to start optimization\nwe use the same set of example as initial data for comparison of three models. One case is where there is no outlier, while for other two models, we introduced outliers at a fixed probability. Using same initial example data helps us to compare the efficiency of outlier detection compared to the no outlier model that works on the dataset with no outliers. Further, using same sampled set with same outliers help starting both models with and without outlier detection from same value and we can see their evolution with iterations.\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nbo_results_set = []  # stores progress of model on data with no outliers (no outliers model)\nbo_results_outliers_set = []  # stores progress of model with no outlier detection on data with outliers (baseline model)\nbo_results_no_outliers_set = []  # stores progress of model with the outlier detection on data with outliers (our model)\n\nBenchmark = Himmelblau()\nsampled = sample(Benchmark.domain)\nsampled_xy = Benchmark.f(sampled, return_complete=True)"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-1",
    "href": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-1",
    "title": "Himmelblau Benchmark with outliers",
    "section": "Performance of models",
    "text": "Performance of models\n\nif not SMOKE_TEST:\n    plt.plot(bo_results_itr.mean(axis=0), color=\"red\", label=\"no outliers\")\n    plt.scatter(range(50), bo_results_itr.mean(axis=0), color=\"red\")\n    plt.fill_between(\n        range(50),\n        (bo_results_itr.mean(axis=0) - bo_results_itr.std(axis=0)),\n        (bo_results_itr.mean(axis=0) + bo_results_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"red\",\n    )\n    plt.plot(bo_results_outliers_itr.mean(axis=0), color=\"blue\", label=\"baseline\")\n    plt.scatter(range(50), bo_results_outliers_itr.mean(axis=0), color=\"blue\")\n    plt.fill_between(\n        range(50),\n        (bo_results_outliers_itr.mean(axis=0) - bo_results_outliers_itr.std(axis=0)),\n        (bo_results_outliers_itr.mean(axis=0) + bo_results_outliers_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"blue\",\n    )\n    plt.plot(bo_results_no_outliers_itr.mean(axis=0), color=\"green\", label=\"ITGP model\")\n    plt.scatter(range(50), bo_results_no_outliers_itr.mean(axis=0), color=\"green\")\n    plt.fill_between(\n        range(50),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            - bo_results_no_outliers_itr.std(axis=0)\n        ),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            + bo_results_no_outliers_itr.std(axis=0)\n        ),\n        alpha=0.3,\n        color=\"green\",\n    )\n\n    plt.ylabel(\"function value\")\n    # plt.yscale('log',base=10)\n    plt.legend()\n    plt.title(\"outliers moderately outrageous\")\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-2",
    "href": "build/lib/docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-2",
    "title": "Himmelblau Benchmark with outliers",
    "section": "Performance of models",
    "text": "Performance of models\n\nif not SMOKE_TEST:\n    plt.plot(bo_results_itr.mean(axis=0), color=\"red\", label=\"no outliers\")\n    plt.scatter(range(50), bo_results_itr.mean(axis=0), color=\"red\")\n    plt.fill_between(\n        range(50),\n        (bo_results_itr.mean(axis=0) - bo_results_itr.std(axis=0)),\n        (bo_results_itr.mean(axis=0) + bo_results_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"red\",\n    )\n    plt.plot(bo_results_outliers_itr.mean(axis=0), color=\"blue\", label=\"baseline\")\n    plt.scatter(range(50), bo_results_outliers_itr.mean(axis=0), color=\"blue\")\n    plt.fill_between(\n        range(50),\n        (bo_results_outliers_itr.mean(axis=0) - bo_results_outliers_itr.std(axis=0)),\n        (bo_results_outliers_itr.mean(axis=0) + bo_results_outliers_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"blue\",\n    )\n    plt.plot(bo_results_no_outliers_itr.mean(axis=0), color=\"green\", label=\"ITGP model\")\n    plt.scatter(range(50), bo_results_no_outliers_itr.mean(axis=0), color=\"green\")\n    plt.fill_between(\n        range(50),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            - bo_results_no_outliers_itr.std(axis=0)\n        ),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            + bo_results_no_outliers_itr.std(axis=0)\n        ),\n        alpha=0.3,\n        color=\"green\",\n    )\n\n    plt.ylabel(\"function value\")\n    # plt.yscale('log',base=10)\n    plt.legend()\n    plt.title(\"outliers too outrageous\")\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html",
    "title": "Local Search Region Bayesian Optimization",
    "section": "",
    "text": "In this notebook the Branin benchmark from paper about local search region BO (https://www.merl.com/publications/docs/TR2023-057.pdf) is reproduced. Note that we use here by purpose qEI as acquisition function to reproduce the paper."
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#imports-and-helper-methods",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#imports-and-helper-methods",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Imports and helper methods",
    "text": "Imports and helper methods\n\nimport os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Branin\nfrom bofire.data_models.acquisition_functions.api import qEI\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.strategies.api import (\n    LSRBO,\n    BotorchOptimizer,\n    RandomStrategy,\n    SoboStrategy,\n)\nfrom bofire.runners.api import run\n\n\nwarnings.filterwarnings(\"ignore\")\n\nbench = Branin(locality_factor=0.5)\n\n\ndef sample(domain):\n    sampled = domain.inputs.sample(10)\n    return sampled\n    # sampled = bench.f(sampled, return_complete = True)\n    # sampled = sampled.sort_values(by=\"y\", ascending=False, ignore_index=True)\n    # return sampled[bench.domain.inputs.get_keys()].copy()\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#random-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#random-optimization",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Random Optimization",
    "text": "Random Optimization\nHere random sampling is performed without any local search region constraints applied.\n\nrandom_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=8.834]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 95.19it/s, Current Best:=8.834]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#sobo-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#sobo-optimization",
    "title": "Local Search Region Bayesian Optimization",
    "section": "SOBO Optimization",
    "text": "SOBO Optimization\nHere standard bayesian optimization is performed without any local search region constraints applied using the qEI acquistion function.\n\nsobo_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(domain=domain, acquisition_function=qEI()),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=12.608]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  3.19it/s, Current Best:=12.608]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  3.18it/s, Current Best:=12.608]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#local-sobo-optimization",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#local-sobo-optimization",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Local SOBO Optimization",
    "text": "Local SOBO Optimization\nHere bayesian optimization is performed with setting gamma parameter of the LSR-BO method to 0 which results in a pure local optimization with respect to the last random sample.\n\nlocal_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qEI(),\n                acquisition_optimizer=BotorchOptimizer(\n                    local_search_config=LSRBO(gamma=0)\n                ),\n            ),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=1.084]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.07it/s, Current Best:=1.084]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.07it/s, Current Best:=1.084]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#global-sobo-optimization-projection",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#global-sobo-optimization-projection",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Global SOBO Optimization (Projection)",
    "text": "Global SOBO Optimization (Projection)\nHere bayesian optimization is performed with setting gamma parameter of the LSR-BO method to 500 which results in taking always the biggest step in the direction of the global candidate. In the original paper, this is called “Projection”.\n\nglobal_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qEI(),\n                acquisition_optimizer=BotorchOptimizer(\n                    local_search_config=LSRBO(gamma=500)\n                ),\n            ),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=1.537]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.59it/s, Current Best:=1.537]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.59it/s, Current Best:=1.537]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#lsr-bo",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#lsr-bo",
    "title": "Local Search Region Bayesian Optimization",
    "section": "LSR-BO",
    "text": "LSR-BO\nHere the actual method from the paper is performed with setting gamma to 0.1.\n\nlsr_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qEI(),\n                acquisition_optimizer=BotorchOptimizer(\n                    local_search_config=LSRBO(gamma=0.1)\n                ),\n            ),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=8.016]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.52it/s, Current Best:=8.016]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.52it/s, Current Best:=8.016]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#plot-the-results",
    "href": "build/lib/docs/tutorials/benchmarks/007-LSRBO.html#plot-the-results",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Plot the results",
    "text": "Plot the results\n\nif not SMOKE_TEST:\n    fig, ax = plt.subplots()\n\n    best_random = np.array(\n        [random_results[i][0][1] for i in range(len(random_results))]\n    )\n    ax.plot(range(80), best_random.mean(axis=0), color=\"gray\", label=\"Random\")\n    ax.fill_between(\n        range(80),\n        (best_random.mean(0) - best_random.std(0)),\n        (best_random.mean(0) + best_random.std(0)),\n        alpha=0.3,\n        color=\"gray\",\n    )\n\n    best_global = np.log10(\n        np.array([global_results[i][0][1] for i in range(len(global_results))])\n        - 0.397887,\n    )\n    ax.plot(range(80), best_global.mean(axis=0), color=\"orange\", label=\"Projection\")\n    ax.fill_between(\n        range(80),\n        (best_global.mean(0) - best_global.std(0)),\n        (best_global.mean(0) + best_global.std(0)),\n        alpha=0.3,\n        color=\"orange\",\n    )\n\n    best_local = np.log10(\n        np.array([local_results[i][0][1] for i in range(len(global_results))])\n        - 0.397887,\n    )\n    ax.plot(range(80), best_local.mean(axis=0), color=\"green\", label=\"Local\")\n    ax.fill_between(\n        range(80),\n        (best_local.mean(0) - best_local.std(0)),\n        (best_local.mean(0) + best_local.std(0)),\n        alpha=0.3,\n        color=\"green\",\n    )\n\n    best_lsr = np.log10(\n        np.array([lsr_results[i][0][1] for i in range(len(lsr_results))]) - 0.397887,\n    )\n    ax.plot(range(80), best_lsr.mean(axis=0), color=\"blue\", label=\"LSR\")\n    ax.fill_between(\n        range(80),\n        (best_lsr.mean(0) - best_lsr.std(0)),\n        (best_lsr.mean(0) + best_lsr.std(0)),\n        alpha=0.3,\n        color=\"blue\",\n    )\n\n    best_unconstrained = np.log10(\n        np.array([sobo_results[i][0][1] for i in range(len(sobo_results))]) - 0.397887,\n    )\n    ax.plot(\n        range(80), best_unconstrained.mean(axis=0), color=\"red\", label=\"Unconstrained\"\n    )\n    ax.fill_between(\n        range(80),\n        (best_unconstrained.mean(0) - best_unconstrained.std(0)),\n        (best_unconstrained.mean(0) + best_unconstrained.std(0)),\n        alpha=0.3,\n        color=\"red\",\n    )\n\n    ax.set_xlabel(\"Iteration\")\n    ax.set_ylabel(\"Log10(SimpleRegret)\")\n    ax.legend()\n\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/009-BNH.html",
    "href": "build/lib/docs/tutorials/benchmarks/009-BNH.html",
    "title": "BNH Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import BNH\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import (\n    MaximizeSigmoidObjective,\n    MinimizeObjective,\n    MinimizeSigmoidObjective,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.plot.api import plot_objective_plotly\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/009-BNH.html#imports",
    "href": "build/lib/docs/tutorials/benchmarks/009-BNH.html#imports",
    "title": "BNH Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import BNH\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import (\n    MaximizeSigmoidObjective,\n    MinimizeObjective,\n    MinimizeSigmoidObjective,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.plot.api import plot_objective_plotly\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/009-BNH.html#random-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/009-BNH.html#random-strategy",
    "title": "BNH Benchmark",
    "section": "Random Strategy",
    "text": "Random Strategy\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    if \"c1\" in experiments.columns:\n        return compute_hypervolume(\n            domain,\n            experiments.loc[(experiments.c1 &lt;= 25) & (experiments.c2 &gt;= 7.7)],\n            ref_point={\"f1\": 140, \"f2\": 50},\n        )\n    return compute_hypervolume(domain, experiments, ref_point={\"f1\": 140, \"f2\": 50})\n\n\nrandom_results = run(\n    BNH(constraints=True),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=4643.892]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 45.97it/s, Current Best:=4643.892]"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/009-BNH.html#mobo-strategy",
    "href": "build/lib/docs/tutorials/benchmarks/009-BNH.html#mobo-strategy",
    "title": "BNH Benchmark",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\n\nWithout Constraints\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(domain=domain, ref_point={\"f1\": 140, \"f2\": 50})\n    return strategies.map(data_model)\n\n\nresults = run(\n    BNH(constraints=False),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s, Current Best:=4640.954]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.80s/it, Current Best:=4640.954]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.80s/it, Current Best:=4640.954]\n\n\n\n\nWith Constraints\n\nManual Setup of the Domain\n\ndomain = Domain(\n    inputs=Inputs(\n        features=[\n            ContinuousInput(key=\"x1\", bounds=(0, 5)),\n            ContinuousInput(key=\"x2\", bounds=(0, 3)),\n        ],\n    ),\n    outputs=Outputs(\n        features=[\n            ContinuousOutput(key=\"f1\", objective=MinimizeObjective()),\n            ContinuousOutput(key=\"f2\", objective=MinimizeObjective()),\n            # these are the output constraints, choose MinimizeSigmoidObjective for lower bound constraints\n            # and MaximizeSigmoidObjective for upper bound constraints\n            # tp is the threshold point, steepness is the steepness of the sigmoid that is applied to the constraint\n            # usually a steepness of 1000 is fine.\n            ContinuousOutput(\n                key=\"c1\",\n                objective=MinimizeSigmoidObjective(tp=25, steepness=1000),\n            ),\n            ContinuousOutput(\n                key=\"c2\",\n                objective=MaximizeSigmoidObjective(tp=7.7, steepness=1000),\n            ),\n        ],\n    ),\n)\n\nOne can visualize the objectives in the following way:\n\nfeat = domain.outputs.get_by_key(\"c1\")\n\nif not SMOKE_TEST:\n    plot_objective_plotly(feat, lower=20, upper=30)  # type: ignore\n\n\nfeat = domain.outputs.get_by_key(\"c2\")\n\nif not SMOKE_TEST:\n    plot_objective_plotly(feat, lower=5, upper=10)  # type: ignore\n\n\n\nRun tbe optimization\nThe warnings can be ignored. They are stemming just from an internal postprocessing step and will be removed soon.\n\nc_results = run(\n    BNH(constraints=True),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/pandas/core/arraylike.py:402: RuntimeWarning:\n\noverflow encountered in exp\n\nRun 0:   0%|          | 0/1 [00:05&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:05&lt;?, ?it/s, Current Best:=4652.814]Run 0: 100%|██████████| 1/1 [00:05&lt;00:00,  5.48s/it, Current Best:=4652.814]Run 0: 100%|██████████| 1/1 [00:05&lt;00:00,  5.48s/it, Current Best:=4652.814]\n\n\n\nif not SMOKE_TEST:\n    fig, ax = plt.subplots()\n    ax.plot(results[0][1], label=\"without constraints\")\n    ax.plot(c_results[0][1], label=\"with constraints\")\n    ax.set_xlabel(\"iteration\")\n    ax.set_ylabel(\"hypervolume\")\n    ax.legend()\n    plt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/011-ActiveLearning.html",
    "href": "build/lib/docs/tutorials/benchmarks/011-ActiveLearning.html",
    "title": "Active Learning",
    "section": "",
    "text": "Showcase of active learning in bofire. Active learning per definition focusses on fitting the model to the experimental observations best possible in an iterative manner reducing some kind of uncertainty. The ActiveLearningStrategy proposes a set of evaluation points that will gain the most information about the problem each iteration. Thus, an unknown black-box-function can be approximated without optimization. It represents an exploration-only strategy.\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import GenericBenchmark\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import MinimizeObjective\nfrom bofire.data_models.strategies.api import RandomStrategy\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\n\n1-D Objective Function\nFor a 1-D objective function. The Himmelblau benchmark is used. \\[\\begin{equation}\n    f: \\mathbb{R}^2 \\rightarrow \\mathbb{R} \\quad | \\quad f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2) ^2\n\\end{equation}\\] To start the active learning strategy we need to supply some initial data points to set up the Gaussian Regression Model in the background.\n\nhimmelblau = Himmelblau()\n\n\ndef sample(domain: Domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ninitial_points = sample(domain=himmelblau.domain)\ninitial_experiments = pd.concat([initial_points, himmelblau.f(initial_points)], axis=1)\ndisplay(initial_experiments)\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n-5.391364\n-4.896203\n307.594479\n1\n\n\n1\n1.078977\n-0.443436\n138.431471\n1\n\n\n2\n3.783375\n-1.386017\n5.395375\n1\n\n\n3\n-1.796789\n0.433966\n127.945756\n1\n\n\n4\n-4.554880\n-3.358206\n40.892751\n1\n\n\n5\n-4.793341\n3.528760\n240.835256\n1\n\n\n6\n-4.925593\n-3.678740\n94.412878\n1\n\n\n7\n5.994651\n4.682854\n1315.071820\n1\n\n\n8\n-4.598745\n-2.179227\n110.427164\n1\n\n\n9\n2.386284\n1.317954\n24.177170\n1\n\n\n\n\n\n\n\n\nActiveLearningStrategy\nThe ActiveLearningstrategy can be set up just as other BO strategies implemented in bofire. Just take a look into the other tutorials. Basic calls are ask() to retrieve new evaluation candidates from the acquisition function and tell() to train the model with a new observation.\nCurrently, the default active learning acquisition function implemented is qNegIntegratedPosteriorVariance. It focuses on minimizing the overall posterior variance by choosing a new candidate.\nThe ActiveLearningStrategy uses Monte-Carlo-integration to evaluate the acquisition function. The number of integration nodes significantly influences the speed of the integration. These can be adjusted by changing the parameter data_model.num_sobol_samples. Note that a sample size representing a power of \\(2\\) increases performance.\n\n# Manual set up of ActiveLearning\nfrom bofire.data_models.acquisition_functions.api import qNegIntPosVar\nfrom bofire.data_models.strategies.api import ActiveLearningStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\n\n\naf = qNegIntPosVar(\n    n_mc_samples=64,  # lower the number of monte carlo samples to improve speed\n)\n\ndata_model = ActiveLearningStrategy(domain=himmelblau.domain, acquisition_function=af)\nrecommender = strategies.map(data_model=data_model)\nrecommender.tell(experiments=initial_experiments)\ncandidates = recommender.ask(candidate_count=1)\n\n\ndisplay(candidates)\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n-2.809906\n5.637718\n802.222761\n288.819453\n-802.222761\n\n\n\n\n\n\n\n\n# Running the active learning strategy\nn_iter = 1 if SMOKE_TEST else 20\nresults = initial_experiments\n\nfor _ in range(n_iter):\n    # run active learning strategy\n    X = recommender.ask(candidate_count=1)[himmelblau.domain.inputs.get_keys()]\n    Y = himmelblau.f(X)\n    XY = pd.concat([X, Y], axis=1)\n    recommender.tell(experiments=XY)  # pass new experimental data\n    results = pd.concat([results, XY], axis=0, ignore_index=True)\n\n\n# Running a random strategy for comparison\ndef strategy_factory(domain: Domain):\n    data_model = RandomStrategy(domain=domain)\n    return strategies.map(data_model)\n\n\nrandom_results = run(\n    himmelblau,\n    strategy_factory=strategy_factory,\n    n_iterations=n_iter,\n    metric=lambda domain, experiments: 1.0,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n\n\nPlotting\n\nif not SMOKE_TEST:\n    plt.rcParams[\"figure.figsize\"] = (10, 4)\n\n    fig, ax = plt.subplots(1, 2)\n\n# contour plot of himmelblau\n    def f(grid):\n        return (grid[0] ** 2 + grid[1] - 11) ** 2 + (grid[0] + grid[1] ** 2) ** 2\n\n\n    X_grid = np.arange(-7, 7, 0.01)\n    Y_grid = np.arange(-7, 7, 0.01)\n    mesh = np.meshgrid(X_grid, Y_grid)\n    Z = f(grid=mesh)\n    levels = np.linspace(Z.min(), Z.max(), 6)\n\n\n    ax[0].contourf(X_grid, Y_grid, Z, cmap=cm.viridis)\n    ax[0].scatter(random_results[0][0].x_1, random_results[0][0].x_2, c=\"white\")\n    ax[1].contourf(X_grid, Y_grid, Z, cmap=cm.viridis)\n    ax[1].scatter(results.x_1, results.x_2, c=\"white\")\n\n    ax[0].axis([-7, 7, -7, 7])\n    ax[0].set_xlabel(\"$x_1$\")\n    ax[1].set_xlabel(\"$x_1$\")\n    ax[0].set_ylabel(\"$x_2$\")\n    ax[0].set_title(\"random strategy\")\n    ax[1].set_title(\"active learning strategy\")\n    fig.show()\n\nThe plot shows the exploratory behavior of the ActiveLearningStrategy.\n\n\n\n2-D (n-D) Objective Function\nNow, we want to actively learn an objective function with a multi-dimensional output space. This is shown by an example function with \\(2\\) output variables. For this, we again utilize the Himmelblau benchmark function and the Ackley function.\n\\[\\begin{equation}\n    f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2   \\quad | \\quad \n    f(x_1, x_2) =\n        \\begin{pmatrix}\n            (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2) ^2 \\\\\n            -20\\exp \\left[-0.2{\\sqrt {0.5\\left(x_1^{2}+x_2^{2}\\right)}}\\right]\n            -\\exp \\left[0.5\\left(\\cos 2\\pi x_1+\\cos 2\\pi x_2\\right)\\right]+e+20\n\n        \\end{pmatrix}\n\\end{equation}\\]\n\ninputs = Inputs(\n    features=[\n        ContinuousInput(key=\"x_1\", bounds=(-6, 6)),\n        ContinuousInput(key=\"x_2\", bounds=(-6, 6)),\n    ],\n)\noutputs = Outputs(\n    features=[\n        ContinuousOutput(key=\"f_0\", objective=MinimizeObjective()),\n        ContinuousOutput(key=\"f_1\", objective=MinimizeObjective()),\n    ],\n)\ndomain = Domain(inputs=inputs, outputs=outputs)\n\n\ndef benchmark_function(candidates):\n    f0 = (candidates[\"x_1\"] ** 2 + candidates[\"x_2\"] - 11) ** 2 + (\n        candidates[\"x_1\"] + candidates[\"x_2\"] ** 2\n    ) ** 2\n    f1 = -20 * np.exp(\n        -0.2 * np.sqrt(0.5 * (candidates[\"x_1\"] ** 2 + candidates[\"x_2\"] ** 2)),\n    ) + (\n        -np.exp(\n            0.5\n            * (\n                np.cos(2 * np.pi * candidates[\"x_1\"])\n                + np.cos(2 * np.pi * candidates[\"x_2\"])\n            ),\n        )\n        + np.exp(1)\n        + 20\n    )\n    return pd.DataFrame({\"f_0\": f0, \"f_1\": f1})\n\n\nfunction = GenericBenchmark(domain=domain, func=benchmark_function)\ninitial_experiments = pd.concat(\n    [initial_points, function.f(candidates=initial_points)],\n    axis=1,\n)\n\nFor the multi-objective function we need to pass two models to the strategy as each individual output is represented by a separate model. By default, the ActiveLearningStrategy focusses on minimizing the negative integrated posterior variance of each model equally. To minimize the variances in a more specific way certain weights can be provided for each output feature. This can be done by passing a dictionary containing the individual weights for each output feature with its corresponding key to the parameter weights.\n\n# Manual set up of ActiveLearning\nweights = {\n    \"f_0\": 0.4,\n    \"f_1\": 0.6,\n}\n# create an instance of the acquisition function with distinct weights\naf = qNegIntPosVar(weights=weights, n_mc_samples=16)\n\ndata_model = ActiveLearningStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[0]]),\n            ),\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[1]]),\n            ),\n        ],\n    ),\n    acquisition_function=af,\n)\nrecommender = strategies.map(data_model=data_model)\nrecommender.tell(experiments=initial_experiments)\ncandidates = recommender.ask(candidate_count=1)\n\n\ndisplay(candidates)\n\n\n\n\n\n\n\n\nx_1\nx_2\nf_0_pred\nf_1_pred\nf_0_sd\nf_1_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n1.791208\n-4.100843\n185.493638\n7.158284\n260.972469\n1.69404\n-185.493638\n-7.158284\n\n\n\n\n\n\n\n\n# Running the active learning strategy\nn_iter = 1 if SMOKE_TEST else 20\nresults = initial_experiments\n\nfor _ in range(n_iter):\n    # run active learning strategy\n    X = recommender.ask(candidate_count=1)[domain.inputs.get_keys()]\n    Y = function.f(candidates=X)\n    XY = pd.concat([X, Y], axis=1)\n    recommender.tell(experiments=XY)  # pass new experimental data\n    results = pd.concat([results, XY], axis=0, ignore_index=True)\n\n\nrandom_results = run(\n    function,\n    strategy_factory=strategy_factory,\n    n_iterations=n_iter,\n    metric=lambda domain, experiments: 1.0,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n\nPlotting\n\nif not SMOKE_TEST:\n    plt.rcParams[\"figure.figsize\"] = (10, 8)\n    fig, ax = plt.subplots(2, 2)\n\n\n    def f1(grid):\n        return (\n            -20 * np.exp(-0.2 * np.sqrt(0.5 * (grid[0] ** 2 + grid[1] ** 2)))\n            - np.exp(0.5 * (np.cos(2 * np.pi * grid[0]) + np.cos(2 * np.pi * grid[1])))\n            + np.exp(1)\n            + 20\n        )\n\n\n    Z1 = f1(mesh)\n    levels = np.linspace(Z1.min(), Z1.max(), 10)\n    ax[0, 0].contourf(\n        X_grid,\n        Y_grid,\n        Z,\n        cmap=cm.viridis,\n    )\n    ax[0, 0].scatter(random_results[0][0].x_1, random_results[0][0].x_2, c=\"white\")\n    ax[0, 1].contourf(\n        X_grid,\n        Y_grid,\n        Z,\n        cmap=cm.viridis,\n    )\n    ax[0, 1].scatter(results.x_1, results.x_2, c=\"white\")\n    ax[1, 0].contourf(X_grid, Y_grid, Z1, cmap=cm.viridis, levels=levels)\n    ax[1, 0].scatter(\n        random_results[0][0].x_1,\n        random_results[0][0].x_2,\n        c=\"white\",\n        edgecolors=\"black\",\n    )\n    ax[1, 1].contourf(X_grid, Y_grid, Z1, cmap=cm.viridis, levels=levels)\n    ax[1, 1].scatter(results.x_1, results.x_2, c=\"white\", edgecolors=\"black\")\n\n    ax[0, 0].axis([-7, 7, -7, 7])\n    ax[1, 0].set_xlabel(\"$x_1$\")\n    ax[1, 1].set_xlabel(\"$x_1$\")\n    ax[0, 0].set_ylabel(\"$x_2$\")\n    ax[1, 0].set_ylabel(\"$x_2$\")\n    ax[0, 0].set_title(\"random strategy\")\n    ax[0, 1].set_title(\"active learning strategy\")\n    fig.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/benchmarks/013-spherical_kernel.html",
    "href": "build/lib/docs/tutorials/benchmarks/013-spherical_kernel.html",
    "title": "Spherical linear kernels for high dimensional BO",
    "section": "",
    "text": "Spherical Linear Kernel is useful for optimizing high-dimensional problems.\n\nfrom bofire.benchmarks.svm import SVM\nfrom bofire.data_models.strategies.api import SoboStrategy\nfrom bofire.data_models.kernels.api import SphericalLinearKernel\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate, BotorchSurrogates\nimport bofire.strategies.api as strategies\n\nWe use the SVM benchmark.\n\n# problem setup for spherical linear kernels\nbenchmark = SVM()\ncandidates = benchmark._domain.inputs.sample(benchmark.dim+1, seed=benchmark.seed)\nexperiments = candidates.copy()\nresult = benchmark._f(experiments)\n# Add empty columns 'y' and 'valid_y' to experiments DataFrame\nexperiments[\"y\"], experiments[\"valid_y\"] = result[\"y\"], result[\"valid_y\"]\nsobo_strategy_data_model = SoboStrategy(\n    domain=benchmark._domain,\n    seed=benchmark.seed,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=benchmark._domain.inputs,\n                outputs=benchmark._domain.outputs,\n                kernel=SphericalLinearKernel(),\n            )\n        ]\n    ),\n)\nstrategy = strategies.map(sobo_strategy_data_model)\n\nDownloading SVM data...\nDownload complete.\n\n\nRunning the optimization loop\n\nstrategy.tell(experiments, replace=True)\nnum_steps = 3 # set the number of steps here (the original paper uses 1000 steps)\nfor step_number in range(num_steps):\n    print(f\"Step {step_number+1}/{num_steps}\")\n    new_candidates = strategy.ask(candidate_count=1)\n    new_experiments = new_candidates.copy()\n    result = benchmark._f(new_candidates)\n    new_experiments[\"y\"], new_experiments[\"valid_y\"] = result[\"y\"], result[\"valid_y\"]\n    print(f\"New experiment:\\n{new_experiments}\")\n    strategy.tell(experiments=new_experiments)\n# save all the experiments\nall_experiments = strategy.experiments\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nStep 1/3\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:\n\nA not p.d., added jitter of 1.0e-08 to the diagonal\n\n\n\nNew experiment:\n        x_1      x_10     x_100     x_101     x_102     x_103     x_104  \\\n0  0.667271  0.645865  0.673514  0.788116  0.515714  0.899225  0.370435   \n\n      x_105     x_106     x_107  ...      x_95      x_96      x_97      x_98  \\\n0  0.745721  0.642221  0.920136  ...  0.456597  0.457589  0.825119  0.552249   \n\n       x_99    y_pred     y_sd     y_des         y  valid_y  \n0  0.946278  0.236777  0.00201 -0.236777  0.229736        1  \n\n[1 rows x 393 columns]\nStep 2/3\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:\n\nA not p.d., added jitter of 1.0e-08 to the diagonal\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .')]\nTrying again with a new set of initial conditions.\n\n\n\nNew experiment:\n        x_1      x_10     x_100     x_101     x_102     x_103     x_104  \\\n0  0.571132  0.402144  0.774951  0.740535  0.325296  0.751865  0.269752   \n\n      x_105     x_106     x_107  ...      x_95      x_96      x_97      x_98  \\\n0  0.163741  0.343008  0.328641  ...  0.959124  0.405325  0.504206  0.866072   \n\n       x_99    y_pred      y_sd     y_des         y  valid_y  \n0  0.324885  0.236599  0.002038 -0.236599  0.228193        1  \n\n[1 rows x 393 columns]\nStep 3/3\nNew experiment:\n        x_1      x_10    x_100  x_101     x_102     x_103     x_104  x_105  \\\n0  0.827119  0.849634  0.12755    1.0  0.285359  0.549768  0.751327    1.0   \n\n      x_106     x_107  ...      x_95  x_96      x_97      x_98      x_99  \\\n0  0.915843  0.460966  ...  0.422644   1.0  0.663223  0.673715  0.730583   \n\n     y_pred      y_sd     y_des         y  valid_y  \n0  0.197406  0.001861 -0.197406  0.237064        1  \n\n[1 rows x 393 columns]\n\n\nOne can use the results obtained in all_experiments to get the evolution of the optimum with respect to the iterations."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/basic_examples.html",
    "href": "build/lib/docs/tutorials/doe/basic_examples.html",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "",
    "text": "The following example has been taken from the paper “The construction of D- and I-optimal designs for mixture experiments with linear constraints on the components” by R. Coetzer and L. M. Haines (https://www.sciencedirect.com/science/article/pii/S0169743917303106).\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    InterpointEqualityConstraint,\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NonlinearEqualityConstraint,\n    NonlinearInequalityConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion, IOptimalityCriterion"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/basic_examples.html#linear-model",
    "href": "build/lib/docs/tutorials/doe/basic_examples.html#linear-model",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "Linear model",
    "text": "Linear model\nCreating an experimental design that is D-optimal with respect to a linear model is done the same way as making proposals using other methods in BoFire; you 1. create a domain 2. construct a stategy data model (here we want DoEStrategy) 3. map the strategy to its functional version, and finally 4. ask the strategy for proposals.\nWe will start with the simplest case: make a design based on a linear model containing main-effects (i.e., simply the inputs themselves and an intercept, without any second-order terms).\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0.1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 0.6)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n        LinearInequalityConstraint(features=[\"x1\", \"x2\"], coefficients=[5, 4], rhs=3.9),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\"],\n            coefficients=[-20, 5],\n            rhs=-3,\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\n\nLet’s visualize the experiments that were chosen. We will see that such a design puts the experiments at the extremes of the experimental space - these are the points that best allow us to estimate the parameters of the linear model we chose.\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"Linear model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/basic_examples.html#cubic-model",
    "href": "build/lib/docs/tutorials/doe/basic_examples.html#cubic-model",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "cubic model",
    "text": "cubic model\nWhile the previous design is optimal for the main-effects model, we might prefer to see something that does not allocate all the experimental effort to values at the boundary of the space. This implies that we think there might be some higher-order effects present in the system - if we were sure that the target variable would follow straight-line behavior across the domain, we would not need to investigate any points away from the extremes.\nWe can address this by specifying our own linear model that includes higher-order terms.\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\"\n    ),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12)\n\nIn this case we can compare with the result reported in the paper of Coetzer and Haines.\n\nd_opt = np.array(\n    [\n        [\n            0.7,\n            0.3,\n            0.2,\n            0.3,\n            0.5902,\n            0.4098,\n            0.2702,\n            0.2279,\n            0.4118,\n            0.5738,\n            0.4211,\n            0.3360,\n        ],\n        [0.1, 0.6, 0.2, 0.1, 0.2373, 0.4628, 0.4808, 0.3117, 0.1, 0.1, 0.2911, 0.2264],\n        [\n            0.2,\n            0.1,\n            0.6,\n            0.6,\n            0.1725,\n            0.1274,\n            0.249,\n            0.4604,\n            0.4882,\n            0.3262,\n            0.2878,\n            0.4376,\n        ],\n    ],\n)  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solution\nax.scatter(\n    xs=d_opt[0],\n    ys=d_opt[1],\n    zs=d_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"D-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=IOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\",\n        n_space_filling_points=60,\n        ipopt_options={\"max_iter\": 500},\n    ),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12).to_numpy().T\n\n\ni_opt = np.array(\n    [\n        [0.7000, 0.1000, 0.2000],\n        [0.3000, 0.6000, 0.1000],\n        [0.2031, 0.1969, 0.6000],\n        [0.5899, 0.2376, 0.1725],\n        [0.4105, 0.4619, 0.1276],\n        [0.2720, 0.4882, 0.2398],\n        [0.2281, 0.3124, 0.4595],\n        [0.3492, 0.1000, 0.5508],\n        [0.5614, 0.1000, 0.3386],\n        [0.4621, 0.2342, 0.3037],\n        [0.3353, 0.2228, 0.4419],\n        [0.3782, 0.3618, 0.2600],\n    ]\n).T  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot I-optimal solution\nax.scatter(\n    xs=i_opt[0],\n    ys=i_opt[1],\n    zs=i_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"I-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[0],\n    ys=candidates[1],\n    zs=candidates[2],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/objective.py:168: UserWarning:\n\nEquality constraints were detected. No equidistant grid of points can be generated. The design space will be filled via SpaceFilling."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/basic_examples.html#nonlinear-constraints",
    "href": "build/lib/docs/tutorials/doe/basic_examples.html#nonlinear-constraints",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "Nonlinear Constraints",
    "text": "Nonlinear Constraints\nDesign generation also supports nonlinear constraints. The following 3 examples show what is possible.\nFirst, a convenience function for plotting.\n\ndef plot_results_3d(result, surface_func):\n    u, v = np.mgrid[0 : 2 * np.pi : 100j, 0 : np.pi : 80j]\n    X = np.cos(u) * np.sin(v)\n    Y = np.sin(u) * np.sin(v)\n    Z = surface_func(X, Y)\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(X, Y, Z, alpha=0.3)\n    ax.scatter(\n        xs=result[\"x1\"],\n        ys=result[\"x2\"],\n        zs=result[\"x3\"],\n        marker=\"o\",\n        s=40,\n        color=\"red\",\n    )\n    ax.set(xlabel=\"x1\", ylabel=\"x2\", zlabel=\"x3\")\n    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n\n\nExample 1: Design inside a cone / nonlinear inequality\nIn the following example we have three design variables. We impose the constraint that all experiments have to be contained in the interior of a cone, which corresponds to the nonlinear inequality constraint \\(\\sqrt{x_1^2 + x_2^2} - x_3 \\leq 0\\). The optimization is done for a linear model and we will see that it places the points on the surface of the cone so as to maximize the distance between them (although this is not explicitly the objective of the optimization).\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:91: UserWarning:\n\nNonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/utils.py:709: OptimizeWarning:\n\nConstraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n\n\n\n\n\n\n\n\n\n\nWe can do the same for a design space limited by an elliptical cone \\(x_1^2 + x_2^2 - x_3 \\leq 0\\).\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"x1**2 + x2**2 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: x1**2 + x2**2)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:91: UserWarning:\n\nNonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/utils.py:709: OptimizeWarning:\n\nConstraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2: Design on the surface of a cone / nonlinear equality\nWe can also limit the design space to the surface of a cone, defined by the equality constraint \\(\\sqrt{x_1^2 + x_2^2} - x_3 = 0\\). Before, we observed that the experimental proposals happened to be on the surface of the cone, but now they are constrained so that this must be the case.\nRemark: Due to missing sampling methods, the initial points provided to IPOPT don’t satisfy the constraints. But this does not matter for the solution.\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearEqualityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12, raise_validation_error=False)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:91: UserWarning:\n\nNonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:119: UserWarning:\n\n1 validation error for RandomStrategy\n  Value error, constraint `&lt;class 'bofire.data_models.constraints.nonlinear.NonlinearEqualityConstraint'&gt;` is not implemented for strategy `RandomStrategy` [type=value_error, input_value={'domain': Domain(type='D....5, 0], [0, 0, 0]]')]))}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:120: UserWarning:\n\nSampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/utils.py:709: OptimizeWarning:\n\nConstraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3: Batch constraints\nBatch constraints can be used to create designs where each set of multiplicity subsequent experiments have the same value for a certain feature. This can be useful for setups where experiments are done in parallel and some parameters must be shared by experiments in the same parallel batch.\nIn the following example we fix the value of the decision variable x1 for each batch of 3 experiments.\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[InterpointEqualityConstraint(features=[\"x1\"], multiplicity=3)],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 500, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12)\nresult.round(3)\n\n\n\n\n\n\n\n\nx1\nx2\nx3\n\n\n\n\n0\n1.0\n1.0\n1.0\n\n\n1\n1.0\n1.0\n1.0\n\n\n2\n1.0\n1.0\n1.0\n\n\n3\n0.0\n1.0\n0.0\n\n\n4\n0.0\n0.0\n0.0\n\n\n5\n0.0\n0.0\n1.0\n\n\n6\n0.0\n0.0\n0.0\n\n\n7\n0.0\n1.0\n0.0\n\n\n8\n0.0\n0.0\n1.0\n\n\n9\n1.0\n0.0\n0.0\n\n\n10\n1.0\n0.0\n0.0\n\n\n11\n1.0\n0.0\n1.0"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html",
    "href": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html",
    "title": "Design with explicit Formula",
    "section": "",
    "text": "This tutorial notebook shows how to setup a D-optimal design with BoFire while providing an explicit formula and not just one of the four available keywords linear, linear-and-interaction, linear-and-quadratic, fully-quadratic.\nMake sure that cyipoptis installed. The recommend way is the installation via conda conda install -c conda-forge cyipopt."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#imports",
    "href": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#imports",
    "title": "Design with explicit Formula",
    "section": "Imports",
    "text": "Imports\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.api import Domain, Inputs\nfrom bofire.data_models.features.api import ContinuousInput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\nfrom bofire.utils.doe import get_confounding_matrix"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#setup-of-the-problem",
    "href": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#setup-of-the-problem",
    "title": "Design with explicit Formula",
    "section": "Setup of the problem",
    "text": "Setup of the problem\n\ninput_features = Inputs(\n    features=[\n        ContinuousInput(key=\"a\", bounds=(0, 5)),\n        ContinuousInput(key=\"b\", bounds=(40, 800)),\n        ContinuousInput(key=\"c\", bounds=(80, 180)),\n        ContinuousInput(key=\"d\", bounds=(200, 800)),\n    ],\n)\ndomain = Domain(inputs=input_features)"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#definition-of-the-formula-for-which-the-optimal-points-should-be-found",
    "href": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#definition-of-the-formula-for-which-the-optimal-points-should-be-found",
    "title": "Design with explicit Formula",
    "section": "Definition of the formula for which the optimal points should be found",
    "text": "Definition of the formula for which the optimal points should be found\n\nmodel_type = \"a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d\"\nmodel_type\n\n'a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d'"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#find-d-optimal-design",
    "href": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#find-d-optimal-design",
    "title": "Design with explicit Formula",
    "section": "Find D-optimal Design",
    "text": "Find D-optimal Design\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=model_type),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ndesign = strategy.ask(17)\ndesign\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.000000\n40.000000\n180.0\n800.000000\n\n\n1\n5.000000\n40.000000\n80.0\n200.000000\n\n\n2\n0.000000\n800.000000\n80.0\n800.000000\n\n\n3\n5.000000\n800.000000\n80.0\n200.000000\n\n\n4\n5.000000\n800.000000\n180.0\n800.000000\n\n\n5\n0.000000\n40.000000\n80.0\n200.000000\n\n\n6\n3.205374\n378.966400\n80.0\n266.510647\n\n\n7\n5.000000\n40.000000\n80.0\n800.000000\n\n\n8\n0.000000\n397.463954\n180.0\n531.694081\n\n\n9\n0.000000\n800.000000\n80.0\n200.000000\n\n\n10\n0.000000\n800.000000\n180.0\n200.000000\n\n\n11\n5.000000\n40.000000\n180.0\n200.000000\n\n\n12\n5.000000\n634.653072\n180.0\n200.000000\n\n\n13\n0.000000\n40.000000\n80.0\n800.000000\n\n\n14\n5.000000\n40.000000\n180.0\n800.000000\n\n\n15\n0.000000\n800.000000\n180.0\n800.000000\n\n\n16\n5.000000\n800.000000\n80.0\n800.000000"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#analyze-confounding",
    "href": "build/lib/docs/tutorials/doe/design_with_explicit_formula.html#analyze-confounding",
    "title": "Design with explicit Formula",
    "section": "Analyze Confounding",
    "text": "Analyze Confounding\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nmatplotlib.rcParams[\"figure.dpi\"] = 120\n\nm = get_confounding_matrix(\n    domain.inputs,\n    design=design,\n    interactions=[2, 3],\n    powers=[2],\n)\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/index.html",
    "href": "build/lib/docs/tutorials/doe/index.html",
    "title": "Design of Experiments (DOE)",
    "section": "",
    "text": "These tutorials demonstrate the usage of traditional design of experiments algorithms implemented in BoFire, including optimal designs and factorial designs.\n\n\n\n\nIntroduction to basic design of experiments concepts and implementations.\n\n\n\nCreating experimental designs using explicit model formulas.\n\n\n\nEfficient screening designs using fractional factorial approaches.\n\n\n\nHandling n-choose-k constraints in experimental designs.\n\n\n\nUsing different optimality criteria (D-optimal, A-optimal, etc.) for design generation."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/index.html#available-tutorials",
    "href": "build/lib/docs/tutorials/doe/index.html#available-tutorials",
    "title": "Design of Experiments (DOE)",
    "section": "",
    "text": "Introduction to basic design of experiments concepts and implementations.\n\n\n\nCreating experimental designs using explicit model formulas.\n\n\n\nEfficient screening designs using fractional factorial approaches.\n\n\n\nHandling n-choose-k constraints in experimental designs.\n\n\n\nUsing different optimality criteria (D-optimal, A-optimal, etc.) for design generation."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/optimality_criteria.html",
    "href": "build/lib/docs/tutorials/doe/optimality_criteria.html",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "",
    "text": "This tutorial notebook demonstrates the impact of different optimality-criteria for generating candidates using the DoE strategy for a two-dimensional fully-quadratic model."
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/optimality_criteria.html#imports",
    "href": "build/lib/docs/tutorials/doe/optimality_criteria.html#imports",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "Imports",
    "text": "Imports\n\nimport matplotlib.pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import (\n    AOptimalityCriterion,\n    DOptimalityCriterion,\n    EOptimalityCriterion,\n    IOptimalityCriterion,\n    KOptimalityCriterion,\n    SpaceFillingCriterion,\n)\nfrom bofire.strategies.doe.objective import get_objective_function"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/optimality_criteria.html#designs-for-different-optimality-criteria",
    "href": "build/lib/docs/tutorials/doe/optimality_criteria.html#designs-for-different-optimality-criteria",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "Designs for different optimality criteria",
    "text": "Designs for different optimality criteria\n\n# Optimal designs for a quadratic model on the unit square\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(2)],\n    outputs=[ContinuousOutput(key=\"y\")],\n)\nmodel_type = \"fully-quadratic\"\nn_experiments = 13\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor crit, label in [\n    (DOptimalityCriterion, \"D-Optimality\"),\n    (AOptimalityCriterion, \"A-Optimality\"),\n    (KOptimalityCriterion, \"K-Optimality\"),\n    (EOptimalityCriterion, \"E-Optimality\"),\n    (IOptimalityCriterion, \"I-Optimality\"),\n]:\n    criterion = crit(formula=model_type)\n    data_model = DoEStrategy(\n        domain=domain,\n        criterion=criterion,\n        ipopt_options={\"max_iter\": 300},\n    )\n    strategy = strategies.map(data_model=data_model)\n    design = strategy.ask(candidate_count=n_experiments)\n    obj_value = get_objective_function(\n        criterion=criterion, domain=domain, n_experiments=n_experiments\n    ).evaluate(design.to_numpy().flatten())\n    ax.scatter(design.x1, design.x2, s=40, label=f\"{label}\")\n\n\nax.set_title(\"Designs with different optimality criteria\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.grid(alpha=0.3)\nax.legend()\n\nplt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/doe/optimality_criteria.html#space-filling-design",
    "href": "build/lib/docs/tutorials/doe/optimality_criteria.html#space-filling-design",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "Space filling design",
    "text": "Space filling design\nBoFire can also generate space filling designs, here it is show three dimensions and a simplex constraint.\n\n# Space filling design on the unit 2-simplex\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(3)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain, criterion=SpaceFillingCriterion(), ipopt_options={\"max_iter\": 500}\n)\nstrategy = strategies.map(data_model=data_model)\nX = strategy.ask(candidate_count=40).to_numpy()\n\nfig = plt.figure(figsize=((10, 8)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 20)\nax.set_title(\"Space filling design\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\n\n# plot feasible polytope\nax.plot(xs=[0, 0, 1, 0], ys=[0, 1, 0, 0], zs=[1, 0, 0, 1], linewidth=2)\n\n# plot design points\nax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], s=40)\n\nplt.show()"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/models_serial.html",
    "href": "build/lib/docs/tutorials/serialization/models_serial.html",
    "title": "Model Building with BoFire",
    "section": "",
    "text": "This notebooks shows how to setup and analyze models trained with BoFire. It is still WIP."
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/models_serial.html#imports",
    "href": "build/lib/docs/tutorials/serialization/models_serial.html#imports",
    "title": "Model Building with BoFire",
    "section": "Imports",
    "text": "Imports\n\nfrom pydantic import TypeAdapter\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.multi import CrossCoupling\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.domain.api import Outputs\nfrom bofire.data_models.enum import CategoricalEncodingEnum\nfrom bofire.data_models.surrogates.api import (\n    AnySurrogate,\n    EmpiricalSurrogate,\n    MixedSingleTaskGPSurrogate,\n    RandomForestSurrogate,\n    RegressionMLPEnsemble,\n    SingleTaskGPSurrogate,\n)"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/models_serial.html#problem-setup",
    "href": "build/lib/docs/tutorials/serialization/models_serial.html#problem-setup",
    "title": "Model Building with BoFire",
    "section": "Problem Setup",
    "text": "Problem Setup\nFor didactic purposes, we sample data from a Himmelblau benchmark function and use them to train a SingleTaskGP.\n\nbenchmark = Himmelblau()\nsamples = benchmark.domain.inputs.sample(n=50)\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments.head(10)\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n-3.060758\n-4.700833\n184.992842\n1\n\n\n1\n4.532513\n5.068375\n752.724086\n1\n\n\n2\n0.592843\n3.322310\n75.115926\n1\n\n\n3\n-0.479599\n-4.348970\n359.317872\n1\n\n\n4\n5.034419\n-3.313098\n202.909893\n1\n\n\n5\n-2.808801\n-3.367185\n44.300409\n1\n\n\n6\n-0.390851\n0.072834\n170.634030\n1\n\n\n7\n-1.092262\n-4.595400\n377.090173\n1\n\n\n8\n-3.757704\n-1.551165\n72.211382\n1\n\n\n9\n0.051352\n2.623895\n70.119034\n1"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/models_serial.html#model-fitting",
    "href": "build/lib/docs/tutorials/serialization/models_serial.html#model-fitting",
    "title": "Model Building with BoFire",
    "section": "Model Fitting",
    "text": "Model Fitting\n\ninput_features = benchmark.domain.inputs\noutput_features = benchmark.domain.outputs\n\n\ninput_features.model_dump_json()\n\n'{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]}'\n\n\n\noutput_features.model_dump_json()\n\n'{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]}'\n\n\n\nSingle Task GP\nGenerate the json spec\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = SingleTaskGPSurrogate(inputs=input_features, outputs=output_features)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}'\n\n\nLoad it from the spec\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n\nMap it\n\nsurrogate = surrogates.map(surrogate_data)\n\nFit it. This is not 100% finished. In the future we will call here hyperfit which will return the CV results etc. This has to be finished. So ignore this for now and just call fit.\n\nsurrogate.fit(experiments=experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nDump it.\n\n# dump it\ndump = surrogate.dumps()\n\nMake predictions.\n\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\nLoad again from spec and dump and make predictions.\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue\n\n\n\n\nRandom Forest\nGenerate the json spec\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = RandomForestSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n    random_state=42,\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":null,\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"RandomForestSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"n_estimators\":100,\"criterion\":\"squared_error\",\"max_depth\":null,\"min_samples_split\":2,\"min_samples_leaf\":1,\"min_weight_fraction_leaf\":0.0,\"max_features\":1.0,\"max_leaf_nodes\":null,\"min_impurity_decrease\":0.0,\"bootstrap\":true,\"oob_score\":false,\"random_state\":42,\"ccp_alpha\":0.0,\"max_samples\":null}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# Fit it\nsurrogate.fit(experiments=experiments)\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\nTrue\n\n\n\n\nMLP Ensemble\nGenerate the json spec\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = RegressionMLPEnsemble(\n    inputs=input_features,\n    outputs=output_features,\n    n_estimators=2,\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":null,\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"RegressionMLPEnsemble\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"IDENTITY\",\"output_scaler\":\"IDENTITY\",\"n_estimators\":2,\"hidden_layer_sizes\":[100],\"activation\":\"relu\",\"dropout\":0.0,\"batch_size\":10,\"n_epochs\":200,\"lr\":0.0001,\"weight_decay\":0.0,\"subsample_fraction\":1.0,\"shuffle\":true,\"final_activation\":\"identity\"}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# Fit it\nsurrogate.fit(experiments=experiments)\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue"
  },
  {
    "objectID": "build/lib/docs/tutorials/serialization/models_serial.html#empirical-surrogate",
    "href": "build/lib/docs/tutorials/serialization/models_serial.html#empirical-surrogate",
    "title": "Model Building with BoFire",
    "section": "Empirical Surrogate",
    "text": "Empirical Surrogate\nThe empirical model is special as it has per default no fit and you need cloudpickle. There can be empirical models which implement a fit, but for this they also have to inherit from Trainable. The current example is the default without any fit functionality.\n\nfrom botorch.models.deterministic import DeterministicModel\nfrom torch import Tensor\n\n\nclass HimmelblauModel(DeterministicModel):\n    def __init__(self):\n        super().__init__()\n        self._num_outputs = 1\n\n    def forward(self, X: Tensor) -&gt; Tensor:\n        return (\n            (X[..., 0] ** 2 + X[..., 1] - 11.0) ** 2\n            + (X[..., 0] + X[..., 1] ** 2 - 7.0) ** 2\n        ).unsqueeze(-1)\n\n\nsurrogate_data = EmpiricalSurrogate(\n\n    inputs=input_features,\n    outputs=output_features,\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"type\":\"EmpiricalSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{}}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# attach the actual model to it\nsurrogate.model = HimmelblauModel()\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue\n\n\n\nMixed GP\nGenerate data for a mixed problem.\n\nbenchmark = CrossCoupling()\nsamples = benchmark.domain.inputs.sample(n=50)\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments.head(10)\n\n\n\n\n\n\n\n\nbase_eq\nt_res\ntemperature\nbase\ncatalyst\nyield\ncost\nvalid_cost\nvalid_yield\n\n\n\n\n0\n1.689718\n714.259563\n50.585137\nTEA\ntBuBrettPhos\n-0.001553\n0.278885\n1\n1\n\n\n1\n1.255766\n606.556684\n79.660707\nTEA\ntBuXPhos\n0.062620\n0.248696\n1\n1\n\n\n2\n1.295070\n244.552650\n90.600736\nBTMG\nAlPhos\n0.938471\n0.475580\n1\n1\n\n\n3\n1.130402\n1741.925820\n80.800082\nTMG\ntBuBrettPhos\n0.665889\n0.278312\n1\n1\n\n\n4\n2.336282\n596.766757\n33.594086\nBTMG\ntBuXPhos\n0.855915\n0.350288\n1\n1\n\n\n5\n1.746800\n1250.912139\n37.222488\nDBU\nAlPhos\n0.798264\n0.420919\n1\n1\n\n\n6\n1.433599\n756.109382\n55.323248\nDBU\nAlPhos\n0.699118\n0.420577\n1\n1\n\n\n7\n1.492249\n949.352283\n33.021765\nTEA\ntBuBrettPhos\n0.052626\n0.278814\n1\n1\n\n\n8\n1.395102\n128.426292\n65.210822\nTEA\ntBuBrettPhos\n-0.001775\n0.278778\n1\n1\n\n\n9\n2.165214\n1595.083659\n40.063186\nTMG\ntBuXPhos\n0.162979\n0.248318\n1\n1\n\n\n\n\n\n\n\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = MixedSingleTaskGPSurrogate(\n    inputs=benchmark.domain.inputs,\n    outputs=Outputs(features=[benchmark.domain.outputs.features[0]]),\n    categorical_encodings={\"catalyst\": CategoricalEncodingEnum.ORDINAL},\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":{\"type\":\"MixedSingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"continuous_kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\"},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"MixedSingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalDescriptorInput\",\"key\":\"catalyst\",\"categories\":[\"tBuXPhos\",\"tBuBrettPhos\",\"AlPhos\"],\"allowed\":[true,true,true],\"descriptors\":[\"area_cat\",\"M2_cat\"],\"values\":[[460.7543,67.2057],[518.8408,89.8738],[819.933,129.0808]]},{\"type\":\"CategoricalDescriptorInput\",\"key\":\"base\",\"categories\":[\"TEA\",\"TMG\",\"BTMG\",\"DBU\"],\"allowed\":[true,true,true,true],\"descriptors\":[\"area\",\"M2\"],\"values\":[[162.2992,25.8165],[165.5447,81.4847],[227.3523,30.554],[192.4693,59.8367]]},{\"type\":\"ContinuousInput\",\"key\":\"base_eq\",\"unit\":null,\"bounds\":[1.0,2.5],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"temperature\",\"unit\":null,\"bounds\":[30.0,100.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"t_res\",\"unit\":null,\"bounds\":[60.0,1800.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"yield\",\"unit\":null,\"objective\":{\"type\":\"MaximizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{\"base\":\"ORDINAL\",\"catalyst\":\"ORDINAL\"},\"dump\":null,\"categorical_encodings\":{\"catalyst\":\"ORDINAL\",\"base\":\"DESCRIPTOR\"},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"continuous_kernel\":{\"type\":\"RBFKernel\",\"features\":[\"temperature\",\"base\",\"t_res\",\"base_eq\"],\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":{\"type\":\"GreaterThan\",\"lower_bound\":0.025}},\"categorical_kernel\":{\"type\":\"HammingDistanceKernel\",\"features\":[\"catalyst\"],\"ard\":true,\"lengthscale_prior\":null,\"lengthscale_constraint\":{\"type\":\"GreaterThan\",\"lower_bound\":1e-6}},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# Fit it\nsurrogate.fit(experiments=experiments)\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue"
  },
  {
    "objectID": "docs/tutorials/advanced_examples/conditional_features_bo.html",
    "href": "docs/tutorials/advanced_examples/conditional_features_bo.html",
    "title": "Conditional Features",
    "section": "",
    "text": "When optimizing chemical processes, we often have some inputs that are dependent on others. For example, the value of a catalyst_concentration input feature is only relevant depending on another feature use_catalyst==True. Whilst it may seem that use_catalyst==False is equivalent to just setting catalyst_concentration==0, there may be some limitations to this approach: - If a catalyst is used, there may be some minimum amount required. It is difficult to model the disjoint bounds of a continuous feature. - It may be the case that some tiny presence of catalyst enables a side reaction that completely changes the reaction. We therefore have a step change at 0, with smoother behaviour everywhere else in the domain, which Gaussian process surrogates cannot model well.\nFor a some examples of literature on these problems, see [Swersky2014Arc] and [Horn2019Wedge].\n[Swersky2014Arc] Swersky et al. 2014, “Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces” arXiv [Horn2019Wedge] Horn et al. “Surrogates for hierarchical search spaces: the wedge-kernel and an automated analysis”, GECCO\nWe consider a test problem as described above, where we wish to optimize the yield of a reaction by controlling the temperature and catalyst concentration.\nfrom bofire.data_models.constraints.api import NonZeroCondition\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.kernels.api import RBFKernel, WedgeKernel\n\n\ncatalyst_domain = Domain.from_lists(\n    inputs=[\n        ContinuousInput(key=\"temperature\", unit=\"°C\", bounds=(50.0, 100.0)),\n        ContinuousInput(key=\"catalyst_concentration\", unit=\"M\", bounds=(0.2, 1.0), allow_zero=True),\n    ],\n    outputs=[ContinuousOutput(key=\"yield\")],\n)\n\nindicator_kernel = WedgeKernel(\n    base_kernel=RBFKernel(),\n    conditions=[\n        (\"catalyst_concentration\", \"catalyst_concentration\", NonZeroCondition())\n    ],\n)\nAfter defining the domain, we can then build the wedge kernel for our GP surrogate.\nfrom bofire.data_models.kernels.api import RBFKernel, WedgeKernel\n# here, we manually build the list of conditions\n# in future we will automatically build them from any inputs with allow_zero==True\nconditions = [\n    (\n        \"catalyst_concentration\",\n        \"catalyst_concentration\",\n        NonZeroCondition(),\n    )\n]\n\nwedge_kernel_data_model = WedgeKernel(\n    base_kernel=RBFKernel(),\n    conditions=conditions,\n)\nimport torch\n\nimport bofire.kernels.api as kernels\n\n\ndef features_to_idx_mapper(feats: list[str]) -&gt; list[int]:\n    return catalyst_domain.inputs.get_feature_indices({}, feats)\n\n\nwedge_kernel = kernels.map(\n    wedge_kernel_data_model,\n    batch_shape=torch.Size([]),\n    active_dims=list(range(2)),\n    features_to_idx_mapper=features_to_idx_mapper,\n)\n# cast the return type from `kernels.map` to the wedge kernel\n# this fixes syntax highlighting in this notebook\nfrom typing import cast\n\nfrom bofire.kernels.conditional import WedgeKernel as WedgeKernelFunctional\n\nwedge_kernel = cast(WedgeKernelFunctional, wedge_kernel)\nwedge_kernel\n\nWedgeKernel(\n  (raw_lengthscale_constraint): Positive()\n  (raw_angle_constraint): Interval(1.000E-04, 9.999E-01)\n  (raw_radius_constraint): Positive()\n  (base_kernel): RBFKernel(\n    (raw_lengthscale_constraint): Positive()\n  )\n)\nWe check below that the kernel behaves as expected. Specifically, we want to check that the indicator function correctly masks the conditional feature to be inactive when the condition is not met.\n# check the order of dimensions in X\nassert features_to_idx_mapper([\"catalyst_concentration\", \"temperature\"]) == [0, 1]\n\nX = torch.tensor(\n    [\n        [0.0, 0.0],\n        [0.05, 0.0],\n        [0.50, 0.6],\n    ]\n)\n\n# the indicator function shows which dimensions are active\n# X[:, 1] should always be active\n# X[:, 0] should only be active if X[:, 0] != 0\nwedge_kernel.indicator_func(X)\n\ntensor([[False,  True],\n        [ True,  True],\n        [ True,  True]])"
  },
  {
    "objectID": "docs/tutorials/advanced_examples/conditional_features_bo.html#using-the-wedge-kernel-for-bayesian-optimizaiton",
    "href": "docs/tutorials/advanced_examples/conditional_features_bo.html#using-the-wedge-kernel-for-bayesian-optimizaiton",
    "title": "Conditional Features",
    "section": "Using the wedge kernel for Bayesian optimizaiton",
    "text": "Using the wedge kernel for Bayesian optimizaiton\nUsing the domain defined at the start of the notebook, we now create a benchmark that we can use to evaluate how well this kernel works for Bayesian optimization.\nWe set up our objective function such that adding a catalyst enables a side reaction, and so any small prescence of the catalyst hurts yield. However, at higher concentration of catalyst, we see improved performance.\nIncreasing temperature increases the yield, however it also causes decomposition of the catalyst at high temperatures.\n\nimport numpy as np\nimport pandas as pd\n\nfrom bofire.benchmarks.api import Benchmark\n\n\nclass ReactionOptimizationBenchmark(Benchmark):\n    _domain = catalyst_domain\n\n    def _f(self, X: pd.DataFrame, **kwargs) -&gt; pd.DataFrame:  # type: ignore\n        cat_conc = X[\"catalyst_concentration\"]\n        temp = X[\"temperature\"]\n        norm_temp = (temp - 50) / 50\n\n        side_product = np.where(cat_conc &gt; 0.0, 10.0, 0.0)\n        effective_catalyst = np.clip(cat_conc - 1.5 * (norm_temp - 0.5), 0.0, cat_conc)\n\n        catalyst_effect = 10 * np.exp(effective_catalyst)\n        temperature_effect = 10 * np.exp(norm_temp)\n        y = temperature_effect + catalyst_effect - side_product\n\n        Y = pd.DataFrame({\"yield\": y, \"valid_yield\": 1})\n        return Y\n\n\nbenchmark = ReactionOptimizationBenchmark()\n\nFirst, we plot the objective function:\n\nimport matplotlib.pyplot as plt\n\n\nN_grid_pts = 50\n# plot active points\nX_c, X_t = np.meshgrid(\n    np.linspace(0.5, 1.0, num=N_grid_pts), np.linspace(50, 100, num=N_grid_pts)\n)\ngrid_X = np.stack((X_c.flatten(), X_t.flatten()), axis=-1)\ngrid_Y = benchmark.f(\n    pd.DataFrame(data=grid_X, columns=[\"catalyst_concentration\", \"temperature\"])\n)[\"yield\"].to_numpy()\nplt.contourf(X_c, X_t, grid_Y.reshape(N_grid_pts, N_grid_pts), vmin=10, vmax=40)\n\n# plot inactive points\nX_c, X_t = np.meshgrid(\n    np.linspace(-0.05, 0.05, num=2), np.linspace(50, 100, num=N_grid_pts)\n)\ngrid_X = np.stack((0.0 * X_c.flatten(), X_t.flatten()), axis=-1)\ngrid_Y = benchmark.f(\n    pd.DataFrame(data=grid_X, columns=[\"catalyst_concentration\", \"temperature\"])\n)[\"yield\"].to_numpy()\nplt.contourf(X_c, X_t, grid_Y.reshape(N_grid_pts, 2), vmin=10, vmax=40)\n\nplt.colorbar()\nplt.xlabel(\"catalyst_concentration\")\nplt.ylabel(\"temperature\")\n\nText(0, 0.5, 'temperature')\n\n\n\n\n\n\n\n\n\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\n\n\nsamples = strategies.map(RandomStrategy(domain=catalyst_domain, seed=0)).ask(10)\n# RandomStrategy doesn't currently support randomly sampling with allow_zero\n# So we manually set the first 2 reactions to have no catalyst.\nsamples.loc[:1, \"catalyst_concentration\"] = 0.0\nexperiments = benchmark.f(samples, return_complete=True)\nexperiments.head(4)\n\n\n\n\n\n\n\n\ncatalyst_concentration\ntemperature\nyield\nvalid_yield\n\n\n\n\n0\n0.000000\n59.171060\n22.013203\n1\n\n\n1\n0.000000\n74.864291\n26.442524\n1\n\n\n2\n0.297314\n92.215550\n23.263744\n1\n\n\n3\n0.406463\n75.872417\n21.404513\n1\n\n\n\n\n\n\n\n\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\n\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=catalyst_domain,\n    seed=1,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=catalyst_domain.inputs,\n                outputs=catalyst_domain.outputs,\n                kernel=wedge_kernel_data_model,\n            )\n        ]\n    ),\n)\nstrategy = strategies.map(sobo_strategy_data_model)\n\n\nstrategy.tell(experiments, replace=True)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nWe would expect the proposed batch to include some experiments with no catalyst, and some with a catalyst. Note that the volume of the search space with catalyst is larger, so we may also expect more experiments with the feature active.\n\nstrategy.ask(5)\n\n\n\n\n\n\n\n\ncatalyst_concentration\ntemperature\nyield_pred\nyield_sd\nyield_des\n\n\n\n\n0\n0.920084\n55.793790\n29.725369\n2.410661\n29.725369\n\n\n1\n0.000000\n97.313192\n27.228390\n3.424580\n27.228390\n\n\n2\n1.000000\n72.165219\n31.275936\n0.585717\n31.275936\n\n\n3\n0.616450\n50.000000\n27.636364\n3.372072\n27.636364\n\n\n4\n1.000000\n50.000000\n28.600987\n3.072161\n28.600987"
  },
  {
    "objectID": "docs/tutorials/advanced_examples/desirability_objectives.html",
    "href": "docs/tutorials/advanced_examples/desirability_objectives.html",
    "title": "Desirability Functions for Multi-Objective Optimization",
    "section": "",
    "text": "This notebook demonstrates the use of desirability functions for multi-objective optimization. The desirability function is a scalar function that maps a vector of objective values to a scalar value, most often in the range [0, 1]. The desirability function is used to aggregate multiple objectives into a single objective value, e.g. by the multiplicative Sobo strategy.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom bofire.data_models.objectives import api as objectives_data_model\n\n\nDesirability Functions map from the input space to the range [0, 1], also by clipping after the bounds\n\nobjectives = {\n    \"Increasing\": objectives_data_model.IncreasingDesirabilityObjective(\n        bounds=(0.0, 5.0)\n    ),\n    \"Decreasing\": objectives_data_model.DecreasingDesirabilityObjective(\n        bounds=(0.0, 5.0)\n    ),\n    \"Peak\": objectives_data_model.PeakDesirabilityObjective(\n        bounds=(0.0, 5.0), peak_position=2.5\n    ),\n    \"In-Range\": objectives_data_model.InRangeDesirability(bounds=(1.0, 3.0)),\n}\n\n\nfor key, objective in objectives.items():\n    x = np.linspace(-2.0, 7.0, 100)\n    y = objective(x, None)\n    plt.plot(x, y, label=key)\nplt.grid(True)\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nClipping is optional, but leads to values outside the [0, 1] range\n\nobjectives = {\n    \"Increasing\": objectives_data_model.IncreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), clip=False\n    ),\n    \"Decreasing\": objectives_data_model.DecreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), clip=False\n    ),\n    \"Peak\": objectives_data_model.PeakDesirabilityObjective(\n        bounds=(0.0, 5.0), peak_position=2.5, clip=False\n    ),\n}\nfor key, objective in objectives.items():\n    x = np.linspace(-2.0, 7.0, 100)\n    y = objective(x, None)\n    plt.plot(x, y, label=key)\nplt.grid(True)\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nA concave or convex desirability function can be created by setting the log_shape_factor\n\nobjectives = {\n    \"Increasing\": objectives_data_model.IncreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), log_shape_factor=1.0\n    ),\n    \"Decreasing\": objectives_data_model.DecreasingDesirabilityObjective(\n        bounds=(0.0, 5.0), log_shape_factor=-1.0\n    ),\n    \"Peak\": objectives_data_model.PeakDesirabilityObjective(\n        bounds=(0.0, 5.0),\n        peak_position=2.5,\n        log_shape_factor=-1.0,\n        log_shape_factor_decreasing=1.0,\n    ),\n}\nfor key, objective in objectives.items():\n    x = np.linspace(-2.0, 7.0, 100)\n    y = objective(x, None)\n    plt.plot(x, y, label=key)\nplt.grid(True)\nplt.legend()",
    "crumbs": [
      "Advanced Examples",
      "Desirability Functions for Multi-Objective Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/index.html",
    "href": "docs/tutorials/advanced_examples/index.html",
    "title": "Advanced Examples",
    "section": "",
    "text": "These notebooks showcase more specialized and advanced use cases in BoFire. These examples are not necessarily better strategies, but represent more complex uses of components within the library.\n\n\n\n\nCreate custom single-objective Bayesian optimization strategies.\n\n\n\nWorking with desirability functions for multi-criteria optimization.\n\n\n\nUsing genetic algorithms for optimization in BoFire.\n\n\n\nTechniques for combining multiple objectives in optimization.\n\n\n\nLeveraging multiple fidelity levels for efficient optimization.\n\n\n\nDefining optimization objectives directly on input parameters.\n\n\n\nUsing Random Forest as a surrogate model instead of Gaussian Processes.\n\n\n\nApplying transfer learning techniques to Bayesian optimization.\n\n\n\nDefine input features that are conditionally active.",
    "crumbs": [
      "Advanced Examples",
      "Advanced Examples"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/index.html#available-tutorials",
    "href": "docs/tutorials/advanced_examples/index.html#available-tutorials",
    "title": "Advanced Examples",
    "section": "",
    "text": "Create custom single-objective Bayesian optimization strategies.\n\n\n\nWorking with desirability functions for multi-criteria optimization.\n\n\n\nUsing genetic algorithms for optimization in BoFire.\n\n\n\nTechniques for combining multiple objectives in optimization.\n\n\n\nLeveraging multiple fidelity levels for efficient optimization.\n\n\n\nDefining optimization objectives directly on input parameters.\n\n\n\nUsing Random Forest as a surrogate model instead of Gaussian Processes.\n\n\n\nApplying transfer learning techniques to Bayesian optimization.\n\n\n\nDefine input features that are conditionally active.",
    "crumbs": [
      "Advanced Examples",
      "Advanced Examples"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/multifidelity_bo.html",
    "href": "docs/tutorials/advanced_examples/multifidelity_bo.html",
    "title": "",
    "section": "",
    "text": "Code\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Ackley, Benchmark, Branin\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import TaskInput\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, MultiTaskGPSurrogate\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nNUM_INIT_HF = 4\nNUM_INIT_LF = 10\nif SMOKE_TEST:\n    num_runs = 5\n    num_iters = 2\n    verbose = False\nelse:\n    num_runs = 10\n    num_iters = 10\n    verbose = True\nThis notebook is a sequel to “Transfer Learning in BO”.",
    "crumbs": [
      "Advanced Examples",
      "Multi-fidelity Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/multifidelity_bo.html#problem-definition",
    "href": "docs/tutorials/advanced_examples/multifidelity_bo.html#problem-definition",
    "title": "",
    "section": "Problem definition",
    "text": "Problem definition\nWe use the same problem as the transfer learning notebook; optimizing the Branin benchmark, with a low-fidelity function biased by the Ackley function (with fewer initial points, to demonstrate the strength of being able to query low fidelities). Below, we define the problem domain, and the strategies we will use to optimize.\nAs a baseline, we use the SoboStrategy with the MultiTaskSurrogate, as in the previous notebook. We also introduce the MultiFidelityStrategy here, which uses the same surrogate, but is able to query the lower fidelity functions using a variance-based acquisition function [Kandasamy et al. 2016, Folch et al. 2023].\nBoth strategies first select a design point \\(x\\) by optimizing the target fidelity. The MultiFidelityStrategy then selects the fidelity, \\(m\\), by selecting the lowest fidelity that has a variance over a fixed threshold. This means that the strategy will explore the cheapest fidelities first, and only query the expensive fidelities when there is no information to be gained by the cheap approximations.\n\nclass BraninMultiTask(Benchmark):\n    def __init__(self, low_fidelity_allowed=False, **kwargs):\n        super().__init__(**kwargs)\n        self._branin = Branin()\n        self._ackley = Ackley()\n        task_input = TaskInput(\n            key=\"task\",\n            categories=[\"task_hf\", \"task_lf\"],\n            allowed=[True, low_fidelity_allowed],\n            fidelities=[0, 1],\n        )\n        self._domain = Domain(\n            inputs=self._branin.domain.inputs + (task_input,),\n            outputs=self._branin.domain.outputs,\n        )\n\n    def _f(self, candidates: pd.DataFrame) -&gt; pd.DataFrame:\n        candidates_no_task = candidates.drop(columns=[\"task\"])\n        f_branin = self._branin.f(candidates_no_task)\n        f_ackley = self._ackley.f(candidates_no_task)\n        bias_scale = np.where(candidates[\"task\"] == \"task_hf\", 0.0, 0.15).reshape(-1, 1)\n        bias_scale = pd.DataFrame(bias_scale, columns=self._domain.outputs.get_keys())\n        bias_scale[\"valid_y\"] = 0.0\n        return f_branin + bias_scale * f_ackley\n\n    def get_optima(self) -&gt; pd.DataFrame:\n        optima = self._branin.get_optima()\n        optima[\"task\"] = \"task_hf\"\n        return optima\n\n\nmf_benchmark = BraninMultiTask(low_fidelity_allowed=True)\ntl_benchmark = BraninMultiTask(low_fidelity_allowed=False)\n\n\ndef create_data_set(seed: int):\n    # use the tl_benchmark to sample without the low fidelity\n    experiments = tl_benchmark.domain.inputs.sample(\n        NUM_INIT_HF + NUM_INIT_LF, seed=seed\n    )\n    experiments[\"task\"] = np.where(\n        experiments.index &lt; NUM_INIT_LF, \"task_lf\", \"task_hf\"\n    )\n\n    # then use the ml_benchmark to evaluate the low fidelity\n    return mf_benchmark.f(experiments, return_complete=True)\n\n\ncreate_data_set(0)\n\n\n\n\n\n\n\n\nx_1\nx_2\ntask\ny\nvalid_y\n\n\n\n\n0\n-2.236178\n5.665713\ntask_lf\n26.409237\n1.0\n\n\n1\n-2.139889\n8.469326\ntask_lf\n9.288236\n1.0\n\n\n2\n1.124986\n10.645193\ntask_lf\n55.820241\n1.0\n\n\n3\n5.262999\n13.213141\ntask_lf\n161.890497\n1.0\n\n\n4\n-1.920241\n6.790379\ntask_lf\n16.125827\n1.0\n\n\n5\n0.962901\n2.647478\ntask_lf\n20.232801\n1.0\n\n\n6\n8.704473\n8.641214\ntask_lf\n50.234493\n1.0\n\n\n7\n7.233487\n7.940435\ntask_lf\n62.729505\n1.0\n\n\n8\n3.697975\n1.518759\ntask_lf\n3.272981\n1.0\n\n\n9\n3.066951\n9.750911\ntask_lf\n57.729416\n1.0\n\n\n10\n1.859665\n14.993475\ntask_hf\n139.663235\n1.0\n\n\n11\n6.712335\n7.141519\ntask_hf\n54.780215\n1.0\n\n\n12\n-1.488379\n4.954397\ntask_hf\n24.485001\n1.0\n\n\n13\n9.025726\n1.709098\ntask_hf\n1.354707\n1.0\n\n\n\n\n\n\n\n\nfrom bofire.data_models.strategies.api import MultiFidelityStrategy\n\n\n# It isn't necessary to define the surrogate specs here, as the MFStrategy\n# will use a MultiTaskGP by default.\n\nmf_data_model = MultiFidelityStrategy(\n    domain=mf_benchmark.domain,\n    acquisition_function=qLogEI(),\n    fidelity_thresholds=0.1,\n)\nmf_data_model.surrogate_specs.surrogates[0].inputs\n\nInputs(type='Inputs', features=[ContinuousInput(type='ContinuousInput', key='x_1', unit=None, bounds=[-5.0, 10.0], local_relative_bounds=None, stepsize=None, allow_zero=False), ContinuousInput(type='ContinuousInput', key='x_2', unit=None, bounds=[0.0, 15.0], local_relative_bounds=None, stepsize=None, allow_zero=False), TaskInput(type='TaskInput', key='task', categories=['task_hf', 'task_lf'], allowed=[True, True], fidelities=[0, 1])])\n\n\n\nfrom bofire.data_models.strategies.api import SoboStrategy\n\n\nsurrogate_specs = BotorchSurrogates(\n    surrogates=[\n        MultiTaskGPSurrogate(\n            inputs=tl_benchmark.domain.inputs,\n            outputs=tl_benchmark.domain.outputs,\n        )\n    ]\n)\n\ntl_data_model = SoboStrategy(\n    domain=tl_benchmark.domain,\n    acquisition_function=qLogEI(),\n    surrogate_specs=surrogate_specs,\n)",
    "crumbs": [
      "Advanced Examples",
      "Multi-fidelity Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/octane_number.html",
    "href": "docs/tutorials/advanced_examples/octane_number.html",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "",
    "text": "This tutorial shows how to predict the Motor Octane Number (MON) of a hydrocarbon mixture. The dataset stems from the paper from Chew et al.. It contains 722 experiments of up to 121 componet mixtures of 423 individual molecules. Each molecule is represented by its SMILES string.",
    "crumbs": [
      "Advanced Examples",
      "Predict Motor Octane Number of Hydrocarbon Mixtures"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/octane_number.html#imports",
    "href": "docs/tutorials/advanced_examples/octane_number.html#imports",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "Imports",
    "text": "Imports\n\nimport os\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.data.octane_number import get_octane_data\nfrom bofire.data_models.domain.api import EngineeredFeatures, Inputs, Outputs\nfrom bofire.data_models.features.api import (\n    ContinuousMolecularInput,\n    ContinuousOutput,\n    MolecularWeightedSumFeature,\n)\nfrom bofire.data_models.molfeatures.api import MordredDescriptors\nfrom bofire.data_models.molfeatures.names import mordred as mordred_names\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Advanced Examples",
      "Predict Motor Octane Number of Hydrocarbon Mixtures"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/octane_number.html#setup-data",
    "href": "docs/tutorials/advanced_examples/octane_number.html#setup-data",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "Setup Data",
    "text": "Setup Data\n\ndf_experiments = get_octane_data()\ndf_experiments[\"valid_MON\"] = 1\n\noutput_key = \"MON\"\n\ninputs = Inputs(\n    features = [\n        ContinuousMolecularInput(key=col, molecule=col, bounds=(0,1))\n        for col in df_experiments.columns\n        if col not in [\"MON\", \"Label\", \"valid_MON\"]\n    ]\n)\noutputs = Outputs(features=[ContinuousOutput(key=output_key)])\n\n/tmp/ipykernel_6115/4204084361.py:2: PerformanceWarning:\n\nDataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`",
    "crumbs": [
      "Advanced Examples",
      "Predict Motor Octane Number of Hydrocarbon Mixtures"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/octane_number.html#setup-surrogate-and-perform-cv",
    "href": "docs/tutorials/advanced_examples/octane_number.html#setup-surrogate-and-perform-cv",
    "title": "Predict Motor Octane Number of Hydrocarbon Mixtures",
    "section": "Setup Surrogate and perform CV",
    "text": "Setup Surrogate and perform CV\nWe model the high-dimensional problem by using an engineered feature called MolecularWeightedSumFeature. In computes the weighted sum of the molecular descriptors of the original ContinuousMolecularInputs that make up the engineered feature. Here we use Mordred descriptors with a correlation cutoff of 0.9.\n\nsurrogate_data = SingleTaskGPSurrogate(\n    inputs=inputs,\n    outputs=outputs,\n    engineered_features = EngineeredFeatures(\n        features=[\n                MolecularWeightedSumFeature(\n                    key=\"mixture\",\n                    features=inputs.get_keys(),\n                    molfeatures=MordredDescriptors(descriptors=mordred_names,ignore_3D=False, correlation_cutoff=0.9),\n                    keep_features=False\n                )\n            ]\n        )\n)\n\nprint(\"Number of molecular features before correlation filtering: \", len(surrogate_data.engineered_features[0].molfeatures.get_descriptor_names()))\nsurrogate = surrogates.map(surrogate_data)\ncv_train, cv_test, _ = surrogate.cross_validate(df_experiments, folds=10 if not SMOKE_TEST else 3)\n\ndisplay(cv_test.get_metrics())\n\nprint(\"Number of molecular features before correlation filtering: \", len(surrogate_data.engineered_features[0].molfeatures.get_descriptor_names()))\n\nNumber of molecular features before correlation filtering:  1826\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n\n\n\n\n\n\nMAE\nMSD\nR2\nMAPE\nPEARSON\nSPEARMAN\nFISHER\n\n\n\n\n0\n4.070695\n67.308609\n0.788384\n6.877943e+14\n0.888053\n0.891514\n6.251366e-123\n\n\n\n\n\n\n\nNumber of molecular features before correlation filtering:  394\n\n\nEven better performance can be achieved by using SAAS based surrogates, like AdditiveMapSaasSingleTaskGPSurrogate or EnsembleMapSaasSingleTaskGPSurrogate. Drawback are higher computational costs.",
    "crumbs": [
      "Advanced Examples",
      "Predict Motor Octane Number of Hydrocarbon Mixtures"
    ]
  },
  {
    "objectID": "docs/tutorials/advanced_examples/transfer_learning_bo.html",
    "href": "docs/tutorials/advanced_examples/transfer_learning_bo.html",
    "title": "",
    "section": "",
    "text": "Code\n\n\n\n\n\n\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport bofire.strategies.api as strategies\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.api import Ackley, Branin\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput, TaskInput\nfrom bofire.data_models.objectives.api import MaximizeObjective\nfrom bofire.data_models.strategies.api import SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    BotorchSurrogates,\n    MultiTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\n\n\nimport matplotlib.pyplot as plt\n\nIn this notebook we show how to use BoFire for the purposes of transfer learning Bayesian optimization. In particular, we assume we have a task \\(f_2\\) with data that is relevant to the optimization of our current task \\(f_1\\). The procedure is simple, we fit a MultiTask GP to both data-sets, however only carry out the BO on \\(f_1\\), i.e., we optimize the acquisition functions on on the task \\(f_1\\).\nWe build a small data-set using the target task:\n\\[ f_1(x) = \\sin(2 \\pi x) \\]\nAnd we will have data the second related task:\n\\[ f_2 = 0.9 \\sin(2 \\pi x) + 0.2 \\cos(3 \\pi x) - 0.2 \\]\nWe begin by defining the functions, generating some data, and plotting it. We generate 15 data-points for Task 2 and just 4 data-points for Task 1, all the data-points in Task 1 will be in a restricted area of the space.\n\ndef task_1_f(x):\n    return np.sin(x * 2 * np.pi)\n\n\ndef task_2_f(x):\n    return 0.9 * np.sin(x * 2 * np.pi) - 0.2 + 0.2 * np.cos(x * 3 * np.pi)\n\n\nx = np.linspace(0, 1, 101)\n\n# generate lots of low fidelity data and a few high fidelity data\n\ntask_1_x = np.linspace(0.6, 1, 4)\ntask_1_y = task_1_f(task_1_x)\n\ntask_2_x = np.linspace(0, 1, 15)\ntask_2_y = task_2_f(task_2_x)\n\n# set the data in the pandas format\nexperiments = pd.DataFrame(\n    {\n        \"x\": np.concatenate([task_1_x, task_2_x]),\n        \"y\": np.concatenate([task_1_y, task_2_y]),\n        \"task\": [\"task_1\"] * len(task_1_x) + [\"task_2\"] * len(task_2_x),\n    },\n)\n\nplt.figure(figsize=(6, 4))\n\nplt.scatter(task_1_x, task_1_y, label=\"Task 1 data\", color=\"red\")\nplt.scatter(task_2_x, task_2_y, label=\"Task 2 data\", color=\"blue\")\n\nplt.plot(x, task_1_f(x), label=\"Task 1\", color=\"red\")\nplt.plot(x, task_2_f(x), label=\"Task 2\", color=\"blue\")\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.legend()\n\n\n\n\n\n\n\n\n\nInference\nAt first we will show to do inference with the model and see make predictions using multiple data-sets.\nWe first set-up the model according to BoFire’s API, by defining the set of input and output features and the corresponding bounds, and create a surrogate data model:\n\nTo define the task we choose the TaskInput feature, everything else follows standard BoFire procedure.\n\n\n# set-up the task model with allowed variable as [\"True\"] for the target task and [\"False\"] for the other task\ntask_input = TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"])\n# define the input features\ninput_features = [ContinuousInput(key=\"x\", bounds=(0, 1)), task_input]\n\nobjective = MaximizeObjective(w=1)\noutput_features = [ContinuousOutput(key=\"y\", objective=objective)]\n\ninputs = Inputs(features=input_features)\noutputs = Outputs(features=output_features)\n\nsurrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\n\nWe map from the surrogate data into the surrogate model and fit the data.\n\nsurrogate = surrogates.map(surrogate_data)\n\nsurrogate.fit(experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/interpolation.py:71: UserWarning:\n\ntorch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:654.)\n\n\n\nPlot to see how we are able to predict outside of the region where there is data for Task 1, since we can use the data from Task 2 and the learnt correlations:\n\n# predict the high fidelity data\nx_predict = np.linspace(0, 1, 101)\ny_predict = surrogate.predict(\n    pd.DataFrame({\"x\": x_predict, \"task\": [\"task_1\"] * len(x_predict)}),\n)\n\n# plot data and predictions\nplt.plot(x_predict, y_predict[\"y_pred\"], label=\"Predictions\", color=\"green\")\nplt.fill_between(\n    x_predict,\n    y_predict[\"y_pred\"] - 2 * y_predict[\"y_sd\"],\n    y_predict[\"y_pred\"] + 2 * y_predict[\"y_sd\"],\n    color=\"green\",\n    alpha=0.2,\n)\n\n# plot the high fidelity function\nplt.plot(x, task_1_f(x), label=\"Task 1\", color=\"red\")\n\n# plot the data too\nplt.scatter(\n    experiments[experiments[\"task\"] == \"task_1\"][\"x\"],\n    experiments[experiments[\"task\"] == \"task_1\"][\"y\"],\n    label=\"Task 1 data\",\n    color=\"red\",\n)\nplt.scatter(\n    experiments[experiments[\"task\"] == \"task_2\"][\"x\"],\n    experiments[experiments[\"task\"] == \"task_2\"][\"y\"],\n    label=\"Task 2 data\",\n    color=\"blue\",\n)\n\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nplt.legend()\n\n\n\n\n\n\n\n\n\n\nTransfer Learning Bayesian Optimisation\nLet us now integrate this into BoFire’s SOBO strategy. This can be done by following the standard BoFire syntax with a small modification.\n\nFor TaskInput we must set the variable allowed as a list, with each element in the list corresponding to one of the categories such that all auxiliary tasks have False and target task has True. For example, we have categories = [\"task_1, task_2\"] and the goal of our optimization is to optimize task_1 therefore we set allowed = [True, False]:\n\n\ninput_features = [\n    ContinuousInput(key=\"x\", bounds=(0, 1)),\n    TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"], allowed=[True, False]),\n]\n\nobjective = MaximizeObjective(w=1)\noutput_features = [ContinuousOutput(key=\"y\", objective=objective)]\n\ninputs = Inputs(features=input_features)\noutputs = Outputs(features=output_features)\n\nsurrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\nsurrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n# define the acquisition function\nacquisition = qLogEI()\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=Domain(\n        inputs=inputs,\n        outputs=outputs,\n    ),\n    acquisition_function=acquisition,\n    surrogate_specs=surrogate_specs,\n)\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\n\nsobo_strategy.tell(experiments)\n\nWe can now generate experimental candidates:\n\ncandidates = sobo_strategy.ask(3)\n\ncandidates\n\n\n\n\n\n\n\n\nx\ntask\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n0.471305\ntask_1\n0.003909\n0.113361\n0.003909\n\n\n1\n0.173199\ntask_1\n0.911355\n0.197954\n0.911355\n\n\n2\n0.207861\ntask_1\n0.910492\n0.193869\n0.910492\n\n\n\n\n\n\n\nIf we instead wanted to optimize task_2 instead of task_1, we simply change allowed = [False, True]:\n\ninput_features = [\n    ContinuousInput(key=\"x\", bounds=(0, 1)),\n    TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"], allowed=[False, True]),\n]\n\nobjective = MaximizeObjective(w=1)\noutput_features = [ContinuousOutput(key=\"y\", objective=objective)]\n\ninputs = Inputs(features=input_features)\noutputs = Outputs(features=output_features)\n\nsurrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\nsurrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n# define the acquisition function\nacquisition = qLogEI()\n\nsobo_strategy_data_model = SoboStrategy(\n    domain=Domain(\n        inputs=inputs,\n        outputs=outputs,\n    ),\n    acquisition_function=acquisition,\n    surrogate_specs=surrogate_specs,\n)\n\nsobo_strategy = strategies.map(sobo_strategy_data_model)\n\nsobo_strategy.tell(experiments)\n\nWe now obtain candidates for task_2:\n\ncandidate = sobo_strategy.ask(1)\n\ncandidate\n\n\n\n\n\n\n\n\nx\ntask\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n0.197145\ntask_2\n0.594692\n0.026343\n0.594692\n\n\n\n\n\n\n\nLet us now run a Bayesian optimization loop on the Branin benchmark to show the usefulness of transfer learning Bayesian optimization in a practical setting. We create a small data-set composed of the Branin benchmark itself, and a large one composed of the Branin function with a small amount of bias added by summing the Branin and Ackley functions.\nWe begin by defining a function that creates random initial data-sets, and create as many data-sets as the number of runs we want to average over:\n\nbenchmark = Branin()\nbias = Ackley()\n\n\ndef create_data_set():\n    # choose the initial data-sets\n    low_fidelity_x = benchmark.domain.inputs.sample(25)\n    high_fidelity_x = benchmark.domain.inputs.sample(4)\n\n    # create the observations\n    high_fidelity_data = benchmark.f(high_fidelity_x, return_complete=True)\n    low_fidelity_bias = bias.f(low_fidelity_x, return_complete=True)\n\n    low_fidelity_data = benchmark.f(low_fidelity_x, return_complete=True)\n    low_fidelity_data[\"y\"] = low_fidelity_data[\"y\"] + 0.15 * low_fidelity_bias[\"y\"]\n\n    # create a joint data-set, with the task variable\n    high_fidelity_data[\"task\"] = \"task_1\"\n    low_fidelity_data[\"task\"] = \"task_2\"\n\n    experiments_joint = pd.concat([low_fidelity_data, high_fidelity_data])\n\n    return high_fidelity_data, experiments_joint\n\n\nsingle_task_all_regrets = []\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\nif SMOKE_TEST:\n    num_runs = 5\n    num_iters = 2\n    verbose = False\nelse:\n    num_runs = 10\n    num_iters = 10\n    verbose = True\n\n# create the initial data-sets for each run\n\nhigh_fidelity_datasets = []\nexperiments_joint_datasets = []\n\nfor _ in range(num_runs):\n    high_fidelity_data, experiments_joint = create_data_set()\n    high_fidelity_datasets.append(high_fidelity_data)\n    experiments_joint_datasets.append(experiments_joint)\n\nLet us now run a Bayesian optimization loop only using the high-fidelity data:\n\nfor run in range(num_runs):\n    high_fidelity_data = high_fidelity_datasets[run]\n\n    inputs = benchmark.domain.inputs\n    outputs = benchmark.domain.outputs\n\n    surrogate_data = SingleTaskGPSurrogate(inputs=inputs, outputs=outputs)\n    surrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n    acquisition = qLogEI()\n\n    sobo_strategy_data_model = SoboStrategy(\n        domain=Domain(\n            inputs=inputs,\n            outputs=outputs,\n        ),\n        acquisition_function=acquisition,\n        surrogate_specs=surrogate_specs,\n    )\n\n    sobo_strategy = strategies.map(sobo_strategy_data_model)\n\n    dataset = high_fidelity_data.drop(columns=[\"task\"])\n\n    sobo_strategy.tell(dataset)\n\n    regrets_single_task = []\n\n    init_regret = (\n        sobo_strategy.experiments[\"y\"][sobo_strategy.experiments[\"y\"].argmin()]\n        - benchmark.get_optima()[\"y\"][0].item()\n    )\n    regrets_single_task.append(init_regret)\n\n    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n    for _iter in pbar:\n        candidate = sobo_strategy.ask(1)\n        y = benchmark.f(candidate, return_complete=True)\n        sobo_strategy.tell(y)\n\n        regret = (\n            sobo_strategy.experiments[\"y\"][sobo_strategy.experiments[\"y\"].argmin()]\n            - benchmark.get_optima()[\"y\"][0].item()\n        )\n        regrets_single_task.append(regret)\n\n        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n\n    single_task_all_regrets.append(regrets_single_task)\n\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=2.1972]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.05it/s, Regret=2.1972]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:00,  1.05it/s, Regret=2.1972]Optimizing: 100%|██████████| 2/2 [00:02&lt;00:00,  1.28s/it, Regret=2.1972]Optimizing: 100%|██████████| 2/2 [00:02&lt;00:00,  1.23s/it, Regret=2.1972]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:01&lt;?, ?it/s, Regret=26.0806]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:01,  1.14s/it, Regret=26.0806]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:01,  1.14s/it, Regret=11.3245]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.18it/s, Regret=11.3245]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.12it/s, Regret=11.3245]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=5.8798]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  2.56it/s, Regret=5.8798]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  2.56it/s, Regret=5.8798]Optimizing: 100%|██████████| 2/2 [00:00&lt;00:00,  2.59it/s, Regret=5.8798]Optimizing: 100%|██████████| 2/2 [00:00&lt;00:00,  2.58it/s, Regret=5.8798]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=55.7364]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  1.34it/s, Regret=55.7364]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:00,  1.34it/s, Regret=8.6383] Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.06it/s, Regret=8.6383]Optimizing: 100%|██████████| 2/2 [00:01&lt;00:00,  1.10it/s, Regret=8.6383]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s, Regret=0.9048]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  2.74it/s, Regret=0.9048]Optimizing:  50%|█████     | 1/2 [00:00&lt;00:00,  2.74it/s, Regret=0.9048]Optimizing: 100%|██████████| 2/2 [00:00&lt;00:00,  2.27it/s, Regret=0.9048]Optimizing: 100%|██████████| 2/2 [00:00&lt;00:00,  2.33it/s, Regret=0.9048]\n\n\nWe now repeat the experiment but using transfer learning BO:\n\nmultitask_all_regrets = []\n\nfor run in range(num_runs):\n    experiments_joint = experiments_joint_datasets[run]\n\n    input_features = benchmark.domain.inputs.features + [\n        TaskInput(key=\"task\", categories=[\"task_1\", \"task_2\"], allowed=[True, False]),\n    ]\n    inputs = Inputs(features=input_features)\n    outputs = benchmark.domain.outputs\n\n    surrogate_data = MultiTaskGPSurrogate(inputs=inputs, outputs=outputs)\n    surrogate_specs = BotorchSurrogates(surrogates=[surrogate_data])\n\n    acquisition = qLogEI()\n\n    sobo_strategy_data_model = SoboStrategy(\n        domain=Domain(\n            inputs=inputs,\n            outputs=outputs,\n        ),\n        acquisition_function=acquisition,\n        surrogate_specs=surrogate_specs,\n    )\n\n    sobo_strategy = strategies.map(sobo_strategy_data_model)\n\n    dataset = experiments_joint.copy()\n\n    sobo_strategy.tell(dataset)\n\n    regrets_transfer_learning = []\n\n    # obtain experiments at the highest fidelity\n    experiments = sobo_strategy.experiments[\n        sobo_strategy.experiments[\"task\"] == \"task_1\"\n    ][\"y\"]\n    init_regret = (\n        experiments[experiments.argmin()] - benchmark.get_optima()[\"y\"][0].item()\n    )\n    regrets_transfer_learning.append(init_regret)\n\n    pbar = tqdm(range(num_iters), desc=\"Optimizing\")\n    for _iter in pbar:\n        candidate = sobo_strategy.ask(1)\n        candidate = candidate.drop(columns=[\"task\"])\n        y = benchmark.f(candidate, return_complete=True)\n        y[\"task\"] = \"task_1\"\n        sobo_strategy.tell(y)\n\n        experiments = sobo_strategy.experiments[\n            sobo_strategy.experiments[\"task\"] == \"task_1\"\n        ][\"y\"].reset_index(drop=True)\n        regret = (\n            experiments[experiments.argmin()] - benchmark.get_optima()[\"y\"][0].item()\n        )\n        regrets_transfer_learning.append(regret)\n\n        pbar.set_postfix({\"Regret\": f\"{regret:.4f}\"})\n\n    multitask_all_regrets.append(regrets_transfer_learning)\n\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:03&lt;?, ?it/s, Regret=2.8369]Optimizing:  50%|█████     | 1/2 [00:03&lt;00:03,  3.32s/it, Regret=2.8369]Optimizing:  50%|█████     | 1/2 [00:05&lt;00:03,  3.32s/it, Regret=1.6767]Optimizing: 100%|██████████| 2/2 [00:05&lt;00:00,  2.73s/it, Regret=1.6767]Optimizing: 100%|██████████| 2/2 [00:05&lt;00:00,  2.82s/it, Regret=1.6767]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:02&lt;?, ?it/s, Regret=11.8976]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:02,  3.00s/it, Regret=11.8976]Optimizing:  50%|█████     | 1/2 [00:05&lt;00:02,  3.00s/it, Regret=10.5630]Optimizing: 100%|██████████| 2/2 [00:05&lt;00:00,  2.50s/it, Regret=10.5630]Optimizing: 100%|██████████| 2/2 [00:05&lt;00:00,  2.58s/it, Regret=10.5630]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:02&lt;?, ?it/s, Regret=5.8798]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:02,  2.57s/it, Regret=5.8798]Optimizing:  50%|█████     | 1/2 [00:04&lt;00:02,  2.57s/it, Regret=2.0079]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.31s/it, Regret=2.0079]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.35s/it, Regret=2.0079]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:01&lt;?, ?it/s, Regret=1.5848]Optimizing:  50%|█████     | 1/2 [00:01&lt;00:01,  1.73s/it, Regret=1.5848]Optimizing:  50%|█████     | 1/2 [00:04&lt;00:01,  1.73s/it, Regret=1.5848]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.08s/it, Regret=1.5848]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.03s/it, Regret=1.5848]\nOptimizing:   0%|          | 0/2 [00:00&lt;?, ?it/s]Optimizing:   0%|          | 0/2 [00:02&lt;?, ?it/s, Regret=0.9048]Optimizing:  50%|█████     | 1/2 [00:02&lt;00:02,  2.08s/it, Regret=0.9048]Optimizing:  50%|█████     | 1/2 [00:04&lt;00:02,  2.08s/it, Regret=0.1622]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.31s/it, Regret=0.1622]Optimizing: 100%|██████████| 2/2 [00:04&lt;00:00,  2.28s/it, Regret=0.1622]\n\n\nWe now plot the quantiles and median simple regret against iteration:\n\n# plot the results\nplt.figure(figsize=(6, 4))\n\nregrets_single_task_median = np.median(np.array(single_task_all_regrets), axis=0)\nregrets_transfer_learning_median = np.median(np.array(multitask_all_regrets), axis=0)\n\n# get the 25 and 75 percentiles\nregrets_single_task_upper_quantile = np.quantile(\n    np.array(single_task_all_regrets),\n    0.75,\n    axis=0,\n)\nregrets_single_task_lower_quantile = np.quantile(\n    np.array(single_task_all_regrets),\n    0.25,\n    axis=0,\n)\n\nregrets_transfer_learning_upper_quantile = np.quantile(\n    np.array(multitask_all_regrets),\n    0.75,\n    axis=0,\n)\nregrets_transfer_learning_lower_quantile = np.quantile(\n    np.array(multitask_all_regrets),\n    0.25,\n    axis=0,\n)\n\nplt.plot(regrets_single_task_median, label=\"Single task\", color=\"red\")\nplt.plot(regrets_transfer_learning_median, label=\"Transfer learning\", color=\"blue\")\n\nplt.fill_between(\n    np.arange(num_iters + 1),\n    regrets_single_task_upper_quantile,\n    regrets_single_task_lower_quantile,\n    color=\"red\",\n    alpha=0.2,\n)\nplt.fill_between(\n    np.arange(num_iters + 1),\n    regrets_transfer_learning_upper_quantile,\n    regrets_transfer_learning_lower_quantile,\n    color=\"blue\",\n    alpha=0.2,\n)\n\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Regret\")\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that using transfer learning leads to significant improvement.",
    "crumbs": [
      "Advanced Examples",
      "generate lots of low fidelity data and a few high fidelity data"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Outlier_Detection_and_Robust_GP.html",
    "href": "docs/tutorials/basic_examples/Outlier_Detection_and_Robust_GP.html",
    "title": "Outlier Detection and Robust GP",
    "section": "",
    "text": "This notebook shows how to use RobustSingleTaskGPSurrogate in Bofire to autmomatically detect outliers in your data and/or fit Gaussian process models robust to outliers.\nIt is based on the Robust Gaussian Processes via Relevance Pursuit paper and is based on the accompanying implementation and tutorial in BoTorch\nIn this approach, the typical GP observation noise \\(\\sigma^2\\) is extended with data-point-specific noise variances \\(\\rho\\). A prior distribution is placed over the number of outliers \\(S\\), and through a sequential greedy optimization algorithm (see for details the links above) a list of models with variying sparsity levels \\(|S|\\) are obtained. The most promising model can then be selected through Bayesian model selection.\nThis tutorial will show how to use the model, how to obtain the data-point specific noise levels, and how to obtain the full model trace.\n\nImports\n\n# Model imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import Tensor\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.domain.api import Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.surrogates.api import (\n    RobustSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\n\n\n\nSetting up a Synthetic example\n\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i+1}\", bounds=(-4, 4)) for i in range(1)],\n)\noutput_features = Outputs(features=[ContinuousOutput(key=\"y_1\")])\n\nexperiments = input_features.sample(n=10)\nexperiments[\"y_1\"] = np.sin(experiments[\"x_1\"])\n\nexperiments[\"valid_y_1\"] = 1\nexperiments[\"valid_y_2\"] = 1\n\n# prediction grid\nx = pd.DataFrame(pd.Series(np.linspace(-4, 4, 100), name=\"x_1\"))\n\n\n\nLet’s add two clear outliers\n\nexperiments.loc[0, \"y_1\"] = 5\nexperiments.loc[1, \"y_1\"] = -5\n\n\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\")\n\n# plot the last two points in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTesting a SingleTaskGP\n\nstgp_data_model = SingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n)\n\nstgp_model = surrogates.map(data_model=stgp_data_model)\nstgp_model.fit(experiments)\nstgp_predictions = stgp_model.predict(x)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:179: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\n\n# plot the surrogate\nplt.figure(figsize=(10, 5))\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\", label=\"observations\")\n\n# plot the outliers in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\nplt.plot(x[\"x_1\"], stgp_predictions[\"y_1_pred\"], label=\"predictions\")\nplt.fill_between(\n    x[\"x_1\"],\n    stgp_predictions[\"y_1_pred\"] - 2 * stgp_predictions[\"y_1_sd\"],\n    stgp_predictions[\"y_1_pred\"] + 2 * stgp_predictions[\"y_1_sd\"],\n    alpha=0.2,\n    label=\"95% confidence interval\",\n)\nplt.xlabel(\"x_1\")\nplt.ylabel(\"y_1\")\nplt.title(\"Single Task GP Predictions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTesting the Robust GP\n\ndata_model = RobustSingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n)\n\nmodel = surrogates.map(data_model=data_model)\nmodel.fit(experiments)\npredictions = model.predict(x)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/relevance_pursuit.py:156: UserWarning:\n\nConverting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\nConsider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:836.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/likelihoods/sparse_outlier_noise.py:104: InputDataWarning:\n\nSparseOutlierNoise: Robust rho not applied because the last dimension of the base noise covariance (100) is not compatible with the last dimension of rho (10). This can happen when the model posterior is evaluated on test data.\n\n\n\n\n# plot the surrogate\nplt.figure(figsize=(10, 5))\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\", label=\"observations\")\nplt.plot(x[\"x_1\"], predictions[\"y_1_pred\"], label=\"predictions\")\n\n# plot the outliers in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\nplt.fill_between(\n    x[\"x_1\"],\n    predictions[\"y_1_pred\"] - 2 * predictions[\"y_1_sd\"],\n    predictions[\"y_1_pred\"] + 2 * predictions[\"y_1_sd\"],\n    alpha=0.2,\n    label=\"95% confidence interval\",\n)\nplt.xlabel(\"x_1\")\nplt.ylabel(\"y_1\")\nplt.title(\"Robust Single Task GP Predictions\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nReturning data point specific noise values \\(\\rho\\)\n\nmodel.predict_outliers(experiments)\n\n\n\n\n\n\n\n\ny_1_pred\ny_1_sd\ny_1_rho\n\n\n\n\n0\n-0.556613\n0.427483\n5.395839\n\n\n1\n0.960449\n0.371247\n6.207615\n\n\n2\n-0.227015\n0.129809\n0.000000\n\n\n3\n0.230208\n0.106240\n0.000000\n\n\n4\n-0.269685\n0.119124\n0.000000\n\n\n5\n0.908518\n0.121699\n0.000000\n\n\n6\n0.447358\n0.107548\n0.000000\n\n\n7\n-0.137626\n0.105501\n0.000000\n\n\n8\n-0.456778\n0.121065\n0.000000\n\n\n9\n0.021041\n0.115477\n0.000000\n\n\n\n\n\n\n\n\n\nPlotting the data point specific noise values\n\n# Get the outlier scores (rho) from the robust GP model\noutlier_scores = model.predict_outliers(experiments)\n\n# Plot the surrogate predictions\nplt.figure(figsize=(10, 5))\nplt.plot(experiments[\"x_1\"], experiments[\"y_1\"], \"o\", label=\"observations\")\nplt.plot(x[\"x_1\"], predictions[\"y_1_pred\"], label=\"predictions\")\n\n# plot the outliers in red\nplt.plot(\n    experiments[\"x_1\"].iloc[:2],\n    experiments[\"y_1\"].iloc[:2],\n    \"ro\",\n    label=\"outliers\",\n)\n\n# Plot sqrt(rho) as error bars on the observations\nplt.errorbar(\n    experiments[\"x_1\"],\n    experiments[\"y_1\"],\n    yerr=outlier_scores[\"y_1_rho\"].values,\n    fmt=\"none\",\n    ecolor=\"orange\",\n    alpha=0.7,\n    label=\"rho\",\n)\n\nplt.fill_between(\n    x[\"x_1\"],\n    predictions[\"y_1_pred\"] - 2 * predictions[\"y_1_sd\"],\n    predictions[\"y_1_pred\"] + 2 * predictions[\"y_1_sd\"],\n    alpha=0.2,\n    label=\"95% confidence interval\",\n)\nplt.xlabel(\"x_1\")\nplt.ylabel(\"y_1\")\nplt.title(\"Robust Single Task GP Predictions with sqrt(rho) errorbars\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nHyperparameter optimization\nThe RobustSingleTaskGPSurrogate also support hyperparameter optimization over different kernels, priors, etc.\n\n# opt_surrogate_data, perf = hyperoptimize(surrogate_data=data_model, training_data=experiments, folds=4)\n# surrogate = surrogates.map(opt_surrogate_data)\n\n\n\nObtaining the full model trace\n\ndef bmc_plot(bmc_support_sizes: Tensor, bmc_probabilities: Tensor) -&gt; None:\n    cmap = plt.colormaps[\"viridis\"]\n    bar_width = 1\n    plt.title(\"Model Evidence\")\n    for i, ss in enumerate(bmc_support_sizes):\n        color = cmap((1 - (len(bmc_support_sizes) - i) / (2 * len(bmc_support_sizes))))\n        plt.bar(ss, bmc_probabilities[i], color=color, width=bar_width)\n\n    i = bmc_probabilities.argmax()\n    map_color = cmap((1 - (len(bmc_support_sizes) - i) / (2 * len(bmc_support_sizes))))\n    plt.bar(\n        bmc_support_sizes[i],\n        bmc_probabilities[i],\n        color=map_color,\n        label=\"MAP\",\n        edgecolor=\"black\",\n        linewidth=1.5,\n        width=bar_width,\n    )\n\n    support_prior = torch.exp(-bmc_support_sizes / model.model.prior_mean_of_support)\n    support_prior = support_prior / support_prior.sum()\n    plt.plot(bmc_support_sizes, support_prior, \"D\", color=\"black\", label=\"Prior\", ms=2)\n\n    plt.xlabel(\n        f\"Support Size (Prior = Exponential(mean={model.model.prior_mean_of_support:.1f}))\"\n    )\n    plt.ylabel(\"Posterior Marginal Likelihood (%)\")\n    plt.legend(loc=\"upper right\")\n\n\ndata_model = RobustSingleTaskGPSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n    cache_model_trace=True,\n)\n\nmodel = surrogates.map(data_model=data_model)\nmodel.fit(experiments)\npredictions = model.predict(x)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/likelihoods/sparse_outlier_noise.py:104: InputDataWarning:\n\nSparseOutlierNoise: Robust rho not applied because the last dimension of the base noise covariance (100) is not compatible with the last dimension of rho (10). This can happen when the model posterior is evaluated on test data.\n\n\n\n\nfigsize = (6, 4)\nfig = plt.figure(dpi=100, figsize=figsize)\nbmc_plot(model.model.bmc_support_sizes.detach(), model.model.bmc_probabilities.detach())",
    "crumbs": [
      "Basic Examples",
      "Outlier Detection and Robust GP"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html",
    "href": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html",
    "title": "Classification Surrogate Tests",
    "section": "",
    "text": "We are interested in testing whether or not a surrogate model can correctly identify unknown constraints based on categorical criteria with classification surrogates. Essentially, we want to account for scenarios where specialists can look at a set of experiments and label outcomes as ‘acceptable’, ‘unacceptable’, ‘ideal’, etc.\nThis involves new models that produce CategoricalOutput’s rather than continuous outputs. Mathematically, if \\(g_{\\theta}:\\mathbb{R}^d\\to[0,1]^c\\) represents the function governed by learnable parameters \\(\\theta\\) which outputs a probability vector over \\(c\\) potential classes (i.e. for input \\(x\\in\\mathbb{R}^d\\), \\(g_{\\theta}(x)^\\top\\mathbf{1}=1\\) where \\(\\mathbf{1}\\) is the vector of all 1’s) and we have acceptibility criteria for the corresponding classes given by \\(a\\in\\{0,1\\}^c\\), we can compute the scalar output \\(g_{\\theta}(x)^\\top a\\in[0,1]\\) which represents the expected value of acceptance as an objective value to be passed in as a constrained function.\nIn this script, we look at the Rosenbrock function constrained to a disk which attains a global minima at \\((x_0^*,x_1^*)=(1.0, 1.0)\\). To facilitate testing the functionality offered by BoFire, we label all points inside of the circle \\(x_0^2+x_1^2\\le2\\) as ‘acceptable’ and further label anything inside of the intersection of this circle and the circle \\((x_0-1)^2+(x_1-1)^2\\le1.0\\) as ‘ideal’; points lying outside of these two locations are labeled as “unacceptable.”\n# Import packages\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import (\n    CategoricalInput,\n    CategoricalOutput,\n    ContinuousInput,\n    ContinuousOutput,\n)\nfrom bofire.data_models.objectives.api import (\n    ConstrainedCategoricalObjective,\n    MinimizeObjective,\n)",
    "crumbs": [
      "Basic Examples",
      "Classification Surrogate Tests"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#manual-setup-of-the-optimization-domain",
    "href": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#manual-setup-of-the-optimization-domain",
    "title": "Classification Surrogate Tests",
    "section": "Manual setup of the optimization domain",
    "text": "Manual setup of the optimization domain\nThe following cells show how to manually setup the optimization problem in BoFire for didactic purposes.\n\n# Write helper functions which give the objective and the constraints\ndef rosenbrock(x: pd.Series) -&gt; pd.Series:\n    assert \"x_0\" in x.columns\n    assert \"x_1\" in x.columns\n    return (1 - x[\"x_0\"]) ** 2 + 100 * (x[\"x_1\"] - x[\"x_0\"] ** 2) ** 2\n\n\ndef constraints(x: pd.Series) -&gt; pd.Series:\n    assert \"x_0\" in x.columns\n    assert \"x_1\" in x.columns\n    feasiblity_vector = []\n    for _, row in x.iterrows():\n        if (row[\"x_0\"] ** 2 + row[\"x_1\"] ** 2 &lt;= 2.0) and (\n            (row[\"x_0\"] - 1.0) ** 2 + (row[\"x_1\"] - 1.0) ** 2 &lt;= 1.0\n        ):\n            feasiblity_vector.append(\"ideal\")\n        elif row[\"x_0\"] ** 2 + row[\"x_1\"] ** 2 &lt;= 2.0:\n            feasiblity_vector.append(\"acceptable\")\n        else:\n            feasiblity_vector.append(\"unacceptable\")\n    return feasiblity_vector\n\n\n# Set-up the inputs and outputs, use categorical domain just as an example\ninput_features = Inputs(\n    features=[ContinuousInput(key=f\"x_{i}\", bounds=(-1.75, 1.75)) for i in range(2)]\n    + [CategoricalInput(key=\"x_3\", categories=[\"0\", \"1\"], allowed=[True, True])],\n)\n\n# here the minimize objective is used, if you want to maximize you have to use the maximize objective.\noutput_features = Outputs(\n    features=[\n        ContinuousOutput(key=\"f_0\", objective=MinimizeObjective(w=1.0)),\n        CategoricalOutput(\n            key=\"f_1\",\n            categories=[\"unacceptable\", \"acceptable\", \"ideal\"],\n            objective=ConstrainedCategoricalObjective(\n                categories=[\"unacceptable\", \"acceptable\", \"ideal\"],\n                desirability=[False, True, True],\n            ),\n        ),  # This function will be associated with learning the categories\n    ],\n)\n\n# Create domain\ndomain1 = Domain(inputs=input_features, outputs=output_features)\n\n# Sample random points\nsample_df = domain1.inputs.sample(100)\n\n# Write a function which outputs one continuous variable and another discrete based on some logic\nsample_df[\"f_0\"] = rosenbrock(x=sample_df)\nsample_df[\"f_1\"] = constraints(x=sample_df)\n\nsample_df.head(5)\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_3\nf_0\nf_1\n\n\n\n\n0\n-0.084525\n-0.735754\n0\n56.365977\nacceptable\n\n\n1\n1.119736\n0.404295\n0\n72.181620\nideal\n\n\n2\n0.171416\n1.536808\n0\n227.919281\nunacceptable\n\n\n3\n-0.020876\n1.660924\n1\n276.764203\nunacceptable\n\n\n4\n-1.641700\n0.461470\n1\n505.923586\nunacceptable\n\n\n\n\n\n\n\n\n# Plot the sample df\nimport math\n\nimport plotly.express as px\n\n\nfig = px.scatter(\n    sample_df,\n    x=\"x_0\",\n    y=\"x_1\",\n    color=\"f_1\",\n    width=550,\n    height=525,\n    title=\"Samples with labels\",\n)\nfig.add_shape(\n    type=\"circle\",\n    xref=\"x\",\n    yref=\"y\",\n    opacity=0.1,\n    fillcolor=\"red\",\n    x0=-math.sqrt(2),\n    y0=-math.sqrt(2),\n    x1=math.sqrt(2),\n    y1=math.sqrt(2),\n    line_color=\"red\",\n)\nfig.add_shape(\n    type=\"circle\",\n    xref=\"x\",\n    yref=\"y\",\n    opacity=0.2,\n    fillcolor=\"LightSeaGreen\",\n    x0=0,\n    y0=0,\n    x1=2,\n    y1=2,\n    line_color=\"LightSeaGreen\",\n)\nfig.show()",
    "crumbs": [
      "Basic Examples",
      "Classification Surrogate Tests"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#evaluate-the-classification-model-performance-outside-of-the-optimization-procedure",
    "href": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#evaluate-the-classification-model-performance-outside-of-the-optimization-procedure",
    "title": "Classification Surrogate Tests",
    "section": "Evaluate the classification model performance (outside of the optimization procedure)",
    "text": "Evaluate the classification model performance (outside of the optimization procedure)\n\n# Import packages\nimport bofire.surrogates.api as surrogates\nfrom bofire.data_models.surrogates.api import ClassificationMLPEnsemble\nfrom bofire.surrogates.diagnostics import ClassificationMetricsEnum\n\n\n# Instantiate the surrogate data model\nsurrogate_data = ClassificationMLPEnsemble(\n    inputs=domain1.inputs,\n    outputs=Outputs(features=[domain1.outputs.get_by_key(\"f_1\")]),\n    lr=0.03,\n    n_epochs=100,\n    hidden_layer_sizes=(\n        4,\n        2,\n    ),\n    weight_decay=0.0,\n    batch_size=10,\n    activation=\"tanh\",\n)\nsurrogate = surrogates.map(surrogate_data)\n\n# Fit the surrogate to the classification data\ncv_df = sample_df.drop([\"f_0\"], axis=1)\ncv_df[\"valid_f_1\"] = 1\ncv_train, cv_test, _ = surrogate.cross_validate(cv_df, folds=3)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:1134: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\n# Print training performance\ncv_train.get_metrics(\n    metrics=ClassificationMetricsEnum,\n    combine_folds=True,\n)\n\n\n\n\n\n\n\n\nACCURACY\nF1\n\n\n\n\n0\n0.72\n0.72\n\n\n\n\n\n\n\n\n# Print test performance\ncv_test.get_metrics(\n    metrics=ClassificationMetricsEnum,\n    combine_folds=True,\n)\n\n\n\n\n\n\n\n\nACCURACY\nF1\n\n\n\n\n0\n0.57\n0.57",
    "crumbs": [
      "Basic Examples",
      "Classification Surrogate Tests"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#setup-strategy-and-ask-for-candidates",
    "href": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#setup-strategy-and-ask-for-candidates",
    "title": "Classification Surrogate Tests",
    "section": "Setup strategy and ask for candidates",
    "text": "Setup strategy and ask for candidates\nNow we setup a SoboStrategy for generating candidates, the categorical output is modelled using the surrogate from above. The categorical output is modelled as an output constraint in the acquistion function optimization (constrained expected improvement). For more details have a look at this notebook: https://github.com/pytorch/botorch/blob/main/notebooks_community/clf_constrained_bo.ipynb and/or this paper: https://arxiv.org/abs/2402.07692.\n\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.strategies.api import SoboStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates\n\n\nstrategy_data = SoboStrategy(\n    domain=domain1,\n    acquisition_function=qLogEI(),\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[surrogate_data],\n    ),\n)\n\nstrategy = strategies.map(strategy_data)\n\nstrategy.tell(sample_df)\n\n\ncandidates = strategy.ask(10)\ncandidates\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-06 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-06 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), NumericalWarning('A not p.d., added jitter of 1.0e-08 to the diagonal'), NumericalWarning('A not p.d., added jitter of 1.0e-07 to the diagonal'), OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.'), RuntimeWarning('Could not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.')]\nTrying again with a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed on the second try, after generating a new set of initial conditions.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _MLPEnsemble does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\n\n\n\n\n\n\nx_0\nx_1\nx_3\nf_1_pred\nf_1_sd\nf_1_unacceptable_prob\nf_1_acceptable_prob\nf_1_ideal_prob\nf_0_pred\nf_1_unacceptable_sd\nf_1_acceptable_sd\nf_1_ideal_sd\nf_0_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n-1.314712\n1.750000\n1\nunacceptable\n0.0\n1.370305\n0.226049\n0.749408\n0.024543\n7.653558\n0.433532\n0.425387\n0.049945\n-0.024543\n0.975457\n\n\n1\n0.183930\n0.035942\n1\nideal\n0.0\n0.044995\n0.001771\n0.997481\n0.000748\n3.954484\n0.001947\n0.001397\n0.000562\n-0.000748\n0.999252\n\n\n2\n1.319202\n1.750000\n1\nacceptable\n0.0\n-3.517232\n0.807034\n0.001456\n0.191510\n5.629126\n0.283888\n0.002581\n0.282702\n-0.191510\n0.808490\n\n\n3\n-0.234049\n0.061555\n1\nunacceptable\n0.0\n1.121220\n0.001742\n0.997646\n0.000612\n3.934215\n0.001951\n0.001566\n0.000452\n-0.000612\n0.999388\n\n\n4\n-0.009006\n0.254392\n1\nunacceptable\n0.0\n6.619221\n0.001767\n0.997602\n0.000631\n3.942263\n0.001982\n0.001575\n0.000459\n-0.000631\n0.999369\n\n\n5\n-0.855391\n1.550704\n0\nunacceptable\n0.0\n71.427356\n0.542135\n0.193828\n0.264037\n6.562004\n0.495186\n0.427765\n0.432219\n-0.264037\n0.735963\n\n\n6\n-1.309076\n1.750000\n1\nunacceptable\n0.0\n1.434173\n0.222668\n0.753628\n0.023704\n7.680310\n0.435081\n0.426181\n0.047014\n-0.023704\n0.976296\n\n\n7\n-0.758667\n0.209802\n1\nunacceptable\n0.0\n16.747444\n0.002204\n0.997079\n0.000717\n3.974722\n0.002047\n0.001520\n0.000552\n-0.000717\n0.999283\n\n\n8\n-1.315718\n1.750000\n0\nunacceptable\n0.0\n1.426984\n0.709680\n0.170018\n0.120301\n7.768017\n0.413196\n0.300159\n0.164036\n-0.120301\n0.879699\n\n\n9\n1.318749\n1.750000\n0\nacceptable\n0.0\n-3.454193\n0.806762\n0.001461\n0.191777\n5.650369\n0.283661\n0.002578\n0.282480\n-0.191777\n0.808223",
    "crumbs": [
      "Basic Examples",
      "Classification Surrogate Tests"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#check-classification-of-proposed-candidates",
    "href": "docs/tutorials/basic_examples/Unknown_Constraint_Classification.html#check-classification-of-proposed-candidates",
    "title": "Classification Surrogate Tests",
    "section": "Check classification of proposed candidates",
    "text": "Check classification of proposed candidates\nUse the logic from above to verify the classification values\n\n# Append to the candidates\ncandidates[\"f_1_true\"] = constraints(x=candidates)\n\n\n# Print results\ncandidates[[\"x_0\", \"x_1\", \"f_1_pred\", \"f_1_true\"]]\n\n\n\n\n\n\n\n\nx_0\nx_1\nf_1_pred\nf_1_true\n\n\n\n\n0\n-1.314712\n1.750000\nunacceptable\nunacceptable\n\n\n1\n0.183930\n0.035942\nideal\nacceptable\n\n\n2\n1.319202\n1.750000\nacceptable\nunacceptable\n\n\n3\n-0.234049\n0.061555\nunacceptable\nacceptable\n\n\n4\n-0.009006\n0.254392\nunacceptable\nacceptable\n\n\n5\n-0.855391\n1.550704\nunacceptable\nunacceptable\n\n\n6\n-1.309076\n1.750000\nunacceptable\nunacceptable\n\n\n7\n-0.758667\n0.209802\nunacceptable\nacceptable\n\n\n8\n-1.315718\n1.750000\nunacceptable\nunacceptable\n\n\n9\n1.318749\n1.750000\nacceptable\nunacceptable",
    "crumbs": [
      "Basic Examples",
      "Classification Surrogate Tests"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/index.html",
    "href": "docs/tutorials/basic_examples/index.html",
    "title": "Basic Examples",
    "section": "",
    "text": "These notebooks demonstrate the basic functionality of BoFire, including:\n\nSetting up reaction domains\nDefining objectives\nRunning Bayesian optimization loops\nModel fitting and analysis\nOutlier detection and robust Gaussian Processes\nUnknown constraint classification\n\n\n\n\n\nIntroduction to fundamental concepts and terminology used throughout BoFire.\n\n\n\nA comprehensive example showing how to optimize chemical reactions using BoFire.\n\n\n\nLearn how to fit surrogate models and analyze their performance.\n\n\n\nTechniques for detecting outliers and building robust Gaussian Process models.\n\n\n\nHandling and classifying unknown constraints in optimization problems.\n\n\n\nLearn how to use Index Kernel and Positive Index Kernel for categorical variables.",
    "crumbs": [
      "Basic Examples",
      "Basic Examples"
    ]
  },
  {
    "objectID": "docs/tutorials/basic_examples/index.html#available-tutorials",
    "href": "docs/tutorials/basic_examples/index.html#available-tutorials",
    "title": "Basic Examples",
    "section": "",
    "text": "Introduction to fundamental concepts and terminology used throughout BoFire.\n\n\n\nA comprehensive example showing how to optimize chemical reactions using BoFire.\n\n\n\nLearn how to fit surrogate models and analyze their performance.\n\n\n\nTechniques for detecting outliers and building robust Gaussian Process models.\n\n\n\nHandling and classifying unknown constraints in optimization problems.\n\n\n\nLearn how to use Index Kernel and Positive Index Kernel for categorical variables.",
    "crumbs": [
      "Basic Examples",
      "Basic Examples"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/001-Himmelblau.html",
    "href": "docs/tutorials/benchmarks/001-Himmelblau.html",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/001-Himmelblau.html#imports",
    "href": "docs/tutorials/benchmarks/001-Himmelblau.html#imports",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/001-Himmelblau.html#random-optimization",
    "href": "docs/tutorials/benchmarks/001-Himmelblau.html#random-optimization",
    "title": "Himmelblau Benchmark",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nrandom_results = run(\n    Himmelblau(),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=95.863]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 78.31it/s, Current Best:=95.863]",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/001-Himmelblau.html#sobo-gpei-optimization",
    "href": "docs/tutorials/benchmarks/001-Himmelblau.html#sobo-gpei-optimization",
    "title": "Himmelblau Benchmark",
    "section": "SOBO (GPEI) Optimization",
    "text": "SOBO (GPEI) Optimization\n\ndef strategy_factory(domain: Domain):\n    data_model = SoboStrategy(domain=domain, acquisition_function=qLogEI())\n    return strategies.map(data_model)\n\n\nbo_results = run(\n    Himmelblau(),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=13.763]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.30it/s, Current Best:=13.763]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.30it/s, Current Best:=13.763]",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html",
    "href": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Hartmann\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import (\n    BotorchOptimizer,\n    RandomStrategy,\n    SoboStrategy,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#imports",
    "href": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#imports",
    "title": "Himmelblau Benchmark",
    "section": "",
    "text": "import os\n\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.single import Hartmann\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.api import Domain\nfrom bofire.data_models.strategies.api import (\n    BotorchOptimizer,\n    RandomStrategy,\n    SoboStrategy,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#random-optimization",
    "href": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#random-optimization",
    "title": "Himmelblau Benchmark",
    "section": "Random Optimization",
    "text": "Random Optimization\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nrandom_results = run(\n    Hartmann(dim=6, allowed_k=4),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=-0.719]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 53.34it/s, Current Best:=-0.719]",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#sobo-gpei-optimization",
    "href": "docs/tutorials/benchmarks/003-Hartmann_with_nchoosek.html#sobo-gpei-optimization",
    "title": "Himmelblau Benchmark",
    "section": "SOBO (GPEI) Optimization",
    "text": "SOBO (GPEI) Optimization\n\ndef strategy_factory(domain: Domain):\n    data_model = SoboStrategy(\n        domain=domain,\n        acquisition_function=qLogEI(),\n        acquisition_optimizer=BotorchOptimizer(n_raw_samples=512, n_restarts=24),\n    )\n    return strategies.map(data_model)\n\n\nbo_results = run(\n    Hartmann(dim=6, allowed_k=4),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=best,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:08&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:08&lt;?, ?it/s, Current Best:=-0.428]Run 0: 100%|██████████| 1/1 [00:08&lt;00:00,  8.86s/it, Current Best:=-0.428]Run 0: 100%|██████████| 1/1 [00:08&lt;00:00,  8.86s/it, Current Best:=-0.428]",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html",
    "href": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html",
    "title": "Himmelblau Benchmark with outliers",
    "section": "",
    "text": "import os\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.benchmark import UniformOutlierPrior\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.features.api import CategoricalInput\nfrom bofire.data_models.outlier_detection.api import (\n    IterativeTrimming,\n    OutlierDetections,\n)\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    MixedSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark with outliers"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#imports",
    "href": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#imports",
    "title": "Himmelblau Benchmark with outliers",
    "section": "",
    "text": "import os\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.benchmark import UniformOutlierPrior\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.acquisition_functions.api import qLogEI\nfrom bofire.data_models.domain.api import Domain, Outputs\nfrom bofire.data_models.features.api import CategoricalInput\nfrom bofire.data_models.outlier_detection.api import (\n    IterativeTrimming,\n    OutlierDetections,\n)\nfrom bofire.data_models.strategies.api import RandomStrategy, SoboStrategy\nfrom bofire.data_models.surrogates.api import (\n    MixedSingleTaskGPSurrogate,\n    SingleTaskGPSurrogate,\n)\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark with outliers"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#sample-set-of-himmelblau-example-to-start-optimization",
    "href": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#sample-set-of-himmelblau-example-to-start-optimization",
    "title": "Himmelblau Benchmark with outliers",
    "section": "sample set of Himmelblau example to start optimization",
    "text": "sample set of Himmelblau example to start optimization\nwe use the same set of example as initial data for comparison of three models. One case is where there is no outlier, while for other two models, we introduced outliers at a fixed probability. Using same initial example data helps us to compare the efficiency of outlier detection compared to the no outlier model that works on the dataset with no outliers. Further, using same sampled set with same outliers help starting both models with and without outlier detection from same value and we can see their evolution with iterations.\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nbo_results_set = []  # stores progress of model on data with no outliers (no outliers model)\nbo_results_outliers_set = []  # stores progress of model with no outlier detection on data with outliers (baseline model)\nbo_results_no_outliers_set = []  # stores progress of model with the outlier detection on data with outliers (our model)\n\nBenchmark = Himmelblau()\nsampled = sample(Benchmark.domain)\nsampled_xy = Benchmark.f(sampled, return_complete=True)",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark with outliers"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-1",
    "href": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-1",
    "title": "Himmelblau Benchmark with outliers",
    "section": "Performance of models",
    "text": "Performance of models\n\nif not SMOKE_TEST:\n    plt.plot(bo_results_itr.mean(axis=0), color=\"red\", label=\"no outliers\")\n    plt.scatter(range(50), bo_results_itr.mean(axis=0), color=\"red\")\n    plt.fill_between(\n        range(50),\n        (bo_results_itr.mean(axis=0) - bo_results_itr.std(axis=0)),\n        (bo_results_itr.mean(axis=0) + bo_results_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"red\",\n    )\n    plt.plot(bo_results_outliers_itr.mean(axis=0), color=\"blue\", label=\"baseline\")\n    plt.scatter(range(50), bo_results_outliers_itr.mean(axis=0), color=\"blue\")\n    plt.fill_between(\n        range(50),\n        (bo_results_outliers_itr.mean(axis=0) - bo_results_outliers_itr.std(axis=0)),\n        (bo_results_outliers_itr.mean(axis=0) + bo_results_outliers_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"blue\",\n    )\n    plt.plot(bo_results_no_outliers_itr.mean(axis=0), color=\"green\", label=\"ITGP model\")\n    plt.scatter(range(50), bo_results_no_outliers_itr.mean(axis=0), color=\"green\")\n    plt.fill_between(\n        range(50),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            - bo_results_no_outliers_itr.std(axis=0)\n        ),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            + bo_results_no_outliers_itr.std(axis=0)\n        ),\n        alpha=0.3,\n        color=\"green\",\n    )\n\n    plt.ylabel(\"function value\")\n    # plt.yscale('log',base=10)\n    plt.legend()\n    plt.title(\"outliers moderately outrageous\")\n    plt.show()",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark with outliers"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-2",
    "href": "docs/tutorials/benchmarks/005-Himmelblau_with_outliers.html#performance-of-models-2",
    "title": "Himmelblau Benchmark with outliers",
    "section": "Performance of models",
    "text": "Performance of models\n\nif not SMOKE_TEST:\n    plt.plot(bo_results_itr.mean(axis=0), color=\"red\", label=\"no outliers\")\n    plt.scatter(range(50), bo_results_itr.mean(axis=0), color=\"red\")\n    plt.fill_between(\n        range(50),\n        (bo_results_itr.mean(axis=0) - bo_results_itr.std(axis=0)),\n        (bo_results_itr.mean(axis=0) + bo_results_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"red\",\n    )\n    plt.plot(bo_results_outliers_itr.mean(axis=0), color=\"blue\", label=\"baseline\")\n    plt.scatter(range(50), bo_results_outliers_itr.mean(axis=0), color=\"blue\")\n    plt.fill_between(\n        range(50),\n        (bo_results_outliers_itr.mean(axis=0) - bo_results_outliers_itr.std(axis=0)),\n        (bo_results_outliers_itr.mean(axis=0) + bo_results_outliers_itr.std(axis=0)),\n        alpha=0.3,\n        color=\"blue\",\n    )\n    plt.plot(bo_results_no_outliers_itr.mean(axis=0), color=\"green\", label=\"ITGP model\")\n    plt.scatter(range(50), bo_results_no_outliers_itr.mean(axis=0), color=\"green\")\n    plt.fill_between(\n        range(50),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            - bo_results_no_outliers_itr.std(axis=0)\n        ),\n        (\n            bo_results_no_outliers_itr.mean(axis=0)\n            + bo_results_no_outliers_itr.std(axis=0)\n        ),\n        alpha=0.3,\n        color=\"green\",\n    )\n\n    plt.ylabel(\"function value\")\n    # plt.yscale('log',base=10)\n    plt.legend()\n    plt.title(\"outliers too outrageous\")\n    plt.show()",
    "crumbs": [
      "Benchmarks",
      "Himmelblau Benchmark with outliers"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html",
    "title": "Local Search Region Bayesian Optimization",
    "section": "",
    "text": "In this notebook the Branin benchmark from paper about local search region BO (https://www.merl.com/publications/docs/TR2023-057.pdf) is reproduced. Note that we use here by purpose qEI as acquisition function to reproduce the paper.",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#imports-and-helper-methods",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#imports-and-helper-methods",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Imports and helper methods",
    "text": "Imports and helper methods\n\nimport os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Branin\nfrom bofire.data_models.acquisition_functions.api import qEI\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.strategies.api import (\n    LSRBO,\n    BotorchOptimizer,\n    RandomStrategy,\n    SoboStrategy,\n)\nfrom bofire.runners.api import run\n\n\nwarnings.filterwarnings(\"ignore\")\n\nbench = Branin(locality_factor=0.5)\n\n\ndef sample(domain):\n    sampled = domain.inputs.sample(10)\n    return sampled\n    # sampled = bench.f(sampled, return_complete = True)\n    # sampled = sampled.sort_values(by=\"y\", ascending=False, ignore_index=True)\n    # return sampled[bench.domain.inputs.get_keys()].copy()\n\n\ndef best(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    return experiments.y.min()\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#random-optimization",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#random-optimization",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Random Optimization",
    "text": "Random Optimization\nHere random sampling is performed without any local search region constraints applied.\n\nrandom_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=1.559]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 98.89it/s, Current Best:=1.559]",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#sobo-optimization",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#sobo-optimization",
    "title": "Local Search Region Bayesian Optimization",
    "section": "SOBO Optimization",
    "text": "SOBO Optimization\nHere standard bayesian optimization is performed without any local search region constraints applied using the qEI acquistion function.\n\nsobo_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(domain=domain, acquisition_function=qEI()),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=4.520]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.19it/s, Current Best:=4.520]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  1.19it/s, Current Best:=4.520]",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#local-sobo-optimization",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#local-sobo-optimization",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Local SOBO Optimization",
    "text": "Local SOBO Optimization\nHere bayesian optimization is performed with setting gamma parameter of the LSR-BO method to 0 which results in a pure local optimization with respect to the last random sample.\n\nlocal_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qEI(),\n                acquisition_optimizer=BotorchOptimizer(\n                    local_search_config=LSRBO(gamma=0)\n                ),\n            ),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=9.953]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.17it/s, Current Best:=9.953]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.17it/s, Current Best:=9.953]",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#global-sobo-optimization-projection",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#global-sobo-optimization-projection",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Global SOBO Optimization (Projection)",
    "text": "Global SOBO Optimization (Projection)\nHere bayesian optimization is performed with setting gamma parameter of the LSR-BO method to 500 which results in taking always the biggest step in the direction of the global candidate. In the original paper, this is called “Projection”.\n\nglobal_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qEI(),\n                acquisition_optimizer=BotorchOptimizer(\n                    local_search_config=LSRBO(gamma=500)\n                ),\n            ),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:01&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:01&lt;?, ?it/s, Current Best:=4.657]Run 0: 100%|██████████| 1/1 [00:01&lt;00:00,  1.07s/it, Current Best:=4.657]Run 0: 100%|██████████| 1/1 [00:01&lt;00:00,  1.07s/it, Current Best:=4.657]",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#lsr-bo",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#lsr-bo",
    "title": "Local Search Region Bayesian Optimization",
    "section": "LSR-BO",
    "text": "LSR-BO\nHere the actual method from the paper is performed with setting gamma to 0.1.\n\nlsr_results = [\n    run(\n        Branin(locality_factor=0.5),\n        strategy_factory=lambda domain: strategies.map(\n            SoboStrategy(\n                domain=domain,\n                acquisition_function=qEI(),\n                acquisition_optimizer=BotorchOptimizer(\n                    local_search_config=LSRBO(gamma=0.1)\n                ),\n            ),\n        ),\n        n_iterations=80 if not SMOKE_TEST else 1,\n        metric=best,\n        initial_sampler=sample,\n        n_runs=1,\n        n_procs=1,\n    )\n    for _ in range(5 if not SMOKE_TEST else 1)\n]\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=3.004]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.00it/s, Current Best:=3.004]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00,  2.00it/s, Current Best:=3.004]",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/007-LSRBO.html#plot-the-results",
    "href": "docs/tutorials/benchmarks/007-LSRBO.html#plot-the-results",
    "title": "Local Search Region Bayesian Optimization",
    "section": "Plot the results",
    "text": "Plot the results\n\nif not SMOKE_TEST:\n    fig, ax = plt.subplots()\n\n    best_random = np.array(\n        [random_results[i][0][1] for i in range(len(random_results))]\n    )\n    ax.plot(range(80), best_random.mean(axis=0), color=\"gray\", label=\"Random\")\n    ax.fill_between(\n        range(80),\n        (best_random.mean(0) - best_random.std(0)),\n        (best_random.mean(0) + best_random.std(0)),\n        alpha=0.3,\n        color=\"gray\",\n    )\n\n    best_global = np.log10(\n        np.array([global_results[i][0][1] for i in range(len(global_results))])\n        - 0.397887,\n    )\n    ax.plot(range(80), best_global.mean(axis=0), color=\"orange\", label=\"Projection\")\n    ax.fill_between(\n        range(80),\n        (best_global.mean(0) - best_global.std(0)),\n        (best_global.mean(0) + best_global.std(0)),\n        alpha=0.3,\n        color=\"orange\",\n    )\n\n    best_local = np.log10(\n        np.array([local_results[i][0][1] for i in range(len(global_results))])\n        - 0.397887,\n    )\n    ax.plot(range(80), best_local.mean(axis=0), color=\"green\", label=\"Local\")\n    ax.fill_between(\n        range(80),\n        (best_local.mean(0) - best_local.std(0)),\n        (best_local.mean(0) + best_local.std(0)),\n        alpha=0.3,\n        color=\"green\",\n    )\n\n    best_lsr = np.log10(\n        np.array([lsr_results[i][0][1] for i in range(len(lsr_results))]) - 0.397887,\n    )\n    ax.plot(range(80), best_lsr.mean(axis=0), color=\"blue\", label=\"LSR\")\n    ax.fill_between(\n        range(80),\n        (best_lsr.mean(0) - best_lsr.std(0)),\n        (best_lsr.mean(0) + best_lsr.std(0)),\n        alpha=0.3,\n        color=\"blue\",\n    )\n\n    best_unconstrained = np.log10(\n        np.array([sobo_results[i][0][1] for i in range(len(sobo_results))]) - 0.397887,\n    )\n    ax.plot(\n        range(80), best_unconstrained.mean(axis=0), color=\"red\", label=\"Unconstrained\"\n    )\n    ax.fill_between(\n        range(80),\n        (best_unconstrained.mean(0) - best_unconstrained.std(0)),\n        (best_unconstrained.mean(0) + best_unconstrained.std(0)),\n        alpha=0.3,\n        color=\"red\",\n    )\n\n    ax.set_xlabel(\"Iteration\")\n    ax.set_ylabel(\"Log10(SimpleRegret)\")\n    ax.legend()\n\n    plt.show()",
    "crumbs": [
      "Benchmarks",
      "Local Search Region Bayesian Optimization"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/009-BNH.html",
    "href": "docs/tutorials/benchmarks/009-BNH.html",
    "title": "BNH Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import BNH\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import (\n    MaximizeSigmoidObjective,\n    MinimizeObjective,\n    MinimizeSigmoidObjective,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.plot.api import plot_objective_plotly\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "BNH Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/009-BNH.html#imports",
    "href": "docs/tutorials/benchmarks/009-BNH.html#imports",
    "title": "BNH Benchmark",
    "section": "",
    "text": "import os\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import BNH\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import (\n    MaximizeSigmoidObjective,\n    MinimizeObjective,\n    MinimizeSigmoidObjective,\n)\nfrom bofire.data_models.strategies.api import MoboStrategy, RandomStrategy\nfrom bofire.plot.api import plot_objective_plotly\nfrom bofire.runners.api import run\nfrom bofire.utils.multiobjective import compute_hypervolume\n\n\nwarnings.simplefilter(\"once\")\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")",
    "crumbs": [
      "Benchmarks",
      "BNH Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/009-BNH.html#random-strategy",
    "href": "docs/tutorials/benchmarks/009-BNH.html#random-strategy",
    "title": "BNH Benchmark",
    "section": "Random Strategy",
    "text": "Random Strategy\n\ndef sample(domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ndef hypervolume(domain: Domain, experiments: pd.DataFrame) -&gt; float:\n    if \"c1\" in experiments.columns:\n        return compute_hypervolume(\n            domain,\n            experiments.loc[(experiments.c1 &lt;= 25) & (experiments.c2 &gt;= 7.7)],\n            ref_point={\"f1\": 140, \"f2\": 50},\n        )\n    return compute_hypervolume(domain, experiments, ref_point={\"f1\": 140, \"f2\": 50})\n\n\nrandom_results = run(\n    BNH(constraints=True),\n    strategy_factory=lambda domain: strategies.map(RandomStrategy(domain=domain)),\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/utils/torch_tools.py:706: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\nRun 0:   0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:00&lt;?, ?it/s, Current Best:=4392.203]Run 0: 100%|██████████| 1/1 [00:00&lt;00:00, 42.69it/s, Current Best:=4392.203]",
    "crumbs": [
      "Benchmarks",
      "BNH Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/009-BNH.html#mobo-strategy",
    "href": "docs/tutorials/benchmarks/009-BNH.html#mobo-strategy",
    "title": "BNH Benchmark",
    "section": "MOBO Strategy",
    "text": "MOBO Strategy\n\nWithout Constraints\n\ndef strategy_factory(domain: Domain):\n    data_model = MoboStrategy(domain=domain, ref_point={\"f1\": 140, \"f2\": 50})\n    return strategies.map(data_model)\n\n\nresults = run(\n    BNH(constraints=False),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:04&lt;?, ?it/s, Current Best:=4719.264]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.96s/it, Current Best:=4719.264]Run 0: 100%|██████████| 1/1 [00:04&lt;00:00,  4.96s/it, Current Best:=4719.264]\n\n\n\n\nWith Constraints\n\nManual Setup of the Domain\n\ndomain = Domain(\n    inputs=Inputs(\n        features=[\n            ContinuousInput(key=\"x1\", bounds=(0, 5)),\n            ContinuousInput(key=\"x2\", bounds=(0, 3)),\n        ],\n    ),\n    outputs=Outputs(\n        features=[\n            ContinuousOutput(key=\"f1\", objective=MinimizeObjective()),\n            ContinuousOutput(key=\"f2\", objective=MinimizeObjective()),\n            # these are the output constraints, choose MinimizeSigmoidObjective for lower bound constraints\n            # and MaximizeSigmoidObjective for upper bound constraints\n            # tp is the threshold point, steepness is the steepness of the sigmoid that is applied to the constraint\n            # usually a steepness of 1000 is fine.\n            ContinuousOutput(\n                key=\"c1\",\n                objective=MinimizeSigmoidObjective(tp=25, steepness=1000),\n            ),\n            ContinuousOutput(\n                key=\"c2\",\n                objective=MaximizeSigmoidObjective(tp=7.7, steepness=1000),\n            ),\n        ],\n    ),\n)\n\nOne can visualize the objectives in the following way:\n\nfeat = domain.outputs.get_by_key(\"c1\")\n\nif not SMOKE_TEST:\n    plot_objective_plotly(feat, lower=20, upper=30)  # type: ignore\n\n\nfeat = domain.outputs.get_by_key(\"c2\")\n\nif not SMOKE_TEST:\n    plot_objective_plotly(feat, lower=5, upper=10)  # type: ignore\n\n\n\nRun tbe optimization\nThe warnings can be ignored. They are stemming just from an internal postprocessing step and will be removed soon.\n\nc_results = run(\n    BNH(constraints=True),\n    strategy_factory=strategy_factory,\n    n_iterations=50 if not SMOKE_TEST else 1,\n    metric=hypervolume,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n  0%|          | 0/1 [00:00&lt;?, ?it/s]/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/pandas/core/arraylike.py:402: RuntimeWarning:\n\noverflow encountered in exp\n\nRun 0:   0%|          | 0/1 [00:05&lt;?, ?it/s]Run 0:   0%|          | 0/1 [00:05&lt;?, ?it/s, Current Best:=4723.576]Run 0: 100%|██████████| 1/1 [00:05&lt;00:00,  5.11s/it, Current Best:=4723.576]Run 0: 100%|██████████| 1/1 [00:05&lt;00:00,  5.11s/it, Current Best:=4723.576]\n\n\n\nif not SMOKE_TEST:\n    fig, ax = plt.subplots()\n    ax.plot(results[0][1], label=\"without constraints\")\n    ax.plot(c_results[0][1], label=\"with constraints\")\n    ax.set_xlabel(\"iteration\")\n    ax.set_ylabel(\"hypervolume\")\n    ax.legend()\n    plt.show()",
    "crumbs": [
      "Benchmarks",
      "BNH Benchmark"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/011-ActiveLearning.html",
    "href": "docs/tutorials/benchmarks/011-ActiveLearning.html",
    "title": "Active Learning",
    "section": "",
    "text": "Showcase of active learning in bofire. Active learning per definition focusses on fitting the model to the experimental observations best possible in an iterative manner reducing some kind of uncertainty. The ActiveLearningStrategy proposes a set of evaluation points that will gain the most information about the problem each iteration. Thus, an unknown black-box-function can be approximated without optimization. It represents an exploration-only strategy.\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import cm\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import GenericBenchmark\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.api import Domain, Inputs, Outputs\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.objectives.api import MinimizeObjective\nfrom bofire.data_models.strategies.api import RandomStrategy\nfrom bofire.runners.api import run\n\n\nSMOKE_TEST = os.environ.get(\"SMOKE_TEST\")\n\n\n1-D Objective Function\nFor a 1-D objective function. The Himmelblau benchmark is used. \\[\\begin{equation}\n    f: \\mathbb{R}^2 \\rightarrow \\mathbb{R} \\quad | \\quad f(x_1, x_2) = (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2) ^2\n\\end{equation}\\] To start the active learning strategy we need to supply some initial data points to set up the Gaussian Regression Model in the background.\n\nhimmelblau = Himmelblau()\n\n\ndef sample(domain: Domain):\n    datamodel = RandomStrategy(domain=domain)\n    sampler = strategies.map(data_model=datamodel)\n    sampled = sampler.ask(10)\n    return sampled\n\n\ninitial_points = sample(domain=himmelblau.domain)\ninitial_experiments = pd.concat([initial_points, himmelblau.f(initial_points)], axis=1)\ndisplay(initial_experiments)\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n-2.572315\n-4.182943\n136.179641\n1\n\n\n1\n5.755339\n0.901911\n530.375408\n1\n\n\n2\n-0.454152\n-4.303777\n350.443355\n1\n\n\n3\n-2.104895\n4.283005\n90.591137\n1\n\n\n4\n-5.976054\n-5.827297\n796.894518\n1\n\n\n5\n3.961876\n2.276960\n53.235721\n1\n\n\n6\n-2.562994\n0.971048\n86.277138\n1\n\n\n7\n-3.362257\n0.990991\n89.667053\n1\n\n\n8\n1.811745\n-4.842215\n491.131847\n1\n\n\n9\n-4.422257\n-1.422631\n139.219598\n1\n\n\n\n\n\n\n\n\nActiveLearningStrategy\nThe ActiveLearningstrategy can be set up just as other BO strategies implemented in bofire. Just take a look into the other tutorials. Basic calls are ask() to retrieve new evaluation candidates from the acquisition function and tell() to train the model with a new observation.\nCurrently, the default active learning acquisition function implemented is qNegIntegratedPosteriorVariance. It focuses on minimizing the overall posterior variance by choosing a new candidate.\nThe ActiveLearningStrategy uses Monte-Carlo-integration to evaluate the acquisition function. The number of integration nodes significantly influences the speed of the integration. These can be adjusted by changing the parameter data_model.num_sobol_samples. Note that a sample size representing a power of \\(2\\) increases performance.\n\n# Manual set up of ActiveLearning\nfrom bofire.data_models.acquisition_functions.api import qNegIntPosVar\nfrom bofire.data_models.strategies.api import ActiveLearningStrategy\nfrom bofire.data_models.surrogates.api import BotorchSurrogates, SingleTaskGPSurrogate\n\n\naf = qNegIntPosVar(\n    n_mc_samples=64,  # lower the number of monte carlo samples to improve speed\n)\n\ndata_model = ActiveLearningStrategy(domain=himmelblau.domain, acquisition_function=af)\nrecommender = strategies.map(data_model=data_model)\nrecommender.tell(experiments=initial_experiments)\ncandidates = recommender.ask(candidate_count=1)\n\n\ndisplay(candidates)\n\n\n\n\n\n\n\n\nx_1\nx_2\ny_pred\ny_sd\ny_des\n\n\n\n\n0\n2.968745\n-0.009181\n215.493316\n158.054856\n-215.493316\n\n\n\n\n\n\n\n\n# Running the active learning strategy\nn_iter = 1 if SMOKE_TEST else 20\nresults = initial_experiments\n\nfor _ in range(n_iter):\n    # run active learning strategy\n    X = recommender.ask(candidate_count=1)[himmelblau.domain.inputs.get_keys()]\n    Y = himmelblau.f(X)\n    XY = pd.concat([X, Y], axis=1)\n    recommender.tell(experiments=XY)  # pass new experimental data\n    results = pd.concat([results, XY], axis=0, ignore_index=True)\n\n\n# Running a random strategy for comparison\ndef strategy_factory(domain: Domain):\n    data_model = RandomStrategy(domain=domain)\n    return strategies.map(data_model)\n\n\nrandom_results = run(\n    himmelblau,\n    strategy_factory=strategy_factory,\n    n_iterations=n_iter,\n    metric=lambda domain, experiments: 1.0,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n\n\nPlotting\n\nif not SMOKE_TEST:\n    plt.rcParams[\"figure.figsize\"] = (10, 4)\n\n    fig, ax = plt.subplots(1, 2)\n\n# contour plot of himmelblau\n    def f(grid):\n        return (grid[0] ** 2 + grid[1] - 11) ** 2 + (grid[0] + grid[1] ** 2) ** 2\n\n\n    X_grid = np.arange(-7, 7, 0.01)\n    Y_grid = np.arange(-7, 7, 0.01)\n    mesh = np.meshgrid(X_grid, Y_grid)\n    Z = f(grid=mesh)\n    levels = np.linspace(Z.min(), Z.max(), 6)\n\n\n    ax[0].contourf(X_grid, Y_grid, Z, cmap=cm.viridis)\n    ax[0].scatter(random_results[0][0].x_1, random_results[0][0].x_2, c=\"white\")\n    ax[1].contourf(X_grid, Y_grid, Z, cmap=cm.viridis)\n    ax[1].scatter(results.x_1, results.x_2, c=\"white\")\n\n    ax[0].axis([-7, 7, -7, 7])\n    ax[0].set_xlabel(\"$x_1$\")\n    ax[1].set_xlabel(\"$x_1$\")\n    ax[0].set_ylabel(\"$x_2$\")\n    ax[0].set_title(\"random strategy\")\n    ax[1].set_title(\"active learning strategy\")\n    fig.show()\n\nThe plot shows the exploratory behavior of the ActiveLearningStrategy.\n\n\n\n2-D (n-D) Objective Function\nNow, we want to actively learn an objective function with a multi-dimensional output space. This is shown by an example function with \\(2\\) output variables. For this, we again utilize the Himmelblau benchmark function and the Ackley function.\n\\[\\begin{equation}\n    f: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2   \\quad | \\quad \n    f(x_1, x_2) =\n        \\begin{pmatrix}\n            (x_1^2 + x_2 - 11)^2 + (x_1 + x_2^2) ^2 \\\\\n            -20\\exp \\left[-0.2{\\sqrt {0.5\\left(x_1^{2}+x_2^{2}\\right)}}\\right]\n            -\\exp \\left[0.5\\left(\\cos 2\\pi x_1+\\cos 2\\pi x_2\\right)\\right]+e+20\n\n        \\end{pmatrix}\n\\end{equation}\\]\n\ninputs = Inputs(\n    features=[\n        ContinuousInput(key=\"x_1\", bounds=(-6, 6)),\n        ContinuousInput(key=\"x_2\", bounds=(-6, 6)),\n    ],\n)\noutputs = Outputs(\n    features=[\n        ContinuousOutput(key=\"f_0\", objective=MinimizeObjective()),\n        ContinuousOutput(key=\"f_1\", objective=MinimizeObjective()),\n    ],\n)\ndomain = Domain(inputs=inputs, outputs=outputs)\n\n\ndef benchmark_function(candidates):\n    f0 = (candidates[\"x_1\"] ** 2 + candidates[\"x_2\"] - 11) ** 2 + (\n        candidates[\"x_1\"] + candidates[\"x_2\"] ** 2\n    ) ** 2\n    f1 = -20 * np.exp(\n        -0.2 * np.sqrt(0.5 * (candidates[\"x_1\"] ** 2 + candidates[\"x_2\"] ** 2)),\n    ) + (\n        -np.exp(\n            0.5\n            * (\n                np.cos(2 * np.pi * candidates[\"x_1\"])\n                + np.cos(2 * np.pi * candidates[\"x_2\"])\n            ),\n        )\n        + np.exp(1)\n        + 20\n    )\n    return pd.DataFrame({\"f_0\": f0, \"f_1\": f1})\n\n\nfunction = GenericBenchmark(domain=domain, func=benchmark_function)\ninitial_experiments = pd.concat(\n    [initial_points, function.f(candidates=initial_points)],\n    axis=1,\n)\n\nFor the multi-objective function we need to pass two models to the strategy as each individual output is represented by a separate model. By default, the ActiveLearningStrategy focusses on minimizing the negative integrated posterior variance of each model equally. To minimize the variances in a more specific way certain weights can be provided for each output feature. This can be done by passing a dictionary containing the individual weights for each output feature with its corresponding key to the parameter weights.\n\n# Manual set up of ActiveLearning\nweights = {\n    \"f_0\": 0.4,\n    \"f_1\": 0.6,\n}\n# create an instance of the acquisition function with distinct weights\naf = qNegIntPosVar(weights=weights, n_mc_samples=16)\n\ndata_model = ActiveLearningStrategy(\n    domain=domain,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[0]]),\n            ),\n            SingleTaskGPSurrogate(\n                inputs=domain.inputs,\n                outputs=Outputs(features=[domain.outputs[1]]),\n            ),\n        ],\n    ),\n    acquisition_function=af,\n)\nrecommender = strategies.map(data_model=data_model)\nrecommender.tell(experiments=initial_experiments)\ncandidates = recommender.ask(candidate_count=1)\n\n\ndisplay(candidates)\n\n\n\n\n\n\n\n\nx_1\nx_2\nf_0_pred\nf_1_pred\nf_0_sd\nf_1_sd\nf_0_des\nf_1_des\n\n\n\n\n0\n-1.281995\n5.421464\n335.796005\n11.949257\n331.339499\n0.917908\n-335.796005\n-11.949257\n\n\n\n\n\n\n\n\n# Running the active learning strategy\nn_iter = 1 if SMOKE_TEST else 20\nresults = initial_experiments\n\nfor _ in range(n_iter):\n    # run active learning strategy\n    X = recommender.ask(candidate_count=1)[domain.inputs.get_keys()]\n    Y = function.f(candidates=X)\n    XY = pd.concat([X, Y], axis=1)\n    recommender.tell(experiments=XY)  # pass new experimental data\n    results = pd.concat([results, XY], axis=0, ignore_index=True)\n\n\nrandom_results = run(\n    function,\n    strategy_factory=strategy_factory,\n    n_iterations=n_iter,\n    metric=lambda domain, experiments: 1.0,\n    initial_sampler=sample,\n    n_runs=1,\n    n_procs=1,\n)\n\n\nPlotting\n\nif not SMOKE_TEST:\n    plt.rcParams[\"figure.figsize\"] = (10, 8)\n    fig, ax = plt.subplots(2, 2)\n\n\n    def f1(grid):\n        return (\n            -20 * np.exp(-0.2 * np.sqrt(0.5 * (grid[0] ** 2 + grid[1] ** 2)))\n            - np.exp(0.5 * (np.cos(2 * np.pi * grid[0]) + np.cos(2 * np.pi * grid[1])))\n            + np.exp(1)\n            + 20\n        )\n\n\n    Z1 = f1(mesh)\n    levels = np.linspace(Z1.min(), Z1.max(), 10)\n    ax[0, 0].contourf(\n        X_grid,\n        Y_grid,\n        Z,\n        cmap=cm.viridis,\n    )\n    ax[0, 0].scatter(random_results[0][0].x_1, random_results[0][0].x_2, c=\"white\")\n    ax[0, 1].contourf(\n        X_grid,\n        Y_grid,\n        Z,\n        cmap=cm.viridis,\n    )\n    ax[0, 1].scatter(results.x_1, results.x_2, c=\"white\")\n    ax[1, 0].contourf(X_grid, Y_grid, Z1, cmap=cm.viridis, levels=levels)\n    ax[1, 0].scatter(\n        random_results[0][0].x_1,\n        random_results[0][0].x_2,\n        c=\"white\",\n        edgecolors=\"black\",\n    )\n    ax[1, 1].contourf(X_grid, Y_grid, Z1, cmap=cm.viridis, levels=levels)\n    ax[1, 1].scatter(results.x_1, results.x_2, c=\"white\", edgecolors=\"black\")\n\n    ax[0, 0].axis([-7, 7, -7, 7])\n    ax[1, 0].set_xlabel(\"$x_1$\")\n    ax[1, 1].set_xlabel(\"$x_1$\")\n    ax[0, 0].set_ylabel(\"$x_2$\")\n    ax[1, 0].set_ylabel(\"$x_2$\")\n    ax[0, 0].set_title(\"random strategy\")\n    ax[0, 1].set_title(\"active learning strategy\")\n    fig.show()",
    "crumbs": [
      "Benchmarks",
      "Active Learning"
    ]
  },
  {
    "objectID": "docs/tutorials/benchmarks/013-spherical_kernel.html",
    "href": "docs/tutorials/benchmarks/013-spherical_kernel.html",
    "title": "Spherical linear kernels for high dimensional BO",
    "section": "",
    "text": "Spherical Linear Kernel is useful for optimizing high-dimensional problems.\n\nfrom bofire.benchmarks.svm import SVM\nfrom bofire.data_models.strategies.api import SoboStrategy\nfrom bofire.data_models.kernels.api import SphericalLinearKernel\nfrom bofire.data_models.surrogates.api import SingleTaskGPSurrogate, BotorchSurrogates\nimport bofire.strategies.api as strategies\n\nWe use the SVM benchmark.\n\n# problem setup for spherical linear kernels\nbenchmark = SVM()\ncandidates = benchmark._domain.inputs.sample(benchmark.dim+1, seed=benchmark.seed)\nexperiments = candidates.copy()\nresult = benchmark._f(experiments)\n# Add empty columns 'y' and 'valid_y' to experiments DataFrame\nexperiments[\"y\"], experiments[\"valid_y\"] = result[\"y\"], result[\"valid_y\"]\nsobo_strategy_data_model = SoboStrategy(\n    domain=benchmark._domain,\n    seed=benchmark.seed,\n    surrogate_specs=BotorchSurrogates(\n        surrogates=[\n            SingleTaskGPSurrogate(\n                inputs=benchmark._domain.inputs,\n                outputs=benchmark._domain.outputs,\n                kernel=SphericalLinearKernel(),\n            )\n        ]\n    ),\n)\nstrategy = strategies.map(sobo_strategy_data_model)\n\nDownloading SVM data...\nDownload complete.\n\n\nRunning the optimization loop\n\nstrategy.tell(experiments, replace=True)\nnum_steps = 3 # set the number of steps here (the original paper uses 1000 steps)\nfor step_number in range(num_steps):\n    print(f\"Step {step_number+1}/{num_steps}\")\n    new_candidates = strategy.ask(candidate_count=1)\n    new_experiments = new_candidates.copy()\n    result = benchmark._f(new_candidates)\n    new_experiments[\"y\"], new_experiments[\"valid_y\"] = result[\"y\"], result[\"valid_y\"]\n    print(f\"New experiment:\\n{new_experiments}\")\n    strategy.tell(experiments=new_experiments)\n# save all the experiments\nall_experiments = strategy.experiments\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nStep 1/3\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:\n\nA not p.d., added jitter of 1.0e-08 to the diagonal\n\n\n\nNew experiment:\n        x_1      x_10     x_100     x_101     x_102     x_103     x_104  \\\n0  0.667271  0.645865  0.673514  0.788116  0.515714  0.899225  0.370435   \n\n      x_105     x_106     x_107  ...      x_95      x_96      x_97      x_98  \\\n0  0.745721  0.642221  0.920136  ...  0.456597  0.457589  0.825119  0.552249   \n\n       x_99    y_pred     y_sd     y_des         y  valid_y  \n0  0.946278  0.236777  0.00201 -0.236777  0.229736        1  \n\n[1 rows x 393 columns]\nStep 2/3\n\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning:\n\nA not p.d., added jitter of 1.0e-08 to the diagonal\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/optim/optimize.py:789: RuntimeWarning:\n\nOptimization failed in `gen_candidates_scipy` with the following warning(s):\n[OptimizationWarning('Optimization failed within `scipy.optimize.minimize` with status 2 and message ABNORMAL: .')]\nTrying again with a new set of initial conditions.\n\n\n\nNew experiment:\n        x_1      x_10     x_100     x_101     x_102     x_103     x_104  \\\n0  0.571132  0.402144  0.774951  0.740535  0.325296  0.751865  0.269752   \n\n      x_105     x_106     x_107  ...      x_95      x_96      x_97      x_98  \\\n0  0.163741  0.343008  0.328641  ...  0.959124  0.405325  0.504206  0.866072   \n\n       x_99    y_pred      y_sd     y_des         y  valid_y  \n0  0.324885  0.236599  0.002038 -0.236599  0.228193        1  \n\n[1 rows x 393 columns]\nStep 3/3\nNew experiment:\n        x_1      x_10    x_100  x_101     x_102     x_103     x_104  x_105  \\\n0  0.827119  0.849634  0.12755    1.0  0.285359  0.549768  0.751327    1.0   \n\n      x_106     x_107  ...      x_95  x_96      x_97      x_98      x_99  \\\n0  0.915843  0.460966  ...  0.422644   1.0  0.663223  0.673715  0.730583   \n\n     y_pred      y_sd     y_des         y  valid_y  \n0  0.197406  0.001861 -0.197406  0.237064        1  \n\n[1 rows x 393 columns]\n\n\nOne can use the results obtained in all_experiments to get the evolution of the optimum with respect to the iterations.",
    "crumbs": [
      "Benchmarks",
      "Spherical linear kernels for high dimensional BO"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/basic_examples.html",
    "href": "docs/tutorials/doe/basic_examples.html",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "",
    "text": "The following example has been taken from the paper “The construction of D- and I-optimal designs for mixture experiments with linear constraints on the components” by R. Coetzer and L. M. Haines (https://www.sciencedirect.com/science/article/pii/S0169743917303106).\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.ticker import FormatStrFormatter\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import (\n    InterpointEqualityConstraint,\n    LinearEqualityConstraint,\n    LinearInequalityConstraint,\n    NonlinearEqualityConstraint,\n    NonlinearInequalityConstraint,\n)\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion, IOptimalityCriterion",
    "crumbs": [
      "Design of Experiments",
      "Basic Examples for the DoE Subpackage"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/basic_examples.html#linear-model",
    "href": "docs/tutorials/doe/basic_examples.html#linear-model",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "Linear model",
    "text": "Linear model\nCreating an experimental design that is D-optimal with respect to a linear model is done the same way as making proposals using other methods in BoFire; you 1. create a domain 2. construct a stategy data model (here we want DoEStrategy) 3. map the strategy to its functional version, and finally 4. ask the strategy for proposals.\nWe will start with the simplest case: make a design based on a linear model containing main-effects (i.e., simply the inputs themselves and an intercept, without any second-order terms).\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0.1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 0.6)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n        LinearInequalityConstraint(features=[\"x1\", \"x2\"], coefficients=[5, 4], rhs=3.9),\n        LinearInequalityConstraint(\n            features=[\"x1\", \"x2\"],\n            coefficients=[-20, 5],\n            rhs=-3,\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(candidate_count=12)\n\nLet’s visualize the experiments that were chosen. We will see that such a design puts the experiments at the extremes of the experimental space - these are the points that best allow us to estimate the parameters of the linear model we chose.\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 45)\nax.set_title(\"Linear model\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solutions\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()",
    "crumbs": [
      "Design of Experiments",
      "Basic Examples for the DoE Subpackage"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/basic_examples.html#cubic-model",
    "href": "docs/tutorials/doe/basic_examples.html#cubic-model",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "cubic model",
    "text": "cubic model\nWhile the previous design is optimal for the main-effects model, we might prefer to see something that does not allocate all the experimental effort to values at the boundary of the space. This implies that we think there might be some higher-order effects present in the system - if we were sure that the target variable would follow straight-line behavior across the domain, we would not need to investigate any points away from the extremes.\nWe can address this by specifying our own linear model that includes higher-order terms.\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\"\n    ),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12)\n\nIn this case we can compare with the result reported in the paper of Coetzer and Haines.\n\nd_opt = np.array(\n    [\n        [\n            0.7,\n            0.3,\n            0.2,\n            0.3,\n            0.5902,\n            0.4098,\n            0.2702,\n            0.2279,\n            0.4118,\n            0.5738,\n            0.4211,\n            0.3360,\n        ],\n        [0.1, 0.6, 0.2, 0.1, 0.2373, 0.4628, 0.4808, 0.3117, 0.1, 0.1, 0.2911, 0.2264],\n        [\n            0.2,\n            0.1,\n            0.6,\n            0.6,\n            0.1725,\n            0.1274,\n            0.249,\n            0.4604,\n            0.4882,\n            0.3262,\n            0.2878,\n            0.4376,\n        ],\n    ],\n)  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot D-optimal solution\nax.scatter(\n    xs=d_opt[0],\n    ys=d_opt[1],\n    zs=d_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"D-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[\"x1\"],\n    ys=candidates[\"x2\"],\n    zs=candidates[\"x3\"],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n\n\n\n\n\n\n\n\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=IOptimalityCriterion(\n        formula=\"x1 + x2 + x3 + {x1**2} + {x2**2} + {x3**2} + {x1**3} + {x2**3} + {x3**3} + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3\",\n        n_space_filling_points=60,\n        ipopt_options={\"max_iter\": 500},\n    ),\n    ipopt_options={\"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ncandidates = strategy.ask(12).to_numpy().T\n\n\ni_opt = np.array(\n    [\n        [0.7000, 0.1000, 0.2000],\n        [0.3000, 0.6000, 0.1000],\n        [0.2031, 0.1969, 0.6000],\n        [0.5899, 0.2376, 0.1725],\n        [0.4105, 0.4619, 0.1276],\n        [0.2720, 0.4882, 0.2398],\n        [0.2281, 0.3124, 0.4595],\n        [0.3492, 0.1000, 0.5508],\n        [0.5614, 0.1000, 0.3386],\n        [0.4621, 0.2342, 0.3037],\n        [0.3353, 0.2228, 0.4419],\n        [0.3782, 0.3618, 0.2600],\n    ]\n).T  # values taken from paper\n\n\nfig = plt.figure(figsize=((10, 10)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.set_title(\"cubic model\")\nax.view_init(45, 45)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\nplt.rcParams[\"figure.figsize\"] = (10, 8)\n\n# plot feasible polytope\nax.plot(\n    xs=[7 / 10, 3 / 10, 1 / 5, 3 / 10, 7 / 10],\n    ys=[1 / 10, 3 / 5, 1 / 5, 1 / 10, 1 / 10],\n    zs=[1 / 5, 1 / 10, 3 / 5, 3 / 5, 1 / 5],\n    linewidth=2,\n)\n\n# plot I-optimal solution\nax.scatter(\n    xs=i_opt[0],\n    ys=i_opt[1],\n    zs=i_opt[2],\n    marker=\"o\",\n    s=40,\n    color=\"darkgreen\",\n    label=\"I-optimal design, 12 points\",\n)\n\nax.scatter(\n    xs=candidates[0],\n    ys=candidates[1],\n    zs=candidates[2],\n    marker=\"o\",\n    s=40,\n    color=\"orange\",\n    label=\"optimal_design solution, 12 points\",\n)\n\nplt.legend()\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/objective.py:168: UserWarning:\n\nEquality constraints were detected. No equidistant grid of points can be generated. The design space will be filled via SpaceFilling.",
    "crumbs": [
      "Design of Experiments",
      "Basic Examples for the DoE Subpackage"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/basic_examples.html#nonlinear-constraints",
    "href": "docs/tutorials/doe/basic_examples.html#nonlinear-constraints",
    "title": "Basic Examples for the DoE Subpackage",
    "section": "Nonlinear Constraints",
    "text": "Nonlinear Constraints\nDesign generation also supports nonlinear constraints. The following 3 examples show what is possible.\nFirst, a convenience function for plotting.\n\ndef plot_results_3d(result, surface_func):\n    u, v = np.mgrid[0 : 2 * np.pi : 100j, 0 : np.pi : 80j]\n    X = np.cos(u) * np.sin(v)\n    Y = np.sin(u) * np.sin(v)\n    Z = surface_func(X, Y)\n\n    fig = plt.figure(figsize=(8, 8))\n    ax = fig.add_subplot(111, projection=\"3d\")\n    ax.plot_surface(X, Y, Z, alpha=0.3)\n    ax.scatter(\n        xs=result[\"x1\"],\n        ys=result[\"x2\"],\n        zs=result[\"x3\"],\n        marker=\"o\",\n        s=40,\n        color=\"red\",\n    )\n    ax.set(xlabel=\"x1\", ylabel=\"x2\", zlabel=\"x3\")\n    ax.xaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n\n\nExample 1: Design inside a cone / nonlinear inequality\nIn the following example we have three design variables. We impose the constraint that all experiments have to be contained in the interior of a cone, which corresponds to the nonlinear inequality constraint \\(\\sqrt{x_1^2 + x_2^2} - x_3 \\leq 0\\). The optimization is done for a linear model and we will see that it places the points on the surface of the cone so as to maximize the distance between them (although this is not explicitly the objective of the optimization).\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:91: UserWarning:\n\nNonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/utils.py:709: OptimizeWarning:\n\nConstraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n\n\n\n\n\n\n\n\n\n\nWe can do the same for a design space limited by an elliptical cone \\(x_1^2 + x_2^2 - x_3 \\leq 0\\).\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearInequalityConstraint(\n            expression=\"x1**2 + x2**2 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(\n    strategy.get_required_number_of_experiments(), raise_validation_error=False\n)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: x1**2 + x2**2)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:91: UserWarning:\n\nNonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/utils.py:709: OptimizeWarning:\n\nConstraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2: Design on the surface of a cone / nonlinear equality\nWe can also limit the design space to the surface of a cone, defined by the equality constraint \\(\\sqrt{x_1^2 + x_2^2} - x_3 = 0\\). Before, we observed that the experimental proposals happened to be on the surface of the cone, but now they are constrained so that this must be the case.\nRemark: Due to missing sampling methods, the initial points provided to IPOPT don’t satisfy the constraints. But this does not matter for the solution.\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x2\", bounds=(-1, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        NonlinearEqualityConstraint(\n            expression=\"(x1**2 + x2**2)**0.5 - x3\",\n            features=[\"x1\", \"x2\", \"x3\"],\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12, raise_validation_error=False)\nresult.round(3)\nplot_results_3d(result, surface_func=lambda x1, x2: np.sqrt(x1**2 + x2**2))\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:91: UserWarning:\n\nNonlinear constraints were detected. Not all features and checks are supported for this type of constraints.                 Using them can lead to unexpected behaviour. Please make sure to provide jacobians for nonlinear constraints.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:119: UserWarning:\n\n1 validation error for RandomStrategy\n  Value error, constraint `&lt;class 'bofire.data_models.constraints.nonlinear.NonlinearEqualityConstraint'&gt;` is not implemented for strategy `RandomStrategy` [type=value_error, input_value={'domain': Domain(type='D....5, 0], [0, 0, 0]]')]))}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/design.py:120: UserWarning:\n\nSampling failed. Falling back to uniform sampling on input domain.                      Providing a custom sampling strategy compatible with the problem can                       possibly improve performance.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/strategies/doe/utils.py:709: OptimizeWarning:\n\nConstraint options `finite_diff_jac_sparsity`, `finite_diff_rel_step`, `keep_feasible`, and `hess`are ignored by this method.\n\n\n\n\n\n\n\n\n\n\n\n\nExample 3: Batch constraints\nBatch constraints can be used to create designs where each set of multiplicity subsequent experiments have the same value for a certain feature. This can be useful for setups where experiments are done in parallel and some parameters must be shared by experiments in the same parallel batch.\nIn the following example we fix the value of the decision variable x1 for each batch of 3 experiments.\n\ndomain = Domain(\n    inputs=[\n        ContinuousInput(key=\"x1\", bounds=(0, 1)),\n        ContinuousInput(key=\"x2\", bounds=(0, 1)),\n        ContinuousInput(key=\"x3\", bounds=(0, 1)),\n    ],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[InterpointEqualityConstraint(features=[\"x1\"], multiplicity=3)],\n)\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=\"linear\"),\n    ipopt_options={\"max_iter\": 500, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\nresult = strategy.ask(12)\nresult.round(3)\n\n\n\n\n\n\n\n\nx1\nx2\nx3\n\n\n\n\n0\n1.0\n1.0\n0.0\n\n\n1\n1.0\n1.0\n0.0\n\n\n2\n1.0\n1.0\n0.0\n\n\n3\n1.0\n0.0\n0.0\n\n\n4\n1.0\n1.0\n0.0\n\n\n5\n1.0\n0.0\n1.0\n\n\n6\n0.0\n1.0\n0.0\n\n\n7\n0.0\n1.0\n1.0\n\n\n8\n0.0\n0.0\n0.0\n\n\n9\n0.0\n0.0\n1.0\n\n\n10\n0.0\n0.0\n1.0\n\n\n11\n0.0\n0.0\n1.0",
    "crumbs": [
      "Design of Experiments",
      "Basic Examples for the DoE Subpackage"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/design_with_explicit_formula.html",
    "href": "docs/tutorials/doe/design_with_explicit_formula.html",
    "title": "Design with explicit Formula",
    "section": "",
    "text": "This tutorial notebook shows how to setup a D-optimal design with BoFire while providing an explicit formula and not just one of the four available keywords linear, linear-and-interaction, linear-and-quadratic, fully-quadratic.\nMake sure that cyipoptis installed. The recommend way is the installation via conda conda install -c conda-forge cyipopt.",
    "crumbs": [
      "Design of Experiments",
      "Design with explicit Formula"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/design_with_explicit_formula.html#imports",
    "href": "docs/tutorials/doe/design_with_explicit_formula.html#imports",
    "title": "Design with explicit Formula",
    "section": "Imports",
    "text": "Imports\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.api import Domain, Inputs\nfrom bofire.data_models.features.api import ContinuousInput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import DOptimalityCriterion\nfrom bofire.utils.doe import get_confounding_matrix",
    "crumbs": [
      "Design of Experiments",
      "Design with explicit Formula"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/design_with_explicit_formula.html#setup-of-the-problem",
    "href": "docs/tutorials/doe/design_with_explicit_formula.html#setup-of-the-problem",
    "title": "Design with explicit Formula",
    "section": "Setup of the problem",
    "text": "Setup of the problem\n\ninput_features = Inputs(\n    features=[\n        ContinuousInput(key=\"a\", bounds=(0, 5)),\n        ContinuousInput(key=\"b\", bounds=(40, 800)),\n        ContinuousInput(key=\"c\", bounds=(80, 180)),\n        ContinuousInput(key=\"d\", bounds=(200, 800)),\n    ],\n)\ndomain = Domain(inputs=input_features)",
    "crumbs": [
      "Design of Experiments",
      "Design with explicit Formula"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/design_with_explicit_formula.html#definition-of-the-formula-for-which-the-optimal-points-should-be-found",
    "href": "docs/tutorials/doe/design_with_explicit_formula.html#definition-of-the-formula-for-which-the-optimal-points-should-be-found",
    "title": "Design with explicit Formula",
    "section": "Definition of the formula for which the optimal points should be found",
    "text": "Definition of the formula for which the optimal points should be found\n\nmodel_type = \"a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d\"\nmodel_type\n\n'a + {a**2} + b + c + d + a:b + a:c + a:d + b:c + b:d + c:d'",
    "crumbs": [
      "Design of Experiments",
      "Design with explicit Formula"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/design_with_explicit_formula.html#find-d-optimal-design",
    "href": "docs/tutorials/doe/design_with_explicit_formula.html#find-d-optimal-design",
    "title": "Design with explicit Formula",
    "section": "Find D-optimal Design",
    "text": "Find D-optimal Design\n\ndata_model = DoEStrategy(\n    domain=domain,\n    criterion=DOptimalityCriterion(formula=model_type),\n    ipopt_options={\"max_iter\": 100, \"print_level\": 0},\n)\nstrategy = strategies.map(data_model=data_model)\ndesign = strategy.ask(17)\ndesign\n\n\n\n\n\n\n\n\na\nb\nc\nd\n\n\n\n\n0\n0.000000\n40.000000\n80.0\n800.000000\n\n\n1\n0.000000\n800.000000\n80.0\n200.000000\n\n\n2\n3.363964\n800.000000\n80.0\n800.000000\n\n\n3\n2.163665\n64.296704\n180.0\n733.175455\n\n\n4\n5.000000\n40.000000\n180.0\n200.000000\n\n\n5\n0.000000\n94.899820\n180.0\n200.000000\n\n\n6\n5.000000\n800.000000\n180.0\n729.469697\n\n\n7\n5.000000\n40.000000\n180.0\n635.743144\n\n\n8\n5.000000\n71.567006\n80.0\n475.458384\n\n\n9\n5.000000\n688.912389\n180.0\n507.209079\n\n\n10\n0.000000\n606.739635\n180.0\n800.000000\n\n\n11\n5.000000\n578.619492\n180.0\n800.000000\n\n\n12\n2.016226\n86.027053\n80.0\n200.000000\n\n\n13\n2.071470\n800.000000\n180.0\n200.000000\n\n\n14\n0.000000\n800.000000\n80.0\n800.000000\n\n\n15\n5.000000\n40.000000\n80.0\n800.000000\n\n\n16\n5.000000\n800.000000\n80.0\n200.000000",
    "crumbs": [
      "Design of Experiments",
      "Design with explicit Formula"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/design_with_explicit_formula.html#analyze-confounding",
    "href": "docs/tutorials/doe/design_with_explicit_formula.html#analyze-confounding",
    "title": "Design with explicit Formula",
    "section": "Analyze Confounding",
    "text": "Analyze Confounding\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nmatplotlib.rcParams[\"figure.dpi\"] = 120\n\nm = get_confounding_matrix(\n    domain.inputs,\n    design=design,\n    interactions=[2, 3],\n    powers=[2],\n)\n\nsns.heatmap(m, annot=True, annot_kws={\"fontsize\": 7}, fmt=\"2.1f\")\nplt.show()",
    "crumbs": [
      "Design of Experiments",
      "Design with explicit Formula"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/index.html",
    "href": "docs/tutorials/doe/index.html",
    "title": "Design of Experiments (DOE)",
    "section": "",
    "text": "These tutorials demonstrate the usage of traditional design of experiments algorithms implemented in BoFire, including optimal designs and factorial designs.\n\n\n\n\nIntroduction to basic design of experiments concepts and implementations.\n\n\n\nCreating experimental designs using explicit model formulas.\n\n\n\nEfficient screening designs using fractional factorial approaches.\n\n\n\nHandling n-choose-k constraints in experimental designs.\n\n\n\nUsing different optimality criteria (D-optimal, A-optimal, etc.) for design generation.",
    "crumbs": [
      "Design of Experiments",
      "Design of Experiments (DOE)"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/index.html#available-tutorials",
    "href": "docs/tutorials/doe/index.html#available-tutorials",
    "title": "Design of Experiments (DOE)",
    "section": "",
    "text": "Introduction to basic design of experiments concepts and implementations.\n\n\n\nCreating experimental designs using explicit model formulas.\n\n\n\nEfficient screening designs using fractional factorial approaches.\n\n\n\nHandling n-choose-k constraints in experimental designs.\n\n\n\nUsing different optimality criteria (D-optimal, A-optimal, etc.) for design generation.",
    "crumbs": [
      "Design of Experiments",
      "Design of Experiments (DOE)"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/optimality_criteria.html",
    "href": "docs/tutorials/doe/optimality_criteria.html",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "",
    "text": "This tutorial notebook demonstrates the impact of different optimality-criteria for generating candidates using the DoE strategy for a two-dimensional fully-quadratic model.",
    "crumbs": [
      "Design of Experiments",
      "DoE Optimality Criteria in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/optimality_criteria.html#imports",
    "href": "docs/tutorials/doe/optimality_criteria.html#imports",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "Imports",
    "text": "Imports\n\nimport matplotlib.pyplot as plt\n\nimport bofire.strategies.api as strategies\nfrom bofire.data_models.constraints.api import LinearEqualityConstraint\nfrom bofire.data_models.domain.api import Domain\nfrom bofire.data_models.features.api import ContinuousInput, ContinuousOutput\nfrom bofire.data_models.strategies.api import DoEStrategy\nfrom bofire.data_models.strategies.doe import (\n    AOptimalityCriterion,\n    DOptimalityCriterion,\n    EOptimalityCriterion,\n    IOptimalityCriterion,\n    KOptimalityCriterion,\n    SpaceFillingCriterion,\n)\nfrom bofire.strategies.doe.objective import get_objective_function",
    "crumbs": [
      "Design of Experiments",
      "DoE Optimality Criteria in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/optimality_criteria.html#designs-for-different-optimality-criteria",
    "href": "docs/tutorials/doe/optimality_criteria.html#designs-for-different-optimality-criteria",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "Designs for different optimality criteria",
    "text": "Designs for different optimality criteria\n\n# Optimal designs for a quadratic model on the unit square\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(2)],\n    outputs=[ContinuousOutput(key=\"y\")],\n)\nmodel_type = \"fully-quadratic\"\nn_experiments = 13\n\nfig, ax = plt.subplots(figsize=(8, 8))\n\nfor crit, label in [\n    (DOptimalityCriterion, \"D-Optimality\"),\n    (AOptimalityCriterion, \"A-Optimality\"),\n    (KOptimalityCriterion, \"K-Optimality\"),\n    (EOptimalityCriterion, \"E-Optimality\"),\n    (IOptimalityCriterion, \"I-Optimality\"),\n]:\n    criterion = crit(formula=model_type)\n    data_model = DoEStrategy(\n        domain=domain,\n        criterion=criterion,\n        ipopt_options={\"max_iter\": 300},\n    )\n    strategy = strategies.map(data_model=data_model)\n    design = strategy.ask(candidate_count=n_experiments)\n    obj_value = get_objective_function(\n        criterion=criterion, domain=domain, n_experiments=n_experiments\n    ).evaluate(design.to_numpy().flatten())\n    ax.scatter(design.x1, design.x2, s=40, label=f\"{label}\")\n\n\nax.set_title(\"Designs with different optimality criteria\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.grid(alpha=0.3)\nax.legend()\n\nplt.show()",
    "crumbs": [
      "Design of Experiments",
      "DoE Optimality Criteria in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/doe/optimality_criteria.html#space-filling-design",
    "href": "docs/tutorials/doe/optimality_criteria.html#space-filling-design",
    "title": "DoE Optimality Criteria in BoFire",
    "section": "Space filling design",
    "text": "Space filling design\nBoFire can also generate space filling designs, here it is show three dimensions and a simplex constraint.\n\n# Space filling design on the unit 2-simplex\ndomain = Domain(\n    inputs=[ContinuousInput(key=f\"x{i+1}\", bounds=(0, 1)) for i in range(3)],\n    outputs=[ContinuousOutput(key=\"y\")],\n    constraints=[\n        LinearEqualityConstraint(\n            features=[\"x1\", \"x2\", \"x3\"],\n            coefficients=[1, 1, 1],\n            rhs=1,\n        ),\n    ],\n)\ndata_model = DoEStrategy(\n    domain=domain, criterion=SpaceFillingCriterion(), ipopt_options={\"max_iter\": 500}\n)\nstrategy = strategies.map(data_model=data_model)\nX = strategy.ask(candidate_count=40).to_numpy()\n\nfig = plt.figure(figsize=((10, 8)))\nax = fig.add_subplot(111, projection=\"3d\")\nax.view_init(45, 20)\nax.set_title(\"Space filling design\")\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.set_zlabel(\"$x_3$\")\n\n# plot feasible polytope\nax.plot(xs=[0, 0, 1, 0], ys=[0, 1, 0, 0], zs=[1, 0, 0, 1], linewidth=2)\n\n# plot design points\nax.scatter(xs=X[:, 0], ys=X[:, 1], zs=X[:, 2], s=40)\n\nplt.show()",
    "crumbs": [
      "Design of Experiments",
      "DoE Optimality Criteria in BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/models_serial.html",
    "href": "docs/tutorials/serialization/models_serial.html",
    "title": "Model Building with BoFire",
    "section": "",
    "text": "This notebooks shows how to setup and analyze models trained with BoFire. It is still WIP.",
    "crumbs": [
      "Serialization",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/models_serial.html#imports",
    "href": "docs/tutorials/serialization/models_serial.html#imports",
    "title": "Model Building with BoFire",
    "section": "Imports",
    "text": "Imports\n\nfrom pydantic import TypeAdapter\n\nimport bofire.surrogates.api as surrogates\nfrom bofire.benchmarks.multi import CrossCoupling\nfrom bofire.benchmarks.single import Himmelblau\nfrom bofire.data_models.domain.api import Outputs\nfrom bofire.data_models.enum import CategoricalEncodingEnum\nfrom bofire.data_models.surrogates.api import (\n    AnySurrogate,\n    EmpiricalSurrogate,\n    MixedSingleTaskGPSurrogate,\n    RandomForestSurrogate,\n    RegressionMLPEnsemble,\n    SingleTaskGPSurrogate,\n)",
    "crumbs": [
      "Serialization",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/models_serial.html#problem-setup",
    "href": "docs/tutorials/serialization/models_serial.html#problem-setup",
    "title": "Model Building with BoFire",
    "section": "Problem Setup",
    "text": "Problem Setup\nFor didactic purposes, we sample data from a Himmelblau benchmark function and use them to train a SingleTaskGP.\n\nbenchmark = Himmelblau()\nsamples = benchmark.domain.inputs.sample(n=50)\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments.head(10)\n\n\n\n\n\n\n\n\nx_1\nx_2\ny\nvalid_y\n\n\n\n\n0\n-3.853727\n2.947165\n50.917946\n1\n\n\n1\n-3.324718\n-5.413721\n389.108710\n1\n\n\n2\n4.411845\n3.972062\n328.617820\n1\n\n\n3\n-0.347501\n4.572293\n223.606819\n1\n\n\n4\n-4.855268\n3.692435\n267.749101\n1\n\n\n5\n-4.651991\n5.426848\n574.969706\n1\n\n\n6\n-0.449582\n-4.658118\n441.907030\n1\n\n\n7\n-4.161159\n-1.265276\n116.900305\n1\n\n\n8\n-0.287016\n-5.515142\n805.022458\n1\n\n\n9\n1.186874\n4.673191\n281.007603\n1",
    "crumbs": [
      "Serialization",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/models_serial.html#model-fitting",
    "href": "docs/tutorials/serialization/models_serial.html#model-fitting",
    "title": "Model Building with BoFire",
    "section": "Model Fitting",
    "text": "Model Fitting\n\ninput_features = benchmark.domain.inputs\noutput_features = benchmark.domain.outputs\n\n\ninput_features.model_dump_json()\n\n'{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]}'\n\n\n\noutput_features.model_dump_json()\n\n'{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]}'\n\n\n\nSingle Task GP\nGenerate the json spec\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = SingleTaskGPSurrogate(inputs=input_features, outputs=output_features)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":{\"type\":\"SingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"scalekernel\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\",\"lengthscale_constraint\":null,\"outputscale_constraint\":null},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"SingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"kernel\":{\"type\":\"RBFKernel\",\"features\":null,\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":null},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}'\n\n\nLoad it from the spec\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n\nMap it\n\nsurrogate = surrogates.map(surrogate_data)\n\nFit it. This is not 100% finished. In the future we will call here hyperfit which will return the CV results etc. This has to be finished. So ignore this for now and just call fit.\n\nsurrogate.fit(experiments=experiments)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/bofire/surrogates/botorch.py:180: UserWarning:\n\nThe given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n\n\n\nDump it.\n\n# dump it\ndump = surrogate.dumps()\n\nMake predictions.\n\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\nLoad again from spec and dump and make predictions.\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue\n\n\n\n\nRandom Forest\nGenerate the json spec\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = RandomForestSurrogate(\n    inputs=input_features,\n    outputs=output_features,\n    random_state=42,\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":null,\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"RandomForestSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"n_estimators\":100,\"criterion\":\"squared_error\",\"max_depth\":null,\"min_samples_split\":2,\"min_samples_leaf\":1,\"min_weight_fraction_leaf\":0.0,\"max_features\":1.0,\"max_leaf_nodes\":null,\"min_impurity_decrease\":0.0,\"bootstrap\":true,\"oob_score\":false,\"random_state\":42,\"ccp_alpha\":0.0,\"max_samples\":null}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# Fit it\nsurrogate.fit(experiments=experiments)\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/botorch/models/ensemble.py:82: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n/opt/hostedtoolcache/Python/3.12.12/x64/lib/python3.12/site-packages/torch/nn/modules/module.py:2916: RuntimeWarning:\n\nCould not update `train_inputs` with transformed inputs since _RandomForest does not have a `train_inputs` attribute. Make sure that the `input_transform` is applied to both the train inputs and test inputs.\n\n\n\nTrue\n\n\n\n\nMLP Ensemble\nGenerate the json spec\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = RegressionMLPEnsemble(\n    inputs=input_features,\n    outputs=output_features,\n    n_estimators=2,\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":null,\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"RegressionMLPEnsemble\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{},\"scaler\":\"IDENTITY\",\"output_scaler\":\"IDENTITY\",\"n_estimators\":2,\"hidden_layer_sizes\":[100],\"activation\":\"relu\",\"dropout\":0.0,\"batch_size\":10,\"n_epochs\":200,\"lr\":0.0001,\"weight_decay\":0.0,\"subsample_fraction\":1.0,\"shuffle\":true,\"final_activation\":\"identity\"}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# Fit it\nsurrogate.fit(experiments=experiments)\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue",
    "crumbs": [
      "Serialization",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "docs/tutorials/serialization/models_serial.html#empirical-surrogate",
    "href": "docs/tutorials/serialization/models_serial.html#empirical-surrogate",
    "title": "Model Building with BoFire",
    "section": "Empirical Surrogate",
    "text": "Empirical Surrogate\nThe empirical model is special as it has per default no fit and you need cloudpickle. There can be empirical models which implement a fit, but for this they also have to inherit from Trainable. The current example is the default without any fit functionality.\n\nfrom botorch.models.deterministic import DeterministicModel\nfrom torch import Tensor\n\n\nclass HimmelblauModel(DeterministicModel):\n    def __init__(self):\n        super().__init__()\n        self._num_outputs = 1\n\n    def forward(self, X: Tensor) -&gt; Tensor:\n        return (\n            (X[..., 0] ** 2 + X[..., 1] - 11.0) ** 2\n            + (X[..., 0] + X[..., 1] ** 2 - 7.0) ** 2\n        ).unsqueeze(-1)\n\n\nsurrogate_data = EmpiricalSurrogate(\n\n    inputs=input_features,\n    outputs=output_features,\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"type\":\"EmpiricalSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"ContinuousInput\",\"key\":\"x_1\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"x_2\",\"unit\":null,\"bounds\":[-6.0,6.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"y\",\"unit\":null,\"objective\":{\"type\":\"MinimizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{},\"dump\":null,\"categorical_encodings\":{}}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# attach the actual model to it\nsurrogate.model = HimmelblauModel()\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue\n\n\n\nMixed GP\nGenerate data for a mixed problem.\n\nbenchmark = CrossCoupling()\nsamples = benchmark.domain.inputs.sample(n=50)\nexperiments = benchmark.f(samples, return_complete=True)\n\nexperiments.head(10)\n\n\n\n\n\n\n\n\nbase_eq\nt_res\ntemperature\nbase\ncatalyst\nyield\ncost\nvalid_cost\nvalid_yield\n\n\n\n\n0\n1.416611\n1175.712002\n30.708945\nDBU\ntBuBrettPhos\n0.515077\n0.279817\n1\n1\n\n\n1\n2.189996\n201.214583\n61.813375\nBTMG\ntBuBrettPhos\n0.986604\n0.373929\n1\n1\n\n\n2\n1.747246\n1697.260591\n56.214862\nTEA\ntBuBrettPhos\n0.037644\n0.278906\n1\n1\n\n\n3\n1.820323\n319.016213\n98.593839\nTMG\ntBuXPhos\n0.684236\n0.248306\n1\n1\n\n\n4\n1.256738\n712.758807\n81.197768\nTEA\nAlPhos\n0.061935\n0.419468\n1\n1\n\n\n5\n2.420830\n327.264280\n93.761409\nBTMG\ntBuBrettPhos\n1.020012\n0.384012\n1\n1\n\n\n6\n1.656334\n1767.917894\n68.096672\nTMG\ntBuBrettPhos\n0.501263\n0.278331\n1\n1\n\n\n7\n1.003067\n1398.147888\n36.187936\nTMG\nAlPhos\n0.158322\n0.419048\n1\n1\n\n\n8\n1.580321\n1685.473707\n50.421270\nTEA\ntBuBrettPhos\n0.036620\n0.278846\n1\n1\n\n\n9\n1.281410\n1760.321832\n73.304972\nBTMG\ntBuXPhos\n0.939331\n0.304211\n1\n1\n\n\n\n\n\n\n\n\n# we setup the data model, here a Single Task GP\nsurrogate_data = MixedSingleTaskGPSurrogate(\n    inputs=benchmark.domain.inputs,\n    outputs=Outputs(features=[benchmark.domain.outputs.features[0]]),\n    categorical_encodings={\"catalyst\": CategoricalEncodingEnum.ORDINAL},\n)\n\n# we generate the json spec\njspec = surrogate_data.model_dump_json()\n\njspec\n\n'{\"hyperconfig\":{\"type\":\"MixedSingleTaskGPHyperconfig\",\"hyperstrategy\":\"FractionalFactorialStrategy\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalInput\",\"key\":\"continuous_kernel\",\"categories\":[\"rbf\",\"matern_1.5\",\"matern_2.5\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"prior\",\"categories\":[\"mbo\",\"threesix\",\"hvarfner\"],\"allowed\":[true,true,true]},{\"type\":\"CategoricalInput\",\"key\":\"ard\",\"categories\":[\"True\",\"False\"],\"allowed\":[true,true]}]},\"n_iterations\":null,\"target_metric\":\"MAE\"},\"engineered_features\":{\"type\":\"EngineeredFeatures\",\"features\":[]},\"type\":\"MixedSingleTaskGPSurrogate\",\"inputs\":{\"type\":\"Inputs\",\"features\":[{\"type\":\"CategoricalDescriptorInput\",\"key\":\"catalyst\",\"categories\":[\"tBuXPhos\",\"tBuBrettPhos\",\"AlPhos\"],\"allowed\":[true,true,true],\"descriptors\":[\"area_cat\",\"M2_cat\"],\"values\":[[460.7543,67.2057],[518.8408,89.8738],[819.933,129.0808]]},{\"type\":\"CategoricalDescriptorInput\",\"key\":\"base\",\"categories\":[\"TEA\",\"TMG\",\"BTMG\",\"DBU\"],\"allowed\":[true,true,true,true],\"descriptors\":[\"area\",\"M2\"],\"values\":[[162.2992,25.8165],[165.5447,81.4847],[227.3523,30.554],[192.4693,59.8367]]},{\"type\":\"ContinuousInput\",\"key\":\"base_eq\",\"unit\":null,\"bounds\":[1.0,2.5],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"temperature\",\"unit\":null,\"bounds\":[30.0,100.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false},{\"type\":\"ContinuousInput\",\"key\":\"t_res\",\"unit\":null,\"bounds\":[60.0,1800.0],\"local_relative_bounds\":null,\"stepsize\":null,\"allow_zero\":false}]},\"outputs\":{\"type\":\"Outputs\",\"features\":[{\"type\":\"ContinuousOutput\",\"key\":\"yield\",\"unit\":null,\"objective\":{\"type\":\"MaximizeObjective\",\"w\":1.0,\"bounds\":[0.0,1.0]}}]},\"input_preprocessing_specs\":{\"base\":\"ORDINAL\",\"catalyst\":\"ORDINAL\"},\"dump\":null,\"categorical_encodings\":{\"catalyst\":\"ORDINAL\",\"base\":\"DESCRIPTOR\"},\"scaler\":\"NORMALIZE\",\"output_scaler\":\"STANDARDIZE\",\"continuous_kernel\":{\"type\":\"RBFKernel\",\"features\":[\"base\",\"base_eq\",\"temperature\",\"t_res\"],\"ard\":true,\"lengthscale_prior\":{\"type\":\"DimensionalityScaledLogNormalPrior\",\"loc\":1.4142135623730951,\"loc_scaling\":0.5,\"scale\":1.7320508075688772,\"scale_scaling\":0.0},\"lengthscale_constraint\":{\"type\":\"GreaterThan\",\"lower_bound\":0.025}},\"categorical_kernel\":{\"type\":\"HammingDistanceKernel\",\"features\":[\"catalyst\"],\"ard\":true,\"lengthscale_prior\":null,\"lengthscale_constraint\":{\"type\":\"GreaterThan\",\"lower_bound\":1e-6}},\"noise_prior\":{\"type\":\"LogNormalPrior\",\"loc\":-4.0,\"scale\":1.0}}'\n\n\n\n# Load it from the spec\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\n# Map it\nsurrogate = surrogates.map(surrogate_data)\n# Fit it\nsurrogate.fit(experiments=experiments)\n# dump it\ndump = surrogate.dumps()\n# predict with it\ndf_predictions = surrogate.predict(experiments)\n# transform to spec\npredictions = surrogate.to_predictions(predictions=df_predictions)\n\n\nsurrogate_data = TypeAdapter(AnySurrogate).validate_json(jspec)\nsurrogate = surrogates.map(surrogate_data)\nsurrogate.loads(dump)\n\n# predict with it\ndf_predictions2 = surrogate.predict(experiments)\n# transform to spec\npredictions2 = surrogate.to_predictions(predictions=df_predictions2)\n\n# check for equality\npredictions == predictions2\n\nTrue",
    "crumbs": [
      "Serialization",
      "Model Building with BoFire"
    ]
  },
  {
    "objectID": "build/lib/docs/userguides/data_models_functionals.html",
    "href": "build/lib/docs/userguides/data_models_functionals.html",
    "title": "Data Models vs. Functional Components",
    "section": "",
    "text": "Data models in BoFire hold static data of an optimization problem. These are input and output features as well as constraints making up the domain. They further include possible optimization objectives, acquisition functions, and kernels.\nAll data models in bofire.data_models, are specified as pydantic models and inherit from bofire.data_models.base.BaseModel. These data models can be (de)serialized via .dict() and .model_dump_json() (provided by pydantic). A json schema of each data model can be obtained using .schema().\nFor surrogates and strategies, all functional parts are located in bofire.surrogates and bofire.strategies. These functionalities include the ask and tell as well as fit and predict methods. All class attributes (used by these method) are also removed from the data models. Each functional entity is initialized using the corresponding data model. As an example, consider the following data model of a RandomStrategy:\n\nimport bofire.data_models.domain.api as dm_domain\nimport bofire.data_models.features.api as dm_features\nimport bofire.data_models.strategies.api as dm_strategies\n\nin1 = dm_features.ContinuousInput(key=\"in1\", bounds=[0.0,1.0])\nin2 = dm_features.ContinuousInput(key=\"in2\", bounds=[0.0,2.0])\nin3 = dm_features.ContinuousInput(key=\"in3\", bounds=[0.0,3.0])\n\nout1 = dm_features.ContinuousOutput(key=\"out1\")\n\ninputs = dm_domain.Inputs(features=[in1, in2, in3])\noutputs = dm_domain.Outputs(features=[out1])\nconstraints = dm_domain.Constraints()\n\ndomain = dm_domain.Domain(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints,\n)\n\ndata_model = dm_strategies.RandomStrategy(domain=domain)\n\nSuch a data model can be (de)serialized as follows:\n\nfrom pydantic import TypeAdapter\nfrom bofire.data_models.strategies.api import AnyStrategy\n\nserialized = data_model.model_dump_json()\n\ndata_model_ = TypeAdapter(AnyStrategy).validate_json(serialized)\n\nassert data_model_ == data_model\n\nThe data model of a strategy contains its hyperparameters. Using this data model of a strategy, we can create an instance of a (functional) strategy:\n\nimport bofire.strategies.api as strategies\nstrategy = strategies.RandomStrategy(data_model=data_model)\n\nAs each strategy data model should be mapped to a specific (functional) strategy, we provide such a mapping:\n\nstrategy = strategies.map(data_model)"
  },
  {
    "objectID": "build/lib/docs/userguides/strategies.html",
    "href": "build/lib/docs/userguides/strategies.html",
    "title": "Strategies",
    "section": "",
    "text": "Strategies are the key ingredient of BoFire that explore the search space defined in the Domain and provide candidates for the next experiment or batch of experiments. Available strategies can be clustered in the following subclasses."
  },
  {
    "objectID": "build/lib/docs/userguides/strategies.html#non-predictive-strategies",
    "href": "build/lib/docs/userguides/strategies.html#non-predictive-strategies",
    "title": "Strategies",
    "section": "Non-predictive Strategies",
    "text": "Non-predictive Strategies\nBoFire offers the following strategies for sampling from the search space (no output features need to be provided in the Domain):\n\nRandomStrategy: This strategy proposes candidates by (quasi-)random sampling from the search space. It is applicable to almost all combinations of input features and constraints.\nDoEStrategy: This strategy offers model-based DoE approaches for proposing candidates.\nFractionalFactorialStrategy: This strategy should be used to generate (fractional-)factorial designs."
  },
  {
    "objectID": "build/lib/docs/userguides/strategies.html#predictive-strategies",
    "href": "build/lib/docs/userguides/strategies.html#predictive-strategies",
    "title": "Strategies",
    "section": "Predictive Strategies",
    "text": "Predictive Strategies\nPredictive strategies are making use of (Bayesian) surrogate models to provide candidates with the intention to achieve certain goals depending on the provided objectives of the output features.\nThe following predictive strategies are available:\n\nSoboStrategy: Bayesian optimization strategy that optimizes a single-objective acquisition function. For multi-objective domains, different scalarizations are possible as implemented in the AdditiveSoboStrategy, MultiplicativeSoboStrategy, AdditiveMultiplicativeSoboStrategy and CustomSoboStrategy.\nMoboStrategy: Bayesian optimization strategy that optimizes a hypervolume based acquisition function for pareto-based multi-objective optimization.\nQparegoStrategy: Parallel ParEGO strategy for multiobjective optimization.\nMultifidelityStrategy: Single objective multi-fidelity BO as described here\nEntingStrategy: Strategy based on the Entmoot package that uses tree-based surrogate models to perform both single-objective and multiobjective optimization."
  },
  {
    "objectID": "build/lib/docs/userguides/strategies.html#combining-strategies",
    "href": "build/lib/docs/userguides/strategies.html#combining-strategies",
    "title": "Strategies",
    "section": "Combining Strategies",
    "text": "Combining Strategies\nIn BoFire, the StepwiseStrategy operates on a sequence of strategies and determines when to switch between them based on customizable logical operators.\nThe StepwiseStrategy is comprised of a sequence of Steps, where each Step consists of the following three attributes:\n\nstrategy_data: data model of the strategy which should be executed in this step.\ncondition: A logical expression that determines when this step’s strategy should be executed. The StepwiseStrategy evaluates each step in order and selects the first strategy whose condition evaluates to True.\ntransform: An object that can be used to transform experiments and/or candidates before they enter/leave the strategy assigned in the step.\n\nThe following example demonstrates how to combine a RandomStrategy with a SoboStrategy using the StepwiseStrategy. In this setup, the RandomStrategy is applied initially to propose candidates until 10 experiments have been completed. Once this threshold is reached, the strategy automatically switches to the SoboStrategy for subsequent candidate generation.\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Himmelblau\nfrom bofire.data_models.strategies.api import (\n    AlwaysTrueCondition,\n    NumberOfExperimentsCondition,\n    RandomStrategy,\n    SoboStrategy,\n    Step,\n    StepwiseStrategy,\n)\n\n\ndomain = Himmelblau().domain\n\nstrategy_data = StepwiseStrategy(\n    domain=domain,\n    steps=[\n        Step(\n            strategy_data=RandomStrategy(domain=domain),\n            condition=NumberOfExperimentsCondition(n_experiments=10),\n        ),\n        Step(\n            strategy_data=SoboStrategy(domain=domain), condition=AlwaysTrueCondition()\n        ),\n    ],\n)\n\nstrategy = strategies.map(strategy_data)\nWhen dealing with output constraints, it is often beneficial to ensure that a certain number of experiments satisfy these constraints before proceeding with the main optimization. If no feasible experiments have been found, the strategy should prioritize generating candidates likely to fulfill the output constraints. This can be accomplished by using the qLogPF (Probability of Feasibility) acquisition function together with the FeasibleExperimentCondition. This approach allows the optimization process to focus first on feasibility, and only switch to the main objective once enough feasible experiments are available.\nIn the following example, a RandomStrategy is applied for the initial 10 experiments to broadly explore the search space. Afterward, the SoboStrategy with the qLogPF acquisition function is used to prioritize finding at least one feasible experiment that satisfies the output constraints. Once this feasibility criterion is met, the strategy transitions to the standard SoboStrategy to focus on optimizing the main objective.\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import DTLZ2\nfrom bofire.data_models.acquisition_functions.api import qLogPF\nfrom bofire.data_models.objectives.api import MaximizeSigmoidObjective\nfrom bofire.data_models.strategies.api import (\n    AlwaysTrueCondition,\n    FeasibleExperimentCondition,\n    NumberOfExperimentsCondition,\n    RandomStrategy,\n    SoboStrategy,\n    Step,\n    StepwiseStrategy,\n)\n\n\n# create a domain with one output constraint by assigning a MaximizeSigmoidObjective\n# to the output with key \"f_1\"\ndomain = DTLZ2(dim=6).domain\ndomain.outputs.get_by_key(\"f_1\").objective = MaximizeSigmoidObjective(\n    tp=0.5, steepness=100\n)\n\n\nstrategy_data = StepwiseStrategy(\n    domain=domain,\n    steps=[\n        Step(\n            strategy_data=RandomStrategy(domain=domain),\n            condition=NumberOfExperimentsCondition(n_experiments=10),\n        ),\n        Step(\n            strategy_data=SoboStrategy(domain=domain, acquisition_function=qLogPF()),\n            condition=FeasibleExperimentCondition(n_required_feasible_experiments=1),\n        ),\n        Step(\n            strategy_data=SoboStrategy(domain=domain), condition=AlwaysTrueCondition()\n        ),\n    ],\n)\n\nstrategy = strategies.map(strategy_data)"
  },
  {
    "objectID": "docs/userguides/data_models_functionals.html",
    "href": "docs/userguides/data_models_functionals.html",
    "title": "Data Models vs. Functional Components",
    "section": "",
    "text": "Data models in BoFire hold static data of an optimization problem. These are input and output features as well as constraints making up the domain. They further include possible optimization objectives, acquisition functions, and kernels.\nAll data models in bofire.data_models, are specified as pydantic models and inherit from bofire.data_models.base.BaseModel. These data models can be (de)serialized via .dict() and .model_dump_json() (provided by pydantic). A json schema of each data model can be obtained using .schema().\nFor surrogates and strategies, all functional parts are located in bofire.surrogates and bofire.strategies. These functionalities include the ask and tell as well as fit and predict methods. All class attributes (used by these method) are also removed from the data models. Each functional entity is initialized using the corresponding data model. As an example, consider the following data model of a RandomStrategy:\n\nimport bofire.data_models.domain.api as dm_domain\nimport bofire.data_models.features.api as dm_features\nimport bofire.data_models.strategies.api as dm_strategies\n\nin1 = dm_features.ContinuousInput(key=\"in1\", bounds=[0.0,1.0])\nin2 = dm_features.ContinuousInput(key=\"in2\", bounds=[0.0,2.0])\nin3 = dm_features.ContinuousInput(key=\"in3\", bounds=[0.0,3.0])\n\nout1 = dm_features.ContinuousOutput(key=\"out1\")\n\ninputs = dm_domain.Inputs(features=[in1, in2, in3])\noutputs = dm_domain.Outputs(features=[out1])\nconstraints = dm_domain.Constraints()\n\ndomain = dm_domain.Domain(\n    inputs=inputs,\n    outputs=outputs,\n    constraints=constraints,\n)\n\ndata_model = dm_strategies.RandomStrategy(domain=domain)\n\nSuch a data model can be (de)serialized as follows:\n\nfrom pydantic import TypeAdapter\nfrom bofire.data_models.strategies.api import AnyStrategy\n\nserialized = data_model.model_dump_json()\n\ndata_model_ = TypeAdapter(AnyStrategy).validate_json(serialized)\n\nassert data_model_ == data_model\n\nThe data model of a strategy contains its hyperparameters. Using this data model of a strategy, we can create an instance of a (functional) strategy:\n\nimport bofire.strategies.api as strategies\nstrategy = strategies.RandomStrategy(data_model=data_model)\n\nAs each strategy data model should be mapped to a specific (functional) strategy, we provide such a mapping:\n\nstrategy = strategies.map(data_model)",
    "crumbs": [
      "Data Models vs. Functional Components"
    ]
  },
  {
    "objectID": "docs/userguides/strategies.html",
    "href": "docs/userguides/strategies.html",
    "title": "Strategies",
    "section": "",
    "text": "Strategies are the key ingredient of BoFire that explore the search space defined in the Domain and provide candidates for the next experiment or batch of experiments. Available strategies can be clustered in the following subclasses.",
    "crumbs": [
      "Strategies"
    ]
  },
  {
    "objectID": "docs/userguides/strategies.html#non-predictive-strategies",
    "href": "docs/userguides/strategies.html#non-predictive-strategies",
    "title": "Strategies",
    "section": "Non-predictive Strategies",
    "text": "Non-predictive Strategies\nBoFire offers the following strategies for sampling from the search space (no output features need to be provided in the Domain):\n\nRandomStrategy: This strategy proposes candidates by (quasi-)random sampling from the search space. It is applicable to almost all combinations of input features and constraints.\nDoEStrategy: This strategy offers model-based DoE approaches for proposing candidates.\nFractionalFactorialStrategy: This strategy should be used to generate (fractional-)factorial designs.",
    "crumbs": [
      "Strategies"
    ]
  },
  {
    "objectID": "docs/userguides/strategies.html#predictive-strategies",
    "href": "docs/userguides/strategies.html#predictive-strategies",
    "title": "Strategies",
    "section": "Predictive Strategies",
    "text": "Predictive Strategies\nPredictive strategies are making use of (Bayesian) surrogate models to provide candidates with the intention to achieve certain goals depending on the provided objectives of the output features.\nThe following predictive strategies are available:\n\nSoboStrategy: Bayesian optimization strategy that optimizes a single-objective acquisition function. For multi-objective domains, different scalarizations are possible as implemented in the AdditiveSoboStrategy, MultiplicativeSoboStrategy, AdditiveMultiplicativeSoboStrategy and CustomSoboStrategy.\nMoboStrategy: Bayesian optimization strategy that optimizes a hypervolume based acquisition function for pareto-based multi-objective optimization.\nQparegoStrategy: Parallel ParEGO strategy for multiobjective optimization.\nMultifidelityStrategy: Single objective multi-fidelity BO as described here\nEntingStrategy: Strategy based on the Entmoot package that uses tree-based surrogate models to perform both single-objective and multiobjective optimization.",
    "crumbs": [
      "Strategies"
    ]
  },
  {
    "objectID": "docs/userguides/strategies.html#combining-strategies",
    "href": "docs/userguides/strategies.html#combining-strategies",
    "title": "Strategies",
    "section": "Combining Strategies",
    "text": "Combining Strategies\nIn BoFire, the StepwiseStrategy operates on a sequence of strategies and determines when to switch between them based on customizable logical operators.\nThe StepwiseStrategy is comprised of a sequence of Steps, where each Step consists of the following three attributes:\n\nstrategy_data: data model of the strategy which should be executed in this step.\ncondition: A logical expression that determines when this step’s strategy should be executed. The StepwiseStrategy evaluates each step in order and selects the first strategy whose condition evaluates to True.\ntransform: An object that can be used to transform experiments and/or candidates before they enter/leave the strategy assigned in the step.\n\nThe following example demonstrates how to combine a RandomStrategy with a SoboStrategy using the StepwiseStrategy. In this setup, the RandomStrategy is applied initially to propose candidates until 10 experiments have been completed. Once this threshold is reached, the strategy automatically switches to the SoboStrategy for subsequent candidate generation.\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import Himmelblau\nfrom bofire.data_models.strategies.api import (\n    AlwaysTrueCondition,\n    NumberOfExperimentsCondition,\n    RandomStrategy,\n    SoboStrategy,\n    Step,\n    StepwiseStrategy,\n)\n\n\ndomain = Himmelblau().domain\n\nstrategy_data = StepwiseStrategy(\n    domain=domain,\n    steps=[\n        Step(\n            strategy_data=RandomStrategy(domain=domain),\n            condition=NumberOfExperimentsCondition(n_experiments=10),\n        ),\n        Step(\n            strategy_data=SoboStrategy(domain=domain), condition=AlwaysTrueCondition()\n        ),\n    ],\n)\n\nstrategy = strategies.map(strategy_data)\nWhen dealing with output constraints, it is often beneficial to ensure that a certain number of experiments satisfy these constraints before proceeding with the main optimization. If no feasible experiments have been found, the strategy should prioritize generating candidates likely to fulfill the output constraints. This can be accomplished by using the qLogPF (Probability of Feasibility) acquisition function together with the FeasibleExperimentCondition. This approach allows the optimization process to focus first on feasibility, and only switch to the main objective once enough feasible experiments are available.\nIn the following example, a RandomStrategy is applied for the initial 10 experiments to broadly explore the search space. Afterward, the SoboStrategy with the qLogPF acquisition function is used to prioritize finding at least one feasible experiment that satisfies the output constraints. Once this feasibility criterion is met, the strategy transitions to the standard SoboStrategy to focus on optimizing the main objective.\n\nimport bofire.strategies.api as strategies\nfrom bofire.benchmarks.api import DTLZ2\nfrom bofire.data_models.acquisition_functions.api import qLogPF\nfrom bofire.data_models.objectives.api import MaximizeSigmoidObjective\nfrom bofire.data_models.strategies.api import (\n    AlwaysTrueCondition,\n    FeasibleExperimentCondition,\n    NumberOfExperimentsCondition,\n    RandomStrategy,\n    SoboStrategy,\n    Step,\n    StepwiseStrategy,\n)\n\n\n# create a domain with one output constraint by assigning a MaximizeSigmoidObjective\n# to the output with key \"f_1\"\ndomain = DTLZ2(dim=6).domain\ndomain.outputs.get_by_key(\"f_1\").objective = MaximizeSigmoidObjective(\n    tp=0.5, steepness=100\n)\n\n\nstrategy_data = StepwiseStrategy(\n    domain=domain,\n    steps=[\n        Step(\n            strategy_data=RandomStrategy(domain=domain),\n            condition=NumberOfExperimentsCondition(n_experiments=10),\n        ),\n        Step(\n            strategy_data=SoboStrategy(domain=domain, acquisition_function=qLogPF()),\n            condition=FeasibleExperimentCondition(n_required_feasible_experiments=1),\n        ),\n        Step(\n            strategy_data=SoboStrategy(domain=domain), condition=AlwaysTrueCondition()\n        ),\n    ],\n)\n\nstrategy = strategies.map(strategy_data)",
    "crumbs": [
      "Strategies"
    ]
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.constraints.Constraints.html",
    "href": "build/lib/docs/reference/data_models.domain.constraints.Constraints.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.constraints.Constraints.html#methods",
    "href": "build/lib/docs/reference/data_models.domain.constraints.Constraints.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nget\nGet constraints of the domain\n\n\nget_reps_df\nProvides a tabular overwiev of all constraints within the domain\n\n\nis_fulfilled\nCheck if all constraints are fulfilled on all rows of the provided dataframe\n\n\njacobian\nNumerically evaluate the jacobians of all constraints\n\n\n\n\nget\ndata_models.domain.constraints.Constraints.get(\n    includes=Constraint,\n    excludes=None,\n    exact=False,\n)\nGet constraints of the domain\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nincludes\nUnion[Type[CIncludes], Sequence[Type[CIncludes]]]\nConstraint class or list of specific constraint classes to be returned. Defaults to Constraint.\nConstraint\n\n\nexcludes\nOptional[Union[Type[CExcludes], List[Type[CExcludes]]]]\nConstraint class or list of specific constraint classes to be excluded from the return. Defaults to None.\nNone\n\n\nexact\nbool\nBoolean to distinguish if only the exact class listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nConstraints\nConstraints[CIncludes]\nconstraints in the domain fitting to the passed requirements.\n\n\n\n\n\n\nget_reps_df\ndata_models.domain.constraints.Constraints.get_reps_df()\nProvides a tabular overwiev of all constraints within the domain\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\npd.DataFrame: DataFrame listing all constraints of the domain with a description\n\n\n\n\n\n\nis_fulfilled\ndata_models.domain.constraints.Constraints.is_fulfilled(experiments, tol=1e-06)\nCheck if all constraints are fulfilled on all rows of the provided dataframe\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with data, the constraint validity should be tested on\nrequired\n\n\ntol\nfloat\ntolerance parameter. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 0.\n1e-06\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nBoolean\npd.Series\nTrue if all constraints are fulfilled for all rows, false if not\n\n\n\n\n\n\njacobian\ndata_models.domain.constraints.Constraints.jacobian(experiments)\nNumerically evaluate the jacobians of all constraints\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\ndata to evaluate the constraint jacobians on\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nlist\nlist\nA list containing the jacobians as pd.DataFrames"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.features.Inputs.html",
    "href": "build/lib/docs/reference/data_models.domain.features.Inputs.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.features.Inputs.html#attributes",
    "href": "build/lib/docs/reference/data_models.domain.features.Inputs.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nfeatures\nList(Inputs\nlist of the features."
  },
  {
    "objectID": "build/lib/docs/reference/data_models.domain.features.Inputs.html#methods",
    "href": "build/lib/docs/reference/data_models.domain.features.Inputs.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nget_bounds\nReturns the boundaries of the optimization problem based on the transformations\n\n\nget_categorical_combinations\nGet a list of tuples pairing the feature keys with a list of valid categories\n\n\nget_feature_indices\nReturns a list of indices of the given feature key list.\n\n\nget_fixed\nGets all features in self that are fixed and returns them as new\n\n\nget_free\nGets all features in self that are not fixed and returns them as\n\n\nget_number_of_categorical_combinations\nGet the total number of unique categorical combinations.\n\n\ninverse_transform\nTransform a dataframe back to the original representations.\n\n\nis_fulfilled\nCheck if the provided experiments fulfill all constraints defined on the\n\n\nsample\nDraw sobol samples\n\n\ntransform\nTransform a dataframe to the representation specified in specs.\n\n\nvalidate_candidates\nValidate a pandas dataframe with input feature values.\n\n\n\n\nget_bounds\ndata_models.domain.features.Inputs.get_bounds(\n    specs,\n    experiments=None,\n    reference_experiment=None,\n)\nReturns the boundaries of the optimization problem based on the transformations defined in the specs dictionary.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\nexperiments\nOptional[pd.DataFrame]\nDataframe with input features. If provided the real feature bounds are returned based on both the opt. feature bounds and the extreme points in the dataframe. Defaults to None,\nNone\n\n\nreference_experiment\nOptional[pd.Series]\nIf a reference experiment provided,\nNone\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf a feature type is not known.\n\n\n\nValueError\nIf no transformation is provided for a categorical feature.\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[List[float], List[float]]\nTuple[List[float], List[float]]: list with lower bounds, list with upper bounds.\n\n\n\n\n\n\nget_categorical_combinations\ndata_models.domain.features.Inputs.get_categorical_combinations(\n    include=Input,\n    exclude=None,\n)\nGet a list of tuples pairing the feature keys with a list of valid categories\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude\nFeature\nFeatures to be included. Defaults to Input.\nInput\n\n\nexclude\nFeature\nFeatures to be excluded, e.g. subclasses of the included features. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[tuple[tuple[str, float] | tuple[str, str], …]]\nList[(str, List[str])]: Returns a list of tuples pairing the feature keys with a list of valid categories (str)\n\n\n\n\n\n\nget_feature_indices\ndata_models.domain.features.Inputs.get_feature_indices(specs, feature_keys)\nReturns a list of indices of the given feature key list.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\nfeature_keys\nList[str]\nList of feature keys.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[int]\nList[int]: The list of indices.\n\n\n\n\n\n\nget_fixed\ndata_models.domain.features.Inputs.get_fixed()\nGets all features in self that are fixed and returns them as new Inputs object.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nInputs\nInputs\nInput features object containing only fixed features.\n\n\n\n\n\n\nget_free\ndata_models.domain.features.Inputs.get_free()\nGets all features in self that are not fixed and returns them as new Inputs object.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nInputs\nInputs\nInput features object containing only non-fixed features.\n\n\n\n\n\n\nget_number_of_categorical_combinations\ndata_models.domain.features.Inputs.get_number_of_categorical_combinations(\n    include=Input,\n    exclude=None,\n)\nGet the total number of unique categorical combinations.\nThis is used before generating all of the categorical combinations, which may cause memory issues if there are too many.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninclude\nFeature\nFeatures to be included. Defaults to Input.\nInput\n\n\nexclude\nFeature\nFeatures to be excluded, e.g. subclasses of the included features. Defaults to None.\nNone\n\n\n\nReturns: int: Returns the number of unique combinations of discrete and categorical features.\n\n\n\ninverse_transform\ndata_models.domain.features.Inputs.inverse_transform(experiments, specs)\nTransform a dataframe back to the original representations.\nThe original applied transformation has to be provided via the specs dictionary. Currently only input categoricals are supported.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nTransformed data dataframe.\nrequired\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Back transformed dataframe. Only input features are included.\n\n\n\n\n\n\nis_fulfilled\ndata_models.domain.features.Inputs.is_fulfilled(experiments)\nCheck if the provided experiments fulfill all constraints defined on the input features itself like the bounds or the allowed categories.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with input features.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nSeries with boolean values indicating if the experiments fulfill the constraints on the input features.\n\n\n\n\n\n\nsample\ndata_models.domain.features.Inputs.sample(\n    n=1,\n    method=SamplingMethodEnum.UNIFORM,\n    seed=None,\n)\nDraw sobol samples\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of samples, has to be larger than 0. Defaults to 1.\n1\n\n\nmethod\nSamplingMethodEnum\nMethod to use, implemented methods are UNIFORM, SOBOL and LHS. Defaults to UNIFORM.\nSamplingMethodEnum.UNIFORM\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe containing the samples.\n\n\n\n\n\n\ntransform\ndata_models.domain.features.Inputs.transform(experiments, specs)\nTransform a dataframe to the representation specified in specs.\nCurrently only input categoricals are supported.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nData dataframe to be transformed.\nrequired\n\n\nspecs\nInputTransformSpecs\nDictionary specifying which input feature is transformed by which encoder.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Transformed dataframe. Only input features are included.\n\n\n\n\n\n\nvalidate_candidates\ndata_models.domain.features.Inputs.validate_candidates(candidates)\nValidate a pandas dataframe with input feature values.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.Dataframe\nInputs to validate.\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nRaises a Valueerror if a feature based validation raises an exception.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.Dataframe: Validated dataframe"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.features.api.html",
    "href": "build/lib/docs/reference/data_models.features.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.features.api.html#classes",
    "href": "build/lib/docs/reference/data_models.features.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nCategoricalInput\nBase class for all categorical input features.\n\n\nCategoricalOutput\n\n\n\nContinuousInput\nBase class for all continuous input features.\n\n\nContinuousOutput\nThe base class for a continuous output feature\n\n\nDiscreteInput\nFeature with discretized ordinal values allowed in the optimization.\n\n\nContinuousDescriptorInput\nClass for continuous input features with descriptors\n\n\nCategoricalDescriptorInput\nClass for categorical input features with descriptors\n\n\nCategoricalMolecularInput\n\n\n\nTaskInput\n\n\n\n\n\nCategoricalInput\ndata_models.features.api.CategoricalInput()\nBase class for all categorical input features.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncategories\nList[str]\nNames of the categories.\n\n\nallowed\nList[bool]\nList of bools indicating if a category is allowed within the optimization.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfixed_value\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\n\nfrom_dummy_encoding\nConvert points back from dummy encoding.\n\n\nfrom_onehot_encoding\nConverts values back from one-hot encoding.\n\n\nfrom_ordinal_encoding\nConvertes values back from ordinal encoding.\n\n\ngenerate_allowed\nGenerates the list of allowed categories if not provided.\n\n\nget_allowed_categories\nReturns the allowed categories.\n\n\nget_forbidden_categories\nReturns the non-allowed categories\n\n\nget_possible_categories\nReturn the superset of categories that have been used in the experimental dataset and\n\n\nis_fixed\nReturns True if there is only one allowed category.\n\n\nis_fulfilled\nMethod to check if the values are all allowed categories.\n\n\nsample\nDraw random samples from the feature.\n\n\nto_dummy_encoding\nConverts values to a dummy-hot encoding, dropping the first categorical level.\n\n\nto_onehot_encoding\nConverts values to a one-hot encoding.\n\n\nto_ordinal_encoding\nConverts values to an ordinal integer based encoding.\n\n\nvalidate_candidental\nMethod to validate the suggested candidates\n\n\nvalidate_experimental\nMethod to validate the experimental dataFrame\n\n\n\n\nfixed_value\ndata_models.features.api.CategoricalInput.fixed_value(transform_type=None)\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[List[str], List[float], None]\nList[str]: List of categories or None\n\n\n\n\n\n\nfrom_dummy_encoding\ndata_models.features.api.CategoricalInput.from_dummy_encoding(values)\nConvert points back from dummy encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDummy-hot encoded values.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf one-hot columns not present in values.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nfrom_onehot_encoding\ndata_models.features.api.CategoricalInput.from_onehot_encoding(values)\nConverts values back from one-hot encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nOne-hot encoded values.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf one-hot columns not present in values.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nfrom_ordinal_encoding\ndata_models.features.api.CategoricalInput.from_ordinal_encoding(values)\nConvertes values back from ordinal encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nOrdinal encoded series.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\ngenerate_allowed\ndata_models.features.api.CategoricalInput.generate_allowed(allowed, info)\nGenerates the list of allowed categories if not provided.\n\n\nget_allowed_categories\ndata_models.features.api.CategoricalInput.get_allowed_categories()\nReturns the allowed categories.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nlist[str]\nlist of str: The allowed categories\n\n\n\n\n\n\nget_forbidden_categories\ndata_models.features.api.CategoricalInput.get_forbidden_categories()\nReturns the non-allowed categories\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[str]: List of the non-allowed categories\n\n\n\n\n\n\nget_possible_categories\ndata_models.features.api.CategoricalInput.get_possible_categories(values)\nReturn the superset of categories that have been used in the experimental dataset and that can be used in the optimization\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries with the values for this feature\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nlist\nlist\nlist of possible categories\n\n\n\n\n\n\nis_fixed\ndata_models.features.api.CategoricalInput.is_fixed()\nReturns True if there is only one allowed category.\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\n[bool]: True if there is only one allowed category\n\n\n\n\n\n\nis_fulfilled\ndata_models.features.api.CategoricalInput.is_fulfilled(values)\nMethod to check if the values are all allowed categories.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA series with values for the input feature.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA series with boolean values indicating if the input feature is fulfilled.\n\n\n\n\n\n\nsample\ndata_models.features.api.CategoricalInput.sample(n, seed=None)\nDraw random samples from the feature.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nnumber of samples.\nrequired\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: drawn samples.\n\n\n\n\n\n\nto_dummy_encoding\ndata_models.features.api.CategoricalInput.to_dummy_encoding(values)\nConverts values to a dummy-hot encoding, dropping the first categorical level.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries to be transformed.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dummy-hot transformed data frame.\n\n\n\n\n\n\nto_onehot_encoding\ndata_models.features.api.CategoricalInput.to_onehot_encoding(values)\nConverts values to a one-hot encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries to be transformed.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: One-hot transformed data frame.\n\n\n\n\n\n\nto_ordinal_encoding\ndata_models.features.api.CategoricalInput.to_ordinal_encoding(values)\nConverts values to an ordinal integer based encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nSeries to be transformed.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Ordinal encoded values.\n\n\n\n\n\n\nvalidate_candidental\ndata_models.features.api.CategoricalInput.validate_candidental(values)\nMethod to validate the suggested candidates\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with candidates\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen not all values for a feature are one of the allowed categories\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The passed dataFrame with candidates\n\n\n\n\n\n\nvalidate_experimental\ndata_models.features.api.CategoricalInput.validate_experimental(\n    values,\n    strict=False,\n)\nMethod to validate the experimental dataFrame\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with experiments\nrequired\n\n\nstrict\nbool\nBoolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen an entry is not in the list of allowed categories\n\n\n\nValueError\nwhen there is no variation in a feature provided by the experimental data\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: A dataFrame with experiments\n\n\n\n\n\n\n\n\nCategoricalOutput\ndata_models.features.api.CategoricalOutput()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_objective_categories\nValidates that objective categories match the output categories\n\n\n\n\nvalidate_objective_categories\ndata_models.features.api.CategoricalOutput.validate_objective_categories()\nValidates that objective categories match the output categories\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen categories do not match objective categories\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nself\n\n\n\n\n\n\n\n\nContinuousInput\ndata_models.features.api.ContinuousInput()\nBase class for all continuous input features.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbounds\nTuple[float, float]\nA tuple that stores the lower and upper bound of the feature.\n\n\nstepsize\nPositiveFloat\nFloat indicating the allowed stepsize between lower and upper. Defaults to None.\n\n\nlocal_relative_bounds\nTuple[float, float]\nA tuple that stores the lower and upper bounds relative to a reference value. Defaults to None.\n\n\nallow_zero\nbool\nA boolean indicating if the input feature can take inactive values. Useful for features that take values between bounds, but can also take a value of 0. One may choose to use a conditional kernel for this, if taking a value of 0 represents a distinct behaviour from non-zero values.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_fulfilled\nMethod to check if the values are within the bounds of the feature.\n\n\nround\nRound values to the stepsize of the feature. If no stepsize is provided return the\n\n\nsample\nDraw random samples from the feature.\n\n\nvalidate_candidental\nMethod to validate the suggested candidates\n\n\n\n\nis_fulfilled\ndata_models.features.api.ContinuousInput.is_fulfilled(values, noise=1e-05)\nMethod to check if the values are within the bounds of the feature.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA series with values for the input feature.\nrequired\n\n\nnoise\nfloat\nA small value to allow for numerical errors. Defaults to 10e-6.\n1e-05\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA series with boolean values indicating if the input feature is fulfilled.\n\n\n\n\n\n\nround\ndata_models.features.api.ContinuousInput.round(values)\nRound values to the stepsize of the feature. If no stepsize is provided return the provided values.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nThe values that should be rounded.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The rounded values\n\n\n\n\n\n\nsample\ndata_models.features.api.ContinuousInput.sample(n, seed=None)\nDraw random samples from the feature.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nnumber of samples.\nrequired\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: drawn samples.\n\n\n\n\n\n\nvalidate_candidental\ndata_models.features.api.ContinuousInput.validate_candidental(values)\nMethod to validate the suggested candidates\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with candidates\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen non numerical values are passed\n\n\n\nValueError\nwhen values are larger than the upper bound of the feature\n\n\n\nValueError\nwhen values are lower than the lower bound of the feature\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The passed dataFrame with candidates\n\n\n\n\n\n\n\n\nContinuousOutput\ndata_models.features.api.ContinuousOutput()\nThe base class for a continuous output feature\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nobjective\nobjective\nobjective of the feature indicating in which direction it should be optimized. Defaults to MaximizeObjective.\n\n\n\n\n\n\nDiscreteInput\ndata_models.features.api.DiscreteInput()\nFeature with discretized ordinal values allowed in the optimization.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nkey(str)\n\nkey of the feature.\n\n\nvalues(List[float])\n\nthe discretized allowed values during the optimization.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfrom_continuous\nRounds continuous values to the closest discrete ones.\n\n\nis_fulfilled\nMethod to check if the values are close to the discrete values.\n\n\nsample\nDraw random samples from the feature.\n\n\nvalidate_candidental\nMethod to validate the provided candidates.\n\n\nvalidate_values_unique\nValidates that provided values are unique.\n\n\n\n\nfrom_continuous\ndata_models.features.api.DiscreteInput.from_continuous(values)\nRounds continuous values to the closest discrete ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDataframe with continuous entries.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with discrete values.\n\n\n\n\n\n\nis_fulfilled\ndata_models.features.api.DiscreteInput.is_fulfilled(values)\nMethod to check if the values are close to the discrete values.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA series with values for the input feature.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nA series with boolean values indicating if the input feature is fulfilled.\n\n\n\n\n\n\nsample\ndata_models.features.api.DiscreteInput.sample(n, seed=None)\nDraw random samples from the feature.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nnumber of samples.\nrequired\n\n\nseed\nint\nrandom seed. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: drawn samples.\n\n\n\n\n\n\nvalidate_candidental\ndata_models.features.api.DiscreteInput.validate_candidental(values)\nMethod to validate the provided candidates.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nsuggested candidates for the feature\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nRaises error when one of the provided values is not contained in the list of allowed values.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: suggested candidates for the feature\n\n\n\n\n\n\nvalidate_values_unique\ndata_models.features.api.DiscreteInput.validate_values_unique(values)\nValidates that provided values are unique.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nList[float]\nList of values\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen values are non-unique.\n\n\n\nValueError\nwhen values contains only one entry.\n\n\n\nValueError\nwhen values is empty.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[values]: Sorted list of values\n\n\n\n\n\n\n\n\nContinuousDescriptorInput\ndata_models.features.api.ContinuousDescriptorInput()\nClass for continuous input features with descriptors\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nlower_bound\nfloat\nLower bound of the feature in the optimization.\n\n\nupper_bound\nfloat\nUpper bound of the feature in the optimization.\n\n\ndescriptors\nList[str]\nNames of the descriptors.\n\n\nvalues\nList[float]\nValues of the descriptors.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nto_df\nTabular overview of the feature as DataFrame\n\n\nvalidate_list_lengths\nCompares the length of the defined descriptors list with the provided values\n\n\n\n\nto_df\ndata_models.features.api.ContinuousDescriptorInput.to_df()\nTabular overview of the feature as DataFrame\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: tabular overview of the feature as DataFrame\n\n\n\n\n\n\nvalidate_list_lengths\ndata_models.features.api.ContinuousDescriptorInput.validate_list_lengths()\nCompares the length of the defined descriptors list with the provided values\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nDict\nDictionary with all attributes\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen the number of descriptors does not math the number of provided values\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nDict\n\nDict with the attributes\n\n\n\n\n\n\n\n\nCategoricalDescriptorInput\ndata_models.features.api.CategoricalDescriptorInput()\nClass for categorical input features with descriptors\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ncategories\nList[str]\nNames of the categories.\n\n\nallowed\nList[bool]\nList of bools indicating if a category is allowed within the optimization.\n\n\ndescriptors\nList[str]\nList of strings representing the names of the descriptors.\n\n\nvalues\nList[List[float]]\nList of lists representing the descriptor values.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfixed_value\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\n\nfrom_descriptor_encoding\nConverts values back from descriptor encoding.\n\n\nfrom_df\nCreates a feature from a dataframe\n\n\nto_descriptor_encoding\nConverts values to descriptor encoding.\n\n\nto_df\nTabular overview of the feature as DataFrame\n\n\nvalidate_experimental\nMethod to validate the experimental dataFrame\n\n\nvalidate_values\nValidates the compatibility of passed values for the descriptors and the defined categories\n\n\n\n\nfixed_value\ndata_models.features.api.CategoricalDescriptorInput.fixed_value(\n    transform_type=None,\n)\nReturns the categories to which the feature is fixed, None if the feature is not fixed\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUnion[List[str], List[float], None]\nList[str]: List of categories or None\n\n\n\n\n\n\nfrom_descriptor_encoding\ndata_models.features.api.CategoricalDescriptorInput.from_descriptor_encoding(\n    values,\n)\nConverts values back from descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDescriptor encoded dataframe.\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf descriptor columns not found in the dataframe.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nfrom_df\ndata_models.features.api.CategoricalDescriptorInput.from_df(key, df)\nCreates a feature from a dataframe\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nstr\nThe name of the feature\nrequired\n\n\ndf\npd.DataFrame\nCategories as rows and descriptors as columns\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\ntype\n\ndescription\n\n\n\n\n\n\nto_descriptor_encoding\ndata_models.features.api.CategoricalDescriptorInput.to_descriptor_encoding(\n    values,\n)\nConverts values to descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nValues to transform.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Descriptor encoded dataframe.\n\n\n\n\n\n\nto_df\ndata_models.features.api.CategoricalDescriptorInput.to_df()\nTabular overview of the feature as DataFrame\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\npd.DataFrame: tabular overview of the feature as DataFrame\n\n\n\n\n\n\nvalidate_experimental\ndata_models.features.api.CategoricalDescriptorInput.validate_experimental(\n    values,\n    strict=False,\n)\nMethod to validate the experimental dataFrame\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.Series\nA dataFrame with experiments\nrequired\n\n\nstrict\nbool\nBoolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen an entry is not in the list of allowed categories\n\n\n\nValueError\nwhen there is no variation in a feature provided by the experimental data\n\n\n\nValueError\nwhen no variation is present or planned for a given descriptor\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: A dataFrame with experiments\n\n\n\n\n\n\nvalidate_values\ndata_models.features.api.CategoricalDescriptorInput.validate_values(v, info)\nValidates the compatibility of passed values for the descriptors and the defined categories\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nList[List[float]]\nNested list with descriptor values\nrequired\n\n\nvalues\nDict\nDictionary with attributes\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen values have different length than categories\n\n\n\nValueError\nwhen rows in values have different length than descriptors\n\n\n\nValueError\nwhen a descriptor shows no variance in the data\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[List[float]]: Nested list with descriptor values\n\n\n\n\n\n\n\n\nCategoricalMolecularInput\ndata_models.features.api.CategoricalMolecularInput()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nfrom_descriptor_encoding\nConverts values back from descriptor encoding.\n\n\nvalidate_smiles\nValidates that categories are valid smiles. Note that this check can only\n\n\n\n\nfrom_descriptor_encoding\ndata_models.features.api.CategoricalMolecularInput.from_descriptor_encoding(\n    transform_type,\n    values,\n)\nConverts values back from descriptor encoding.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\npd.DataFrame\nDescriptor encoded dataframe.\nrequired\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf descriptor columns not found in the dataframe.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: Series with categorical values.\n\n\n\n\n\n\nvalidate_smiles\ndata_models.features.api.CategoricalMolecularInput.validate_smiles(categories)\nValidates that categories are valid smiles. Note that this check can only be executed when rdkit is available.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncategories\nList[str]\nList of smiles\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen string is not a smiles\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[str]: List of the smiles\n\n\n\n\n\n\n\n\nTaskInput\ndata_models.features.api.TaskInput()"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.strategies.api.html",
    "href": "build/lib/docs/reference/data_models.strategies.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/data_models.strategies.api.html#classes",
    "href": "build/lib/docs/reference/data_models.strategies.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nDoEStrategy\n\n\n\nFactorialStrategy\nFactorial design strategy.\n\n\nFractionalFactorialStrategy\nFractional factorial design strategy.\n\n\nActiveLearningStrategy\nDatamodel for an ActiveLearningStrategy that focusses on pure exploration of the input space.\n\n\nBotorchStrategy\n\n\n\nEntingStrategy\n\n\n\nMoboStrategy\n\n\n\nMultiFidelityStrategy\n\n\n\nMultiobjectiveStrategy\n\n\n\nAdditiveSoboStrategy\n\n\n\nCustomSoboStrategy\n\n\n\n\n\nDoEStrategy\ndata_models.strategies.api.DoEStrategy()\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\nreturn_fixed_candidates\nDatamodel for strategy for design of experiments. This strategy is used to generate a set of\n\n\n\n\n\n\nFactorialStrategy\ndata_models.strategies.api.FactorialStrategy()\nFactorial design strategy.\nThis strategy is deprecated, please use FractionalFactorialStrategy instead.\n\n\nFractionalFactorialStrategy\ndata_models.strategies.api.FractionalFactorialStrategy()\nFractional factorial design strategy.\nThis strategy generates a fractional factorial two level design for the continuous part of the domain, which is then combined with the categorical part of the domain. For every categorical combination, the continuous part of the design is repeated.\n\n\nActiveLearningStrategy\ndata_models.strategies.api.ActiveLearningStrategy()\nDatamodel for an ActiveLearningStrategy that focusses on pure exploration of the input space. This type of strategy chooses new candidate points in order to minimize the uncertainty.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_feature_implemented\nMethod to check if a specific feature type is implemented for the strategy\n\n\nis_objective_implemented\nMethod to check if a objective type is implemented for the strategy\n\n\n\n\nis_feature_implemented\ndata_models.strategies.api.ActiveLearningStrategy.is_feature_implemented(\n    my_type,\n)\nMethod to check if a specific feature type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Feature]\nFeature class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the feature type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nis_objective_implemented\ndata_models.strategies.api.ActiveLearningStrategy.is_objective_implemented(\n    my_type,\n)\nMethod to check if a objective type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Objective]\nObjective class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the objective type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\n\n\nBotorchStrategy\ndata_models.strategies.api.BotorchStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_constraint_implemented\nMethod to check if a specific constraint type is implemented for the strategy. For optimizer-specific\n\n\nvalidate_multitask_allowed\nEnsures that if a multitask model is used there is only a single allowed task category\n\n\nvalidate_outlier_detection_specs_for_domain\nEnsures that a outlier_detection model is specified for each output feature\n\n\nvalidate_surrogate_specs\nEnsures that a prediction model is specified for each output feature\n\n\n\n\nis_constraint_implemented\ndata_models.strategies.api.BotorchStrategy.is_constraint_implemented(my_type)\nMethod to check if a specific constraint type is implemented for the strategy. For optimizer-specific strategies, this is passed to the optimizer check.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Constraint]\nConstraint class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the constraint type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nvalidate_multitask_allowed\ndata_models.strategies.api.BotorchStrategy.validate_multitask_allowed()\nEnsures that if a multitask model is used there is only a single allowed task category\n\n\nvalidate_outlier_detection_specs_for_domain\ndata_models.strategies.api.BotorchStrategy.validate_outlier_detection_specs_for_domain(\n)\nEnsures that a outlier_detection model is specified for each output feature\n\n\nvalidate_surrogate_specs\ndata_models.strategies.api.BotorchStrategy.validate_surrogate_specs()\nEnsures that a prediction model is specified for each output feature\n\n\n\n\nEntingStrategy\ndata_models.strategies.api.EntingStrategy()\n\n\nMoboStrategy\ndata_models.strategies.api.MoboStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_feature_implemented\nMethod to check if a specific feature type is implemented for the strategy\n\n\nis_objective_implemented\nMethod to check if a objective type is implemented for the strategy\n\n\nvalidate_ref_point\nValidate that the provided refpoint matches the provided domain.\n\n\n\n\nis_feature_implemented\ndata_models.strategies.api.MoboStrategy.is_feature_implemented(my_type)\nMethod to check if a specific feature type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Feature]\nFeature class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the feature type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nis_objective_implemented\ndata_models.strategies.api.MoboStrategy.is_objective_implemented(my_type)\nMethod to check if a objective type is implemented for the strategy\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmy_type\nType[Objective]\nObjective class\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if the objective type is valid for the strategy chosen, False otherwise\n\n\n\n\n\n\nvalidate_ref_point\ndata_models.strategies.api.MoboStrategy.validate_ref_point()\nValidate that the provided refpoint matches the provided domain.\n\n\n\n\nMultiFidelityStrategy\ndata_models.strategies.api.MultiFidelityStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_multitask_allowed\nOverwrites BotorchSurrogate.validate_multitask_allowed, as multiple tasks are allowed.\n\n\nvalidate_only_one_target_fidelity\nEnsures that there is only one target fidelity (task where fidelity==0).\n\n\nvalidate_surrogate_specs\nEnsures that a multi-task model is specified for each output feature\n\n\nvalidate_tasks_and_fidelity_thresholds\nEnsures that there is one threshold per fidelity\n\n\n\n\nvalidate_multitask_allowed\ndata_models.strategies.api.MultiFidelityStrategy.validate_multitask_allowed()\nOverwrites BotorchSurrogate.validate_multitask_allowed, as multiple tasks are allowed.\n\n\nvalidate_only_one_target_fidelity\ndata_models.strategies.api.MultiFidelityStrategy.validate_only_one_target_fidelity(\n)\nEnsures that there is only one target fidelity (task where fidelity==0).\n\n\nvalidate_surrogate_specs\ndata_models.strategies.api.MultiFidelityStrategy.validate_surrogate_specs()\nEnsures that a multi-task model is specified for each output feature\n\n\nvalidate_tasks_and_fidelity_thresholds\ndata_models.strategies.api.MultiFidelityStrategy.validate_tasks_and_fidelity_thresholds(\n)\nEnsures that there is one threshold per fidelity\n\n\n\n\nMultiobjectiveStrategy\ndata_models.strategies.api.MultiobjectiveStrategy()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nvalidate_domain_is_multiobjective\nValidate that the domain is multiobjective.\n\n\n\n\nvalidate_domain_is_multiobjective\ndata_models.strategies.api.MultiobjectiveStrategy.validate_domain_is_multiobjective(\n    v,\n)\nValidate that the domain is multiobjective.\n\n\n\n\nAdditiveSoboStrategy\ndata_models.strategies.api.AdditiveSoboStrategy()\n\n\nCustomSoboStrategy\ndata_models.strategies.api.CustomSoboStrategy()"
  },
  {
    "objectID": "build/lib/docs/reference/index.html",
    "href": "build/lib/docs/reference/index.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#data-models---domain",
    "href": "build/lib/docs/reference/index.html#data-models---domain",
    "title": "",
    "section": "Data Models - Domain",
    "text": "Data Models - Domain\nDomain configuration data models\n\n\n\ndata_models.domain.domain.Domain\n\n\n\ndata_models.domain.features.Inputs\nContainer of input features, only input features are allowed.\n\n\ndata_models.domain.features.Outputs\nContainer of output features, only output features are allowed.\n\n\ndata_models.domain.constraints.Constraints"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#data-models---features",
    "href": "build/lib/docs/reference/index.html#data-models---features",
    "title": "",
    "section": "Data Models - Features",
    "text": "Data Models - Features\nFeature data models\n\n\n\ndata_models.features.api"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#data-models---strategies",
    "href": "build/lib/docs/reference/index.html#data-models---strategies",
    "title": "",
    "section": "Data Models - Strategies",
    "text": "Data Models - Strategies\nStrategy configuration data models\n\n\n\ndata_models.strategies.api"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#data-models---kernels",
    "href": "build/lib/docs/reference/index.html#data-models---kernels",
    "title": "",
    "section": "Data Models - Kernels",
    "text": "Data Models - Kernels\nKernel configuration data models\n\n\n\ndata_models.kernels.api"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#data-models---surrogates",
    "href": "build/lib/docs/reference/index.html#data-models---surrogates",
    "title": "",
    "section": "Data Models - Surrogates",
    "text": "Data Models - Surrogates\nSurrogate model configuration data models\n\n\n\ndata_models.surrogates.api"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#strategies",
    "href": "build/lib/docs/reference/index.html#strategies",
    "title": "",
    "section": "Strategies",
    "text": "Strategies\nStrategy implementations\n\n\n\nstrategies.api"
  },
  {
    "objectID": "build/lib/docs/reference/index.html#surrogates",
    "href": "build/lib/docs/reference/index.html#surrogates",
    "title": "",
    "section": "Surrogates",
    "text": "Surrogates\nSurrogate model implementations\n\n\n\nsurrogates.api"
  },
  {
    "objectID": "build/lib/docs/reference/surrogates.api.html",
    "href": "build/lib/docs/reference/surrogates.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "build/lib/docs/reference/surrogates.api.html#classes",
    "href": "build/lib/docs/reference/surrogates.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nSurrogate\n\n\n\nTrainableSurrogate\n\n\n\nSingleTaskGPSurrogate\n\n\n\nMultiTaskGPSurrogate\n\n\n\nBotorchSurrogates\n\n\n\nLinearDeterministicSurrogate\n\n\n\nEmpiricalSurrogate\nAll necessary functions has to be implemented in the model which can then be loaded\n\n\nRandomForestSurrogate\nBoFire Random Forest model.\n\n\nMLPEnsemble\n\n\n\nPiecewiseLinearGPSurrogate\n\n\n\n\n\nSurrogate\nsurrogates.api.Surrogate(data_model)\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\nis_fitted\nReturn True if model is fitted, else False.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ndumps\nDumps the actual model to a string as this is not directly json serializable.\n\n\nloads\nLoads the actual model from a string and writes it to the model attribute.\n\n\n\n\ndumps\nsurrogates.api.Surrogate.dumps()\nDumps the actual model to a string as this is not directly json serializable.\n\n\nloads\nsurrogates.api.Surrogate.loads(data)\nLoads the actual model from a string and writes it to the model attribute.\n\n\n\n\nTrainableSurrogate\nsurrogates.api.TrainableSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ncross_validate\nPerform a cross validation for the provided training data.\n\n\nfit\nFit the surrogate model to the provided experiments.\n\n\n\n\ncross_validate\nsurrogates.api.TrainableSurrogate.cross_validate(\n    experiments,\n    folds=-1,\n    include_X=False,\n    include_labcodes=False,\n    random_state=None,\n    stratified_feature=None,\n    group_split_column=None,\n    hooks=None,\n    hook_kwargs=None,\n)\nPerform a cross validation for the provided training data.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nData on which the cross validation should be performed.\nrequired\n\n\nfolds\nint\nNumber of folds. -1 is equal to LOO CV. Defaults to -1.\n-1\n\n\ninclude_X\nbool\nIf true the X values of the fold are written to respective CvResult objects for later analysis. Defaults to False.\nFalse\n\n\nrandom_state\nint\nControls the randomness of the indices in the train and test sets of each fold. Defaults to None.\nNone\n\n\nstratified_feature\nstr\nThe feature name to preserve the percentage of samples for each class in the stratified folds. Defaults to None.\nNone\n\n\ngroup_split_column\nstr\nThe column name of the group id. This parameter is used to ensure that the splits are made such that the same group is not present in both training and testing sets. This is useful in scenarios where data points are related or dependent on each other, and splitting them into different sets would violate the assumption of independence. The number of unique groups must be greater than or equal to the number of folds. Defaults to None.\nNone\n\n\nhooks\nDict[str, Callable[[Model, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame], Any]]\nDictionary of callable hooks that are called within the CV loop. The callable retrieves the current trained modeld and the current CV folds in the following order: X_train, y_train, X_test, y_test. Defaults to {}.\nNone\n\n\nhook_kwargs\nDict[str, Dict[str, Any]]\nDictionary holding hook specific keyword arguments. Defaults to {}.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[CvResults, CvResults, Dict[str, List[Any]]]\nTuple[CvResults, CvResults, Dict[str, List[Any]]]: First CvResults object reflects the training data, second CvResults object the test data, dictionary object holds the return values of the applied hooks.\n\n\n\n\n\n\nfit\nsurrogates.api.TrainableSurrogate.fit(experiments, options=None)\nFit the surrogate model to the provided experiments.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nThe experimental data to fit the model.\nrequired\n\n\noptions\nOptional[Dict]\nAdditional options for fitting the model. Defaults to None.\nNone\n\n\n\n\n\n\n\n\nSingleTaskGPSurrogate\nsurrogates.api.SingleTaskGPSurrogate(data_model, **kwargs)\n\n\nMultiTaskGPSurrogate\nsurrogates.api.MultiTaskGPSurrogate(data_model, **kwargs)\n\n\nBotorchSurrogates\nsurrogates.api.BotorchSurrogates(data_model, **kwargs)\n\n\nLinearDeterministicSurrogate\nsurrogates.api.LinearDeterministicSurrogate(data_model, **kwargs)\n\n\nEmpiricalSurrogate\nsurrogates.api.EmpiricalSurrogate(data_model, **kwargs)\nAll necessary functions has to be implemented in the model which can then be loaded from cloud pickle.\n\nAttributes\n\n\n\nName\nType\nDescription\n\n\n\n\nmodel\nDeterministicModel\nBotorch model instance.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nloads\nLoads the actual model from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nloads\nsurrogates.api.EmpiricalSurrogate.loads(data)\nLoads the actual model from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nRandomForestSurrogate\nsurrogates.api.RandomForestSurrogate(data_model, **kwargs)\nBoFire Random Forest model.\nThe same hyperparameters are available as for the wrapped sklearn RandomForestRegreesor.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nloads\nLoads the actual random forest from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nloads\nsurrogates.api.RandomForestSurrogate.loads(data)\nLoads the actual random forest from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\n\n\nMLPEnsemble\nsurrogates.api.MLPEnsemble(data_model, **kwargs)\n\n\nPiecewiseLinearGPSurrogate\nsurrogates.api.PiecewiseLinearGPSurrogate(data_model, **kwargs)"
  },
  {
    "objectID": "docs/reference/data_models.domain.domain.Domain.html",
    "href": "docs/reference/data_models.domain.domain.Domain.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.domain.domain.Domain.html#attributes",
    "href": "docs/reference/data_models.domain.domain.Domain.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ncandidate_column_names\nThe columns in the candidate dataframe\n\n\nconstraints\nRepresentation of the optimization problem/domain\n\n\nexperiment_column_names\nThe columns in the experimental dataframe"
  },
  {
    "objectID": "docs/reference/data_models.domain.domain.Domain.html#methods",
    "href": "docs/reference/data_models.domain.domain.Domain.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate_by_duplicates\nAggregate the dataframe by duplicate experiments\n\n\ncoerce_invalids\nCoerces all invalid output measurements to np.nan\n\n\ndescribe_experiments\nMethod to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\n\nget_nchoosek_combinations\nGet all possible NChooseK combinations\n\n\nis_fulfilled\nCheck if all constraints are fulfilled on all rows of the provided dataframe\n\n\nvalidate_candidates\nMethod to check the validty of proposed candidates\n\n\nvalidate_constraints\nValidate that the constraints defined in the domain fit to the input features.\n\n\nvalidate_experiments\nChecks the experimental data on validity\n\n\nvalidate_unique_feature_keys\nValidates if provided input and output feature keys are unique\n\n\n\n\naggregate_by_duplicates\ndata_models.domain.domain.Domain.aggregate_by_duplicates(\n    experiments,\n    prec,\n    delimiter='-',\n    method='mean',\n)\nAggregate the dataframe by duplicate experiments\nDuplicates are identified based on the experiments with the same input features. Continuous input features are rounded before identifying the duplicates. Aggregation is performed by taking the average of the involved output features.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe containing experimental data\nrequired\n\n\nprec\nint\nPrecision of the rounding of the continuous input features\nrequired\n\n\ndelimiter\nstr\nDelimiter used when combining the orig. labcodes to a new one. Defaults to “-”.\n'-'\n\n\nmethod\nLiteral['mean', 'median']\nWhich aggregation method to use. Defaults to “mean”.\n'mean'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[pd.DataFrame, list]\nTuple[pd.DataFrame, list]: Dataframe holding the aggregated experiments, list of lists holding the labcodes of the duplicates\n\n\n\n\n\n\ncoerce_invalids\ndata_models.domain.domain.Domain.coerce_invalids(experiments)\nCoerces all invalid output measurements to np.nan\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe containing experimental data\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: coerced dataframe\n\n\n\n\n\n\ndescribe_experiments\ndata_models.domain.domain.Domain.describe_experiments(experiments)\nMethod to get a tabular overview of how many measurements and how many valid entries are included in the input data for each output feature\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with counts how many measurements and how many valid entries are included in the input data for each output feature\n\n\n\n\n\n\nget_nchoosek_combinations\ndata_models.domain.domain.Domain.get_nchoosek_combinations(exhaustive=False)\nGet all possible NChooseK combinations\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexhaustive\nbool\nif True all combinations are returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nTuple\n(used_features_list, unused_features_list)\nused_features_list is a list of lists containing features used in each NChooseK combination. unused_features_list is a list of lists containing features unused in each NChooseK combination.\n\n\n\n\n\n\nis_fulfilled\ndata_models.domain.domain.Domain.is_fulfilled(\n    experiments,\n    tol=1e-06,\n    exlude_interpoint=True,\n)\nCheck if all constraints are fulfilled on all rows of the provided dataframe both constraints and inputs are checked.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with data, the constraint validity should be tested on\nrequired\n\n\ntol\nfloat\nTolerance for checking the constraints. Defaults to 1e-6.\n1e-06\n\n\nexlude_interpoint\nbool\nIf True, InterpointConstraints are excluded from the check. Defaults to True.\nTrue\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\nBoolean series indicating if all constraints are fulfilled for all rows.\n\n\n\n\n\n\nvalidate_candidates\ndata_models.domain.domain.Domain.validate_candidates(\n    candidates,\n    only_inputs=False,\n    tol=1e-05,\n    raise_validation_error=True,\n)\nMethod to check the validty of proposed candidates\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\nDataframe with suggested new experiments (candidates)\nrequired\n\n\nonly_inputs\n(bool, optional)\nIf True, only the input columns are validated. Defaults to False.\nFalse\n\n\ntol\n(float, optional)\ntolerance parameter for constraints. A constraint is considered as not fulfilled if the violation is larger than tol. Defaults to 1e-6.\n1e-05\n\n\nraise_validation_error\nbool\nIf true an error will be raised if candidates violate constraints, otherwise only a warning will be displayed. Defaults to True.\nTrue\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nwhen a column is missing for a defined input feature\n\n\n\nValueError\nwhen a column is missing for a defined output feature\n\n\n\nValueError\nwhen a non-numerical value is proposed\n\n\n\nValueError\nwhen an additional column is found\n\n\n\nConstraintNotFulfilledError\nwhen the constraints are not fulfilled and raise_validation_error = True\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: dataframe with suggested experiments (candidates)\n\n\n\n\n\n\nvalidate_constraints\ndata_models.domain.domain.Domain.validate_constraints()\nValidate that the constraints defined in the domain fit to the input features.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nList[Constraint]\nList of constraints or empty if no constraints are defined\nrequired\n\n\nvalues\nList[Input]\nList of input features of the domain\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nFeature key in constraint is unknown.\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n\nList[Constraint]: List of constraints defined for the domain\n\n\n\n\n\n\nvalidate_experiments\ndata_models.domain.domain.Domain.validate_experiments(experiments, strict=False)\nChecks the experimental data on validity\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\nstrict\nbool\nBoolean to distinguish if the occurrence of fixed features in the dataset should be considered or not. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nempty dataframe\n\n\n\nValueError\nthe column for a specific feature is missing the provided data\n\n\n\nValueError\nthere are labcodes with null value\n\n\n\nValueError\nthere are labcodes with nan value\n\n\n\nValueError\nlabcodes are not unique\n\n\n\nValueError\nthe provided columns do no match to the defined domain\n\n\n\nValueError\nthe provided columns do no match to the defined domain\n\n\n\nValueError\nInput with null values\n\n\n\nValueError\nInput with nan values\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: The provided dataframe with experimental data\n\n\n\n\n\n\nvalidate_unique_feature_keys\ndata_models.domain.domain.Domain.validate_unique_feature_keys()\nValidates if provided input and output feature keys are unique\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nv\nOutputs\nList of all output features of the domain.\nrequired\n\n\nvalue\nDict[str, Inputs]\nDict containing a list of input features as single entry.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nFeature keys are not unique.\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\nOutputs\n\nKeeps output features as given."
  },
  {
    "objectID": "docs/reference/data_models.domain.features.Outputs.html",
    "href": "docs/reference/data_models.domain.features.Outputs.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.domain.features.Outputs.html#attributes",
    "href": "docs/reference/data_models.domain.features.Outputs.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nfeatures\nList(Outputs\nlist of the features."
  },
  {
    "objectID": "docs/reference/data_models.domain.features.Outputs.html#methods",
    "href": "docs/reference/data_models.domain.features.Outputs.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nadd_valid_columns\nAdd the valid_{feature.key} columns to the experiments dataframe,\n\n\nget_by_objective\nGet output features filtered by the type of the attached objective.\n\n\nget_keys_by_objective\nGet keys of output features filtered by the type of the attached objective.\n\n\npreprocess_experiments_all_valid_outputs\nMethod to get a dataframe where non-valid entries of all output feature are removed\n\n\npreprocess_experiments_any_valid_output\nMethod to get a dataframe where at least one output feature has a valid entry\n\n\npreprocess_experiments_one_valid_output\nMethod to get a dataframe where non-valid entries of the provided output feature are removed\n\n\n\n\nadd_valid_columns\ndata_models.domain.features.Outputs.add_valid_columns(experiments)\nAdd the valid_{feature.key} columns to the experiments dataframe, in case that they are not present.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe holding the experiments.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe holding the experiments.\n\n\n\n\n\n\nget_by_objective\ndata_models.domain.features.Outputs.get_by_objective(\n    includes=Objective,\n    excludes=None,\n    exact=False,\n)\nGet output features filtered by the type of the attached objective.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nincludes\nUnion[List[TObjective], TObjective]\nObjective class or list of objective classes to be returned. Defaults to Objective.\nObjective\n\n\nexcludes\nUnion[List[TObjective], TObjective, None]\nObjective class or list of specific objective classes to be excluded from the return. Defaults to None.\nNone\n\n\nexact\nbool\nBoolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOutputs\nList[AnyOutput]: List of output features fitting to the passed requirements.\n\n\n\n\n\n\nget_keys_by_objective\ndata_models.domain.features.Outputs.get_keys_by_objective(\n    includes=Objective,\n    excludes=None,\n    exact=False,\n)\nGet keys of output features filtered by the type of the attached objective.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nincludes\nUnion[List[TObjective], TObjective]\nObjective class or list of objective classes to be returned. Defaults to Objective.\nObjective\n\n\nexcludes\nUnion[List[TObjective], TObjective, None]\nObjective class or list of specific objective classes to be excluded from the return. Defaults to None.\nNone\n\n\nexact\nbool\nBoolean to distinguish if only the exact classes listed in includes and no subclasses inherenting from this class shall be returned. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nList[str]\nList[str]: List of output feature keys fitting to the passed requirements.\n\n\n\n\n\n\npreprocess_experiments_all_valid_outputs\ndata_models.domain.features.Outputs.preprocess_experiments_all_valid_outputs(\n    experiments,\n    output_feature_keys=None,\n)\nMethod to get a dataframe where non-valid entries of all output feature are removed\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\noutput_feature_keys\nOptional[List]\nList of output feature keys which should be considered for removal of invalid values. Defaults to None.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with all experiments where only valid entries of the selected features are included\n\n\n\n\n\n\npreprocess_experiments_any_valid_output\ndata_models.domain.features.Outputs.preprocess_experiments_any_valid_output(\n    experiments,\n)\nMethod to get a dataframe where at least one output feature has a valid entry\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with all experiments where at least one output feature has a valid entry\n\n\n\n\n\n\npreprocess_experiments_one_valid_output\ndata_models.domain.features.Outputs.preprocess_experiments_one_valid_output(\n    output_feature_key,\n    experiments,\n)\nMethod to get a dataframe where non-valid entries of the provided output feature are removed\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experimental data\nrequired\n\n\noutput_feature_key\nstr\nThe feature based on which non-valid entries rows are removed\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: Dataframe with all experiments where only valid entries of the specific feature are included"
  },
  {
    "objectID": "docs/reference/data_models.kernels.api.html",
    "href": "docs/reference/data_models.kernels.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.kernels.api.html#classes",
    "href": "docs/reference/data_models.kernels.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nKernel\n\n\n\nAdditiveKernel\n\n\n\nMultiplicativeKernel\n\n\n\nScaleKernel\n\n\n\nMaternKernel\n\n\n\nRBFKernel\n\n\n\nLinearKernel\n\n\n\nPolynomialKernel\n\n\n\nHammingDistanceKernel\n\n\n\nTanimotoKernel\n\n\n\nWassersteinKernel\nKernel based on the Wasserstein distance.\n\n\n\n\nKernel\ndata_models.kernels.api.Kernel()\n\n\nAdditiveKernel\ndata_models.kernels.api.AdditiveKernel()\n\n\nMultiplicativeKernel\ndata_models.kernels.api.MultiplicativeKernel()\n\n\nScaleKernel\ndata_models.kernels.api.ScaleKernel()\n\n\nMaternKernel\ndata_models.kernels.api.MaternKernel()\n\n\nRBFKernel\ndata_models.kernels.api.RBFKernel()\n\n\nLinearKernel\ndata_models.kernels.api.LinearKernel()\n\n\nPolynomialKernel\ndata_models.kernels.api.PolynomialKernel()\n\n\nHammingDistanceKernel\ndata_models.kernels.api.HammingDistanceKernel()\n\n\nTanimotoKernel\ndata_models.kernels.api.TanimotoKernel()\n\n\nWassersteinKernel\ndata_models.kernels.api.WassersteinKernel()\nKernel based on the Wasserstein distance.\nIt only works for 1D data that is monotonically increasing, as it is just calculating the integral of the absolute difference between two shapes. Only when both shapes are monotonically increasing, this integral is also a Wasserstein distance (https://arxiv.org/abs/2002.01878).\nThe shape are assumed to be discretized as a set of points. Make sure that the discretization is fine enough to capture the shape of the data.\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nsquared\nbool\nIf True, the squared exponential Wasserstein distance is used. Note that the squared exponential Wasserstein distance kernel is not positive definite for all lengthscales. For this reason, as default the absolute exponential Wasserstein distance is used.\n\n\nlengthscale_prior\nOptional[AnyPrior]\nPrior for the lengthscale of the kernel."
  },
  {
    "objectID": "docs/reference/data_models.surrogates.api.html",
    "href": "docs/reference/data_models.surrogates.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/data_models.surrogates.api.html#classes",
    "href": "docs/reference/data_models.surrogates.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nSurrogate\n\n\n\nSingleTaskGPSurrogate\n\n\n\nMixedSingleTaskGPSurrogate\n\n\n\nMultiTaskGPSurrogate\n\n\n\nRobustSingleTaskGPSurrogate\nRobust Relevance Pursuit Single Task Gaussian Process Surrogate.\n\n\nTanimotoGPSurrogate\n\n\n\nLinearSurrogate\n\n\n\nPolynomialSurrogate\n\n\n\nRandomForestSurrogate\n\n\n\nMLPEnsemble\n\n\n\nBotorchSurrogates\n“List of botorch surrogates.\n\n\n\n\nSurrogate\ndata_models.surrogates.api.Surrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.Surrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: outputs: objective functions for the surrogate my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nSingleTaskGPSurrogate\ndata_models.surrogates.api.SingleTaskGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.SingleTaskGPSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nMixedSingleTaskGPSurrogate\ndata_models.surrogates.api.MixedSingleTaskGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.MixedSingleTaskGPSurrogate.is_output_implemented(\n    my_type,\n)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nMultiTaskGPSurrogate\ndata_models.surrogates.api.MultiTaskGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.MultiTaskGPSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nRobustSingleTaskGPSurrogate\ndata_models.surrogates.api.RobustSingleTaskGPSurrogate()\nRobust Relevance Pursuit Single Task Gaussian Process Surrogate.\nA robust single-task GP that learns a data-point specific noise level and is therefore more robust to outliers. See: https://botorch.org/docs/tutorials/relevance_pursuit_robust_regression/ Paper: https://arxiv.org/pdf/2410.24222\n\nAttributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nprior_mean_of_support\nOptional[int]\nThe prior mean of the support.\n\n\nconvex_parametrization\nbool\nWhether to use convex parametrization of the sparse noise model.\n\n\ncache_model_trace\nbool\nWhether to cache the model trace. This needs no be set to True if you want to view the model trace after optimization.\n\n\n\n\n\nNote\nThe definition of “outliers” depends on the model capacity, so what is an outlier with respect to a simple model might not be an outlier with respect to a complex model. For this reason, it is necessary to bound the lengthscale of the GP kernel from below.\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.RobustSingleTaskGPSurrogate.is_output_implemented(\n    my_type,\n)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nTanimotoGPSurrogate\ndata_models.surrogates.api.TanimotoGPSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\nvalidate_moleculars\nChecks that at least one of fingerprints, fragments, or fingerprintsfragments features are present.\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.TanimotoGPSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\nvalidate_moleculars\ndata_models.surrogates.api.TanimotoGPSurrogate.validate_moleculars()\nChecks that at least one of fingerprints, fragments, or fingerprintsfragments features are present.\n\n\n\n\nLinearSurrogate\ndata_models.surrogates.api.LinearSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.LinearSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nPolynomialSurrogate\ndata_models.surrogates.api.PolynomialSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.PolynomialSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nRandomForestSurrogate\ndata_models.surrogates.api.RandomForestSurrogate()\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nis_output_implemented\nAbstract method to check output type for surrogate models\n\n\n\n\nis_output_implemented\ndata_models.surrogates.api.RandomForestSurrogate.is_output_implemented(my_type)\nAbstract method to check output type for surrogate models Args: my_type: continuous or categorical output Returns: bool: True if the output type is valid for the surrogate chosen, False otherwise\n\n\n\n\nMLPEnsemble\ndata_models.surrogates.api.MLPEnsemble()\n\n\nBotorchSurrogates\ndata_models.surrogates.api.BotorchSurrogates()\n“List of botorch surrogates.\nBehaves similar to a Surrogate."
  },
  {
    "objectID": "docs/reference/strategies.api.html",
    "href": "docs/reference/strategies.api.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "docs/reference/strategies.api.html#classes",
    "href": "docs/reference/strategies.api.html#classes",
    "title": "",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nStrategy\nBase class for all strategies\n\n\nRandomStrategy\nStrategy for randomly selecting new candidates.\n\n\nDoEStrategy\nStrategy for design of experiments. This strategy is used to generate a set of\n\n\nFractionalFactorialStrategy\n\n\n\nSoboStrategy\n\n\n\nAdditiveSoboStrategy\n\n\n\nMultiplicativeSoboStrategy\n\n\n\nCustomSoboStrategy\n\n\n\nQparegoStrategy\n\n\n\nMoboStrategy\n\n\n\nBotorchStrategy\n\n\n\nEntingStrategy\nStrategy for selecting new candidates using ENTMOOT\n\n\nMultiFidelityStrategy\n\n\n\nActiveLearningStrategy\nActiveLearningStrategy that uses an acquisition function which focuses on\n\n\nShortestPathStrategy\n\n\n\nStepwiseStrategy\n\n\n\n\n\nStrategy\nstrategies.api.Strategy(data_model)\nBase class for all strategies\nAttributes:\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\ncandidates\nReturns the (pending) candidates of the strategy.\n\n\ndomain\nReturns the domain of the strategy.\n\n\nexperiments\nReturns the experiments of the strategy.\n\n\ninputs\nShortcut to access the inputs of the strategy’s domain.\n\n\nnum_candidates\nReturns number of (pending) candidates\n\n\nnum_experiments\nReturns number of experiments\n\n\noutputs\nShortcut to access the outputs of the strategy’s domain.\n\n\nseed\nReturns the seed of the strategy.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nadd_candidates\nAdd pending candidates to the strategy. Appends to existing ones.\n\n\nadd_experiments\nAdd experiments to the strategy. Appends to existing ones.\n\n\nask\nFunction to generate new candidates.\n\n\nfrom_spec\nUsed by the mapper to map from data model to functional strategy.\n\n\nhas_sufficient_experiments\nAbstract method to check if sufficient experiments are available.\n\n\npostprocess_candidates\nMethod to allow for postprocessing of candidates.\n\n\nreset_candidates\nResets the pending candidates of the strategy.\n\n\nset_candidates\nSet pending candidates of the strategy. Overwrites existing ones.\n\n\nset_experiments\nSet experiments of the strategy. Overwrites existing ones.\n\n\ntell\nThis function passes new experimental data to the optimizer\n\n\nto_candidates\nTransform candiadtes dataframe to a list of Candidate objects.\n\n\n\n\nadd_candidates\nstrategies.api.Strategy.add_candidates(candidates)\nAdd pending candidates to the strategy. Appends to existing ones.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with candidates.\nrequired\n\n\n\n\n\n\nadd_experiments\nstrategies.api.Strategy.add_experiments(experiments)\nAdd experiments to the strategy. Appends to existing ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experiments.\nrequired\n\n\n\n\n\n\nask\nstrategies.api.Strategy.ask(\n    candidate_count=None,\n    add_pending=False,\n    raise_validation_error=True,\n)\nFunction to generate new candidates.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidate_count\nPositiveInt\nNumber of candidates to be generated. If not provided, the number of candidates is determined automatically. Defaults to None.\nNone\n\n\nadd_pending\nbool\nIf true the proposed candidates are added to the set of pending experiments. Defaults to False.\nFalse\n\n\nraise_validation_error\nbool\nIf true an error will be raised if candidates violate constraints, otherwise only a warning will be displayed. Defaults to True.\nTrue\n\n\n\n\n\nRaises\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nif candidate count is smaller than 1\n\n\n\nValueError\nif not enough experiments are available to execute the strategy\n\n\n\nValueError\nif the number of generated candidates does not match the requested number\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.DataFrame\npd.DataFrame: DataFrame with candidates (proposed experiments)\n\n\n\n\n\n\nfrom_spec\nstrategies.api.Strategy.from_spec(data_model)\nUsed by the mapper to map from data model to functional strategy.\n\n\nhas_sufficient_experiments\nstrategies.api.Strategy.has_sufficient_experiments()\nAbstract method to check if sufficient experiments are available.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if number of passed experiments is sufficient, False otherwise\n\n\n\n\n\n\npostprocess_candidates\nstrategies.api.Strategy.postprocess_candidates(candidates)\nMethod to allow for postprocessing of candidates.\nBy default this methods applies the stepsize of continuous features if applicable.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\nDataFrame with candidates.\nrequired\n\n\n\nReturns: DataFrame with postprocessed candidates.\n\n\n\nreset_candidates\nstrategies.api.Strategy.reset_candidates()\nResets the pending candidates of the strategy.\n\n\nset_candidates\nstrategies.api.Strategy.set_candidates(candidates)\nSet pending candidates of the strategy. Overwrites existing ones.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with candidates.\nrequired\n\n\n\n\n\n\nset_experiments\nstrategies.api.Strategy.set_experiments(experiments)\nSet experiments of the strategy. Overwrites existing ones.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataframe with experiments.\nrequired\n\n\n\n\n\n\ntell\nstrategies.api.Strategy.tell(experiments, replace=False)\nThis function passes new experimental data to the optimizer\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexperiments\npd.DataFrame\nDataFrame with experimental data\nrequired\n\n\nreplace\nbool\nBoolean to decide if the experimental data should replace the former DataFrame or if the new experiments should be attached. Defaults to False.\nFalse\n\n\n\n\n\n\nto_candidates\nstrategies.api.Strategy.to_candidates(candidates)\nTransform candiadtes dataframe to a list of Candidate objects.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\ncandidates formatted as dataframe\nrequired\n\n\n\nReturns: List[Candidate]: candidates formatted as list of Candidate objects.\n\n\n\n\n\nRandomStrategy\nstrategies.api.RandomStrategy(data_model, **kwargs)\nStrategy for randomly selecting new candidates.\nThis strategy generates candidate samples using the random strategy. It first checks if the domain is compatible with polytope sampling. If so, it uses polytope sampling to generate candidate samples. If not, it performs rejection sampling by repeatedly generating candidates with polytope sampling until the desired number of valid samples is obtained.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata_model\ndata_models.RandomStrategy\nThe data model for the random strategy.\nrequired\n\n\n**kwargs\n\nAdditional keyword arguments.\n{}\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nhas_sufficient_experiments\nCheck if there are sufficient experiments for the strategy.\n\n\nmake\nCreate a new instance of the RandomStrategy class.\n\n\n\n\nhas_sufficient_experiments\nstrategies.api.RandomStrategy.has_sufficient_experiments()\nCheck if there are sufficient experiments for the strategy.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if there are sufficient experiments, False otherwise.\n\n\n\n\n\n\nmake\nstrategies.api.RandomStrategy.make(\n    domain,\n    fallback_sampling_method=None,\n    n_burnin=None,\n    n_thinning=None,\n    num_base_samples=None,\n    max_iters=None,\n    seed=None,\n    sampler_kwargs=None,\n)\nCreate a new instance of the RandomStrategy class. Args: domain: The domain we randomly sample from. fallback_sampling_method: The fallback sampling method to use when the domain has no constraints. n_burnin: The number of burn-in samples for the polytope sampler. n_thinning: The thinning factor for the polytope sampler. num_base_samples: The number of base samples for rejection sampling. max_iters: The maximum number of iterations for rejection sampling. seed: The seed value for random number generation. sampler_kwargs: Additional arguments for the sampler. Defaults to None. Returns: RandomStrategy: A new instance of the RandomStrategy class.\n\n\n\n\nDoEStrategy\nstrategies.api.DoEStrategy(data_model, **kwargs)\nStrategy for design of experiments. This strategy is used to generate a set of experiments for a given domain. The experiments are generated via minimization of a user defined optimality criterion.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nget_additional_experiments_needed\nCalculate the additional number of experiments needed beyond current candidates.\n\n\nget_candidate_rank\nGet the rank of the model matrix with the current candidates.\n\n\nhas_sufficient_experiments\nAbstract method to check if sufficient experiments are available.\n\n\nmake\nCreate a new design of experimence strategy instance.\n\n\n\n\nget_additional_experiments_needed\nstrategies.api.DoEStrategy.get_additional_experiments_needed()\nCalculate the additional number of experiments needed beyond current candidates. This method computes: get_required_number_of_experiments() - get_candidate_rank()\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nOptional[int]\nOptional[int]: Number of additional experiments needed, or None if required number cannot be calculated (e.g., for SpaceFillingCriterion).\n\n\n\n\n\n\nget_candidate_rank\nstrategies.api.DoEStrategy.get_candidate_rank()\nGet the rank of the model matrix with the current candidates.\n\n\nhas_sufficient_experiments\nstrategies.api.DoEStrategy.has_sufficient_experiments()\nAbstract method to check if sufficient experiments are available.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if number of passed experiments is sufficient, False otherwise\n\n\n\n\n\n\nmake\nstrategies.api.DoEStrategy.make(\n    domain,\n    seed=None,\n    criterion=None,\n    verbose=None,\n    ipopt_options=None,\n    scip_params=None,\n    use_hessian=None,\n    use_cyipopt=None,\n    sampling=None,\n    return_fixed_candidates=None,\n)\nCreate a new design of experimence strategy instance. Args: domain: The domain for the strategy. seed: Random seed for reproducibility. criterion: Optimality criterion for the strategy. Default is d-optimality. verbose: Verbosity level. ipopt_options: Options for IPOPT solver. IPOPT is used to minize the optimality criterion. scip_params: Parameters for SCIP solver. SCIP is used to for backprojection of discrete and categorical variables. use_hessian: Whether to use Hessian information. use_cyipopt: Whether to use cyipopt. sampling: Initial points for the strategy. return_fixed_candidates: Whether to return fixed candidates. Returns: DoEStrategy: A new instance of the DoEStrategy class.\n\n\n\n\nFractionalFactorialStrategy\nstrategies.api.FractionalFactorialStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreate a new instance of the strategy with the given parameters. This method will create the datamodel\n\n\nrandomize_design\nRandomize the run order of the design if self.randomize_runorder is True.\n\n\n\n\nmake\nstrategies.api.FractionalFactorialStrategy.make(\n    domain,\n    n_repetitions=None,\n    n_center=None,\n    block_feature_key=None,\n    generator=None,\n    n_generators=None,\n    randomize_runorder=None,\n    seed=None,\n)\nCreate a new instance of the strategy with the given parameters. This method will create the datamodel under the hood and pass it to the constructor of the strategy. Args: domain: The domain of the strategy. n_repetitions: The number of repetitions of the continuous part of the design. n_center: The number of center points in the continuous part of the design per block. block_feature_key: The feature key to use for blocking the design. generator: The generator for the continuous part of the design. n_generators: The number of reducing factors. randomize_runorder: If true, the run order is randomized, else it is deterministic. seed: The seed for the random number generator.\n\n\nrandomize_design\nstrategies.api.FractionalFactorialStrategy.randomize_design(design)\nRandomize the run order of the design if self.randomize_runorder is True.\n\n\n\n\nSoboStrategy\nstrategies.api.SoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates a single objective Bayesian optimization strategy.\n\n\n\n\nmake\nstrategies.api.SoboStrategy.make(\n    domain,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates a single objective Bayesian optimization strategy. Args: domain: The optimization domain of the strategy. acquisition_function: The acquisition function to use. acquisition_optimizer: The optimizer to use for the acquisition function. surrogate_specs: The specifications for the surrogate model. outlier_detection_specs: The specifications for the outlier detection. min_experiments_before_outlier_check: The minimum number of experiments before checking for outliers. frequency_check: The frequency of checking for outliers. frequency_hyperopt: The frequency of hyperparameter optimization. folds: The number of folds for cross-validation. seed: The random seed to use. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\n\n\n\n\nAdditiveSoboStrategy\nstrategies.api.AdditiveSoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates a Bayesian optimization strategy that adds multiple objectives.\n\n\n\n\nmake\nstrategies.api.AdditiveSoboStrategy.make(\n    domain,\n    use_output_constraints=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates a Bayesian optimization strategy that adds multiple objectives. The weights of the objectives are defines in the outputs of the domain. Args: domain: The optimization domain of the strategy. use_output_constraints: Whether to use output constraints. acquisition_function: The acquisition function to use. acquisition_optimizer: The optimizer to use for the acquisition function. surrogate_specs: The specifications for the surrogate model. outlier_detection_specs: The specifications for the outlier detection. min_experiments_before_outlier_check: The minimum number of experiments before checking for outliers. frequency_check: The frequency of checking for outliers. frequency_hyperopt: The frequency of hyperparameter optimization. folds: The number of folds for cross-validation for hyperparameter optimization. seed: The random seed to use. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\n\n\n\n\nMultiplicativeSoboStrategy\nstrategies.api.MultiplicativeSoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates Bayesian optimization strategy that multiplies multiple objectives. The weights of\n\n\n\n\nmake\nstrategies.api.MultiplicativeSoboStrategy.make(\n    domain,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates Bayesian optimization strategy that multiplies multiple objectives. The weights of the objectives are defines in the outputs of the domain. Args: domain: The optimization domain of the strategy. acquisition_function: The acquisition function to use. acquisition_optimizer: The optimizer to use for the acquisition function. surrogate_specs: The specifications for the surrogate model. outlier_detection_specs: The specifications for the outlier detection. min_experiments_before_outlier_check: The minimum number of experiments before checking for outliers. frequency_check: The frequency of checking for outliers. frequency_hyperopt: The frequency of hyperparameter optimization. folds: The number of folds for cross-validation for hyperparameter optimization. seed: The random seed to use. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\n\n\n\n\nCustomSoboStrategy\nstrategies.api.CustomSoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ndumps\nDumps the function to a string via pickle as this is not directly json serializable.\n\n\nloads\nLoads the function from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\nmake\nThe CustomSoboStrategy can be used to design custom objectives or objective combinations for optimizations.\n\n\n\n\ndumps\nstrategies.api.CustomSoboStrategy.dumps()\nDumps the function to a string via pickle as this is not directly json serializable.\n\n\nloads\nstrategies.api.CustomSoboStrategy.loads(data)\nLoads the function from a base64 encoded pickle bytes object and writes it to the model attribute.\n\n\nmake\nstrategies.api.CustomSoboStrategy.make(\n    domain,\n    use_output_constraints=None,\n    dump=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nThe CustomSoboStrategy can be used to design custom objectives or objective combinations for optimizations. In this tutorial notebook, it is shown how to use it to optimize a quantity that depends on a combination of an inferred quantity and one of the inputs. See tutorials/advanced_examples/custom_sobo.ipynb.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe optimization domain of the strategy.\nrequired\n\n\nuse_output_constraints\nbool | None\nWhether to use output constraints.\nNone\n\n\ndump\nstr | None\nThe function to use for the optimization.\nNone\n\n\nacquisition_function\nAnySingleObjectiveAcquisitionFunction | None\nThe acquisition function to use.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nThe optimizer to use for the acquisition function.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nThe specifications for the surrogate model.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nThe specifications for the outlier detection.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nThe minimum number of experiments before checking for outliers.\nNone\n\n\nfrequency_check\nPositiveInt | None\nThe frequency of checking for outliers.\nNone\n\n\nfrequency_hyperopt\nint | None\nThe frequency of hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nThe number of folds for cross-validation.\nNone\n\n\nseed\nint | None\nThe random seed to use.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\n\n\n\n\n\nQparegoStrategy\nstrategies.api.QparegoStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates an instance of the multi-objective strategy ParEGO using the provided configuration parameters.\n\n\n\n\nmake\nstrategies.api.QparegoStrategy.make(\n    domain,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates an instance of the multi-objective strategy ParEGO using the provided configuration parameters.\nJ. Knowles. Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. IEEE Transactions on Evolutionary Computation, 10(1):50-66, 2006\nS. Daulton, M. Balandat, and E. Bakshy. Differentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe optimization domain of the strategy.\nrequired\n\n\nacquisition_function\nqEI | qLogEI | qLogNEI | qNEI | None\nThe acquisition function to use.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nThe optimizer for the acquisition function.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nSpecifications for the surrogate model.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nSpecifications for outlier detection.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nMinimum number of experiments before checking for outliers.\nNone\n\n\nfrequency_check\nPositiveInt | None\nFrequency of outlier checks.\nNone\n\n\nfrequency_hyperopt\nint | None\nFrequency of hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nNumber of folds for cross-validation for hyperparameter optimization.\nNone\n\n\nseed\nint | None\nRandom seed for reproducibility.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\nReturns: An instance of the strategy configured with the provided parameters.\n\n\n\n\n\nMoboStrategy\nstrategies.api.MoboStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates an instance of a multi-objective strategy based on expected hypervolume improvement.\n\n\n\n\nmake\nstrategies.api.MoboStrategy.make(\n    domain,\n    ref_point=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates an instance of a multi-objective strategy based on expected hypervolume improvement.\nS. Daulton, M. Balandat, and E. Bakshy. Parallel Bayesian Optimization of Multiple Noisy Objectives with Expected Hypervolume Improvement. Advances in Neural Information Processing Systems 34, 2021.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe domain specifying the search space.\nrequired\n\n\nref_point\nExplicitReferencePoint | Dict[str, float] | None\nReference point for hypervolume computation.\nNone\n\n\nacquisition_function\nAnyMultiObjectiveAcquisitionFunction | None\nAcquisition function.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nOptimizer for the acquisition function.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nSurrogate model specifications.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nOutlier detection configuration.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nMinimum number of experiments before performing outlier detection.\nNone\n\n\nfrequency_check\nPositiveInt | None\nFrequency at which to perform outlier checks.\nNone\n\n\nfrequency_hyperopt\nint | None\nFrequency at which to perform hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nNumber of folds for cross-validation for hyperparameter optimization.\nNone\n\n\nseed\nint | None\nRandom seed for reproducibility.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\nReturns: An instance of the strategy configured with the specified parameters.\n\n\n\n\n\nBotorchStrategy\nstrategies.api.BotorchStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\ncalc_acquisition\nCalculate the acquisition value for a set of experiments.\n\n\nget_acqf_input_tensors\n\n\n\n\n\ncalc_acquisition\nstrategies.api.BotorchStrategy.calc_acquisition(candidates, combined=False)\nCalculate the acquisition value for a set of experiments.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncandidates\npd.DataFrame\nDataframe with experimentes for which the acqf value should be calculated.\nrequired\n\n\ncombined\nbool\nIf combined an acquisition value for the whole batch is calculated, else individual ones. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nnp.ndarray\nnp.ndarray: Dataframe with the acquisition values.\n\n\n\n\n\n\nget_acqf_input_tensors\nstrategies.api.BotorchStrategy.get_acqf_input_tensors()\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nX_train\nTensor\nTensor of shape (n, d) with n training points and d input dimensions.\n\n\nX_pending\nTensor | None\nTensor of shape (m, d) with m pending points\n\n\n\n\n\n\n\n\nEntingStrategy\nstrategies.api.EntingStrategy(data_model, **kwargs)\nStrategy for selecting new candidates using ENTMOOT\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreate an enting strategy instance with the specified parameters.\n\n\n\n\nmake\nstrategies.api.EntingStrategy.make(\n    domain,\n    beta=None,\n    bound_coeff=None,\n    acq_sense=None,\n    dist_trafo=None,\n    dist_metric=None,\n    cat_metric=None,\n    kappa_fantasy=None,\n    num_boost_round=None,\n    max_depth=None,\n    min_data_in_leaf=None,\n    min_data_per_group=None,\n    verbose=None,\n    solver_name=None,\n    solver_verbose=None,\n    solver_params=None,\n    seed=None,\n)\nCreate an enting strategy instance with the specified parameters.\nhttps://github.com/cog-imperial/entmoot\nENTMOOT: A Framework for Optimization over Ensemble Tree Models A. Thebelt, J. Kronqvist, M. Mistry, R. Lee, N. Sudermann-Merx, R. Misener Computers & Chemical Engineering, 2021\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe domain object defining the problem space.\nrequired\n\n\nbeta\nPositiveFloat | None\nParameter controlling the trade-off in acquisition.\nNone\n\n\nbound_coeff\nPositiveFloat | None\nCoefficient for bounding constraints.\nNone\n\n\nacq_sense\nLiteral['exploration', 'penalty'] | None\nAcquisition sense, either “exploration” or “penalty”.\nNone\n\n\ndist_trafo\nLiteral['normal', 'standard'] | None\nTransformation applied to distances, either “normal” or “standard”.\nNone\n\n\ndist_metric\nLiteral['euclidean_squared', 'l1', 'l2'] | None\nMetric used for distance calculations, e.g., “euclidean_squared”, “l1”, or “l2”.\nNone\n\n\ncat_metric\nLiteral['overlap', 'of', 'goodall4'] | None\nMetric for categorical variables, e.g., “overlap”, “of”, or “goodall4”.\nNone\n\n\nkappa_fantasy\nfloat | None\nKappa parameter for fantasy strategy for batch proposals.\nNone\n\n\nnum_boost_round\nPositiveInt | None\nNumber of boosting rounds for the model.\nNone\n\n\nmax_depth\nPositiveInt | None\nMaximum depth of the model.\nNone\n\n\nmin_data_in_leaf\nPositiveInt | None\nMinimum data points required in a leaf.\nNone\n\n\nmin_data_per_group\nPositiveInt | None\nMinimum data points required per group.\nNone\n\n\nverbose\nLiteral[-1, 0, 1, 2] | None\nVerbosity level of the process.\nNone\n\n\nsolver_name\nstr | None\nName of the solver to be used.\nNone\n\n\nsolver_verbose\nbool | None\nWhether to enable verbose output for the solver.\nNone\n\n\nsolver_params\nDict[str, Any] | None\nAdditional parameters for the solver.\nNone\n\n\nseed\nint | None\nRandom seed for reproducibility.\nNone\n\n\n\nReturns: A strategy instance configured with the provided parameters.\n\n\n\n\n\nMultiFidelityStrategy\nstrategies.api.MultiFidelityStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreate a new instance of the multi-fidelity optimization strategy with the given parameters. This strategy\n\n\n\n\nmake\nstrategies.api.MultiFidelityStrategy.make(\n    domain,\n    fidelity_thresholds=None,\n    acquisition_function=None,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreate a new instance of the multi-fidelity optimization strategy with the given parameters. This strategy is useful if you have different measurement fidelities that measure the same thing with different cost and accuracy. As an example, you can have a simulation that is fast but inaccurate and the real experiment that is slow and expensive, but more accurate.\nK. Kandasamy, G. Dasarathy, J. B. Oliva, J. Schneider, B. Póczos. Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations. Advances in Neural Information Processing Systems, 29, 2016.\nJose Pablo Folch, Robert M Lee, Behrang Shafei, David Walz, Calvin Tsay, Mark van der Wilk, Ruth Misener. Combining Multi-Fidelity Modelling and Asynchronous Batch Bayesian Optimization. Computers & Chemical Engineering Volume 172, 2023.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndomain\nDomain\nThe optimization domain of the strategy.\nrequired\n\n\nfidelity_thresholds\nList[float] | float | None\nThe thresholds for the fidelity. If a single value is provided, it will be used for all fidelities.\nNone\n\n\nacquisition_function\nAnySingleObjectiveAcquisitionFunction | None\nThe acquisition function to use.\nNone\n\n\nacquisition_optimizer\nAnyAcqfOptimizer | None\nThe acquisition optimizer to use.\nNone\n\n\nsurrogate_specs\nBotorchSurrogates | None\nThe specifications for the surrogate model.\nNone\n\n\noutlier_detection_specs\nOutlierDetections | None\nThe specifications for the outlier detection.\nNone\n\n\nmin_experiments_before_outlier_check\nPositiveInt | None\nThe minimum number of experiments before checking for outliers.\nNone\n\n\nfrequency_check\nPositiveInt | None\nThe frequency of outlier checks.\nNone\n\n\nfrequency_hyperopt\nint | None\nThe frequency of hyperparameter optimization.\nNone\n\n\nfolds\nint | None\nThe number of folds for cross-validation.\nNone\n\n\nseed\nint | None\nThe random seed to use.\nNone\n\n\ninclude_infeasible_exps_in_acqf_calc\nbool | None\nWhether infeasible experiments should be included in the set of experiments used to compute the acquisition function.\nFalse\n\n\n\n\n\n\n\n\nActiveLearningStrategy\nstrategies.api.ActiveLearningStrategy(data_model, **kwargs)\nActiveLearningStrategy that uses an acquisition function which focuses on pure exploration of the objective function only. Can be used for single and multi-objective functions.\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nmake\nCreates an ActiveLearningStrategy instance. ActiveLearningStrategy that uses an acquisition function which focuses on\n\n\n\n\nmake\nstrategies.api.ActiveLearningStrategy.make(\n    domain,\n    acquisition_optimizer=None,\n    surrogate_specs=None,\n    outlier_detection_specs=None,\n    min_experiments_before_outlier_check=None,\n    frequency_check=None,\n    frequency_hyperopt=None,\n    folds=None,\n    acquisition_function=None,\n    seed=None,\n    include_infeasible_exps_in_acqf_calc=False,\n)\nCreates an ActiveLearningStrategy instance. ActiveLearningStrategy that uses an acquisition function which focuses on pure exploration of the objective function only. Can be used for single and multi-objective functions. Args: domain: Domain of the strategy. acquisition_optimizer: Acquisition optimizer to use. surrogate_specs: Surrogate specifications. outlier_detection_specs: Outlier detection specifications. min_experiments_before_outlier_check: Minimum number of experiments before checking for outliers. frequency_check: Frequency of outlier checks. frequency_hyperopt: Frequency of hyperparameter optimization. folds: Number of folds for cross-validation in hyperparameter optimization. acquisition_function: Acquisition function to use. seed: Seed for the random number generator. include_infeasible_exps_in_acqf_calc: Whether infeasible experiments should be included in the set of experiments used to compute the acquisition function. Returns: ActiveLearningStrategy: An instance of the ActiveLearningStrategy class.\n\n\n\n\nShortestPathStrategy\nstrategies.api.ShortestPathStrategy(data_model, **kwargs)\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\ncontinuous_inputs\nReturns the continuous inputs from the domain.\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nget_linear_constraints\nReturns the linear constraints in the form of matrices A and b, where Ax = b for\n\n\nhas_sufficient_experiments\nChecks if there are sufficient experiments available.\n\n\nmake\nRepresents a strategy for finding the shortest path between two points\n\n\nstep\nTakes a starting point and returns the next step in the shortest path.\n\n\n\n\nget_linear_constraints\nstrategies.api.ShortestPathStrategy.get_linear_constraints(constraints)\nReturns the linear constraints in the form of matrices A and b, where Ax = b for equality constraints and Ax &lt;= b for inequality constraints.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nconstraints\nConstraints\nThe Constraints object containing the linear constraints.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTuple[np.ndarray, np.ndarray]\nTuple[np.ndarray, np.ndarray]: A tuple containing the matrices A and b.\n\n\n\n\n\n\nhas_sufficient_experiments\nstrategies.api.ShortestPathStrategy.has_sufficient_experiments()\nChecks if there are sufficient experiments available.\n\nReturns\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nbool\nbool\nTrue if there are sufficient experiments, False otherwise.\n\n\n\n\n\n\nmake\nstrategies.api.ShortestPathStrategy.make(\n    domain,\n    start=None,\n    end=None,\n    atol=None,\n    seed=None,\n)\nRepresents a strategy for finding the shortest path between two points Args: start: The starting point of the path. end: The ending point of the path. atol: The absolute tolerance used for numerical comparisons.\n\n\nstep\nstrategies.api.ShortestPathStrategy.step(start)\nTakes a starting point and returns the next step in the shortest path.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\npd.Series\nThe starting point for the shortest path.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\npd.Series\npd.Series: The next step in the shortest path.\n\n\n\n\n\n\n\n\nStepwiseStrategy\nstrategies.api.StepwiseStrategy(data_model, **kwargs)\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nget_step\nReturns the strategy at the current step and the corresponding transform if given.\n\n\nmake\nCreate a StepwiseStrategy from a list of steps. Each step is a strategy the runs until a\n\n\n\n\nget_step\nstrategies.api.StepwiseStrategy.get_step()\nReturns the strategy at the current step and the corresponding transform if given.\n\n\nmake\nstrategies.api.StepwiseStrategy.make(domain, steps=None, seed=None)\nCreate a StepwiseStrategy from a list of steps. Each step is a strategy the runs until a condition is satisfied. One example of a stepwise strategy is is to start with a few random samples to gather initial data for subsequent Bayesian optimization. An example steps-list for two random experiments followed by a SoboStrategy is:\nfrom bofire.data_models.strategies.api import (\n    Step,\n    RandomStrategy,\n    SoboStrategy,\n    NumberOfExperimentsCondition,\n    AlwaysTrueCondition\n)\nfrom bofire.stratgies.api import StepwiseStrategy\nsteps = [\n    Step(\n        strategy_data=RandomStrategy(domain=domain),\n        condition=NumberOfExperimentsCondition(n_experiments=2),\n    ),\n    Step(\n        strategy_data=SoboStrategy(domain=domain), condition=AlwaysTrueCondition()\n    )\n]\nstepwise_strategy = StepwiseStrategy.make(domain, steps)\nAll passed domains need to compatible, i.e.,\n\nthey have the same number of features,\nthe same feature keys and\nthe features with the same key have the same type and categories.\nThe bounds and allowed categories of the features can vary.\n\nFurther, the data and domain are passed to the next step. They can also be transformed before being passed to the next step. Args: steps: List of steps to be used in the strategy. seed: Seed for random number generation."
  }
]